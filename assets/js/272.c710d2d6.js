(window.webpackJsonp=window.webpackJsonp||[]).push([[272],{600:function(s,t,a){"use strict";a.r(t);var n=a(4),r=Object(n.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"_10-14-大数据与空间限制题目"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_10-14-大数据与空间限制题目"}},[s._v("#")]),s._v(" 10.14.大数据与空间限制题目")]),s._v(" "),t("p",[s._v("[TOC]")]),s._v(" "),t("h3",{attrs:{id:"大数据与空间限制题目"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大数据与空间限制题目"}},[s._v("#")]),s._v(" 大数据与空间限制题目")]),s._v(" "),t("h4",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[s._v("#")]),s._v(" 总结")]),s._v(" "),t("p",[s._v("海量数据问题处理方法")]),s._v(" "),t("ul",[t("li",[s._v("Hash")]),s._v(" "),t("li",[s._v("Bit-Map")]),s._v(" "),t("li",[s._v("布隆过滤器 (Bloom Filter)")]),s._v(" "),t("li",[s._v("堆 (Heap)")]),s._v(" "),t("li",[s._v("双层桶划分")]),s._v(" "),t("li",[s._v("数据库索引")]),s._v(" "),t("li",[s._v("倒排索引 (Inverted Index)")]),s._v(" "),t("li",[s._v("B+树")]),s._v(" "),t("li",[s._v("Trie树")]),s._v(" "),t("li",[s._v("MapReduce")])]),s._v(" "),t("h4",{attrs:{id:"如何从大量的-url-中找出相同的-url"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何从大量的-url-中找出相同的-url"}},[s._v("#")]),s._v(" 如何从大量的 URL 中找出相同的 URL?")]),s._v(" "),t("h5",{attrs:{id:"题目描述"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("给定 a, b 两个文件, 各存放 50 亿个 URL, 每个 URL 各占 64B, 内存限制是 4G. 请找出 a, b 两个文件共同的 URL.")]),s._v(" "),t("h5",{attrs:{id:"解答思路"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("每个 URL 占 64B, 那么 50 亿个 URL占用的空间大小约为 320GB.")]),s._v(" "),t("blockquote",[t("p",[s._v("5, 000, 000, 000 * 64B ≈ 5GB * 64 = 320GB")])]),s._v(" "),t("p",[s._v("由于内存大小只有 4G, 因此, 我们不可能一次性把所有 URL 加载到内存中处理. 对于这种类型的题目, 一般采用"),t("strong",[s._v("分治策略")]),s._v(", 即: 把一个文件中的 URL 按照某个特征划分为多个小文件, 使得每个小文件大小不超过 4G, 这样就可以把这个小文件读到内存中进行处理了.")]),s._v(" "),t("p",[t("strong",[s._v("思路如下")]),s._v(":")]),s._v(" "),t("p",[s._v("首先遍历文件 a, 对遍历到的 URL 求 "),t("code",[s._v("hash(URL) % 1000")]),s._v(" , 根据计算结果把遍历到的 URL 存储到 a"),t("sub",[s._v("0")]),s._v(", a"),t("sub",[s._v("1")]),s._v(", a"),t("sub",[s._v("2")]),s._v(", ..., a"),t("sub",[s._v("999")]),s._v(", 这样每个大小约为 300MB. 使用同样的方法遍历文件 b, 把文件 b 中的 URL 分别存储到文件 b"),t("sub",[s._v("0")]),s._v(", b"),t("sub",[s._v("1")]),s._v(", b"),t("sub",[s._v("2")]),s._v(", ..., b"),t("sub",[s._v("999")]),s._v(" 中. 这样处理过后, 所有可能相同的 URL 都在对应的小文件中, 即 a"),t("sub",[s._v("0")]),s._v(" 对应 b"),t("sub",[s._v("0")]),s._v(", ..., a"),t("sub",[s._v("999")]),s._v(" 对应 b"),t("sub",[s._v("999")]),s._v(", 不对应的小文件不可能有相同的 URL. 那么接下来, 我们只需要求出这 1000 对小文件中相同的 URL 就好了.")]),s._v(" "),t("p",[s._v("接着遍历 a"),t("sub",[s._v("i")]),s._v("( "),t("code",[s._v("i∈[0,999]")]),s._v(" ), 把 URL 存储到一个 HashSet  集合中. 然后遍历 b"),t("sub",[s._v("i")]),s._v(" 中每个 URL, 看在 HashSet 集合中是否存在, 若存在, 说明这就是共同的 URL, 可以把这个 URL 保存到一个单独的文件中.")]),s._v(" "),t("h5",{attrs:{id:"方法总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("ol",[t("li",[s._v("分而治之, 进行哈希取余;")]),s._v(" "),t("li",[s._v("对每个子文件进行 HashSet 统计.")])]),s._v(" "),t("h4",{attrs:{id:"如何从大量数据中找出高频词"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何从大量数据中找出高频词"}},[s._v("#")]),s._v(" 如何从大量数据中找出高频词?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-2"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("有一个 1GB 大小的文件, 文件里每一行是一个词, 每个词的大小不超过 16B, 内存大小限制是 1MB, 要求返回频数最高的 100 个词(Top 100).")]),s._v(" "),t("h5",{attrs:{id:"解答思路-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-2"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("由于内存限制, 我们依然无法直接将大文件的所有词一次读到内存中. 因此, 同样可以采用"),t("strong",[s._v("分治策略")]),s._v(", 把一个大文件分解成多个小文件, 保证每个文件的大小小于 1MB, 进而直接将单个小文件读取到内存中进行处理.")]),s._v(" "),t("p",[t("strong",[s._v("思路如下")]),s._v(":")]),s._v(" "),t("p",[s._v("首先遍历大文件, 对遍历到的每个词x, 执行 "),t("code",[s._v("hash(x) % 5000")]),s._v(" , 将结果为 i 的词存放到文件 a"),t("sub",[s._v("i")]),s._v(" 中. 遍历结束后, 我们可以得到 5000 个小文件. 每个小文件的大小为 200KB 左右. 如果有的小文件大小仍然超过 1MB, 则采用同样的方式继续进行分解.")]),s._v(" "),t("p",[s._v("接着统计每个小文件中出现频数最高的 100 个词. 最简单的方式是使用 HashMap 来实现. 其中 key 为词, value 为该词出现的频率. 具体方法是: 对于遍历到的词 x, 如果在 map 中不存在, 则执行 "),t("code",[s._v("map.put(x, 1)")]),s._v(" ; 若存在, 则执行 "),t("code",[s._v("map.put(x, map.get(x)+1)")]),s._v(" , 将该词频数加 1.")]),s._v(" "),t("p",[s._v("上面我们统计了每个小文件单词出现的频数. 接下来, 我们可以通过维护一个"),t("strong",[s._v("小顶堆")]),s._v("来找出所有词中出现频数最高的 100 个. 具体方法是: 依次遍历每个小文件, 构建一个"),t("strong",[s._v("小顶堆")]),s._v(", 堆大小为 100. 如果遍历到的词的出现次数大于堆顶词的出现次数, 则用新词替换堆顶的词, 然后重新调整为"),t("strong",[s._v("小顶堆")]),s._v(", 遍历结束后, 小顶堆上的词就是出现频数最高的 100 个词.")]),s._v(" "),t("h5",{attrs:{id:"方法总结-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-2"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("ol",[t("li",[s._v("分而治之, 进行哈希取余;")]),s._v(" "),t("li",[s._v("使用 HashMap 统计频数;")]),s._v(" "),t("li",[s._v("求解"),t("strong",[s._v("最大")]),s._v("的 TopN 个, 用"),t("strong",[s._v("小顶堆")]),s._v("; 求解"),t("strong",[s._v("最小")]),s._v("的 TopN 个, 用"),t("strong",[s._v("大顶堆")]),s._v(".")])]),s._v(" "),t("h4",{attrs:{id:"如何找出某一天访问百度网站最多的-ip"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何找出某一天访问百度网站最多的-ip"}},[s._v("#")]),s._v(" 如何找出某一天访问百度网站最多的 IP?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-3"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("现有海量日志数据保存在一个超大文件中, 该文件无法直接读入内存, 要求从中提取某天访问百度次数最多的那个 IP.")]),s._v(" "),t("h5",{attrs:{id:"解答思路-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-3"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("这道题只关心某一天访问百度最多的 IP, 因此, 可以首先对文件进行一次遍历, 把这一天访问百度 IP 的相关信息记录到一个单独的大文件中. 接下来采用的方法与上一题一样, 大致就是先对 IP 进行哈希映射, 接着使用 HashMap 统计重复 IP 的次数, 最后计算出重复次数最多的 IP.")]),s._v(" "),t("blockquote",[t("p",[s._v("注: 这里只需要找出出现次数最多的 IP, 可以不必使用堆, 直接用一个变量 max 即可.")])]),s._v(" "),t("h5",{attrs:{id:"方法总结-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-3"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("ol",[t("li",[s._v("分而治之, 进行哈希取余;")]),s._v(" "),t("li",[s._v("使用 HashMap 统计频数;")]),s._v(" "),t("li",[s._v("求解"),t("strong",[s._v("最大")]),s._v("的 TopN 个, 用"),t("strong",[s._v("小顶堆")]),s._v("; 求解"),t("strong",[s._v("最小")]),s._v("的 TopN 个, 用"),t("strong",[s._v("大顶堆")]),s._v(".")])]),s._v(" "),t("h4",{attrs:{id:"如何在大量的数据中找出不重复的整数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何在大量的数据中找出不重复的整数"}},[s._v("#")]),s._v(" 如何在大量的数据中找出不重复的整数?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-4"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-4"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("在 2.5 亿个整数中找出不重复的整数. 注意: 内存不足以容纳这 2.5 亿个整数.")]),s._v(" "),t("h5",{attrs:{id:"解答思路-4"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-4"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("h6",{attrs:{id:"方法一-分治法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法一-分治法"}},[s._v("#")]),s._v(" 方法一: 分治法")]),s._v(" "),t("p",[s._v("与前面的题目方法类似, 先将 2.5 亿个数划分到多个小文件, 用 HashSet/HashMap 找出每个小文件中不重复的整数, 再合并每个子结果, 即为最终结果.")]),s._v(" "),t("h6",{attrs:{id:"方法二-位图法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法二-位图法"}},[s._v("#")]),s._v(" 方法二: 位图法")]),s._v(" "),t("p",[t("strong",[s._v("位图")]),s._v(", 就是用一个或多个 bit 来标记某个元素对应的值, 而键就是该元素. 采用位作为单位来存储数据, 可以大大节省存储空间.")]),s._v(" "),t("p",[s._v("位图通过使用位数组来表示某些元素是否存在. 它可以用于快速查找, 判重, 排序等. 不是很清楚? 我先举个小例子.")]),s._v(" "),t("p",[s._v("假设我们要对 "),t("code",[s._v("[0,7]")]),s._v(" 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序, 可以采用位图法. 0~7 范围总共有 8 个数, 只需要 8bit, 即 1 个字节. 首先将每个位都置 0:")]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("0 0 0 0 0 0 0 0\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("然后遍历 5 个元素, 首先遇到 6, 那么将下标为 6 的位的 0 置为 1; 接着遇到 4, 把下标为 4 的位 的 0 置为 1:")]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("0 0 0 0 1 0 1 0\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("依次遍历, 结束后, 位数组是这样的:")]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("0 1 1 0 1 1 1 0\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("p",[s._v("每个为 1 的位, 它的下标都表示了一个数:")]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("for i in range(8):\n    if bits[i] == 1:\n        print(i)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("p",[s._v("这样我们其实就已经实现了排序.")]),s._v(" "),t("p",[s._v("对于整数相关的算法的求解, "),t("strong",[s._v("位图法")]),s._v("是一种非常实用的算法. 假设 int 整数占用 4B, 即 32bit, 那么我们可以表示的整数的个数为 2"),t("sup",[s._v("32")]),s._v(".")]),s._v(" "),t("p",[t("strong",[s._v("那么对于这道题")]),s._v(", 我们用 2 个 bit 来表示各个数字的状态:")]),s._v(" "),t("ul",[t("li",[s._v("00 表示这个数字没出现过;")]),s._v(" "),t("li",[s._v("01 表示这个数字出现过一次(即为题目所找的不重复整数);")]),s._v(" "),t("li",[s._v("10 表示这个数字出现了多次.")])]),s._v(" "),t("p",[s._v("那么这 2"),t("sup",[s._v("32")]),s._v(" 个整数, 总共所需内存为 2"),t("sup",[s._v("32")]),s._v("*2b=1GB. 因此, 当可用内存超过 1GB 时, 可以采用位图法. 假设内存满足位图法需求, 进行下面的操作:")]),s._v(" "),t("p",[s._v("遍历 2.5 亿个整数, 查看位图中对应的位, 如果是 00, 则变为 01, 如果是 01 则变为 10, 如果是 10 则保持不变. 遍历结束后, 查看位图, 把对应位是 01 的整数输出即可.")]),s._v(" "),t("h5",{attrs:{id:"方法总结-4"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-4"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("p",[t("strong",[s._v("判断数字是否重复的问题")]),s._v(", 位图法是一种非常高效的方法.")]),s._v(" "),t("h4",{attrs:{id:"如何在大量的数据中判断一个数是否存在"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何在大量的数据中判断一个数是否存在"}},[s._v("#")]),s._v(" 如何在大量的数据中判断一个数是否存在?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-5"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-5"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("给定 40 亿个不重复的没排过序的 unsigned int 型整数, 然后再给定一个数, 如何快速判断这个数是否在这 40 亿个整数当中?")]),s._v(" "),t("h5",{attrs:{id:"解答思路-5"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-5"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("h6",{attrs:{id:"方法一-分治法-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法一-分治法-2"}},[s._v("#")]),s._v(" 方法一: 分治法")]),s._v(" "),t("p",[s._v("依然可以用分治法解决, 方法与前面类似, 就不再次赘述了.")]),s._v(" "),t("h6",{attrs:{id:"方法二-位图法-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法二-位图法-2"}},[s._v("#")]),s._v(" 方法二: 位图法")]),s._v(" "),t("p",[s._v("40 亿个不重复整数, 我们用 40 亿个 bit 来表示, 初始位均为 0, 那么总共需要内存: 4, 000, 000, 000b≈512M.")]),s._v(" "),t("p",[s._v("我们读取这 40 亿个整数, 将对应的 bit 设置为 1. 接着读取要查询的数, 查看相应位是否为 1, 如果为 1 表示存在, 如果为 0 表示不存在.")]),s._v(" "),t("h5",{attrs:{id:"方法总结-5"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-5"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("p",[t("strong",[s._v("判断数字是否存在, 判断数字是否重复的问题")]),s._v(", 位图法是一种非常高效的方法.")]),s._v(" "),t("h4",{attrs:{id:"如何查询最热门的查询串"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何查询最热门的查询串"}},[s._v("#")]),s._v(" 如何查询最热门的查询串?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-6"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-6"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来, 每个查询串的长度不超过 255 字节.")]),s._v(" "),t("p",[s._v("假设目前有 1000w 个记录(这些查询串的重复度比较高, 虽然总数是 1000w, 但如果除去重复后, 则不超过 300w 个). 请统计最热门的 10 个查询串, 要求使用的内存不能超过 1G. (一个查询串的重复度越高, 说明查询它的用户越多, 也就越热门. )")]),s._v(" "),t("h5",{attrs:{id:"解答思路-6"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-6"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("每个查询串最长为 255B, 1000w 个串需要占用 约 2.55G 内存, 因此, 我们无法将所有字符串全部读入到内存中处理.")]),s._v(" "),t("h6",{attrs:{id:"方法一-分治法-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法一-分治法-3"}},[s._v("#")]),s._v(" 方法一: 分治法")]),s._v(" "),t("p",[s._v("分治法依然是一个非常实用的方法.")]),s._v(" "),t("p",[s._v("划分为多个小文件, 保证单个小文件中的字符串能被直接加载到内存中处理, 然后求出每个文件中出现次数最多的 10 个字符串; 最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串.")]),s._v(" "),t("p",[s._v("方法可行, 但不是最好, 下面介绍其他方法.")]),s._v(" "),t("h6",{attrs:{id:"方法二-hashmap法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法二-hashmap法"}},[s._v("#")]),s._v(" 方法二: HashMap法")]),s._v(" "),t("p",[s._v("虽然字符串总数比较多, 但去重后不超过 300w, 因此, 可以考虑把所有字符串及出现次数保存在一个 HashMap 中, 所占用的空间为 300w*(255+4)≈777M(其中, 4表示整数占用的4个字节). 由此可见, 1G 的内存空间完全够用.")]),s._v(" "),t("p",[t("strong",[s._v("思路如下")]),s._v(":")]),s._v(" "),t("p",[s._v("首先, 遍历字符串, 若不在 map 中, 直接存入 map, value 记为 1; 若在 map 中, 则把对应的 value 加 1, 这一步时间复杂度 "),t("code",[s._v("O(N)")]),s._v(" .")]),s._v(" "),t("p",[s._v("接着遍历 map, 构建一个 10 个元素的小顶堆, 若遍历到的字符串的出现次数大于堆顶字符串的出现次数, 则进行替换, 并将堆调整为小顶堆.")]),s._v(" "),t("p",[s._v("遍历结束后, 堆中 10 个字符串就是出现次数最多的字符串. 这一步时间复杂度 "),t("code",[s._v("O(Nlog10)")]),s._v(" .")]),s._v(" "),t("h6",{attrs:{id:"方法三-前缀树法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法三-前缀树法"}},[s._v("#")]),s._v(" 方法三: 前缀树法")]),s._v(" "),t("p",[s._v("方法二使用了 HashMap 来统计次数, 当这些字符串有大量相同前缀时, 可以考虑使用前缀树来统计字符串出现的次数, 树的结点保存字符串出现次数, 0 表示没有出现.")]),s._v(" "),t("p",[t("strong",[s._v("思路如下")]),s._v(":")]),s._v(" "),t("p",[s._v("在遍历字符串时, 在前缀树中查找, 如果找到, 则把结点中保存的字符串次数加 1, 否则为这个字符串构建新结点, 构建完成后把叶子结点中字符串的出现次数置为 1.")]),s._v(" "),t("p",[s._v("最后依然使用小顶堆来对字符串的出现次数进行排序.")]),s._v(" "),t("h5",{attrs:{id:"方法总结-6"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-6"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("p",[s._v("前缀树经常被用来统计字符串的出现次数. 它的另外一个大的用途是字符串查找, 判断是否有重复的字符串等.")]),s._v(" "),t("h4",{attrs:{id:"如何统计不同电话号码的个数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何统计不同电话号码的个数"}},[s._v("#")]),s._v(" 如何统计不同电话号码的个数?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-7"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-7"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("已知某个文件内包含一些电话号码, 每个号码为 8 位数字, 统计不同号码的个数.")]),s._v(" "),t("h5",{attrs:{id:"解答思路-7"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-7"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("这道题本质还是求解"),t("strong",[s._v("数据重复")]),s._v("的问题, 对于这类问题, 一般首先考虑"),t("strong",[s._v("位图法")]),s._v(".")]),s._v(" "),t("p",[s._v("对于本题, 8 位电话号码可以表示的号码个数为 10"),t("sup",[s._v("8")]),s._v(" 个, 即 1 亿个. 我们每个号码用一个 bit 来表示, 则总共需要 1 亿个 bit, 内存占用约 100M.")]),s._v(" "),t("p",[t("strong",[s._v("思路如下")]),s._v(":")]),s._v(" "),t("p",[s._v("申请一个位图数组, 长度为 1 亿, 初始化为 0. 然后遍历所有电话号码, 把号码对应的位图中的位置置为 1. 遍历完成后, 如果 bit 为 1, 则表示这个电话号码在文件中存在, 否则不存在. bit 值为 1 的数量即为 不同电话号码的个数.")]),s._v(" "),t("h5",{attrs:{id:"方法总结-7"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-7"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("p",[s._v("求解数据重复问题, 记得考虑位图法.")]),s._v(" "),t("h4",{attrs:{id:"如何从5亿个数中找出中位数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何从5亿个数中找出中位数"}},[s._v("#")]),s._v(" 如何从5亿个数中找出中位数?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-8"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-8"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("从 5 亿个数中找出中位数. 数据排序后, 位置在最中间的数就是中位数. 当样本数为奇数时, 中位数为 第 "),t("code",[s._v("(N+1)/2")]),s._v(" 个数; 当样本数为偶数时, 中位数为 第 "),t("code",[s._v("N/2")]),s._v(" 个数与第 "),t("code",[s._v("1+N/2")]),s._v(" 个数的均值.")]),s._v(" "),t("h5",{attrs:{id:"解答思路-8"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-8"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("如果这道题没有内存大小限制, 则可以把所有数读到内存中排序后找出中位数. 但是最好的排序算法的时间复杂度都为 "),t("code",[s._v("O(NlogN)")]),s._v(" . 这里使用其他方法.")]),s._v(" "),t("h6",{attrs:{id:"方法一-双堆法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法一-双堆法"}},[s._v("#")]),s._v(" 方法一: 双堆法")]),s._v(" "),t("p",[s._v("维护两个堆, 一个大顶堆, 一个小顶堆. 大顶堆中最大的数"),t("strong",[s._v("小于等于")]),s._v("小顶堆中最小的数; 保证这两个堆中的元素个数的差不超过 1.")]),s._v(" "),t("p",[s._v("若数据总数为"),t("strong",[s._v("偶数")]),s._v(", 当这两个堆建好之后, "),t("strong",[s._v("中位数就是这两个堆顶元素的平均值")]),s._v(". 当数据总数为"),t("strong",[s._v("奇数")]),s._v("时, 根据两个堆的大小, "),t("strong",[s._v("中位数一定在数据多的堆的堆顶")]),s._v(".")]),s._v(" "),t("div",{staticClass:"language-java line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("MedianFinder")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    \n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("private")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Integer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("private")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Integer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/** initialize your data structure here. */")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("MedianFinder")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        maxHeap "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Comparator")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("reverseOrder")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        minHeap "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Integer")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("::")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("compareTo")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    \n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("void")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("addNum")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" num"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("isEmpty")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("||")]),s._v(" maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("peek")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" num"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n            maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("offer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n            minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("offer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n        \n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" size1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("size")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" size2 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("size")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("size1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" size2 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n            minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("offer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("poll")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("size2 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" size1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n            maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("offer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("poll")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n    \n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("double")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("findMedian")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" size1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("size")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" size2 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("size")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        \n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" size1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" size2 \n            "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("?")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("peek")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("peek")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("/")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("size1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" size2 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("?")]),s._v(" maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("peek")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" minHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("peek")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br")])]),t("blockquote",[t("p",[s._v("见 LeetCode No.295: "),t("a",{attrs:{href:"https://leetcode.com/problems/find-median-from-data-stream/",target:"_blank",rel:"noopener noreferrer"}},[s._v("https://leetcode.com/problems/find-median-from-data-stream/"),t("OutboundLink")],1)])]),s._v(" "),t("p",[s._v("以上这种方法, 需要把所有数据都加载到内存中. 当数据量很大时, 就不能这样了, 因此, 这种方法"),t("strong",[s._v("适用于数据量较小的情况")]),s._v(". 5 亿个数, 每个数字占用 4B, 总共需要 2G 内存. 如果可用内存不足 2G, 就不能使用这种方法了, 下面介绍另一种方法.")]),s._v(" "),t("h6",{attrs:{id:"方法二-分治法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法二-分治法"}},[s._v("#")]),s._v(" 方法二: 分治法")]),s._v(" "),t("p",[s._v("分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解.")]),s._v(" "),t("p",[s._v("对于这道题, 顺序读取这 5 亿个数字, 对于读取到的数字 num, 如果它对应的二进制中最高位为 1, 则把这个数字写到 f1 中, 否则写入 f0 中. 通过这一步, 可以把这 5 亿个数划分为两部分, 而且 f0 中的数都大于 f1 中的数(最高位是符号位).")]),s._v(" "),t("p",[s._v("划分之后, 可以非常容易地知道中位数是在 f0 还是 f1 中. 假设 f1 中有 1 亿个数, 那么中位数一定在 f0 中, 且是在 f0 中, 从小到大排列的第 1.5 亿个数与它后面的一个数的平均值.")]),s._v(" "),t("blockquote",[t("p",[t("strong",[s._v("提示")]),s._v(", 5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值. 若 f1 有一亿个数, 那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值.")])]),s._v(" "),t("p",[s._v("对于 f0 可以用次高位的二进制继续将文件一分为二, 如此划分下去, 直到划分后的文件可以被加载到内存中, 把数据加载到内存中以后直接排序, 找出中位数.")]),s._v(" "),t("blockquote",[t("p",[t("strong",[s._v("注意")]),s._v(", 当数据总数为偶数, 如果划分后两个文件中的数据有相同个数, 那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值.")])]),s._v(" "),t("h5",{attrs:{id:"方法总结-8"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-8"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("p",[s._v("分治法, 真香!")]),s._v(" "),t("h4",{attrs:{id:"如何按照query的频度排序"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何按照query的频度排序"}},[s._v("#")]),s._v(" 如何按照query的频度排序?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-9"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-9"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("有 10 个文件, 每个文件大小为 1G, 每个文件的每一行存放的都是用户的 query, 每个文件的 query 都可能重复. 要求按照 query 的频度排序.")]),s._v(" "),t("h5",{attrs:{id:"解答思路-9"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-9"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[s._v("如果 query 的重复度比较大, 可以考虑一次性把所有 query 读入内存中处理; 如果 query 的重复率不高, 那么可用内存不足以容纳所有的 query, 这时候就需要采用分治法或其他的方法来解决.")]),s._v(" "),t("h6",{attrs:{id:"方法一-hashmap-法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法一-hashmap-法"}},[s._v("#")]),s._v(" 方法一: HashMap 法")]),s._v(" "),t("p",[s._v("如果 query 重复率高, 说明不同 query 总数比较小, 可以考虑把所有的 query 都加载到内存中的 HashMap 中. 接着就可以按照 query 出现的次数进行排序.")]),s._v(" "),t("h6",{attrs:{id:"方法二-分治法-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法二-分治法-2"}},[s._v("#")]),s._v(" 方法二: 分治法")]),s._v(" "),t("p",[s._v("分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模. 对于这道题, 可以顺序遍历 10 个文件中的 query, 通过 Hash 函数 "),t("code",[s._v("hash(query) % 10")]),s._v(" 把这些 query 划分到 10 个小文件中. 之后对每个小文件使用 HashMap 统计 query 出现次数, 根据次数排序并写入到零外一个单独文件中.")]),s._v(" "),t("p",[s._v("接着对所有文件按照 query 的次数进行排序, 这里可以使用归并排序(由于无法把所有 query 都读入内存, 因此需要使用外排序).")]),s._v(" "),t("h5",{attrs:{id:"方法总结-9"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-9"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("ul",[t("li",[s._v("内存若够, 直接读入进行排序;")]),s._v(" "),t("li",[s._v("内存不够, 先划分为小文件, 小文件排好序后, 整理使用外排序进行归并.")])]),s._v(" "),t("h4",{attrs:{id:"如何找出排名前500的数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何找出排名前500的数"}},[s._v("#")]),s._v(" 如何找出排名前500的数?")]),s._v(" "),t("h5",{attrs:{id:"题目描述-10"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#题目描述-10"}},[s._v("#")]),s._v(" 题目描述")]),s._v(" "),t("p",[s._v("有 20 个数组, 每个数组有 500 个元素, 并且有序排列. 如何在这 20*500 个数中找出前 500 的数?")]),s._v(" "),t("h5",{attrs:{id:"解答思路-10"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解答思路-10"}},[s._v("#")]),s._v(" 解答思路")]),s._v(" "),t("p",[t("strong",[s._v("对于 TopK 问题")]),s._v(", 最常用的方法是使用堆排序. 对本题而言, 假设数组降序排列, 可以采用以下方法:")]),s._v(" "),t("p",[s._v("首先建立大顶堆, 堆的大小为数组的个数, 即为 20, 把每个数组最大的值存到堆中.")]),s._v(" "),t("p",[s._v("接着删除堆顶元素, 保存到另一个大小为 500 的数组中, 然后向大顶堆插入删除的元素所在数组的下一个元素.")]),s._v(" "),t("p",[s._v("重复上面的步骤, 直到删除完第 500 个元素, 也即找出了最大的前 500 个数.")]),s._v(" "),t("blockquote",[t("p",[s._v("为了在堆中取出一个数据后, 能知道它是从哪个数组中取出的, 从而可以从这个数组中取下一个值, 可以把数组的指针存放到堆中, 对这个指针提供比较大小的方法.")])]),s._v(" "),t("div",{staticClass:"language-java line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token import"}},[t("span",{pre:!0,attrs:{class:"token namespace"}},[s._v("lombok"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Data")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token import"}},[t("span",{pre:!0,attrs:{class:"token namespace"}},[s._v("java"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("util"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Arrays")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token import"}},[t("span",{pre:!0,attrs:{class:"token namespace"}},[s._v("java"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("util"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/**\n * @author https://github.com/yanglbme\n */")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[s._v("@Data")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("implements")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Comparable")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/**\n     * 数值\n     */")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("private")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/**\n     * 记录数值来源的数组\n     */")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("private")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" source"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/**\n     * 记录数值在数组中的索引\n     */")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("private")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" index"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" source"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" index"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("this")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("value "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("this")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("source "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" source"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("this")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("index "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" index"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/**\n     *\n     * 由于 PriorityQueue 使用小顶堆来实现, 这里通过修改\n     * 两个整数的比较逻辑来让 PriorityQueue 变成大顶堆\n     */")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[s._v("@Override")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("compareTo")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),s._v(" o"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Integer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("compare")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("o"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getValue")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("this")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Test")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getTop")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" rowSize "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("length"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" columnSize "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("length"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n        "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 创建一个columnSize大小的数组, 存放结果")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" result "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("columnSize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n        "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" maxHeap "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("PriorityQueue")]),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" i "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" i "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v(" rowSize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("++")]),s._v("i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 将每个数组的最大一个元素放入堆中")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),s._v(" d "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n            maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("add")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" num "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("while")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v(" columnSize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 删除堆顶元素")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DataWithSource")]),s._v(" d "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("poll")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n            result"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("num"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("++")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getValue")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),s._v(" columnSize"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n                "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("break")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n            "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n            d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("setValue")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getSource")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getIndex")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n            d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("setIndex")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getIndex")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n            maxHeap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("add")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("d"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" result"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("public")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("static")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("void")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("main")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" args"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" data "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n                "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("29")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("17")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("19")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("17")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("16")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("6")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("25")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" top "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getTop")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n        "),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("System")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("out"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("println")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Arrays")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("toString")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("top"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// [30, 29, 25, 20, 19]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br"),t("span",{staticClass:"line-number"},[s._v("37")]),t("br"),t("span",{staticClass:"line-number"},[s._v("38")]),t("br"),t("span",{staticClass:"line-number"},[s._v("39")]),t("br"),t("span",{staticClass:"line-number"},[s._v("40")]),t("br"),t("span",{staticClass:"line-number"},[s._v("41")]),t("br"),t("span",{staticClass:"line-number"},[s._v("42")]),t("br"),t("span",{staticClass:"line-number"},[s._v("43")]),t("br"),t("span",{staticClass:"line-number"},[s._v("44")]),t("br"),t("span",{staticClass:"line-number"},[s._v("45")]),t("br"),t("span",{staticClass:"line-number"},[s._v("46")]),t("br"),t("span",{staticClass:"line-number"},[s._v("47")]),t("br"),t("span",{staticClass:"line-number"},[s._v("48")]),t("br"),t("span",{staticClass:"line-number"},[s._v("49")]),t("br"),t("span",{staticClass:"line-number"},[s._v("50")]),t("br"),t("span",{staticClass:"line-number"},[s._v("51")]),t("br"),t("span",{staticClass:"line-number"},[s._v("52")]),t("br"),t("span",{staticClass:"line-number"},[s._v("53")]),t("br"),t("span",{staticClass:"line-number"},[s._v("54")]),t("br"),t("span",{staticClass:"line-number"},[s._v("55")]),t("br"),t("span",{staticClass:"line-number"},[s._v("56")]),t("br"),t("span",{staticClass:"line-number"},[s._v("57")]),t("br"),t("span",{staticClass:"line-number"},[s._v("58")]),t("br"),t("span",{staticClass:"line-number"},[s._v("59")]),t("br"),t("span",{staticClass:"line-number"},[s._v("60")]),t("br"),t("span",{staticClass:"line-number"},[s._v("61")]),t("br"),t("span",{staticClass:"line-number"},[s._v("62")]),t("br"),t("span",{staticClass:"line-number"},[s._v("63")]),t("br"),t("span",{staticClass:"line-number"},[s._v("64")]),t("br"),t("span",{staticClass:"line-number"},[s._v("65")]),t("br"),t("span",{staticClass:"line-number"},[s._v("66")]),t("br"),t("span",{staticClass:"line-number"},[s._v("67")]),t("br"),t("span",{staticClass:"line-number"},[s._v("68")]),t("br"),t("span",{staticClass:"line-number"},[s._v("69")]),t("br"),t("span",{staticClass:"line-number"},[s._v("70")]),t("br"),t("span",{staticClass:"line-number"},[s._v("71")]),t("br"),t("span",{staticClass:"line-number"},[s._v("72")]),t("br"),t("span",{staticClass:"line-number"},[s._v("73")]),t("br"),t("span",{staticClass:"line-number"},[s._v("74")]),t("br"),t("span",{staticClass:"line-number"},[s._v("75")]),t("br"),t("span",{staticClass:"line-number"},[s._v("76")]),t("br"),t("span",{staticClass:"line-number"},[s._v("77")]),t("br"),t("span",{staticClass:"line-number"},[s._v("78")]),t("br"),t("span",{staticClass:"line-number"},[s._v("79")]),t("br"),t("span",{staticClass:"line-number"},[s._v("80")]),t("br"),t("span",{staticClass:"line-number"},[s._v("81")]),t("br"),t("span",{staticClass:"line-number"},[s._v("82")]),t("br"),t("span",{staticClass:"line-number"},[s._v("83")]),t("br"),t("span",{staticClass:"line-number"},[s._v("84")]),t("br"),t("span",{staticClass:"line-number"},[s._v("85")]),t("br")])]),t("h5",{attrs:{id:"方法总结-10"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#方法总结-10"}},[s._v("#")]),s._v(" 方法总结")]),s._v(" "),t("p",[s._v("求 TopK, 不妨考虑一下堆排序?")]),s._v(" "),t("h4",{attrs:{id:"两个大文件中找出共同记录"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#两个大文件中找出共同记录"}},[s._v("#")]),s._v(" 两个大文件中找出共同记录")]),s._v(" "),t("h5",{attrs:{id:"_1-题目描述"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-题目描述"}},[s._v("#")]),s._v(" 1. 题目描述")]),s._v(" "),t("p",[s._v("给定 a, b 两个文件, 各存放 50 亿个 url, 每个 url 各占 64 字节, 内存限制是 4G, 让你找出 a, b 文件共同的 url?")]),s._v(" "),t("h5",{attrs:{id:"_2-解题思路"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-解题思路"}},[s._v("#")]),s._v(" 2. 解题思路")]),s._v(" "),t("h6",{attrs:{id:"_1-方案-1"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-方案-1"}},[s._v("#")]),s._v(" (1) "),t("strong",[s._v("方案 1")])]),s._v(" "),t("p",[s._v("首先我们最常想到的方法是读取文件 a, 建立哈希表方便后面查找, 然后再读取文件 b, 遍历文件 b 中每个 url, 对于每个遍历, 我们都执行查找 hash 表的操作, 若 hash 表中搜索到了, 则说明两文件共有, 存入一个集合.")]),s._v(" "),t("p",[s._v("可以估计每个文件安的大小为 5G×64 =320G, 远远大于内存限制的 4G. 所以不可能将其完全加载到内存中处理.")]),s._v(" "),t("p",[s._v("针对上述问题, 我们分治算法的思想.")]),s._v(" "),t("ul",[t("li",[s._v("遍历文件 a, 对每个 url 求取 "),t("code",[s._v("hash(url)%1000")]),s._v(", 然后根据所取得的值将 url 分别存储到 1000 个小文件(记为 a0,a1,...,a999 每个小文件约 300M), 为什么是 1000? 主要根据内存大小和要分治的文件大小来计算, 我们就大致可以把 320G 大小分为 1000 份, 每份大约 300M(当然, 到底能不能分布尽量均匀, 得看 hash 函数的设计)")]),s._v(" "),t("li",[s._v("遍历文件 b, 采取和 a 相同的方式将 url 分别存储到 1000 个小文件(记为 b0,b1,...,b999 ). 为什么要这样做? 文件 a 的 hash 映射和文件 b 的 hash 映射函数要保持一致, 这样的话相同的 url 就会保存在对应的小文件中, 比如, 如果 a 中有一个 url 记录 data1 被 hash 到了 a99 文件中, 那么如果 b 中也有相同 url, 则一定被 hash 到了 b99 中. 所以现在问题转换成了: 找出 1000 对小文件中每一对相同的 url("),t("strong",[s._v("不对应的小文件不可能有相同的 url")]),s._v(")")]),s._v(" "),t("li",[s._v("求每对小文件中相同的 url 时, 可以把其中一个小文件的 url 存储到 hash_set 中. 然后遍历另一个小文件的每个 url, 看其是否在刚才构建的 hash_set 中, 如果是, 那么就是共同的 url, 存到文件里面就可以了.")])]),s._v(" "),t("h6",{attrs:{id:"_2-方案2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-方案2"}},[s._v("#")]),s._v(" (2) "),t("strong",[s._v("方案2")])]),s._v(" "),t("p",[s._v("如果允许有一定的错误率, 可以使用 Bloom filter, 4G 内存大概可以表示 340 亿 bit. 将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit, 然后挨个读取另外一个文件的 url, 检查是否与 Bloom filter, 如果是, 那么该 ur l应该是共同的 url(注意会有一定的错误率).")]),s._v(" "),t("h4",{attrs:{id:"如果有一个500g的超大文件-里面都是数值-如何对这些数值排序"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如果有一个500g的超大文件-里面都是数值-如何对这些数值排序"}},[s._v("#")]),s._v(" 如果有一个500G的超大文件, 里面都是数值, 如何对这些数值排序?")]),s._v(" "),t("p",[s._v("首先, 对于这个这样一个问题, 我们肯定不能直接一次性全部将数据加载到内存里面.")]),s._v(" "),t("p",[s._v("解决思路:")]),s._v(" "),t("ol",[t("li",[t("strong",[s._v("先将这个文件里面的值拆分成多个文件, 每个文件大小差不多 512M. ​")])]),s._v(" "),t("li",[s._v("在这1000个小文件里面的值进行排序去重.")])]),s._v(" "),t("p",[s._v("分两种情况:")]),s._v(" "),t("p",[s._v("① 如果里面的数值不是很大, 这样拼接 1000 文件数值, 拼接, 去重, 排序. 对于 8G 的内存计算机应该是可以处理的.")]),s._v(" "),t("p",[s._v("② 文件里面的数值就是坑爹的大.")]),s._v(" "),t("ol",{attrs:{start:"3"}},[t("li",[s._v("对于 ② 处理也很简单, 对于 1000 小文件, 比如就按升序排序, 我们不是已经拿到了每个的 排序么. 我们把 1000 个文件里面最小的值(也就是第一个)拿出来, 并把他们从这些文件中删除, 拿这些最小值去重排序作为第一个文件,")]),s._v(" "),t("li",[s._v("重复上面的步骤, 这样我们也得到1000个 这样排好序的文件.")])]),s._v(" "),t("p",[s._v("举例")]),s._v(" "),t("p",[s._v("如果有个文件(-1, 5, 2, 1, 4, 3, 2, 4, 5).")]),s._v(" "),t("p",[s._v("第一步: 截取, 每 3 个一个文件得到(-1, 5, 2), (1, 4, 3), (2, 4, 5).")]),s._v(" "),t("p",[s._v("第二步: 去重排序(-1, 2, 5), (1, 3, 4), (2, 4, 5).")]),s._v(" "),t("p",[s._v("第三步: 拿第一个去重排序得到新的 3 个小文件(-1, 1, 2), (2, 3, 4), (4, 5).")]),s._v(" "),t("p",[s._v("看到了吧! 按从小到大的顺序就出来了, 我们最后都知道(-1, 5, 2, 1, 4, 3, 2, 4, 5)文件从小到大的顺序是 -1, 1, 2,  3, 4, 5.")]),s._v(" "),t("h4",{attrs:{id:"参考资料"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[s._v("#")]),s._v(" 参考资料")]),s._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://www.cnblogs.com/chenhuan001/p/5866916.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("海量数据处理: 十道面试题与十个海量数据处理方法总结 - chenhuan001 - 博客园"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"https://blog.csdn.net/qq_21688757/article/details/53993096",target:"_blank",rel:"noopener noreferrer"}},[s._v("大数据和空间限制问题专题(一) - CSDN博客"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"https://github.com/ZuoAndroid/Interview-Book/blob/df3b37cf80de59015c6c3651b1e588d9cbeac889/%E9%9D%A2%E8%AF%95%E9%A2%98/2018-03-31-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88%E5%B8%B8%E8%A7%81%E7%9A%847%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98.md",target:"_blank",rel:"noopener noreferrer"}},[s._v("Interview-Book/2018-03-31-数据分析师常见的7道面试题.md"),t("OutboundLink")],1)])])])}),[],!1,null,null,null);t.default=r.exports}}]);