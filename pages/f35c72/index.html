<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>深入剖析Kubernetes(极客时间)🌸 | Pangolin Note</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="大道至简">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.56073ad3.css" as="style"><link rel="preload" href="/assets/js/app.3f3e0e10.js" as="script"><link rel="preload" href="/assets/js/2.e9fcb30c.js" as="script"><link rel="preload" href="/assets/js/3.1998f389.js" as="script"><link rel="preload" href="/assets/js/194.8cce60a9.js" as="script"><link rel="prefetch" href="/assets/js/10.0b55747f.js"><link rel="prefetch" href="/assets/js/100.b8912a99.js"><link rel="prefetch" href="/assets/js/101.a6740c8a.js"><link rel="prefetch" href="/assets/js/102.e31e89b2.js"><link rel="prefetch" href="/assets/js/103.2c5dbdae.js"><link rel="prefetch" href="/assets/js/104.0e9c02f6.js"><link rel="prefetch" href="/assets/js/105.8288396c.js"><link rel="prefetch" href="/assets/js/106.62f43c11.js"><link rel="prefetch" href="/assets/js/107.70c90211.js"><link rel="prefetch" href="/assets/js/108.b488ce56.js"><link rel="prefetch" href="/assets/js/109.12942650.js"><link rel="prefetch" href="/assets/js/11.ac3fd1ca.js"><link rel="prefetch" href="/assets/js/110.1e57ecba.js"><link rel="prefetch" href="/assets/js/111.6e33b4ea.js"><link rel="prefetch" href="/assets/js/112.bd52831b.js"><link rel="prefetch" href="/assets/js/113.868c1ac9.js"><link rel="prefetch" href="/assets/js/114.090549d1.js"><link rel="prefetch" href="/assets/js/115.d97f6c2b.js"><link rel="prefetch" href="/assets/js/116.097c4b4e.js"><link rel="prefetch" href="/assets/js/117.4024cfe2.js"><link rel="prefetch" href="/assets/js/118.e3057cba.js"><link rel="prefetch" href="/assets/js/119.4ef1fa3b.js"><link rel="prefetch" href="/assets/js/12.863eaabe.js"><link rel="prefetch" href="/assets/js/120.78f9df50.js"><link rel="prefetch" href="/assets/js/121.8e16df87.js"><link rel="prefetch" href="/assets/js/122.f21c0107.js"><link rel="prefetch" href="/assets/js/123.ea1cb375.js"><link rel="prefetch" href="/assets/js/124.01c04c6e.js"><link rel="prefetch" href="/assets/js/125.d383e301.js"><link rel="prefetch" href="/assets/js/126.3162cb47.js"><link rel="prefetch" href="/assets/js/127.c6bebae6.js"><link rel="prefetch" href="/assets/js/128.d659db60.js"><link rel="prefetch" href="/assets/js/129.2afdcf48.js"><link rel="prefetch" href="/assets/js/13.4f59ce35.js"><link rel="prefetch" href="/assets/js/130.beb9ed9d.js"><link rel="prefetch" href="/assets/js/131.35b05f8a.js"><link rel="prefetch" href="/assets/js/132.d77f4f93.js"><link rel="prefetch" href="/assets/js/133.4ceda6e5.js"><link rel="prefetch" href="/assets/js/134.11390c5f.js"><link rel="prefetch" href="/assets/js/135.32f9e38f.js"><link rel="prefetch" href="/assets/js/136.6d8bb0f2.js"><link rel="prefetch" href="/assets/js/137.9c0ffeef.js"><link rel="prefetch" href="/assets/js/138.88d86e5f.js"><link rel="prefetch" href="/assets/js/139.8c2c3d6c.js"><link rel="prefetch" href="/assets/js/14.11c82d35.js"><link rel="prefetch" href="/assets/js/140.c5c375d3.js"><link rel="prefetch" href="/assets/js/141.d703f30b.js"><link rel="prefetch" href="/assets/js/142.6a005a44.js"><link rel="prefetch" href="/assets/js/143.9b2edf6f.js"><link rel="prefetch" href="/assets/js/144.3fd013c9.js"><link rel="prefetch" href="/assets/js/145.b05079c5.js"><link rel="prefetch" href="/assets/js/146.ed5360a6.js"><link rel="prefetch" href="/assets/js/147.7408d86f.js"><link rel="prefetch" href="/assets/js/148.f390b370.js"><link rel="prefetch" href="/assets/js/149.95017f0a.js"><link rel="prefetch" href="/assets/js/15.bd143bd5.js"><link rel="prefetch" href="/assets/js/150.f0ede764.js"><link rel="prefetch" href="/assets/js/151.b7093623.js"><link rel="prefetch" href="/assets/js/152.a82a6c83.js"><link rel="prefetch" href="/assets/js/153.965e3fb2.js"><link rel="prefetch" href="/assets/js/154.9e49311e.js"><link rel="prefetch" href="/assets/js/155.cef33ba5.js"><link rel="prefetch" href="/assets/js/156.bcc397ae.js"><link rel="prefetch" href="/assets/js/157.90824739.js"><link rel="prefetch" href="/assets/js/158.efd429b9.js"><link rel="prefetch" href="/assets/js/159.122ff5b7.js"><link rel="prefetch" href="/assets/js/16.2449162b.js"><link rel="prefetch" href="/assets/js/160.0b806535.js"><link rel="prefetch" href="/assets/js/161.835052c6.js"><link rel="prefetch" href="/assets/js/162.ca34e2d2.js"><link rel="prefetch" href="/assets/js/163.08000d04.js"><link rel="prefetch" href="/assets/js/164.a1cc0109.js"><link rel="prefetch" href="/assets/js/165.65444686.js"><link rel="prefetch" href="/assets/js/166.7d73c84f.js"><link rel="prefetch" href="/assets/js/167.009d47a3.js"><link rel="prefetch" href="/assets/js/168.0afeae2a.js"><link rel="prefetch" href="/assets/js/169.7654cb31.js"><link rel="prefetch" href="/assets/js/17.eb7f9def.js"><link rel="prefetch" href="/assets/js/170.58569530.js"><link rel="prefetch" href="/assets/js/171.7db9ed40.js"><link rel="prefetch" href="/assets/js/172.3cb50ed4.js"><link rel="prefetch" href="/assets/js/173.0846425f.js"><link rel="prefetch" href="/assets/js/174.9e95f111.js"><link rel="prefetch" href="/assets/js/175.ef2098f2.js"><link rel="prefetch" href="/assets/js/176.c2635fe9.js"><link rel="prefetch" href="/assets/js/177.4fa38e8e.js"><link rel="prefetch" href="/assets/js/178.2be7037f.js"><link rel="prefetch" href="/assets/js/179.bf3bba28.js"><link rel="prefetch" href="/assets/js/18.0455f4d1.js"><link rel="prefetch" href="/assets/js/180.72c5f597.js"><link rel="prefetch" href="/assets/js/181.42287bcc.js"><link rel="prefetch" href="/assets/js/182.6cd4bf1a.js"><link rel="prefetch" href="/assets/js/183.05dbbfd9.js"><link rel="prefetch" href="/assets/js/184.03de7e00.js"><link rel="prefetch" href="/assets/js/185.42dd210e.js"><link rel="prefetch" href="/assets/js/186.28ee0f8a.js"><link rel="prefetch" href="/assets/js/187.bb58697b.js"><link rel="prefetch" href="/assets/js/188.1bc8be96.js"><link rel="prefetch" href="/assets/js/189.fac747e3.js"><link rel="prefetch" href="/assets/js/19.ae9e35d6.js"><link rel="prefetch" href="/assets/js/190.ea533ac7.js"><link rel="prefetch" href="/assets/js/191.6dc6eb13.js"><link rel="prefetch" href="/assets/js/192.184d249f.js"><link rel="prefetch" href="/assets/js/193.4a06bc12.js"><link rel="prefetch" href="/assets/js/195.93231a13.js"><link rel="prefetch" href="/assets/js/196.4a596bde.js"><link rel="prefetch" href="/assets/js/197.96c113cf.js"><link rel="prefetch" href="/assets/js/198.73ba3f67.js"><link rel="prefetch" href="/assets/js/199.74bab595.js"><link rel="prefetch" href="/assets/js/20.b542d0e7.js"><link rel="prefetch" href="/assets/js/200.69a6928a.js"><link rel="prefetch" href="/assets/js/201.9ffa7c5a.js"><link rel="prefetch" href="/assets/js/202.41edb652.js"><link rel="prefetch" href="/assets/js/203.7fabcef3.js"><link rel="prefetch" href="/assets/js/204.b0ae2f62.js"><link rel="prefetch" href="/assets/js/205.add8738b.js"><link rel="prefetch" href="/assets/js/206.f3a712ec.js"><link rel="prefetch" href="/assets/js/207.cd7dd729.js"><link rel="prefetch" href="/assets/js/208.78b33afa.js"><link rel="prefetch" href="/assets/js/209.9d3329ff.js"><link rel="prefetch" href="/assets/js/21.5a050318.js"><link rel="prefetch" href="/assets/js/210.d285d5ac.js"><link rel="prefetch" href="/assets/js/211.8791bb3f.js"><link rel="prefetch" href="/assets/js/212.84ed81a8.js"><link rel="prefetch" href="/assets/js/213.7b990580.js"><link rel="prefetch" href="/assets/js/214.da31f20c.js"><link rel="prefetch" href="/assets/js/215.9eeed659.js"><link rel="prefetch" href="/assets/js/216.9539f0ec.js"><link rel="prefetch" href="/assets/js/217.11b575be.js"><link rel="prefetch" href="/assets/js/218.a67f12f1.js"><link rel="prefetch" href="/assets/js/219.bfbb817a.js"><link rel="prefetch" href="/assets/js/22.2bc6f7e3.js"><link rel="prefetch" href="/assets/js/220.8b01342f.js"><link rel="prefetch" href="/assets/js/221.b450decd.js"><link rel="prefetch" href="/assets/js/222.97468507.js"><link rel="prefetch" href="/assets/js/223.b6dafd73.js"><link rel="prefetch" href="/assets/js/224.c86c18c6.js"><link rel="prefetch" href="/assets/js/225.b0dcf86e.js"><link rel="prefetch" href="/assets/js/226.02cc2999.js"><link rel="prefetch" href="/assets/js/227.8474ef5a.js"><link rel="prefetch" href="/assets/js/228.0298e421.js"><link rel="prefetch" href="/assets/js/229.6aeaf595.js"><link rel="prefetch" href="/assets/js/23.9995fce4.js"><link rel="prefetch" href="/assets/js/230.a5785286.js"><link rel="prefetch" href="/assets/js/231.d791daa4.js"><link rel="prefetch" href="/assets/js/232.97aeeb00.js"><link rel="prefetch" href="/assets/js/233.46e51e28.js"><link rel="prefetch" href="/assets/js/234.2cffe82f.js"><link rel="prefetch" href="/assets/js/235.14965da6.js"><link rel="prefetch" href="/assets/js/236.00a5afc0.js"><link rel="prefetch" href="/assets/js/237.3b73d52f.js"><link rel="prefetch" href="/assets/js/238.6e1db765.js"><link rel="prefetch" href="/assets/js/239.75866c5e.js"><link rel="prefetch" href="/assets/js/24.9141eeb2.js"><link rel="prefetch" href="/assets/js/240.af9c2cc9.js"><link rel="prefetch" href="/assets/js/241.388acab2.js"><link rel="prefetch" href="/assets/js/242.c60f2b48.js"><link rel="prefetch" href="/assets/js/243.d8e81b13.js"><link rel="prefetch" href="/assets/js/244.58b0b21d.js"><link rel="prefetch" href="/assets/js/245.c3768497.js"><link rel="prefetch" href="/assets/js/246.ac8bbe7a.js"><link rel="prefetch" href="/assets/js/247.d095f70a.js"><link rel="prefetch" href="/assets/js/248.e9f210b5.js"><link rel="prefetch" href="/assets/js/249.a9fad023.js"><link rel="prefetch" href="/assets/js/25.98e8593e.js"><link rel="prefetch" href="/assets/js/250.ae7f0ebb.js"><link rel="prefetch" href="/assets/js/251.9a617d55.js"><link rel="prefetch" href="/assets/js/252.ce082446.js"><link rel="prefetch" href="/assets/js/253.4575302d.js"><link rel="prefetch" href="/assets/js/254.5899a61b.js"><link rel="prefetch" href="/assets/js/255.66c69f31.js"><link rel="prefetch" href="/assets/js/256.8fd6c706.js"><link rel="prefetch" href="/assets/js/257.31b77e5e.js"><link rel="prefetch" href="/assets/js/258.eba48891.js"><link rel="prefetch" href="/assets/js/259.edf74b59.js"><link rel="prefetch" href="/assets/js/26.e69924ea.js"><link rel="prefetch" href="/assets/js/260.cf2ed36d.js"><link rel="prefetch" href="/assets/js/261.9e7792d8.js"><link rel="prefetch" href="/assets/js/262.e7b9433e.js"><link rel="prefetch" href="/assets/js/263.1185b25c.js"><link rel="prefetch" href="/assets/js/264.5e0f89cb.js"><link rel="prefetch" href="/assets/js/265.a38d3b94.js"><link rel="prefetch" href="/assets/js/266.2ef170b3.js"><link rel="prefetch" href="/assets/js/267.a7d14651.js"><link rel="prefetch" href="/assets/js/268.05b18748.js"><link rel="prefetch" href="/assets/js/269.dad48d6f.js"><link rel="prefetch" href="/assets/js/27.13612515.js"><link rel="prefetch" href="/assets/js/270.4f5a6b8d.js"><link rel="prefetch" href="/assets/js/271.7ebf6682.js"><link rel="prefetch" href="/assets/js/272.7c2fbfd1.js"><link rel="prefetch" href="/assets/js/273.8ee20732.js"><link rel="prefetch" href="/assets/js/274.d525242a.js"><link rel="prefetch" href="/assets/js/275.a8a19acb.js"><link rel="prefetch" href="/assets/js/276.df3a4eb4.js"><link rel="prefetch" href="/assets/js/277.336debda.js"><link rel="prefetch" href="/assets/js/278.c470625f.js"><link rel="prefetch" href="/assets/js/279.a91e1a64.js"><link rel="prefetch" href="/assets/js/28.ab7ae1df.js"><link rel="prefetch" href="/assets/js/280.87e25c9a.js"><link rel="prefetch" href="/assets/js/281.87c1ba25.js"><link rel="prefetch" href="/assets/js/282.dcd4dce0.js"><link rel="prefetch" href="/assets/js/283.abac2e00.js"><link rel="prefetch" href="/assets/js/284.f6079659.js"><link rel="prefetch" href="/assets/js/285.f1b39879.js"><link rel="prefetch" href="/assets/js/286.f6a79242.js"><link rel="prefetch" href="/assets/js/287.06bebe07.js"><link rel="prefetch" href="/assets/js/288.89e325df.js"><link rel="prefetch" href="/assets/js/289.3aa1bedd.js"><link rel="prefetch" href="/assets/js/29.ebe50f76.js"><link rel="prefetch" href="/assets/js/290.ee059c92.js"><link rel="prefetch" href="/assets/js/291.fa9a921a.js"><link rel="prefetch" href="/assets/js/292.2a8811cd.js"><link rel="prefetch" href="/assets/js/293.eec09cdf.js"><link rel="prefetch" href="/assets/js/294.dfac20dc.js"><link rel="prefetch" href="/assets/js/295.825d2070.js"><link rel="prefetch" href="/assets/js/296.f645861e.js"><link rel="prefetch" href="/assets/js/297.424fdb17.js"><link rel="prefetch" href="/assets/js/298.ebf87cdc.js"><link rel="prefetch" href="/assets/js/299.b8f19cbb.js"><link rel="prefetch" href="/assets/js/30.75237511.js"><link rel="prefetch" href="/assets/js/300.10fb6d4f.js"><link rel="prefetch" href="/assets/js/301.ef77c612.js"><link rel="prefetch" href="/assets/js/302.1b839763.js"><link rel="prefetch" href="/assets/js/303.609f7d98.js"><link rel="prefetch" href="/assets/js/304.1d255f19.js"><link rel="prefetch" href="/assets/js/305.b0234f2c.js"><link rel="prefetch" href="/assets/js/306.48677f64.js"><link rel="prefetch" href="/assets/js/307.14390d4b.js"><link rel="prefetch" href="/assets/js/308.fa730b28.js"><link rel="prefetch" href="/assets/js/309.0496b9e0.js"><link rel="prefetch" href="/assets/js/31.cf3f471a.js"><link rel="prefetch" href="/assets/js/310.a31676dc.js"><link rel="prefetch" href="/assets/js/311.ed53adc5.js"><link rel="prefetch" href="/assets/js/312.1b0ff2f1.js"><link rel="prefetch" href="/assets/js/313.bf123f32.js"><link rel="prefetch" href="/assets/js/314.4e6ce06b.js"><link rel="prefetch" href="/assets/js/315.8eb18560.js"><link rel="prefetch" href="/assets/js/32.74fef842.js"><link rel="prefetch" href="/assets/js/33.bc2d190b.js"><link rel="prefetch" href="/assets/js/34.d23438fc.js"><link rel="prefetch" href="/assets/js/35.7675c4d0.js"><link rel="prefetch" href="/assets/js/36.96a4161b.js"><link rel="prefetch" href="/assets/js/37.d1824f2f.js"><link rel="prefetch" href="/assets/js/38.4effff9e.js"><link rel="prefetch" href="/assets/js/39.6509914b.js"><link rel="prefetch" href="/assets/js/4.4d01750f.js"><link rel="prefetch" href="/assets/js/40.1509d08b.js"><link rel="prefetch" href="/assets/js/41.f2c1124f.js"><link rel="prefetch" href="/assets/js/42.e541d077.js"><link rel="prefetch" href="/assets/js/43.369de999.js"><link rel="prefetch" href="/assets/js/44.43959db0.js"><link rel="prefetch" href="/assets/js/45.282fbd31.js"><link rel="prefetch" href="/assets/js/46.1b83cfe9.js"><link rel="prefetch" href="/assets/js/47.c3a88e41.js"><link rel="prefetch" href="/assets/js/48.bc7c0a1b.js"><link rel="prefetch" href="/assets/js/49.92a4e5ba.js"><link rel="prefetch" href="/assets/js/5.c3991e24.js"><link rel="prefetch" href="/assets/js/50.9c488c6c.js"><link rel="prefetch" href="/assets/js/51.546ea632.js"><link rel="prefetch" href="/assets/js/52.0d2ccee1.js"><link rel="prefetch" href="/assets/js/53.6f52b5b1.js"><link rel="prefetch" href="/assets/js/54.c838b295.js"><link rel="prefetch" href="/assets/js/55.13af99ee.js"><link rel="prefetch" href="/assets/js/56.6be6d1ed.js"><link rel="prefetch" href="/assets/js/57.67c98b39.js"><link rel="prefetch" href="/assets/js/58.107e82ec.js"><link rel="prefetch" href="/assets/js/59.b3e5edc7.js"><link rel="prefetch" href="/assets/js/6.36a535f9.js"><link rel="prefetch" href="/assets/js/60.3b0b4ba5.js"><link rel="prefetch" href="/assets/js/61.3df843df.js"><link rel="prefetch" href="/assets/js/62.1213823d.js"><link rel="prefetch" href="/assets/js/63.e8f3f926.js"><link rel="prefetch" href="/assets/js/64.e1219050.js"><link rel="prefetch" href="/assets/js/65.5bf5bb0b.js"><link rel="prefetch" href="/assets/js/66.a2502afb.js"><link rel="prefetch" href="/assets/js/67.690dd59b.js"><link rel="prefetch" href="/assets/js/68.de7b0915.js"><link rel="prefetch" href="/assets/js/69.ac61dd61.js"><link rel="prefetch" href="/assets/js/7.5d254238.js"><link rel="prefetch" href="/assets/js/70.3edd41b1.js"><link rel="prefetch" href="/assets/js/71.d23407e9.js"><link rel="prefetch" href="/assets/js/72.a5bd01bc.js"><link rel="prefetch" href="/assets/js/73.c9f4dd57.js"><link rel="prefetch" href="/assets/js/74.66323fc1.js"><link rel="prefetch" href="/assets/js/75.e1a72e00.js"><link rel="prefetch" href="/assets/js/76.801268b4.js"><link rel="prefetch" href="/assets/js/77.3a5fda67.js"><link rel="prefetch" href="/assets/js/78.54d84cd4.js"><link rel="prefetch" href="/assets/js/79.fa10f4e3.js"><link rel="prefetch" href="/assets/js/8.e060e47d.js"><link rel="prefetch" href="/assets/js/80.d5b24fea.js"><link rel="prefetch" href="/assets/js/81.0e9da714.js"><link rel="prefetch" href="/assets/js/82.e96ff6d7.js"><link rel="prefetch" href="/assets/js/83.771cced1.js"><link rel="prefetch" href="/assets/js/84.220b69e7.js"><link rel="prefetch" href="/assets/js/85.7dcc5659.js"><link rel="prefetch" href="/assets/js/86.44ba2100.js"><link rel="prefetch" href="/assets/js/87.281da75d.js"><link rel="prefetch" href="/assets/js/88.12d88449.js"><link rel="prefetch" href="/assets/js/89.f28c86d3.js"><link rel="prefetch" href="/assets/js/9.8f9fef32.js"><link rel="prefetch" href="/assets/js/90.715cabb2.js"><link rel="prefetch" href="/assets/js/91.6ced7c82.js"><link rel="prefetch" href="/assets/js/92.c05b33ca.js"><link rel="prefetch" href="/assets/js/93.6bf5fb66.js"><link rel="prefetch" href="/assets/js/94.3561200e.js"><link rel="prefetch" href="/assets/js/95.0f2cf716.js"><link rel="prefetch" href="/assets/js/96.ac8487ea.js"><link rel="prefetch" href="/assets/js/97.48042b5a.js"><link rel="prefetch" href="/assets/js/98.441bb7f8.js"><link rel="prefetch" href="/assets/js/99.4b214d92.js">
    <link rel="stylesheet" href="/assets/css/0.styles.56073ad3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/magic.png" alt="Pangolin Note" class="logo"> <span class="site-name can-hide">Pangolin Note</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">🏡首页</a></div><div class="nav-item"><a href="/basic/" class="nav-link">基础</a></div><div class="nav-item"><a href="/develop/" class="nav-link">开发</a></div><div class="nav-item"><a href="/middleware/" class="nav-link">系统</a></div><div class="nav-item"><a href="/algorithm/" class="nav-link">算法</a></div><div class="nav-item"><a href="/books/" class="nav-link">🍀读书笔记</a></div><div class="nav-item"><a href="/work/" class="nav-link">工作</a></div><div class="nav-item"><a href="/pangolin/" class="nav-link">🌸达尔文的猹</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/avatarnew.png"> <div class="blogger-info"><h3>达尔文的猹</h3> <span>大道至简 悟在天成</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">🏡首页</a></div><div class="nav-item"><a href="/basic/" class="nav-link">基础</a></div><div class="nav-item"><a href="/develop/" class="nav-link">开发</a></div><div class="nav-item"><a href="/middleware/" class="nav-link">系统</a></div><div class="nav-item"><a href="/algorithm/" class="nav-link">算法</a></div><div class="nav-item"><a href="/books/" class="nav-link">🍀读书笔记</a></div><div class="nav-item"><a href="/work/" class="nav-link">工作</a></div><div class="nav-item"><a href="/pangolin/" class="nav-link">🌸达尔文的猹</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>系统</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>分布式系统理论</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/129626/" class="sidebar-link">分布式系统基础</a></li><li><a href="/pages/fb5d35/" class="sidebar-link">分布式共识算法</a></li><li><a href="/pages/12ac37/" class="sidebar-link">分布式系统组件</a></li><li><a href="/pages/d03ebf/" class="sidebar-link">分布式技术原理与算法解析(极客时间)🌸</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>系统接入层</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/f1b6c4/" class="sidebar-link">Nginx基础</a></li><li><a href="/pages/d4123d/" class="sidebar-link">深入拆解Tomcat与Jetty(极客时间)🌸</a></li><li><a href="/pages/baee2f/" class="sidebar-link">Netty</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>服务治理-注册发现与RPC</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/05077d/" class="sidebar-link">基础</a></li><li><a href="/pages/51b6aa/" class="sidebar-link">RPC实战与核心原理(极客时间)🌸</a></li><li><a href="/pages/0966ee/" class="sidebar-link">Zookeeper</a></li><li><a href="/pages/3b7e05/" class="sidebar-link">Nacos</a></li><li><a href="/pages/7f31f8/" class="sidebar-link">Dubbo</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>服务治理-流量控制</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/44bfa8/" class="sidebar-link">负载均衡</a></li><li><a href="/pages/4d5a6c/" class="sidebar-link">限流</a></li><li><a href="/pages/e0c561/" class="sidebar-link">熔断</a></li><li><a href="/pages/12ae40/" class="sidebar-link">网关路由</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>服务治理-系统监控与安全</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/8c9210/" class="sidebar-link">系统安全性</a></li><li><a href="/pages/c9bf40/" class="sidebar-link">系统监控组件</a></li><li><a href="/pages/3f3cf7/" class="sidebar-link">运维监控系统实战(极客时间)🌸</a></li><li><a href="/pages/e20e02/" class="sidebar-link">OAuth2.0实战课(极客时间)🌟</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>中间件-消息队列</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/96d94c/" class="sidebar-link">消息队列基础</a></li><li><a href="/pages/abf16c/" class="sidebar-link">RabbitMQ</a></li><li><a href="/pages/4fc3f1/" class="sidebar-link">Kafka</a></li><li><a href="/pages/013cfe/" class="sidebar-link">RocketMQ</a></li><li><a href="/pages/ed8d92/" class="sidebar-link">Disruptor</a></li><li><a href="/pages/249149/" class="sidebar-link">消息队列高手课(极客时间)🌟</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>中间件-缓存</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/946847/" class="sidebar-link">缓存基础</a></li><li><a href="/pages/e46d56/" class="sidebar-link">本地缓存</a></li><li><a href="/pages/0abfb9/" class="sidebar-link">Redis基础</a></li><li><a href="/pages/09236a/" class="sidebar-link">Redis持久化</a></li><li><a href="/pages/867f9b/" class="sidebar-link">Redis主从复制</a></li><li><a href="/pages/50cae1/" class="sidebar-link">Redis哨兵</a></li><li><a href="/pages/43b45c/" class="sidebar-link">Redis集群</a></li><li><a href="/pages/a32379/" class="sidebar-link">Redis内存管理与运维</a></li><li><a href="/pages/386037/" class="sidebar-link">Redis核心技术与实战(极客时间)🌸</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>中间件-其他</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/23044d/" class="sidebar-link">定时任务-XXLJob</a></li><li><a href="/pages/459117/" class="sidebar-link">ES与检索</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>系统设计与优化</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/41a845/" class="sidebar-link">凤凰架构</a></li><li><a href="/pages/68cc0b/" class="sidebar-link">左耳听风(极客时间)🌟</a></li><li><a href="/pages/e3e99c/" class="sidebar-link">从0开始学微服务(极客时间)🌸</a></li><li><a href="/pages/1e5368/" class="sidebar-link">高并发系统设计40问(极客时间)🌸</a></li><li><a href="/pages/33599f/" class="sidebar-link">系统性能调优必知必会(极客时间)🌸</a></li><li><a href="/pages/c83472/" class="sidebar-link">后端技术面试38讲(极客时间)</a></li><li><a href="/pages/4404b6/" class="sidebar-link">架构实战案例解析(极客时间)🌸</a></li><li><a href="/pages/8f1c1d/" class="sidebar-link">如何设计一个秒杀系统(极客时间)</a></li></ul></section></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>容器</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>容器</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/582acf/" class="sidebar-link">部署Minikube</a></li><li><a href="/pages/98e5e4/" class="sidebar-link">容器实战高手课(极客时间)🌸</a></li><li><a href="/pages/c6a42c/" class="sidebar-link">Kubernetes实战🌸</a></li><li><a href="/pages/f35c72/" aria-current="page" class="active sidebar-link">深入剖析Kubernetes(极客时间)🌸</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/pages/caa314/" class="sidebar-link">Istio</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>自动化运维</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/beb97f/" class="sidebar-link">持续集成(CICD)</a></li><li><a href="/pages/a0df2d/" class="sidebar-link">DevOps</a></li><li><a href="/pages/765815/" class="sidebar-link">SRE实战手册(极客时间)</a></li></ul></section></li></ul></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06970110><div class="articleInfo" data-v-06970110><ul class="breadcrumbs" data-v-06970110><li data-v-06970110><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06970110></a></li> <li data-v-06970110><a href="/middleware/#系统" data-v-06970110>系统</a></li><li data-v-06970110><a href="/middleware/#容器" data-v-06970110>容器</a></li><li data-v-06970110><a href="/middleware/#容器" data-v-06970110>容器</a></li></ul> <div class="info" data-v-06970110><div title="作者" class="author iconfont icon-touxiang" data-v-06970110><a href="https://github.com/nanodaemony" target="_blank" title="作者" class="beLink" data-v-06970110>NanoDaemony</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06970110><a href="javascript:;" data-v-06970110>2025-02-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">本文目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">深入剖析Kubernetes(极客时间)🌸<!----></h1>  <div class="theme-vdoing-content content__default"><h1 id="_210-深入剖析kubernetes-极客时间-🌸"><a href="#_210-深入剖析kubernetes-极客时间-🌸" class="header-anchor">#</a> 210.深入剖析Kubernetes(极客时间)🌸</h1> <h3 id="开篇词"><a href="#开篇词" class="header-anchor">#</a> 开篇词</h3> <h4 id="开篇词-打通-容器技术-的任督二脉"><a href="#开篇词-打通-容器技术-的任督二脉" class="header-anchor">#</a> 开篇词-打通&quot;容器技术&quot;的任督二脉</h4> <p>2012 年, 我还在浙大读书的时候, 就有幸组建了一个云计算与 PaaS 基础设施相关的科研团队, 就这样, 我从早期的 Cloud Foundry 社区开始, 正式与容器结缘.</p> <p>这几年里, 我大多数时间都在 Kubernetes 项目里从事上游技术工作, 也得以作为一名从业者和社区成员的身份, 参与和亲历了容器技术从初出茅庐到尘埃落定的全过程.</p> <p>而即使从 2013 年 Docker 项目发布开始算起, 这次变革也不过短短 5 年时间, 可在现如今的技术圈儿里, 不懂容器, 没听过 Kubernetes, 你还真不好意思跟人打招呼.</p> <p>容器技术这样一个新生事物, 完全重塑了整个云计算市场的形态. 它不仅催生出了一批年轻有为的容器技术人, 更培育出了一个具有相当规模的开源基础设施技术市场.</p> <p>在这个市场里, 不仅有 Google, Microsoft 等技术巨擘们厮杀至今, 更有无数的国内外创业公司前仆后继. 而在国内, 甚至连以前对开源基础设施领域涉足不多的 BAT, 蚂蚁, 滴滴这样的巨头们, 也都从 AI, 云计算, 微服务, 基础设施等维度多管齐下, 争相把容器和 Kubernetes 项目树立为战略重心之一.</p> <p><mark><strong>就在这场因 &quot;容器&quot; 而起的技术变革中, Kubernetes 项目已然成为容器技术的事实标准, 重新定义了基础设施领域对应用编排与管理的种种可能.</strong></mark></p> <p>2014 年后, 我开始以远程的方式, 全职在 Kubernetes 和 Kata Containers 社区从事上游开发工作, 先后发起了容器镜像亲密性调度, 基于等价类的调度优化等多个核心特性, 参与了容器运行时接口, 安全容器沙盒等多个基础特性的设计和研发. 还有幸作为主要的研发人员和维护者之一, 亲历了 Serverless Container 概念的诞生与崛起.</p> <p>在 2015 年, 我发起和组织撰写了《Docker 容器与容器云》一书, 希望帮助更多的人利用容器解决实际场景中的问题. 时至今日, 这本书的第 2 版也已经出版快 2 年了, 受到了广大容器技术读者们的好评.</p> <p>2018 年, 我又赴西雅图, 在微软研究院(MSR)云计算与存储研究组, 专门从事基于 Kubernetes 的深度学习基础设施相关的研究工作.</p> <p>我与容器打交道的这些年, 一直在与关注容器生态的工程师们交流, 并经常探讨容器在落地过程中遇到的问题. 从这些交流中, 我发现总有很多相似的问题被反复提及, 比如:</p> <ol><li>为什么容器里只能跑&quot;一个进程&quot;?</li> <li>为什么我原先一直在用的某个 JVM 参数, 在容器里就不好使了?</li> <li>为什么 Kubernetes 就不能固定 IP 地址? 容器网络连不通又该如何去 Debug?</li> <li>Kubernetes 中 StatefulSet 和 Operator 到底什么区别? PV 和 PVC 这些概念又该怎么用?</li></ol> <p>这些问题乍一看与我们平常的认知非常矛盾, 但它们的答案和原理却并不复杂. 不过很遗憾, 对于刚刚开始学习容器的技术人员来说, 它们却很难用一两句话就能解释清楚.</p> <p><mark>究其原因在于, </mark>​<mark>**从过去以物理机和虚拟机为主体的开发运维环境, 向以容器为核心的基础设施的转变过程, 并不是一次温和的改革, 而是涵盖了对网络, 存储, 调度, 操作系统, 分布式原理等各个方面的容器化理解和改造. **</mark></p> <p>这就导致了很多初学者, 对于容器技术栈表现出来的这些难题, 要么知识储备不足, 要么杂乱无章, 无法形成体系. 这也是很多初次参与 PaaS 项目的从业者们共同面临的一个困境.</p> <p>其实, 容器技术体系看似纷乱繁杂, 却存在着很多可以&quot;牵一发而动全身&quot;的主线. 比如, <strong>Linux 的进程模型对于容器本身的重要意义</strong>; 或者, &quot;控制器&quot;模式对整个 Kubernetes 项目提纲挈领的作用.</p> <p><strong>但是, 这些关于 Linux 内核, 分布式系统, 网络, 存储等方方面面的积累, 并不会在 Docker 或者 Kubernetes 的文档中交代清楚. 可偏偏就是它们, 才是真正掌握容器技术体系的精髓所在, 是每一位技术从业者需要悉心修炼的&quot;内功&quot;.</strong></p> <p>**而这, 也正是开设这个专栏的初衷. ** 希望借由这个专栏, 讲清楚容器背后的这些技术本质与设计思想, 并结合着对核心特性的剖析与实践, 加深你对容器技术的理解.</p> <p>为此, 专栏划分成了 4 大模块:</p> <ol><li>&quot;**白话&quot;容器技术基础: ** 用饶有趣味的解解说来梳理容器技术生态的发展脉络, 用最通俗易懂的语言描述容器底层技术的实现方式, 让你知其然, 也知其所以然.</li> <li>**Kubernetes 集群的搭建与实践: ** Kubernetes 集群号称&quot;非常复杂&quot;, 但是如果明白了其中的架构和原理, 选择了正确的工具和方法, 它的搭建却也可以&quot;一键安装&quot;, 它的应用部署也可以浅显易懂.</li> <li><mark><strong>容器编排与 Kubernetes 核心特性剖析</strong></mark>​ **: ** 这是这个专栏最重要的内容.  <strong>&quot;编排&quot;永远都是容器云项目的灵魂所在</strong>, 也是 Kubernetes 社区持久生命力的源泉. 在这一模块会从分布式系统设计的视角出发, 抽象和归纳出这些特性中体现出来的普遍方法, 然后带着这些指导思想去逐一阐述 Kubernetes 项目关于编排, 调度和作业管理的各项核心特性.</li> <li><strong>Kubernetes 开源社区与生态: &quot;</strong> 开源生态&quot;永远都是容器技术和 Kubernetes 项目成功的关键. 在这个模块, 我会和你一起探讨, 容器社区在开源软件工程指导下的演进之路; 带你思考, 如何同团队一起平衡内外部需求, 让自己逐渐成为社区中不可或缺的一员.</li></ol> <p>一句名言: <strong>一个人的命运啊, 当然要靠自我奋斗, 但是也要考虑到历史的行程</strong>.</p> <p>**在专栏开始, 我首先为你准备了 4 篇预习文章, 详细地梳理了容器技术自兴起到现在的发展历程, 同时也回答了 &quot;Kubernetes 为什么会赢&quot; 这个重要的问题, 算是我额外为你准备的一份开学礼物. **</p> <h3 id="容器技术预习篇"><a href="#容器技术预习篇" class="header-anchor">#</a> 容器技术预习篇</h3> <h4 id="_01-预习篇·小鲸鱼大事记-一-初出茅庐"><a href="#_01-预习篇·小鲸鱼大事记-一-初出茅庐" class="header-anchor">#</a> 01 | 预习篇·小鲸鱼大事记(一):初出茅庐</h4> <p>2013 年的后端技术领域, 已经太久没有出现过令人兴奋的东西了. 曾经被人们寄予厚望的云计算技术, 也已经从当初虚无缥缈的概念蜕变成了实实在在的虚拟机和账单. 而相比于的如日中天 AWS 和盛极一时的 OpenStack, 以 <strong>Cloud Foundry</strong> 为代表的开源 PaaS 项目, 却成为了当时云计算技术中的一股清流.</p> <p>这时, Cloud Foundry 项目已经基本度过了最艰难的概念普及和用户教育阶段, 吸引了包括百度, 京东, 华为, IBM 等一大批国内外技术厂商, 开启了以开源 PaaS 为核心构建平台层服务能力的变革. 如果你有机会问问当时的云计算从业者们, 他们十有八九都会告诉你: PaaS 的时代就要来了!</p> <p>这个说法其实一点儿没错, 如果不是后来一个叫 Docker 的开源项目突然冒出来的话.</p> <p>事实上, 当时还名叫 dotCloud 的 Docker 公司, 也是这股 PaaS 热潮中的一份子. 只不过相比于 Heroku, Pivotal, Red Hat 等 PaaS 弄潮儿们, dotCloud 公司实在是太微不足道了, 而它的主打产品由于跟主流的 Cloud Foundry 社区脱节, 长期以来也无人问津. 眼看就要被如火如荼的 PaaS 风潮抛弃, dotCloud 公司却做出了这样一个决定: <strong>开源自己的容器项目 Docker</strong>.</p> <p>显然, 这个决定在当时根本没人在乎.</p> <p>&quot;容器&quot;这个概念从来就不是什么新鲜的东西, 也不是 Docker 公司发明的. 即使在当时最热门的 PaaS 项目 Cloud Foundry 中, 容器也只是其最底层, 最没人关注的那一部分. 说到这里, 我正好以当时的事实标准 Cloud Foundry 为例, 来解说一下 PaaS 技术.</p> <p>**PaaS 项目被大家接纳的一个主要原因, 就是它提供了一种名叫&quot;应用托管&quot;的能力. ** 在当时, 虚拟机和云计算已经是比较普遍的技术和服务了, 那时主流用户的普遍用法, 就是租一批 AWS 或者 OpenStack 的虚拟机, 然后像以前管理物理服务器那样, 用脚本或者手工的方式在这些机器上部署应用.</p> <p>当然, 这个部署过程难免会碰到云端虚拟机和本地环境不一致的问题, 所以当时的云计算服务, 比的就是谁能更好地模拟本地服务器环境, 能带来更好的&quot;上云&quot;体验. 而 PaaS 开源项目的出现, 就是当时解决这个问题的一个最佳方案.</p> <p>举个例子, 虚拟机创建好之后, 运维人员只需要在这些机器上部署一个 Cloud Foundry 项目, 然后开发者只要执行一条命令就能把本地的应用部署到云上, 这条命令就是:</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>$ cf push <span class="token string">&quot; 我的应用 &quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>是不是很神奇?</p> <p>事实上, **像 Cloud Foundry 这样的 PaaS 项目, 最核心的组件就是一套应用的打包和分发机制. ** Cloud Foundry 为每种主流编程语言都定义了一种打包格式, <strong>而 &quot;cf push&quot; 的作用, 基本上等同于用户把应用的可执行文件和启动脚本打进一个压缩包内, 上传到云上 Cloud Foundry 的存储中. 接着, Cloud Foundry 会通过调度器选择一个可以运行这个应用的虚拟机, 然后通知这个机器上的 Agent 把应用压缩包下载下来启动</strong>.</p> <p>这时候关键来了, <strong>由于需要在一个虚拟机上启动很多个来自不同用户的应用, Cloud Foundry 会调用</strong>​<mark><strong>操作系统的 Cgroups 和 Namespace 机制为每一个应用单独创建一个称作&quot;沙盒&quot;的隔离环境</strong></mark>​ <strong>, 然后在&quot;沙盒&quot;中启动这些应用进程. 这样就实现了把多个用户的应用互不干涉地在虚拟机里批量地, 自动地运行起来的目的</strong>.</p> <p><strong>这, 正是 PaaS 项目最核心的能力.</strong>  而这些 Cloud Foundry 用来运行应用的隔离环境, 或者说&quot;沙盒&quot;, 就是所谓的&quot;容器&quot;.</p> <p>而 Docker 项目, 实际上跟 Cloud Foundry 的容器并没有太大不同, 所以在它发布后不久, Cloud Foundry 的首席产品经理 James Bayer 就在社区里做了一次详细对比, 告诉用户 Docker 实际上只是一个同样使用 Cgroups 和 Namespace 实现的&quot;沙盒&quot;而已, 没有什么特别的黑科技, 也不需要特别关注.</p> <p>然而, 短短几个月, Docker 项目就迅速崛起了. 它的崛起速度如此之快, 以至于 Cloud Foundry 以及所有的 PaaS 社区还没来得及成为它的竞争对手, 就直接被宣告出局了. 那时候, 一位多年的 PaaS 从业者曾经如此感慨道: 这简直就是一场降维打击啊.</p> <p>难道这一次, 连闯荡多年的&quot;老江湖&quot;James Bayer 也看走眼了么?</p> <p>并没有. 事实上, Docker 项目确实与 Cloud Foundry 的容器在大部分功能和实现原理上都是一样的, 可偏偏就是这剩下的一<strong>小部分不一样的功能</strong>, 成了 Docker 项目接下来&quot;呼风唤雨&quot;的不二法宝.</p> <p>**这个功能, **​<mark><strong>就是 Docker 镜像</strong></mark>​ **. **</p> <p>恐怕连 Docker 项目的作者 Solomon Hykes 自己当时都没想到, 这个小小的创新, 在短短几年内就如此迅速地改变了整个云计算领域的发展历程.</p> <p>前面介绍过, PaaS 之所以能够帮助用户大规模部署应用到集群里, 是因为它提供了一套应用<strong>打包</strong>的功能. 可偏偏就是这个打包功能, 却成了 PaaS 日后不断遭到用户诟病的一个&quot;软肋&quot;. 出现这个问题的根本原因是, <strong>一旦用上了 PaaS, 用户就必须为每种语言, 每种框架, 甚至每个版本的应用维护一个打好的包</strong>. 这个打包过程, 没有任何章法可循, 更麻烦的是, 明明在本地运行得好好的应用, 却需要做很多修改和配置工作才能在 PaaS 里运行起来. 而这些修改和配置, 并没有什么经验可以借鉴, 基本上得靠不断试错, 直到你摸清楚了本地应用和远端 PaaS 匹配的&quot;脾气&quot;才能够搞定.</p> <p>最后结局就是, &quot;cf push&quot; 确实是能一键部署了, 但是为了实现这个一键部署, 用户<strong>为每个应用打包</strong>的工作可谓一波三折, 费尽心机.</p> <p><mark>而 </mark>​<mark><strong>Docker 镜像解决的, 恰恰就是打包这个根本性的问题</strong></mark>​ <strong>. 所谓 Docker 镜像, 其实就是一个压缩包. 但是这个压缩包里的内容, 比 PaaS 的应用可执行文件 + 启停脚本的组合就要丰富多了. 实际上, 大多数 Docker 镜像是直接由一个完整操作系统的所有文件和目录构成的, 所以这个压缩包里的内容跟你本地开发和测试环境用的操作系统是完全一样的</strong>.</p> <p>这就有意思了: 假设你的应用在本地运行时, 能看见的环境是 CentOS 7.2 操作系统的所有文件和目录, 那么只要用 CentOS 7.2 的 ISO 做一个压缩包, 再把你的应用可执行文件也压缩进去, 那么无论在哪里解压这个压缩包, 都可以得到与你本地测试时一样的环境. 当然, 你的应用也在里面!</p> <p>这就是 Docker 镜像最厉害的地方: 只要有这个压缩包在手, 你就可以使用某种技术创建一个&quot;沙盒&quot;, 在&quot;沙盒&quot;中解压这个压缩包, 然后就可以运行你的程序了.</p> <p>**更重要的是, 这个压缩包包含了完整的操作系统文件和目录, 也就是包含了这个应用运行所需要的所有依赖, 所以你可以先用这个压缩包在本地进行开发和测试, 完成之后, 再把这个压缩包上传到云端运行. **</p> <p>在这个过程中, 你完全不需要进行任何配置或者修改, 因为这个压缩包赋予了你一种极其宝贵的能力: <strong>本地环境和云端环境的高度一致</strong>!</p> <p>这, **正是 Docker 镜像的精髓. **</p> <p>那么, 有了 Docker 镜像这个利器, PaaS 里最核心的打包系统一下子就没了用武之地, 最让用户抓狂的打包过程也随之消失了. 相比之下, 在当今的互联网里, Docker 镜像需要的操作系统文件和目录, 可谓唾手可得.</p> <p>所以, 你只需要提供一个下载好的操作系统文件与目录, 然后使用它制作一个压缩包即可, 这个命令就是:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> build <span class="token string">&quot; 我的镜像 &quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>一旦镜像制作完成, 用户就可以让 Docker 创建一个&quot;沙盒&quot;来解压这个镜像, 然后在&quot;沙盒&quot;中运行自己的应用, 这个命令就是:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token string">&quot; 我的镜像 &quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>当然, docker run 创建的&quot;沙盒&quot;, 也是<strong>使用 Cgroups 和 Namespace 机制创建出来的隔离环境</strong>. 后面的文章会详细介绍这个机制的实现原理.</p> <p>所以, <mark><strong>Docker 项目给 PaaS 世界带来的降维打击, 其实是提供了一种非常便利的打包机制. 这种机制直接打包了应用运行所需要的整个操作系统, 从而保证了本地环境和云端环境的高度一致, 避免了用户通过&quot;试错&quot;来匹配两种不同运行环境之间差异的痛苦过程</strong></mark>​ **. **</p> <p>而对于开发者们来说, 在终于体验到了生产力解放所带来的痛快之后, 他们自然选择了用脚投票, 直接宣告了 PaaS 时代的结束.</p> <p>不过, Docker 项目固然解决了应用打包的难题, 但正如前面所介绍的那样, 它<strong>并不能代替 PaaS 完成大规模部署应用的职责</strong>.</p> <p>遗憾的是, 考虑到 Docker 公司是一个与自己有潜在竞争关系的商业实体, 再加上对 Docker 项目普及程度的错误判断, Cloud Foundry 项目并没有第一时间使用 Docker 作为自己的核心依赖, 去替换自己那套饱受诟病的打包流程.</p> <p>反倒是一些机敏的创业公司, 纷纷在第一时间推出了 Docker 容器集群管理的开源项目(比如 Deis 和 Flynn), 它们一般称自己为 CaaS, 即 Container-as-a-Service, 用来跟&quot;过时&quot;的 PaaS 们划清界限.</p> <p>而在 2014 年底的 DockerCon 上, Docker 公司雄心勃勃地对外发布了自家研发的&quot;Docker 原生&quot;容器集群管理项目 <strong>Swarm</strong>, 不仅将这波&quot;CaaS&quot;热推向了一个前所未有的高潮, 更是寄托了整个 Docker 公司重新定义 PaaS 的宏伟愿望.</p> <p>在 2014 年的这段巅峰岁月里, Docker 公司离自己的理想真的只有一步之遥.</p> <blockquote><p>总结</p></blockquote> <p>2013~2014 年, 以 Cloud Foundry 为代表的 PaaS 项目, 逐渐完成了教育用户和开拓市场的艰巨任务, 也正是在这个将概念逐渐落地的过程中, <strong>应用&quot;打包&quot;困难这个问题</strong>, 成了整个后端技术圈子的一块心病.</p> <p>Docker 项目的出现, 则为这个根本性的问题提供了一个近乎完美的解决方案. 这正是 Docker 项目刚刚开源不久, 就能够带领一家原本默默无闻的 PaaS 创业公司脱颖而出, 然后迅速占领了所有云计算领域头条的技术原因.</p> <p>而在成为了基础设施领域近十年难得一见的技术明星之后, dotCloud 公司则在 2013 年底大胆改名为 Docker 公司. 不过, 这个在当时就颇具争议的改名举动, 也成为了日后容器技术圈风云变幻的一个关键伏笔.</p> <h4 id="_02-预习篇·小鲸鱼大事记-二-崭露头角"><a href="#_02-预习篇·小鲸鱼大事记-二-崭露头角" class="header-anchor">#</a> 02 | 预习篇·小鲸鱼大事记(二):崭露头角</h4> <p>在上一篇文章说到, 一个并不引人瞩目的 PaaS 创业公司 dotCloud, 选择了开源自家的一个容器项目 Docker. 出人意料的是, **就是这样一个普通到不能再普通的技术, 却开启了一个名为 &quot;Docker&quot; 的全新时代. **</p> <p>你可能会有疑问, Docker 项目的崛起, 是不是偶然呢?</p> <p>事实上, **Docker 公司最重要的战略之一就是: 坚持把开发者群体放在至高无上的位置. **</p> <p>相比于其他正在企业级市场里厮杀得头破血流的经典 PaaS 项目们, Docker 项目的推广策略从一开始就呈现出一副憨态可掬的亲人姿态, 把每一位后端技术人员(而不是他们的老板)作为主要的传播对象.</p> <p>简洁的 UI, 有趣的 demo, &quot;1 分钟部署一个 WordPress 网站&quot;, &quot;3 分钟部署一个 Nginx 集群&quot;, 这种同开发者之间与生俱来的亲近关系, 使 Docker 项目迅速成为了全世界 Meetup 上最受欢迎的一颗新星.</p> <p>在过去的很长一段时间里, 相较于前端和互联网技术社区, 服务器端技术社区一直是一个相对沉闷而小众的圈子. 在这里, 从事 Linux 内核开发的极客们自带不合群的&quot;光环&quot;, 后端开发者们啃着多年不变的 TCP/IP 发着牢骚, 运维更是天生注定的幕后英雄.</p> <p>而 Docker 项目, 却给后端开发者提供了走向聚光灯的机会. 就比如 Cgroups 和 Namespace 这种已经存在多年却很少被人们关心的特性, 在 2014 年和 2015 年竟然频繁入选各大技术会议的分享议题, 就因为听众们想要知道 Docker 这个东西到底是怎么一回事儿.</p> <p>**而 Docker 项目之所以能取得如此高的关注, 一方面正如前面所说的那样, 它解决了应用打包和发布这一困扰运维人员多年的技术难题; 而另一方面, 就是因为它第一次把一个纯后端的技术概念, 通过非常友好的设计和封装, 交到了最广大的开发者群体手里. **</p> <p>在这种独特的氛围烘托下, 你不需要精通 TCP/IP, 也无需深谙 Linux 内核原理, 哪怕只是一个前端或者网站的 PHP 工程师, 都会对如何把自己的代码打包成一个随处可以运行的 Docker 镜像充满好奇和兴趣.</p> <p>这种<strong>受众群体的变革</strong>, 正是 Docker 这样一个后端开源项目取得巨大成功的关键. 这也是经典 PaaS 项目想做却没有做好的一件事情: PaaS 的最终用户和受益者, 一定是为这个 PaaS 编写应用的开发者们, 而在 Docker 项目开源之前, PaaS 与开发者之间的关系却从未如此紧密过.</p> <p>**解决了应用打包这个根本性的问题, 同开发者与生俱来的的亲密关系, 再加上 PaaS 概念已经深入人心的完美契机, 成为 Docker 这个技术上看似平淡无奇的项目一举走红的重要原因. **</p> <p>一时之间, &quot;容器化&quot;取代&quot;PaaS 化&quot;成为了基础设施领域最炙手可热的关键词, 一个以&quot;容器&quot;中心的, 全新的云计算市场, 正呼之欲出. 而作为这个生态的一手缔造者, 此时的 dotCloud 公司突然宣布将公司名称改为&quot;Docker&quot;.</p> <p>这个举动, 在当时颇受质疑. 在大家印象中, Docker 只是一个开源项目的名字. 可是现在, 这个单词却成了 Docker 公司的注册商标, 任何人在商业活动中使用这个单词, 以及鲸鱼的 Logo, 都会立刻受到法律警告.</p> <p>那么, Docker 公司这个举动到底卖的什么药? 这个问题不妨后面再做解读, 因为相较于这件小事儿, Docker 公司在 2014 年发布 Swarm 项目才是真正的大事儿.</p> <p>那么, Docker 公司为什么一定要发布 Swarm 项目呢?</p> <p>Docker 项目和 Docker 公司, 兜兜转转了一年多, 却还是回到了 PaaS 项目原本深耕了多年的那个战场: <mark><strong>如何让开发者把应用部署在我的项目上</strong></mark>​ **. **</p> <p>没错, Docker 项目从发布之初就全面发力, 从技术, 社区, 商业, 市场全方位争取到的开发者群体, 实际上是为此后吸引整个生态到自家&quot;PaaS&quot;上的一个铺垫. **只不过这时, &quot;PaaS&quot;的定义已经全然不是 Cloud Foundry 描述的那个样子, 而是变成了一套以 Docker 容器为技术核心, 以 Docker 镜像为打包标准的, 全新的&quot;容器化&quot;思路. **</p> <p>**这, 正是 Docker 项目从一开始悉心运作&quot;容器化&quot;理念和经营整个 Docker 生态的主要目的. **</p> <p>而 Swarm 项目, 正是接下来承接 Docker 公司所有这些努力的关键所在.</p> <blockquote><p>总结</p></blockquote> <p>本节着重介绍了 Docker 项目在短时间内迅速崛起的三个重要原因:</p> <ol><li>Docker 镜像通过技术手段解决了 PaaS 的根本性问题;</li> <li>Docker 容器同开发者之间有着与生俱来的密切关系;</li> <li>PaaS 概念已经深入人心的完美契机.</li></ol> <h4 id="_03-预习篇·小鲸鱼大事记-三-群雄并起"><a href="#_03-预习篇·小鲸鱼大事记-三-群雄并起" class="header-anchor">#</a> 03 | 预习篇·小鲸鱼大事记(三):群雄并起</h4> <p>上一篇文章剖析了 Docker 项目迅速走红背后的技术与非技术原因, 也介绍了 Docker 公司开启平台化战略的野心. 可是, Docker 公司为什么在 Docker 项目已经取得巨大成功之后, 却执意要重新走回那条已经让无数先驱们尘沙折戟的 PaaS 之路呢?</p> <p>实际上, Docker 项目一日千里的发展势头, 一直伴随着公司管理层和股东们的阵阵担忧. 他们心里明白, 虽然 Docker 项目备受追捧, <strong>但用户们最终要部署的, 还是他们的网站, 服务, 数据库, 甚至是云计算业务</strong>.</p> <p>这就意味着, <strong>只有那些能够为用户提供平台层能力的工具, 才会真正成为开发者们关心和愿意付费的产品</strong>. 而 Docker 项目这样一个只能用来创建和启停容器的小工具, 最终只能充当这些平台项目的&quot;幕后英雄&quot;.</p> <p>而谈到 Docker 项目的定位问题, 就不得不说说 Docker 公司的老朋友和老对手 CoreOS 了.</p> <p>CoreOS 是一个基础设施领域创业公司. 它的核心产品是一个定制化的操作系统, 用户可以按照分布式集群的方式, 管理所有安装了这个操作系统的节点. 从而, 用户在集群里部署和管理应用就像使用单机一样方便了.</p> <p>Docker 项目发布后, CoreOS 公司很快就认识到可以把&quot;容器&quot;的概念无缝集成到自己的这套方案中, 从而为用户提供更高层次的 PaaS 能力. 所以, CoreOS 很早就成了 Docker 项目的贡献者, 并在短时间内成为了 Docker 项目中第二重要的力量.</p> <p>然而, 这段短暂的蜜月期到 2014 年底就草草结束了. CoreOS 公司以强烈的措辞宣布与 Docker 公司停止合作, 并直接推出了自己研制的 <strong>Rocket(后来叫 rkt)容器</strong>.</p> <p>这次决裂的根本原因, 正是源于 Docker 公司对 Docker 项目定位的不满足. Docker 公司解决这种不满足的方法就是, 让 Docker 项目提供更多的平台层能力, 即向 PaaS 项目进化. 而这, 显然与 CoreOS 公司的核心产品和战略发生了严重冲突.</p> <p>也就是说, Docker 公司在 2014 年就已经定好了<strong>平台化</strong>的发展方向, 并且绝对不会跟 CoreOS 在平台层面开展任何合作. 这样看来, Docker 公司在 2014 年 12 月的 DockerCon 上发布 Swarm 的举动, 也就一点都不突然了.</p> <p>相较于 CoreOS 是依托于一系列开源项目(比如 Container Linux 操作系统, Fleet 作业调度工具, systemd 进程管理和 rkt 容器), 一层层搭建起来的平台产品, <strong>Swarm 项目则是以一个完整的整体来对外提供集群管理功能. 而 Swarm 的最大亮点, 则是它完全使用 Docker 项目原本的容器管理 API 来完成集群管理, 比如</strong>:</p> <ul><li>单机 Docker 项目:</li></ul> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run &quot; 我的容器
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>多机 Docker 项目:</li></ul> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-H</span> <span class="token string">&quot; 我的 Swarm 集群 API 地址 &quot;</span> <span class="token string">&quot; 我的容器 &quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>所以在部署了 Swarm 的多机环境下, <strong>用户只需要使用原先的 Docker 指令创建一个容器, 这个请求就会被 Swarm 拦截下来处理, 然后通过具体的调度算法找到一个合适的 Docker Daemon 运行起来</strong>.</p> <p>这个操作方式简洁明了, 对于已经了解过 Docker 命令行的开发者们也很容易掌握. 所以, 这样一个&quot;原生&quot;的 Docker 容器集群管理项目一经发布, 就受到了已有 Docker 用户群的热捧. 而相比之下, CoreOS 的解决方案就显得非常另类, 更不用说用户还要去接受完全让人摸不着头脑, 新造的容器项目 rkt 了.</p> <p>当然, Swarm 项目只是 Docker 公司重新定义&quot;PaaS&quot;的关键一环而已. 在 2014 年到 2015 年这段时间里, Docker 项目的迅速走红催生出了一个非常繁荣的&quot;Docker 生态&quot;. 在这个生态里, 围绕着 Docker 在各个层次进行集成和创新的项目层出不穷.</p> <p>而此时已经大红大紫到不差钱的 <strong>Docker 公司, 开始及时地借助这波浪潮通过并购来完善自己的平台层能力</strong>. 其中一个最成功的案例, 莫过于对 Fig 项目的收购.</p> <p>要知道, Fig 项目基本上只是靠两个人全职开发和维护的, 可它却是当时 GitHub 上热度堪比 Docker 项目的明星.</p> <p>**Fig 项目之所以受欢迎, 在于它在开发者面前第一次提出了&quot;**​<mark><strong>容器编排</strong></mark>​ **&quot;(Container Orchestration)的概念. **</p> <p>其实,  <strong>&quot;编排&quot;(Orchestration)在云计算行业里不算是新词汇, 它主要是指用户如何通过某些工具或者配置来完成一组虚拟机以及关联资源的定义, 配置, 创建, 删除等工作, 然后由云计算平台按照这些指定的逻辑来完成的过程</strong>.</p> <p>而容器时代, &quot;编排&quot;显然就是对 Docker 容器的一系列定义, 配置和创建动作的管理. 而 Fig 的工作实际上非常简单: 假如现在用户需要部署的是应用容器 A, 数据库容器 B, 负载均衡容器 C, 那么 Fig 就允许用户把 A, B, C 三个容器定义在一个配置文件中, 并且可以指定它们之间的关联关系, 比如容器 A 需要访问数据库容器 B.</p> <p>接下来, 你只需要执行一条非常简单的指令:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ fig up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>Fig 就会把这些容器的定义和配置交给 Docker API 按照访问逻辑依次创建, 你的一系列容器就都启动了; 而容器 A 与 B 之间的关联关系, 也会交给 Docker 的 Link 功能通过写入 hosts 文件的方式进行配置. 更重要的是, 还可以在 Fig 的配置文件里定义各种容器的副本个数等编排参数, 再加上 Swarm 的集群管理能力, 一个活脱脱的 PaaS 呼之欲出.</p> <p>Fig 项目被收购后改名为 <strong>Compose</strong>, 它成了 Docker 公司到目前为止第二大受欢迎的项目, 一直到今天也依然被很多人使用.</p> <p>一时之间, 整个后端和云计算领域的聪明才俊都汇集在了这个&quot;小鲸鱼&quot;的周围, 为 Docker 生态的蓬勃发展献上了自己的智慧.</p> <p>而除了这个异常繁荣的, 围绕着 Docker 项目和公司的生态之外, 还有一个势力在当时也是风头无两, 这就是老牌集群管理项目 <strong>Mesos</strong> 和它背后的创业公司 Mesosphere.</p> <p>Mesos 作为 Berkeley 主导的大数据套件之一, 是大数据火热时最受欢迎的资源管理项目, 也是跟 Yarn 项目杀得难舍难分的实力派选手.</p> <p>不过, 大数据所关注的计算密集型离线业务, 其实并不像常规的 Web 服务那样适合用容器进行托管和扩容, 也没有对应用打包的强烈需求, 所以 Hadoop, Spark 等项目到现在也没在容器技术上投下更大的赌注; 但是对于 Mesos 来说, 天生的两层调度机制让它非常容易从大数据领域抽身, 转而去支持受众更加广泛的 PaaS 业务.</p> <p>在这种思路的指导下, Mesosphere 公司发布了一个名为 Marathon 的项目, 而这个项目很快就成为了 Docker Swarm 的一个有力竞争对手.</p> <p>**虽然不能提供像 Swarm 那样的原生 Docker API, Mesos 社区却拥有一个独特的竞争力: 超大规模集群的管理经验. **</p> <p>早在几年前, Mesos 就已经通过了万台节点的验证, 2014 年之后又被广泛使用在 eBay 等大型互联网公司的生产环境中. 而这次通过 Marathon 实现了诸如应用托管和负载均衡的 PaaS 功能之后, Mesos+Marathon 的组合实际上进化成了一个高度成熟的 PaaS 项目, 同时还能很好地支持大数据业务.</p> <p>所以, 在这波容器化浪潮中, Mesosphere 公司不失时机地提出了一个名叫&quot;DC/OS&quot;(数据中心操作系统)的口号和产品, 旨在使用户能够像管理一台机器那样管理一个万级别的物理机集群, 并且使用 Docker 容器在这个集群里自由地部署应用. 而这对很多大型企业来说具有着非同寻常的吸引力.</p> <p>这时, 如果你再去审视当时的容器技术生态, 就不难发现 CoreOS 公司竟然显得有些尴尬了. 它的 rkt 容器完全打不开局面, Fleet 集群管理项目更是少有人问津, CoreOS 完全被 Docker 公司压制了.</p> <p>而处境同样不容乐观的似乎还有 RedHat, 作为 Docker 项目早期的重要贡献者, RedHat 也是因为对 Docker 公司平台化战略不满而愤愤退出. 但此时, 它竟只剩下 OpenShift 这个跟 Cloud Foundry 同时代的经典 PaaS 一张牌可以打, 跟 Docker Swarm 和转型后的 Mesos 完全不在同一个&quot;竞技水平&quot;之上.</p> <p>那么, 事实果真如此吗?</p> <p>2014 年注定是一个神奇的年份. 就在这一年的 6 月, 基础设施领域的翘楚 Google 公司突然发力, <strong>正式宣告了一个名叫 Kubernetes 项目的诞生</strong>. 而这个项目, 不仅挽救了当时的 CoreOS 和 RedHat, 还如同当年 Docker 项目的横空出世一样, 再一次改变了整个容器市场的格局.</p> <blockquote><p>总结</p></blockquote> <p>本节介绍了 Docker 公司平台化战略的来龙去脉, 阐述了 Docker Swarm 项目发布的意义和它背后的设计思想, 介绍了 Fig(后来的 Compose)项目如何成为了继 Docker 之后最受瞩目的新星.</p> <p>同时也回顾了 2014~2015 年间如火如荼的容器化浪潮里群雄并起的繁荣姿态. 在这次生态大爆发中, Docker 公司和 Mesosphere 公司, 依托自身优势率先占据了有利位置.</p> <p>但是, 更强大的挑战者们, 即将在不久后纷至沓来.</p> <h4 id="_04-预习篇·小鲸鱼大事记-四-尘埃落定"><a href="#_04-预习篇·小鲸鱼大事记-四-尘埃落定" class="header-anchor">#</a> 04 | 预习篇·小鲸鱼大事记(四):尘埃落定</h4> <p>前一节提到, 伴随着 Docker 公司一手打造出来的容器技术生态在云计算市场中站稳了脚跟, 围绕着 Docker 项目进行的各个层次的集成与创新产品, 也如雨后春笋般出现在这个新兴市场当中. 而 Docker 公司, 不失时机地发布了 <strong>Docker Compose, Swarm 和 Machine</strong> 三件套, 在重新定义 PaaS 的方向上走出了最关键的一步. 这段时间, 也正是 Docker 生态创业公司们的春天, 大量围绕着 Docker 项目的网络, 存储, 监控, CI/CD, 甚至 UI 项目纷纷出台, 也涌现出了很多 Rancher, Tutum 这样在开源与商业上均取得了巨大成功的创业公司.</p> <p>在 2014~2015 年间, 整个容器社区可谓热闹非凡. 这令人兴奋的繁荣背后, 却浮现出了更多的担忧. 这其中最主要的负面情绪, 是对 Docker 公司商业化战略的种种顾虑.</p> <p>事实上, 很多从业者也都看得明白, Docker 项目此时已经成为 Docker 公司一个<strong>商业产品</strong>. 而开源, 只是 Docker 公司吸引开发者群体的一个重要手段. 不过这么多年来, 开源社区的商业化其实都是类似的思路, 无非是高不高调, 心不心急的问题罢了.</p> <p>而真正令大多数人不满意的是, <strong>Docker 公司在 Docker 开源项目的发展上, 始终保持着绝对的权威和发言权, 并在多个场合用实际行动挑战到了其他玩家(比如, CoreOS, RedHat, 甚至谷歌和微软)的切身利益</strong>.</p> <p>所以大家的不满也就不再是在 GitHub 上发发牢骚这么简单了.</p> <p>相信很多容器领域的老玩家们都听说过, Docker 项目刚刚兴起时, Google 也开源了一个在内部使用多年, 经历过生产环境验证的 Linux 容器: lmctfy(Let Me Container That For You). 然而, 面对 Docker 项目的强势崛起, 这个对用户没那么友好的 Google 容器项目根本没有招架之力. 所以, 知难而退的 Google 公司, 向 Docker 公司表示了合作的愿望: 关停这个项目, 和 Docker 公司共同推进一个中立的容器运行时(container runtime)库作为 Docker 项目的核心依赖. 不过, Docker 公司并没有认同这个明显会削弱自己地位的提议, 还在不久后, 自己发布了一个容器运行时库 <strong>Libcontainer</strong>. 这次匆忙的, 由一家主导的, 并带有战略性考量的重构, 成了 Libcontainer 被社区长期诟病代码可读性差, 可维护性不强的一个重要原因.</p> <p>至此, Docker 公司在容器运行时层面上的强硬态度, 以及 Docker 项目在高速迭代中表现出来的不稳定和频繁变更的问题, 开始让社区叫苦不迭.</p> <p>这种情绪在 2015 年达到了一个小高潮, 容器领域的其他几位玩家开始商议&quot;切割&quot; Docker 项目的话语权. 而&quot;切割&quot;的手段也非常经典, 那就是成立一个<strong>中立的基金会</strong>.</p> <p>于是, 2015 年 6 月 22 日, 由 Docker 公司牵头, CoreOS, Google, RedHat 等公司共同宣布, Docker 公司将 Libcontainer 捐出, <strong>并改名为 RunC 项目, 交由一个完全中立的基金会管理, 然后以 RunC 为依据, 大家共同制定一套容器和镜像的标准和规范</strong>.</p> <p>这套<strong>标准和规范</strong>, 就是 OCI( Open Container Initiative ). <strong>OCI 的提出, 意在将容器运行时和镜像的实现从 Docker 项目中完全剥离出来</strong>. 这样做, 一方面可以改善 Docker 公司在容器技术上一家独大的现状, 另一方面也为其他玩家不依赖于 Docker 项目构建各自的平台层能力提供了可能.</p> <p>不过不难看出, OCI 的成立更多的是这些容器玩家出于自身利益进行干涉的一个妥协结果. 所以尽管 Docker 是 OCI 的发起者和创始成员, 它却很少在 OCI 的技术推进和标准制定等事务上扮演关键角色, 也没有动力去积极地推进这些所谓的标准. 这也正是迄今为止 OCI 组织效率持续低下的根本原因.</p> <p>眼看着 OCI 并没能改变 Docker 公司在容器领域一家独大的现状, Google 和 RedHat 等公司于是把与第二把武器摆上了台面.</p> <p>Docker 之所以不担心 OCI 的威胁, 原因就在于它的 Docker 项目是容器生态的事实标准, 而它所维护的 Docker 社区也足够庞大. 可是, 一旦这场斗争被转移到容器之上的平台层, 或者说 PaaS 层, Docker 公司的竞争优势便立刻捉襟见肘了. 在这个领域里, 像 Google 和 RedHat 这样的成熟公司, 都拥有着深厚的技术积累; 而像 CoreOS 这样的创业公司, 也拥有像 Etcd 这样被广泛使用的开源基础设施项目. 可是 Docker 公司呢? 它却只有一个 Swarm.</p> <p>所以这次, Google, RedHat 等开源基础设施领域玩家们, 共同牵头发起了一个名为 CNCF(Cloud Native Computing Foundation)的基金会. 这个基金会的目的其实很容易理解: 它<strong>希望以 Kubernetes 项目为基础, 建立一个由开源基础设施领域厂商主导的, 按照独立基金会方式运营的平台级社区, 来对抗以 Docker 公司为核心的容器商业生态</strong>.</p> <p>而为了打造出这样一个围绕 Kubernetes 项目的护城河, CNCF 社区就需要至少确保两件事情:</p> <ol><li>Kubernetes 项目必须能够在容器编排领域取得足够大的竞争优势;</li> <li>CNCF 社区必须以 Kubernetes 项目为核心, 覆盖足够多的场景.</li></ol> <p>**先来看看 CNCF 社区如何解决 Kubernetes 项目在编排领域的竞争力的问题. **</p> <p>在容器编排领域, Kubernetes 项目需要面对来自 Docker 公司和 Mesos 社区两个方向的压力. 不难看出, Swarm 和 Mesos 实际上分别从两个不同的方向讲出了自己最擅长的故事: <strong>Swarm 擅长的是跟 Docker 生态的无缝集成, 而 Mesos 擅长的则是大规模集群的调度与管理</strong>.</p> <p>这两个方向, 也是大多数人做容器集群管理项目时最容易想到的两个出发点. 也正因为如此, <strong>Kubernetes 项目如果继续在这两个方向上做文章恐怕就不太明智了</strong>.</p> <p>所以这一次, <strong>Kubernetes 选择的应对方式是: Borg</strong>.</p> <p>如果你看过 Kubernetes 项目早期的 GitHub Issue 和 Feature 的话, 就会发现它们<strong>大多来自于 Borg 和 Omega 系统的内部特性, 这些特性落到 Kubernetes 项目上, 就是 Pod, Sidecar 等功能和设计模式</strong>.</p> <p>这就解释了为什么 Kubernetes 发布后, 很多人&quot;抱怨&quot;其设计思想过于&quot;超前&quot;的原因: Kubernetes 项目的基础特性, 并不是几个工程师突然&quot;拍脑袋&quot;想出来的东西, 而是 Google 公司在容器化基础设施领域多年来实践经验的沉淀与升华. <strong>这正是 Kubernetes 项目能够从一开始就避免同 Swarm 和 Mesos 社区同质化的重要手段</strong>.</p> <p>于是, CNCF 接下来的任务就是, 如何把这些先进的思想通过技术手段在开源社区落地, 并培育出一个认同这些理念的生态? 这时, RedHat 就发挥了重要作用.</p> <p>当时, Kubernetes 团队规模很小, 能够投入的工程能力也十分紧张, 而这恰恰是 RedHat 的长处. 更难得的是, RedHat 是世界上为数不多的, 能真正理解开源社区运作和项目研发真谛的合作伙伴. 所以 RedHat 与 Google 联盟的成立, 不仅保证了 RedHat 在 Kubernetes 项目上的影响力, 也正式开启了容器编排领域&quot;三国鼎立&quot;的局面.</p> <p>这时再重新审视容器生态的格局, 就不难发现 Kubernetes 项目, Docker 公司和 Mesos 社区这三大玩家的关系已经发生了微妙的变化.</p> <p>其中, Mesos 社区与容器技术的关系, 更像是&quot;借势&quot;, 而不是这个领域真正的参与者和领导者. 这个事实, 加上它所属的 Apache 社区固有的封闭性, 导致了 Mesos 社区虽然技术最为成熟, 却在容器编排领域鲜有创新. 这也是为何, Google 公司很快就把注意力转向了动作更加激进的 Docker 公司.</p> <p>有意思的是, Docker 公司对 Mesos 社区也是类似的看法. 所以从一开始, Docker 公司就把应对 Kubernetes 项目的竞争摆在了首要位置: 一方面, 不断强调 &quot;Docker Native&quot; 的重要性, 另一方面, 与 Kubernetes 项目在多个场合进行了直接的碰撞.</p> <p>不过, 这次竞争的发展态势, 很快就超过了 Docker 公司的预期.</p> <p>Kubernetes 项目并没有跟 Swarm 项目展开同质化的竞争, 所以 &quot;Docker Native&quot; 的说辞并没有太大的杀伤力. 相反地, <strong>Kubernetes 项目让人耳目一新的设计理念和号召力, 很快就构建出了一个与众不同的容器编排与管理的生态</strong>.</p> <p>就这样, Kubernetes 项目在 GitHub 上的各项指标开始一骑绝尘, 将 Swarm 项目远远地甩在了身后.</p> <p>**有了这个基础, CNCF 社区就可以放心地解决第二个问题了. **</p> <p>在已经囊括了容器监控事实标准的 Prometheus 项目之后, CNCF 社区迅速在成员项目中添加了 Fluentd, OpenTracing, CNI 等一系列容器生态的知名工具和项目. 而在看到了 CNCF 社区对用户表现出来的巨大吸引力之后, 大量的公司和创业团队也开始专门针对 CNCF 社区而非 Docker 公司制定推广策略.</p> <p>面对这样的竞争态势, Docker 公司决定更进一步. 在 2016 年, Docker 公司宣布了一个震惊所有人的计划: 放弃现有的 Swarm 项目, 将容器编排和集群管理功能全部内置到 Docker 项目当中. 显然, Docker 公司意识到了 Swarm 项目目前唯一的竞争优势, 就是跟 Docker 项目的无缝集成. 那么, 如何让这种优势最大化呢? 那就是把 Swarm 内置到 Docker 项目当中. 实际上, 从工程角度来看, 这种做法的风险很大. 内置容器编排, 集群管理和负载均衡能力, 固然可以使得 Docker 项目的边界直接扩大到一个完整的 PaaS 项目的范畴, 但这种变更带来的技术复杂度和维护难度, 长远来看对 Docker 项目是不利的.</p> <p>不过, 在当时的大环境下, Docker 公司的选择恐怕也带有一丝孤注一掷的意味.</p> <p>而 <strong>Kubernetes 的应对策略则是反其道而行之, 开始在整个社区推进&quot;民主化&quot;架构</strong>, 即: <mark><strong>从 API 到容器运行时的每一层, Kubernetes 项目都为开发者暴露出了可以扩展的插件机制, 鼓励用户通过代码的方式介入到 Kubernetes 项目的每一个阶段</strong></mark>.</p> <p>Kubernetes 项目的这个变革的效果立竿见影, 很快在整个容器社区中催生出了大量的, 基于 Kubernetes API 和扩展接口的二次创新工作, 比如:</p> <ul><li>目前热度极高的微服务治理项目 <strong>Istio</strong>;</li> <li>被广泛采用的有状态应用部署框架 Operator;</li> <li>还有像 Rook 这样的开源创业项目, 它通过 Kubernetes 的可扩展接口, 把 Ceph 这样的重量级产品封装成了简单易用的容器存储插件.</li></ul> <p>就这样, 在这种<strong>鼓励二次创新</strong>的整体氛围当中, Kubernetes 社区在 2016 年之后得到了空前的发展. 更重要的是, 不同于之前局限于 &quot;打包, 发布&quot; 这样的 PaaS 化路线, <strong>这一次容器社区的繁荣, 是一次完全以 Kubernetes 项目为核心的&quot;百家争鸣&quot;</strong> .</p> <p>面对 Kubernetes 社区的崛起和壮大, Docker 公司也不得不面对自己豪赌失败的现实. 但在早前拒绝了微软的天价收购之后, Docker 公司实际上已经没有什么回旋余地, 只能选择逐步放弃开源社区而专注于自己的商业化转型. 所以, 从 2017 年开始, Docker 公司先是将 Docker 项目的容器运行时部分 Containerd 捐赠给 CNCF 社区, <strong>标志着 Docker 项目已经全面升级成为一个 PaaS 平台</strong>; 紧接着, Docker 公司宣布将 Docker 项目改名为 Moby, 然后交给社区自行维护, 而 Docker 公司的商业产品将占有 Docker 这个注册商标.</p> <p>Docker 公司这些举措背后的含义非常明确: <strong>它将全面放弃在开源社区同 Kubernetes 生态的竞争, 转而专注于自己的商业业务, 并且通过将 Docker 项目改名为 Moby 的举动, 将原本属于 Docker 社区的用户转化成了自己的客户</strong>.</p> <p>2017 年 10 月, Docker 公司出人意料地宣布, 将在自己的主打产品 Docker 企业版中内置 Kubernetes 项目, 这标志着持续了近两年之久的&quot;编排之争&quot;至此落下帷幕.</p> <p>2018 年 1 月 30 日, RedHat 宣布斥资 2.5 亿美元收购 CoreOS.</p> <p>2018 年 3 月 28 日, 这一切纷争的始作俑者, Docker 公司的 CTO Solomon Hykes 宣布辞职, 曾经纷纷扰扰的容器技术圈子, 到此尘埃落定.</p> <blockquote><p>总结</p></blockquote> <p>容器技术圈子在短短几年里发生了很多变数, 但很多事情其实也都在情理之中. 就像 Docker 这样一家创业公司, 在通过开源社区的运作取得了巨大的成功之后, 就不得不面对来自整个云计算产业的竞争和围剿. 而这个产业的垄断特性, 对于 Docker 这样的技术型创业公司其实天生就不友好.</p> <p>在这种局势下, 接受微软的天价收购, 在大多数人看来都是一个非常明智和实际的选择. 可是 Solomon Hykes 却多少带有一些理想主义的影子, 既然不甘于&quot;寄人篱下&quot;, 那他就必须带领 Docker 公司去对抗来自整个云计算产业的压力.</p> <p>只不过, <mark><strong>Docker 公司最后选择的对抗方式, 是将开源项目与商业产品紧密绑定, 打造了一个极端封闭的技术生态. 而这, 其实违背了 Docker 项目与开发者保持亲密关系的初衷</strong></mark>. 相比之下, Kubernetes 社区, 正是以一种更加温和的方式, 承接了 Docker 项目的未尽事业, 即: <mark><strong>以开发者为核心, 构建一个相对民主和开放的容器生态</strong></mark>. 这也是为何, Kubernetes 项目的成功其实是必然的.</p> <h3 id="容器技术概念入门篇"><a href="#容器技术概念入门篇" class="header-anchor">#</a> 容器技术概念入门篇</h3> <h4 id="_05-白话容器基础-一-从进程说开去"><a href="#_05-白话容器基础-一-从进程说开去" class="header-anchor">#</a> 05 | 白话容器基础(一):从进程说开去</h4> <p>在前面的 4 篇预习文章中梳理了容器这项技术的来龙去脉, 通过这些内容, 希望你能理解如下几个事实:</p> <ul><li>**容器技术的兴起源于 PaaS 技术的普及; **</li> <li>Docker 公司发布的 Docker 项目具有里程碑式的意义;</li> <li>Docker 项目通过&quot;容器镜像&quot;, 解决了应用打包这个根本性难题.</li></ul> <p>紧接着详细介绍了容器技术圈在过去五年里的&quot;风云变幻&quot;, 而通过这部分内容, 希望你能理解这样一个道理:</p> <p><mark><strong>容器本身没有价值, 有价值的是&quot;容器编排&quot;</strong></mark>​ **. **</p> <p>也正因为如此, 容器技术生态才爆发了一场关于&quot;容器编排&quot;的&quot;战争&quot;. 而这次战争, 最终以 Kubernetes 项目和 CNCF 社区的胜利而告终. 所以这个专栏后面的内容, 会以 Docker 和 Kubernetes 项目为核心, 详细介绍容器技术的各项实践与其中的原理.</p> <p>不过在此之前, 还需要搞清楚一个更为基础的问题:</p> <blockquote><p>容器, 到底是怎么一回事儿?</p></blockquote> <p>前面提到过, <strong>容器其实是一种沙盒技术</strong>. 顾名思义, 沙盒就是能够像一个集装箱一样, 把你的应用&quot;装&quot;起来的技术. 这样, <strong>应用与应用之间, 就因为有了边界而不至于相互干扰; 而被装进集装箱的应用, 也可以被方便地搬来搬去, 这不就是 PaaS 最理想的状态</strong>嘛.</p> <p>不过, 这两个能力说起来简单, 但要用技术手段去实现它们, 可能大多数人就无从下手了.</p> <p>**所以就先来说说这个&quot;边界&quot;的实现手段. **</p> <p>假如现在要写一个计算加法的小程序, 这个程序需要的输入来自于一个文件, 计算完成后的结果则输出到另一个文件中. 由于计算机只认识 0 和 1, 所以无论用哪种语言编写这段代码, 最后都需要通过某种方式翻译成二进制文件, 才能在计算机操作系统中运行起来. 而为了能够让这些代码正常运行, 往往还要给它提供数据, 比如这个加法程序所需要的输入文件. 这些数据加上代码本身的二进制文件, 放在磁盘上, 就是平常所说的一个&quot;程序&quot;, 也叫代码的<strong>可执行镜像</strong>(executable image).</p> <p>然后, 就可以在计算机上运行这个&quot;程序&quot;了.</p> <p>首先, 操作系统从&quot;程序&quot;中发现输入数据保存在一个文件中, 所以这些数据就被会加载到内存中待命. 同时, 操作系统又读取到了计算加法的指令, 这时它就需要指示 CPU 完成加法操作. 而 CPU 与内存协作进行加法计算, 又会使用寄存器存放数值, 内存堆栈保存执行的命令和变量. 同时, 计算机里还有被打开的文件, 以及各种各样的 I/O 设备在不断地调用中修改自己的状态.</p> <p>就这样, 一旦&quot;程序&quot;被执行起来, 它就从磁盘上的二进制文件, 变成了计算机内存中的数据, 寄存器里的值, 堆栈中的指令, 被打开的文件, 以及各种设备的状态信息的一个集合. **像这样一个程序运起来后的计算机执行环境的总和, 就是本节的主角: 进程. **</p> <p>所以对于进程来说, 它的<strong>静态表现就是程序</strong>, 平常都安安静静地待在磁盘上; 而一旦运行起来, 它就变成了计算机里的<strong>数据和状态的总和, 这就是它的动态表现</strong>.</p> <p><mark>而</mark>​<mark><strong>容器技术的核心功能, 就是通过约束和修改进程的动态表现, 从而为其创造出一个&quot;边界&quot;.</strong></mark></p> <p>对于 Docker 等大多数 Linux 容器来说, <mark><strong>Cgroups 技术是用来制造约束的主要手段, 而 Namespace 技术则是用来修改进程视图的主要方法</strong></mark>.</p> <p>Cgroups 和 Namespace 这两个概念很抽象, 接下来动手实践一下, 就很容易理解这两项技术了.</p> <p>假设已经有了一个 Linux 操作系统上的 Docker 项目在运行, 比如环境是 Ubuntu 16.04 和 Docker CE 18.05.</p> <p>接下来, 让首先创建一个容器来试试.</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-it</span> busybox /bin/sh/
/ <span class="token comment">#</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个命令是 Docker 项目最重要的一个操作, 即大名鼎鼎的 <strong>docker run</strong>.</p> <p>而 -it 参数告诉了 Docker 项目在启动容器后, 需要给我们分配一个文本输入/输出环境, 也就是 TTY, 跟容器的<strong>标准输入</strong>相关联, 这样就可以和这个 Docker 容器进行交互了. 而 /bin/sh 就是要在 Docker 容器里<strong>运行的程序</strong>.</p> <p>所以, 上面这条指令翻译成人类的语言就是: <strong>请启动一个容器, 在容器里执行 /bin/sh, 并且分配一个命令行终端跟这个容器交互</strong>.</p> <p>这样, 这台 Ubuntu 16.04 机器就变成了一个<strong>宿主机</strong>, 而一个运行着 /bin/sh 的容器, 就跑在了这个宿主机里面.</p> <p>此时, 如果在容器里执行一下 ps 指令, 就会发现一些更有趣的事情:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>/ <span class="token comment"># ps</span>
PID  <span class="token environment constant">USER</span>   TIME COMMAND
  <span class="token number">1</span> root   <span class="token number">0</span>:00 /bin/sh
  <span class="token number">10</span> root   <span class="token number">0</span>:00 <span class="token function">ps</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 在 Docker 里最开始执行的 /bin/sh, 就是这个容器内部的<strong>第 1 号进程(PID=1)</strong> , 而这个容器里<strong>一共只有两个进程</strong>在运行. 这就意味着, 前面执行的 /bin/sh, 以及刚刚执行的 ps, 已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中.</p> <p>这究竟是怎么做到呢?</p> <p>本来, 每当在宿主机上运行了一个 /bin/sh 程序, 操作系统都会给它分配一个<strong>进程编号</strong>, 比如 PID=100. 这个编号是进程的唯一标识, 就像员工的工牌一样. 所以 PID=100, 可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工, 而第 1 号员工就自然是比尔·盖茨这样统领全局的人物.</p> <p>而现在, 要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中. 这时候, <strong>Docker 就会在这个第 100 号员工入职时给他施一个&quot;障眼法&quot;, 让他永远看不到前面的其他 99 个员工, 更看不到比尔·盖茨. 这样他就会错误地以为自己就是公司里的第 1 号员工</strong>.</p> <p><strong>==这种机制, 其实就是对被隔离应用的进程空间做了手脚, 使得这些进程只能看到重新计算过的进程编号, 比如 PID=1. 可实际上, 他们在宿主机的操作系统里, 还是原来的第 100 号进程. ==</strong></p> <p><mark><strong>这种技术, 就是 Linux 里面的 Namespace 机制</strong></mark>. 而 Namespace 的使用方式也非常有意思: 它其实只是 Linux 创建<strong>新进程的一个可选参数</strong>. 我们知道, 在 Linux 系统中创建线程的系统调用是 clone(), 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>int pid <span class="token operator">=</span> clone<span class="token punctuation">(</span>main_function, stack_size, SIGCHLD, NULL<span class="token punctuation">)</span><span class="token punctuation">;</span> 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个系统调用就会为我们<strong>创建一个新的进程</strong>, 并且返回它的进程号 pid.</p> <p>而当用 clone() 系统调用创建一个新进程时, 就可以在参数中<strong>指定 CLONE_NEWPID 参数</strong>, 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>int pid <span class="token operator">=</span> clone<span class="token punctuation">(</span>main_function, stack_size, CLONE_NEWPID <span class="token operator">|</span> SIGCHLD, NULL<span class="token punctuation">)</span><span class="token punctuation">;</span> 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时, <strong>新创建的这个进程将会&quot;看到&quot;一个全新的进程空间, 在这个进程空间里, 它的 PID 是 1</strong>. 之所以说&quot;看到&quot;, 是因为这<strong>只是一个&quot;障眼法&quot;, 在宿主机真实的进程空间里, 这个进程的 PID 还是真实的数值, 比如 100</strong>.</p> <p>当然, 还可以多次执行上面的 clone() 调用, 这样就会创建多个 PID Namespace, 而每个 Namespace 里的应用进程, 都会认为自己是<strong>当前容器里的第 1 号进程, 它们既看不到宿主机里真正的进程空间, 也看不到其他 PID Namespace 里的具体情况</strong>.</p> <p><mark>而</mark>​<mark>**除了刚刚用到的 PID Namespace, Linux 操作系统还提供了 Mount, UTS, IPC, Network 和 User 这些 Namespace, 用来对各种不同的进程上下文进行&quot;障眼法&quot;操作. **</mark></p> <p>比如, Mount Namespace, 用于让被隔离进程只看到当前 Namespace 里的<strong>挂载点</strong>信息; Network Namespace, 用于让被隔离进程看到当前 Namespace 里的<strong>网络设备和配置</strong>.</p> <p>**这就是 Linux 容器最基本的实现原理了. **</p> <p>所以, Docker 容器这个听起来玄而又玄的概念, <mark><strong>实际上是在创建容器进程时, 指定了这个进程所需要启用的一组 Namespace 参数. 这样, 容器就只能&quot;看&quot;到当前 Namespace 所限定的资源, 文件, 设备, 状态, 或者配置. 而对于宿主机以及其他不相关的程序, 它就完全看不到了</strong></mark>.</p> <p>**所以说, 容器其实是一种特殊的进程而已. **</p> <blockquote><p>总结</p></blockquote> <p>谈到为&quot;进程划分一个独立空间&quot;的思想, 相信你一定会联想到虚拟机. 而且, 你应该还看过一张虚拟机和容器的对比图.</p> <p><img src="/img/5825a259ddd0b64861e3f1ccdf6ccf7f-20230731162150-2uswj1n.png" alt=""></p> <p>这幅图的左边, 画出了虚拟机的工作原理. 其中名为 Hypervisor 的软件是虚拟机最主要的部分. 它通过硬件虚拟化功能, 模拟出了运行一个操作系统需要的各种硬件, 比如 CPU, 内存, I/O 设备等等. 然后, 它在这些虚拟的硬件上安装了一个新的操作系统, 即 Guest OS. 这样用户的应用进程就可以运行在这个虚拟的机器中, 它能看到的自然也只有 Guest OS 的文件和目录, 以及这个机器里的虚拟设备. 这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用.</p> <p>而这幅图的右边, 则用一个名为 Docker Engine 的软件替换了 Hypervisor. 这也是为什么, 很多人会把 Docker 项目称为&quot;轻量级&quot;虚拟化技术的原因, 实际上就是把虚拟机的概念套在了容器上.</p> <p>**可是这样的说法, 却并不严谨. **</p> <p>在理解了 Namespace 的工作方式之后, 你就会明白, 跟真实存在的虚拟机不同, 在使用 Docker 的时候, **并没有一个真正的&quot;Docker 容器&quot;运行在宿主机里面. **​<mark><strong>Docker 项目帮助用户启动的, 还是原来的应用进程, 只不过在创建这些进程时, Docker 为它们加上了各种各样的 Namespace 参数</strong></mark>.</p> <p>**这时, 这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程, 只能看到各自 Mount Namespace 里挂载的目录和文件, 只能访问到各自 Network Namespace 里的网络设备, 就仿佛运行在一个个&quot;容器&quot;里面, 与世隔绝. **</p> <h4 id="_06-白话容器基础-二-隔离与限制"><a href="#_06-白话容器基础-二-隔离与限制" class="header-anchor">#</a> 06 | 白话容器基础(二):隔离与限制</h4> <p>上一节详细介绍了 Linux 容器中用来实现&quot;隔离&quot;的技术手段: <strong>Namespace</strong>. 通过这些讲解, 你应该能够明白, <mark><strong>Namespace 技术实际上修改了应用进程看待整个计算机&quot;视图&quot;, 即它的&quot;视线&quot;被操作系统做了限制, 只能&quot;看到&quot;某些指定的内容</strong></mark>. 但对于宿主机来说, 这些被&quot;隔离&quot;了的进程跟其他进程并没有太大区别.</p> <p>上一节的在虚拟机与容器技术的对比图里, 不应该把 Docker Engine 或者任何容器管理工具放在跟 Hypervisor 相同的位置, 因为它们并不像 Hypervisor 那样对应用进程的隔离环境负责, 也不会创建任何实体的&quot;容器&quot;, <strong>真正对隔离环境负责的是宿主机操作系统本身</strong>:</p> <p><img src="/img/7d3ec2c0304dd065d0b9dbafd7db60ea-20230731162150-jyegecv.png" alt=""></p> <p>所以在这个对比图里, 应该把 Docker 画在跟应用同级别并且靠边的位置. 这意味着, <strong>用户运行在容器里的应用进程, 跟宿主机上的其他进程一样, 都由宿主机操作系统统一管理, 只不过这些被隔离的进程拥有额外设置过的 Namespace 参数. 而 Docker 项目在这里扮演的角色, 更多的是旁路式的辅助和管理工作</strong>.</p> <p>后续分享 CRI 和容器运行时的时候还会专门介绍到, 其实像 Docker 这样的角色甚至可以去掉.</p> <p>这样的架构也解释了为什么 Docker 项目比虚拟机更受欢迎的原因. 这是因为, 使用虚拟化技术作为应用沙盒, 就必须要由 Hypervisor 来负责创建虚拟机, 这个<strong>虚拟机是真实存在</strong>的, 并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程. 这就不可避免地带来了<strong>额外的资源消耗和占用</strong>.</p> <p>根据实验, 一个运行着 CentOS 的 KVM 虚拟机启动后, 在不做优化的情况下, 虚拟机自己就需要占用 100~200 MB 内存. 此外, 用户应用运行在虚拟机里面, 它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理, 这本身又是一层性能损耗, 尤其对计算资源, 网络和磁盘 I/O 的损耗非常大.</p> <p>而相比之下, 容器化后的用户应用, 却依然还是一个<strong>宿主机上的普通进程</strong>, 这就意味着这些因为虚拟化而带来的性能损耗都是不存在的; 而另一方面, 使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS, 这就使得容器额外的资源占用几乎可以忽略不计.</p> <p>所以说,  **&quot;敏捷&quot;和&quot;高性能&quot;是容器相较于虚拟机最大的优势, 也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因. **</p> <p>不过, 有利就有弊, 基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处, 其中最主要的问题就是: <mark><strong>隔离得不彻底</strong></mark>​ **. **</p> <p>首先, 既然容器只是运行在宿主机上的一种特殊的进程, 那么<strong>多个容器之间使用的就还是同一个宿主机的操作系统内核</strong>.</p> <p>尽管可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件, 比如 CentOS 或者 Ubuntu, 但这并不能改变<strong>共享宿主机内核</strong>的事实. 这意味着, 如果你要在 Windows 宿主机上运行 Linux 容器, 或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器, 都是行不通的.</p> <p>而相比之下, 拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了. 最极端的例子是, Microsoft 的云计算平台 Azure, 实际上就是运行在 Windows 服务器集群上的, 但这并不妨碍你在它上面创建各种 Linux 虚拟机出来.</p> <p>其次, 在 Linux 内核中, <strong>有很多资源和对象是不能被 Namespace 化</strong>的, 最典型的例子就是: <strong>时间</strong>. 这就意味着, 如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间, <strong>整个宿主机的时间都会被随之修改</strong>, 这显然不符合用户的预期. 相比于在虚拟机里面可以随便折腾的自由度, 在容器里部署应用的时候, &quot;什么能做, 什么不能做&quot;, 就是用户必须考虑的一个问题.</p> <p>此外, 由于上述问题, 尤其是共享宿主机内核的事实, 容器给应用暴露出来的攻击面是相当大的, 应用&quot;越狱&quot;的难度自然也比虚拟机低得多.</p> <p>更为棘手的是, 尽管在实践中确实可以使用 Seccomp 等技术, 对容器内部发起的所有系统调用进行过滤和甄别来进行安全加固, 但这种方法因为多了一层对系统调用的过滤, 一定会拖累容器的性能. 何况, 默认情况下, 谁也不知道到底该开启哪些系统调用, 禁止哪些系统调用.</p> <p>所以, 在生产环境中, <strong>没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上</strong>. 当然, 后续会讲到的基于虚拟化或者独立内核技术的容器实现, 则可以比较好地在隔离与性能之间做出平衡.</p> <p>**在介绍完容器的&quot;隔离&quot;技术之后, 再来研究一下容器的&quot;**​<mark><strong>限制</strong></mark>​ **&quot;问题. **</p> <p>也许你会好奇, 前面不是已经通过 Linux Namespace 创建了一个&quot;容器&quot;吗, 为什么还需要对容器做&quot;限制&quot;呢?</p> <p>还是以 PID Namespace 为例来解释这个问题.</p> <p>虽然容器内的第 1 号进程在&quot;障眼法&quot;的干扰下只能看到容器里的情况, 但是宿主机上, 它作为第 100 号进程与其他所有进程之间依然是<strong>平等的竞争关系</strong>. 这就意味着, 虽然第 100 号进程表面上被隔离了起来, <strong>但是它所能够使用到的资源(比如 CPU, 内存), 却是可以随时被宿主机上的其他进程(或者其他容器)占用的</strong>. 当然, 这个 100 号进程自己也可能把所有资源吃光. 这些情况, 显然都不是一个&quot;沙盒&quot;应该表现出来的合理行为.</p> <p><mark>而 </mark>​<mark>**Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能. **</mark></p> <p>有意思的是, Google 的工程师在 2006 年发起这项特性的时候, 曾将它命名为&quot;进程容器&quot;(process container). 实际上, 在 Google 内部, &quot;容器&quot;这个术语长期以来都被用于形容被 Cgroups 限制过的进程组. 后来 Google 的工程师们说, 他们的 KVM 虚拟机也运行在 Borg 所管理的&quot;容器&quot;里, 其实也是运行在 Cgroups &quot;容器&quot;当中. 这和今天说的 Docker 容器差别很大.</p> <p><mark>**Linux Cgroups 的全称是 Linux Control Group. 它最主要的作用, 就是限制一个进程组能够使用的资源上限, 包括 CPU, 内存, 磁盘, 网络带宽等等. **</mark></p> <p>此外, Cgroups 还能够对进程进行<strong>优先级设置, 审计, 以及将进程挂起和恢复</strong>等操作. 本节重点探讨它与容器关系最紧密的&quot;限制&quot;能力, 并通过一组实践来认识一下 Cgroups.</p> <p>在 Linux 中, Cgroups 给用户暴露出来的操作接口是<strong>文件系统</strong>, 即它<mark><strong>以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下</strong></mark>. 在 Ubuntu 16.04 机器里, 可以用 mount 指令把它们展示出来, 这条命令是:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">mount</span> <span class="token parameter variable">-t</span> cgroup 
cpuset on /sys/fs/cgroup/cpuset <span class="token builtin class-name">type</span> cgroup <span class="token punctuation">(</span>rw,nosuid,nodev,noexec,relatime,cpuset<span class="token punctuation">)</span>
cpu on /sys/fs/cgroup/cpu <span class="token builtin class-name">type</span> cgroup <span class="token punctuation">(</span>rw,nosuid,nodev,noexec,relatime,cpu<span class="token punctuation">)</span>
cpuacct on /sys/fs/cgroup/cpuacct <span class="token builtin class-name">type</span> cgroup <span class="token punctuation">(</span>rw,nosuid,nodev,noexec,relatime,cpuacct<span class="token punctuation">)</span>
blkio on /sys/fs/cgroup/blkio <span class="token builtin class-name">type</span> cgroup <span class="token punctuation">(</span>rw,nosuid,nodev,noexec,relatime,blkio<span class="token punctuation">)</span>
memory on /sys/fs/cgroup/memory <span class="token builtin class-name">type</span> cgroup <span class="token punctuation">(</span>rw,nosuid,nodev,noexec,relatime,memory<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>它的输出结果, 是一系列<strong>文件系统目录</strong>. 如果你在自己的机器上没有看到这些目录, 那就需要自己去挂载 Cgroups, 具体做法可以自行 Google.</p> <p>可以看到, 在 /sys/fs/cgroup 下面有很多诸如 <mark><strong>cpuset, cpu,  memory 这样的子目录, 也叫子系统</strong></mark>. 这些都是这台机器当前<strong>可以被 Cgroups 进行限制的资源种类</strong>. 而在子系统对应的资源种类下, 就可以看到该类资源具体可以被限制的方法. 比如, 对 CPU 子系统来说, 就可以看到如下几个配置文件, 这个指令是:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /sys/fs/cgroup/cpu
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>如果熟悉 Linux CPU 管理的话, 你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词. <strong>这两个参数需要组合使用, 可以用来限制进程在长度为 cfs_period 的一段时间内, 只能被分配到总量为 cfs_quota 的 CPU 时间</strong>.</p> <p>而这样的配置文件又如何使用呢?</p> <p>你需要在<strong>对应的子系统下面创建一个目录</strong>, 比如, 现在进入 /sys/fs/cgroup/cpu 目录下:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>root@ubuntu:/sys/fs/cgroup/cpu$ <span class="token function">mkdir</span> container
root@ubuntu:/sys/fs/cgroup/cpu$ <span class="token function">ls</span> container/
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这个目录就称为一个&quot;<strong>控制组</strong>&quot;. 你会发现, <mark><strong>操作系统会在你新创建的 container 目录下, 自动生成该子系统对应的资源限制文件</strong></mark>.</p> <p>现在, 我们在后台执行这样一条脚本:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token keyword">while</span> <span class="token builtin class-name">:</span> <span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token builtin class-name">:</span> <span class="token punctuation">;</span> <span class="token keyword">done</span> <span class="token operator">&amp;</span>
<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token number">226</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>显然, 它执行了一个死循环, 可以把计算机的 CPU 吃到 100%, 根据它的输出, 可以看到这个脚本在后台运行的进程号(PID)是 226.</p> <p>这样, 可以用 top 指令来确认一下 CPU 有没有被打满:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">top</span>
%Cpu0 :100.0 us, <span class="token number">0.0</span> sy, <span class="token number">0.0</span> ni, <span class="token number">0.0</span> id, <span class="token number">0.0</span> wa, <span class="token number">0.0</span> hi, <span class="token number">0.0</span> si, <span class="token number">0.0</span> st
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在输出里可以看到, CPU 的使用率已经 100% 了(%Cpu0 :100.0 us).</p> <p>而此时, 可以通过查看 container 目录下的文件, 看到 container 控制组里的 <strong>CPU quota 还没有任何限制(即: -1)</strong> , CPU period 则是默认的 100 ms(100000 us):</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cat</span> /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 
<span class="token parameter variable">-1</span>
$ <span class="token function">cat</span> /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 
<span class="token number">100000</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>接下来, 可以通过<strong>修改这些文件的内容</strong>来设置限制.</p> <p>比如, 向 container 组里的 cfs_quota 文件写入 20 ms(20000 us):</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token builtin class-name">echo</span> <span class="token number">20000</span> <span class="token operator">&gt;</span> /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>结合前面的介绍, 你应该能明白这个操作的含义, 它意味着<strong>在每 100 ms 的时间里, 被该控制组限制的进程只能使用 20 ms 的 CPU 时间, 也就是说这个进程只能使用到 20% 的 CPU 带宽</strong>.</p> <p>接下来, 把被限制的<strong>进程的 PID 写入 container 组里的 tasks 文件</strong>, 上面的设置就会<strong>对该进程生效</strong>了:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token builtin class-name">echo</span> <span class="token number">226</span> <span class="token operator">&gt;</span> /sys/fs/cgroup/cpu/container/tasks 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>再次用 top 指令查看一下:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">top</span>
%Cpu0 <span class="token builtin class-name">:</span> <span class="token number">20.3</span> us, <span class="token number">0.0</span> sy, <span class="token number">0.0</span> ni, <span class="token number">79.7</span> id, <span class="token number">0.0</span> wa, <span class="token number">0.0</span> hi, <span class="token number">0.0</span> si, <span class="token number">0.0</span> st
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以看到, 计算机的 CPU 使用率立刻降到了 20%(%Cpu0 : 20.3 us).</p> <p>除 CPU 子系统外, Cgroups 的<strong>每一项子系统都有其独有的资源限制能力</strong>, 比如:</p> <ul><li><strong>blkio</strong>, 为块设备设定 I/O 限制, 一般用于磁盘等设备;</li> <li><strong>cpuset</strong>, 为进程分配单独的 CPU 核和对应的内存节点;</li> <li><strong>memory</strong>, 为进程设定<strong>内存</strong>使用的限制.</li></ul> <p><mark><strong>Linux Cgroups 的设计还是比较易用的, 简单粗暴地理解呢, 它就是一个子系统目录加上一组资源限制文件的组合. 而对于 Docker 等 Linux 容器项目来说, 它们只需要在每个子系统下面, 为每个容器创建一个控制组(即创建一个新目录), 然后在启动容器进程之后, 把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了.</strong></mark></p> <p>而<strong>至于在这些控制组下面的资源文件里填上什么值, 就靠用户执行 docker run 时的参数指定了</strong>, 比如这样一条命令:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-it</span> --cpu-period<span class="token operator">=</span><span class="token number">100000</span> --cpu-quota<span class="token operator">=</span><span class="token number">20000</span> ubuntu /bin/bash
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在启动这个容器后, 可以通过查看 Cgroups 文件系统下, CPU 子系统中, &quot;docker&quot; 这个<strong>控制组</strong>里的资源限制文件的内容来确认:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cat</span> /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 
<span class="token number">100000</span>
$ <span class="token function">cat</span> /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 
<span class="token number">20000</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这就意味着这个 Docker 容器, 只能使用到 20% 的 CPU 带宽.</p> <blockquote><p>总结</p></blockquote> <p>在这篇文章中, 首先介绍了容器使用 Linux Namespace 作为隔离手段的优势和劣势, 对比了 Linux 容器跟虚拟机技术的不同, 进一步明确了&quot;容器只是一种特殊的进程&quot;这个结论.</p> <p>除了创建 Namespace 之外, 在后续关于容器网络的分享中, 还会介绍一些其他 Namespace 的操作, 比如看不见摸不着的 Linux Namespace 在计算机中到底如何表示, 一个进程如何&quot;加入&quot;到其他进程的 Namespace 当中, 等等.</p> <p>紧接着详细介绍了容器在做好了隔离工作之后, 又如何<strong>通过 Linux Cgroups 实现资源的限制</strong>, 并通过一系列简单的实验, 模拟了 Docker 项目创建容器限制的过程.</p> <p>通过以上讲述, 你现在应该能够理解, <mark><strong>一个正在运行的 Docker 容器, 其实就是一个启用了多个 Linux Namespace 的应用进程, 而这个进程能够使用的资源量, 则受 Cgroups 配置的限制</strong></mark>.</p> <p>这也是容器技术中一个非常重要的概念, 即: <mark><strong>容器是一个&quot;单进程&quot;模型</strong></mark>​ **. **</p> <p>由于一个容器的本质就是一个进程, 用户的应用进程实际上就是容器里 PID=1 的进程, 也是其他后续创建的所有进程的父进程. 这就意味着, <strong>在一个容器中, 没办法同时运行两个不同的应用</strong>, 除非能事先找到一个公共的 PID=1 的程序来充当两个不同应用的父进程, 这也是为什么很多人都会用 systemd 或者 supervisord 这样的软件来代替应用本身作为容器的启动进程.</p> <p>但是, 在后面分享容器设计模式时, 还会推荐其他更好的解决办法. <strong>这是因为容器本身的设计, 就是</strong>​<mark><strong>希望容器和应用能够同生命周期</strong></mark>​ <strong>, 这个概念对后续的容器编排非常重要. 否则, 一旦出现类似于&quot;容器是正常运行的, 但是里面的应用早已经挂了&quot;的情况, 编排系统处理起来就非常麻烦了</strong>.</p> <p>另外, 跟 Namespace 的情况类似, Cgroups 对资源的限制能力也有很多不完善的地方, 被提及最多的自然是  <strong>/proc 文件系统</strong>的问题.</p> <p>众所周知, Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件, 用户可以通过访问这些文件, 查看系统以及当前正在运行的进程的信息, 比如 CPU 使用情况, 内存占用率等, 这些文件也是 top 指令查看系统信息的主要数据来源.</p> <p>但是, 如果在容器里执行 top 指令, 就会发现, <strong>它显示的信息居然是宿主机的 CPU 和内存数据, 而不是当前容器的数据</strong>.</p> <p>造成这个问题的原因就是, /proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制, 即:  <mark><strong>/proc 文件系统不了解 Cgroups 限制的存在</strong></mark>.</p> <p>在生产环境中, 这个问题必须进行修正, <strong>否则应用程序在容器里读取到的 CPU 核数, 可用内存等信息都是宿主机上的数据, 这会给应用的运行带来非常大的困惑和风险</strong>. 这也是在企业中, 容器化应用碰到的一个常见问题, 也是容器相较于虚拟机另一个不尽如人意的地方.</p> <h4 id="_07-白话容器基础-三-深入理解容器镜像"><a href="#_07-白话容器基础-三-深入理解容器镜像" class="header-anchor">#</a> 07 | 白话容器基础(三):深入理解容器镜像</h4> <p>前两节讲解了 Linux 容器最基础的两种技术: <strong>Namespace 和 Cgroups</strong>. 希望此时, 你已经彻底理解了 &quot;<strong>容器的本质是一种特殊的进程</strong>&quot; 这个最重要的概念.</p> <p>总结一下前面的内容, <mark><strong>Namespace 的作用是 &quot;隔离&quot;, 它让应用进程只能看到该 Namespace 内的&quot;世界&quot;; 而 Cgroups 的作用是 &quot;限制&quot;, 它给这个&quot;世界&quot;围上了一圈看不见的墙</strong></mark>. 这么一折腾, 进程就真的被&quot;装&quot;在了一个与世隔绝的房间里, 而这些房间就是 PaaS 项目赖以生存的应用&quot;沙盒&quot;.</p> <p>可是, 还有一个问题不知道你有没有仔细思考过: 这个房间四周虽然有了墙, 但是如果容器进程低头一看地面, 又是怎样一副景象呢?</p> <p>换句话说, <mark><strong>容器里的进程看到的文件系统又是什么样子的呢</strong></mark>​ **? **</p> <p>可能你立刻就能想到, 这一定是一个关于 Mount Namespace 的问题: 容器里的应用进程, 理应看到一份<strong>完全独立的文件系统</strong>. 这样, 它就可以在自己的容器目录(比如 /tmp)下进行操作, 而完全不会受宿主机以及其他容器的影响.</p> <p>那么, 真实情况是这样吗?</p> <p>&quot;左耳朵耗子&quot;叔在多年前写的一篇<a href="https://coolshell.cn/articles/17010.html" target="_blank" rel="noopener noreferrer">关于 Docker 基础知识的博客<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>里, 曾经介绍过一段小程序. 这段小程序的作用是, 在创建子进程时开启指定的 Namespace.</p> <p>下面不妨使用它来验证一下刚刚提到的问题.</p> <div class="language-c line-numbers-mode"><pre class="language-c"><code><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">_GNU_SOURCE</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/mount.h&gt;</span> </span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/types.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/wait.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sched.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;signal.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">STACK_SIZE</span> <span class="token expression"><span class="token punctuation">(</span><span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">)</span></span></span>
<span class="token keyword">static</span> <span class="token keyword">char</span> container_stack<span class="token punctuation">[</span>STACK_SIZE<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token keyword">char</span><span class="token operator">*</span> <span class="token keyword">const</span> container_args<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;/bin/bash&quot;</span><span class="token punctuation">,</span>
   <span class="token constant">NULL</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
 
<span class="token keyword">int</span> <span class="token function">container_main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span> arg<span class="token punctuation">)</span>
<span class="token punctuation">{</span>  
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Container - inside the container!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">execv</span><span class="token punctuation">(</span>container_args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> container_args<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Something's wrong!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
 
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Parent - start a container!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> container_pid <span class="token operator">=</span> <span class="token function">clone</span><span class="token punctuation">(</span>container_main<span class="token punctuation">,</span> container_stack<span class="token operator">+</span>STACK_SIZE<span class="token punctuation">,</span> CLONE_NEWNS <span class="token operator">|</span> SIGCHLD <span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">waitpid</span><span class="token punctuation">(</span>container_pid<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Parent - container stopped!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br></div></div><p>这段代码的功能非常简单: 在 main 函数里, 通过 clone() 系统调用<strong>创建了一个新的子进程 container_main, 并且声明要为它启用 Mount Namespace(即: CLONE_NEWNS 标志)</strong> .</p> <p>而这个子进程执行的, 是一个 &quot;/bin/bash&quot; 程序, 也就是一个 shell. 所以<strong>这个 shell 就运行在了 Mount Namespace 的隔离环境中</strong>.</p> <p>编译一下这个程序:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ gcc <span class="token parameter variable">-o</span> ns ns.c
$ ./ns
Parent - start a container<span class="token operator">!</span>
Container - inside the container<span class="token operator">!</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这样, 就进入了这个&quot;容器&quot;当中. 可是, 如果在 &quot;容器&quot; 里执行一下 ls 指令的话, 就会发现一个有趣的现象: /tmp 目录下的内容跟宿主机的内容是一样的.</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /tmp
<span class="token comment"># 你会看到好多宿主机的文件</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>也就是说:</p> <blockquote><p>即使开启了 Mount Namespace, 容器进程看到的文件系统也跟宿主机完全一样.</p></blockquote> <p>这是怎么回事呢?</p> <p>仔细思考一下, 可以发现这其实并不难理解: <mark><strong>Mount Namespace 修改的, 是容器进程对文件系统&quot;挂载点&quot;的认知</strong></mark>. 但是, 这也就意味着, <strong>只有在&quot;挂载&quot;这个操作发生之后, 进程的视图才会被改变. 而在此之前, 新创建的容器会直接继承宿主机的各个挂载点</strong>.</p> <p>这时, 你可能已经想到了一个解决办法: 创建新进程时, 除了声明要启用 Mount Namespace 之外, 还可以告诉容器进程, 有哪些目录需要重新挂载, 就比如这个 /tmp 目录. 于是, 在容器进程执行前可以添加一步重新挂载 /tmp 目录的操作:</p> <div class="language-c line-numbers-mode"><pre class="language-c"><code><span class="token keyword">int</span> <span class="token function">container_main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span> arg<span class="token punctuation">)</span>
<span class="token punctuation">{</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Container - inside the container!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// 如果你的机器的根目录的挂载类型是 shared, 那必须先重新挂载根目录</span>
    <span class="token comment">// mount(&quot;&quot;, &quot;/&quot;, NULL, MS_PRIVATE, &quot;&quot;);</span>
    <span class="token function">mount</span><span class="token punctuation">(</span><span class="token string">&quot;none&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;/tmp&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;tmpfs&quot;</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">&quot;&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">execv</span><span class="token punctuation">(</span>container_args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> container_args<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Something's wrong!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到, 在修改后的代码里, 在容器进程启动之前, 加上了一句 mount(&quot;none&quot;, &quot;/tmp&quot;, &quot;tmpfs&quot;, 0, &quot;&quot;) 语句. 就这样, 告诉了容器<strong>以 tmpfs(内存盘)格式, 重新挂载了 /tmp 目录</strong>.</p> <p>这段修改后的代码, 编译执行后的结果又如何呢? 可以试验一下:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ gcc <span class="token parameter variable">-o</span> ns ns.c
$ ./ns
Parent - start a container<span class="token operator">!</span>
Container - inside the container<span class="token operator">!</span>
$ <span class="token function">ls</span> /tmp
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, 这次 /tmp 变成了一个空目录, 这意味着重新挂载生效了. 可以用 mount -l 检查一下:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">mount</span> <span class="token parameter variable">-l</span> <span class="token operator">|</span> <span class="token function">grep</span> tmpfs
none on /tmp <span class="token builtin class-name">type</span> tmpfs <span class="token punctuation">(</span>rw,relatime<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以看到, 容器里的 /tmp 目录是以 <strong>tmpfs</strong> 方式单独挂载的.</p> <p>更重要的是, 因为创建的新进程启用了 Mount Namespace, 所以这次重新挂载的操作, <strong>只在容器进程的 Mount Namespace 中有效</strong>. 如果在宿主机上用 mount -l 来检查一下这个挂载, 你会发现它是<strong>不存在</strong>的:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 在宿主机上</span>
$ <span class="token function">mount</span> <span class="token parameter variable">-l</span> <span class="token operator">|</span> <span class="token function">grep</span> tmpfs
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>**这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方: **​<mark><strong>它对容器进程视图的改变, 一定是伴随着挂载操作(mount)才能生效</strong></mark>​ **. **</p> <p>可是, 作为一个普通用户, 我们希望的是一个更友好的情况: <strong>每当创建一个新容器时, 我希望容器进程看到的文件系统就是一个独立的隔离环境, 而不是继承自宿主机的文件系统. 怎么才能做到这一点呢</strong>?</p> <p>不难想到, <strong>可以在容器进程启动之前重新挂载它的整个根目录 &quot;/&quot;</strong> . 而由于 Mount Namespace 的存在, 这个挂载对宿主机不可见, 所以容器进程就可以在里面随便折腾了.</p> <p>在 Linux 操作系统里, 有一个名为 chroot 的命令可以帮助你在 shell 中方便地完成这个工作. 顾名思义, 它的作用就是帮你 &quot;change root file system&quot;, 即<strong>改变进程的根目录到你指定的位置</strong>. 它的用法也非常简单.</p> <p>假设, 现在有一个  <strong>$HOME/test</strong> 目录, 想要把它作为一个 /bin/bash 进程的<strong>根目录</strong>.</p> <p>首先, 创建一个 test 目录和几个 lib 文件夹:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> <span class="token environment constant">$HOME</span>/test
$ <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> <span class="token environment constant">$HOME</span>/test/<span class="token punctuation">{</span>bin,lib64,lib<span class="token punctuation">}</span>
$ <span class="token builtin class-name">cd</span> <span class="token variable">$T</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后, 把 bash 命令拷贝到 test 目录对应的 bin 路径下:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cp</span> <span class="token parameter variable">-v</span> /bin/<span class="token punctuation">{</span>bash,ls<span class="token punctuation">}</span> <span class="token environment constant">$HOME</span>/test/bin
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>接下来, 把 bash 命令需要的所有 so 文件, 也拷贝到 test 目录对应的 lib 路径下. 找到 so 文件可以用 ldd 命令:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token assign-left variable">T</span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/test
$ <span class="token assign-left variable">list</span><span class="token operator">=</span><span class="token string">&quot;<span class="token variable"><span class="token variable">$(</span>ldd /bin/ls <span class="token operator">|</span> <span class="token function">egrep</span> <span class="token parameter variable">-o</span> <span class="token string">'/lib.*\.[0-9]'</span><span class="token variable">)</span></span>&quot;</span>
$ <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> <span class="token variable">$list</span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">cp</span> <span class="token parameter variable">-v</span> <span class="token string">&quot;<span class="token variable">$i</span>&quot;</span> <span class="token string">&quot;<span class="token variable">${T}</span><span class="token variable">${i}</span>&quot;</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>最后, 执行 chroot 命令, <strong>告诉操作系统, 将使用 $HOME/test 目录作为 /bin/bash 进程的根目录</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">chroot</span> <span class="token environment constant">$HOME</span>/test /bin/bash
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时, 如果执行 &quot;ls /&quot;, 就会看到, 它返回的<strong>都是 $HOME/test 目录下面的内容, 而不是宿主机的内容</strong>.</p> <p>更重要的是, 对于被 chroot 的进程来说, 它并不会感受到自己的根目录已经被&quot;修改&quot;成 $HOME/test 了.</p> <p>这种视图被修改的原理, 是不是跟之前介绍的 Linux Namespace 很类似呢?</p> <p>没错!</p> <p><mark><strong>实际上, Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的, 它也是 Linux 操作系统里的第一个 Namespace.</strong></mark></p> <p>当然, <strong>为了能够让容器的这个根目录看起来更&quot;真实&quot;, 一般会在这个容器的根目录下挂载一个</strong>​<mark><strong>完整操作系统的文件系统</strong></mark>, 比如 Ubuntu16.04 的 <strong>ISO</strong>. 这样, 在容器启动之后, 在容器里通过执行 &quot;ls /&quot; 查看根目录下的内容, 就是 Ubuntu 16.04 的所有目录和文件.</p> <p>**而这个挂载在容器根目录上, 用来为容器进程提供隔离后执行环境的文件系统, 就是所谓的 &quot;**​<mark><strong>容器镜像</strong></mark>​ **&quot;. 它还有一个更为专业的名字, 叫作: **​<mark><strong>rootfs</strong></mark>​ **(根文件系统). **</p> <p>所以, 一个最常见的 rootfs, 或者说容器镜像, 会包括如下所示的一些目录和文件, 比如 /bin, /etc, /proc 等等:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /
bin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>而进入容器之后执行的 /bin/bash, 就是 /bin 目录下的可执行文件, 与宿主机的 /bin/bash 完全不同.</p> <p>现在, 你应该可以理解, 对 Docker 项目来说, 它最核心的原理实际上就是<strong>为待创建的用户进程</strong>:</p> <ol><li><mark><strong>启用 Linux Namespace 配置</strong></mark>​<mark>;</mark></li> <li><mark><strong>设置指定的 Cgroups 参数</strong></mark>​<mark>;</mark></li> <li><mark><strong>切换进程的根目录(Change Root)</strong></mark>​<mark>.</mark></li></ol> <p>这样, 一个完整的容器就诞生了 <strong>(牛逼)</strong> .</p> <p>不过, Docker 项目在最后一步的切换上会优先使用 <strong>pivot_root</strong> 系统调用, 如果系统不支持, 才会使用 chroot. 这两个系统调用虽然功能类似, 但是也有细微的区别.</p> <p>另外, <mark><strong>需要明确的是, rootfs 只是一个操作系统所包含的文件, 配置和目录, 并不包括操作系统内核. 在 Linux 操作系统中, 这两部分是分开存放的, 操作系统只有在开机启动时才会加载指定版本的内核镜像</strong></mark>​ **. **</p> <p>所以说, <strong>rootfs 只包括了操作系统的&quot;躯壳&quot;, 并没有包括操作系统的&quot;灵魂&quot;</strong> .</p> <p>那么, 对于容器来说, 这个操作系统的&quot;灵魂&quot;又在哪里呢?</p> <p>实际上, <strong>同一台机器上的所有容器, 都共享宿主机操作系统的内核</strong>. 这就意味着, 如果你的应用程序需要配置内核参数, 加载额外的内核模块, 以及跟内核进行直接的交互, 就需要注意了: <strong>这些操作和依赖的对象, 都是宿主机操作系统的内核, 它对于该机器上的所有容器来说是一个&quot;全局变量&quot;, 牵一发而动全身</strong>.</p> <p>这也是容器相比于虚拟机的主要缺陷之一: 毕竟后者不仅有模拟出来的硬件机器充当沙盒, 而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾.</p> <p>不过, **正是由于 rootfs 的存在, 容器才有了一个被反复宣传至今的重要特性: **​<mark><strong>一致性</strong></mark>​ **. **</p> <p>什么是容器的&quot;一致性&quot;呢? 前面曾经提到过: 由于云端与本地服务器<strong>环境不同</strong>, 应用的<strong>打包过程</strong>, 一直是使用 PaaS 时最&quot;痛苦&quot;的一个步骤. 但有了容器之后, 更准确地说, 有了<strong>容器镜像(即 rootfs)</strong> 之后, 这个问题被非常优雅地解决了.</p> <p><mark><strong>由于 rootfs 里打包的不只是应用, 而是整个操作系统的文件和目录, 也就意味着, 应用以及它运行所需要的所有依赖, 都被封装在了一起.</strong></mark></p> <p>事实上, 对于大多数开发者而言, 他们对应用依赖的理解, 一直局限在编程语言层面. 比如 Golang 的 Godeps.json. 但实际上, 一个一直以来很容易被忽视的事实是, <mark><strong>对一个应用来说, 操作系统本身才是它运行所需要的最完整的&quot;依赖库&quot;</strong></mark>​ **. **</p> <p>有了<strong>容器镜像&quot;打包操作系统&quot;的能力</strong>, 这个最基础的依赖环境也终于变成了应用沙盒的一部分. 这就赋予了容器所谓的一致性: <strong>无论在本地, 云端, 还是在一台任何地方的机器上, 用户只需要解压打包好的容器镜像, 那么这个应用运行所需要的完整的执行环境就被重现出来了</strong>.</p> <p><mark><strong>这种深入到操作系统级别的运行环境一致性, 打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟.</strong></mark></p> <p>不过, 这时你可能已经发现了另一个非常棘手的问题: 难道我每开发一个应用, 或者升级一下现有的应用, 都要<strong>重复制作一次</strong> rootfs 吗?</p> <p>比如, 我现在用 Ubuntu 操作系统的 ISO 做了一个 rootfs, 然后又在里面安装了 Java 环境, 用来部署我的 Java 应用. 那么, 另一个同事在发布他的 Java 应用时, 显然希望能够直接使用我安装过 Java 环境的 rootfs, 而不是重复这个流程.</p> <p>一种比较直观的解决办法是, 在制作 rootfs 的时候, 每做一步&quot;有意义&quot;的操作, 就保存一个 rootfs 出来, 这样其他同事就可以按需求去用他需要的 rootfs 了. 但这个解决办法并不具备推广性. 原因在于, 一旦有同事修改了这个 rootfs, 新旧两个 rootfs 之间就没有任何关系了. 这样做的结果就是极度的碎片化.</p> <p>那么, 既然这些修改都基于一个旧的 rootfs, 那能不能<strong>以增量的方式去做这些修改</strong>呢? 这样做的好处是, 所有人都<strong>只需要维护相对于 base rootfs 修改的增量内容</strong>, 而不是每次修改都制造一个 &quot;fork&quot;.</p> <p>答案当然是肯定的.</p> <p>这也正是为何, Docker 公司在实现 Docker 镜像时并没有沿用以前制作 rootfs 的标准流程, 而是做了一个小小的创新:</p> <blockquote><p>Docker 在镜像的设计中, 引入了层(layer)的概念. 也就是说, 用户制作镜像的每一步操作, 都会生成一个层, 也就是一个增量 rootfs.</p></blockquote> <p>当然, 这个想法不是凭空臆造出来的, 而是用到了一种叫作<strong>联合文件系统</strong>(Union File System)的能力.</p> <p>Union File System 也叫 UnionFS, 最主要的功能是将多个不同位置的目录联合挂载(union mount)到同一个目录下. 比如, 现在有两个目录 A 和 B, 它们分别有两个文件:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ tree
<span class="token builtin class-name">.</span>
├── A
│  ├── a
│  └── x
└── B
  ├── b
  └── x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>然后, 使用联合挂载的方式, 将这两个目录挂载到一个公共的目录 C 上:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">mkdir</span> C
$ <span class="token function">mount</span> <span class="token parameter variable">-t</span> aufs <span class="token parameter variable">-o</span> <span class="token assign-left variable">dirs</span><span class="token operator">=</span>./A:./B none ./C
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这时, 再查看目录 C 的内容, 就能看到<strong>目录 A 和 B 下的文件被合并到了一起</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ tree ./C
./C
├── a
├── b
└── x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, 在这个合并后的目录 C 里, 有 a, b, x 三个文件, 并且 x 文件<strong>只有一份</strong>. 这, 就是&quot;合并&quot;的含义. 此外, 如果你在目录 C 里对 a, b, x 文件做修改, <strong>这些修改也会在对应的目录 A, B 中生效</strong>.</p> <p>那么, 在 Docker 项目中, 又是如何使用这种 Union File System 的呢?</p> <p>我的环境是 Ubuntu 16.04 和 Docker CE 18.05, 这对组合默认使用的是 <strong>AuFS</strong> 这个联合文件系统的实现. 可以通过 docker info 命令, 查看到这个信息.</p> <p>AuFS 的全称是 Another UnionFS, 后改名为 Alternative UnionFS, 再后来干脆改名叫作 Advance UnionFS, 从这些名字中你应该能看出这样两个事实:</p> <ol><li>它是对 Linux 原生 UnionFS 的<strong>重写和改进</strong>;</li> <li>它的作者怨气好像很大. 可能是 Linus Torvalds(Linux 之父)一直不让 AuFS 进入 Linux 内核主干的缘故, 所以只能在 Ubuntu 和 Debian 这些发行版上使用它.</li></ol> <p>对于 AuFS 来说, 它最关键的目录结构在  <strong>/var/lib/docker 路径下的 diff 目录</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>/var/lib/docker/aufs/diff/<span class="token operator">&lt;</span>layer_id<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>而这个目录的作用, 不妨通过一个具体例子来看一下.</p> <p>现在, 启动一个容器, 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-d</span> ubuntu:latest <span class="token function">sleep</span> <span class="token number">3600</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时候, Docker 就会从 Docker Hub 上拉取一个 Ubuntu 镜像到本地.</p> <p>这个<strong>所谓的&quot;镜像&quot;, 实际上就是一个 Ubuntu 操作系统的 rootfs, 它的内容是 Ubuntu 操作系统的所有文件和目录</strong>. 不过, 与之前讲述的 rootfs 稍微不同的是, Docker 镜像使用的 rootfs, 往往<strong>由多个&quot;层&quot;组成</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> image inspect ubuntu:latest
<span class="token punctuation">..</span>.
     <span class="token string">&quot;RootFS&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
      <span class="token string">&quot;Type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;layers&quot;</span>,
      <span class="token string">&quot;Layers&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
        <span class="token string">&quot;sha256:f49017d4d5ce9c0f544c...&quot;</span>,
        <span class="token string">&quot;sha256:8f2b771487e9d6354080...&quot;</span>,
        <span class="token string">&quot;sha256:ccd4d61916aaa2159429...&quot;</span>,
        <span class="token string">&quot;sha256:c01d74f99de40e097c73...&quot;</span>,
        <span class="token string">&quot;sha256:268a067217b5fe78e000...&quot;</span>
      <span class="token punctuation">]</span>
    <span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>可以看到, 这个 Ubuntu 镜像, 实际上由五个层组成. 这<strong>五个层就是五个增量 rootfs, 每一层都是 Ubuntu 操作系统文件与目录的一部分; 而在使用镜像时, Docker 会把这些</strong>​<mark><strong>增量联合挂载在一个统一的挂载点上</strong></mark>​ <strong>(等价于前面例子里的&quot;/C&quot;目录)</strong> .</p> <p>这个挂载点就是  <strong>/var/lib/docker/aufs/mnt/</strong> , 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>不出意外的, <strong>这个目录里面正是一个完整的 Ubuntu 操作系统</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e
bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>那么, 前面提到的五个镜像层, 又是如何被联合挂载成这样一个完整的 Ubuntu 文件系统的呢?</p> <p>这个信息记录在 AuFS 的系统目录  <strong>/sys/fs/aufs</strong> 下面.</p> <p>首先, 通过查看 AuFS 的挂载信息, 可以找到这个目录对应的 AuFS 的内部 ID(也叫: si):</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cat</span> /proc/mounts<span class="token operator">|</span> <span class="token function">grep</span> aufs
none /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc<span class="token punctuation">..</span>. aufs rw,relatime,si<span class="token operator">=</span>972c6d361e6b32ba,dio,dirperm1 <span class="token number">0</span> <span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>即, si=972c6d361e6b32ba.</p> <p>然后使用这个 ID, 就可以在  <strong>/sys/fs/aufs 下查看被联合挂载在一起的各个层的信息</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cat</span> /sys/fs/aufs/si_972c6d361e6b32ba/br<span class="token punctuation">[</span><span class="token number">0</span>-9<span class="token punctuation">]</span>*
/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc<span class="token punctuation">..</span>.<span class="token operator">=</span>rw
/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc<span class="token punctuation">..</span>.-init<span class="token operator">=</span>ro+wh
/var/lib/docker/aufs/diff/32e8e20064858c0f2<span class="token punctuation">..</span>.<span class="token operator">=</span>ro+wh
/var/lib/docker/aufs/diff/2b8858809bce62e62<span class="token punctuation">..</span>.<span class="token operator">=</span>ro+wh
/var/lib/docker/aufs/diff/20707dce8efc0d267<span class="token punctuation">..</span>.<span class="token operator">=</span>ro+wh
/var/lib/docker/aufs/diff/72b0744e06247c7d0<span class="token punctuation">..</span>.<span class="token operator">=</span>ro+wh
/var/lib/docker/aufs/diff/a524a729adadedb90<span class="token punctuation">..</span>.<span class="token operator">=</span>ro+wh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>从这些信息里, 可以看到, <strong>镜像的层都放置在 /var/lib/docker/aufs/diff 目录下, 然后被联合挂载在 /var/lib/docker/aufs/mnt 里面</strong>.</p> <p>**而且, 从这个结构可以看出来, 这个容器的 rootfs 由如下图所示的三部分组成: **</p> <p><img src="/img/f975b9f0ab15ddbf4bace538d0effb76-20230731162150-g2su4jh.png" alt=""></p> <p><mark><strong>第一部分, 只读层</strong></mark>​ **. **</p> <p>它是这个容器的 rootfs 最下面的五层, 对应的正是 ubuntu:latest 镜像的<strong>五层</strong>. 可以看到, 它们的<strong>挂载方式都是只读的</strong>(ro+wh, 即 readonly+whiteout, 至于什么是 whiteout, 下面马上会讲到).</p> <p>这时, 可以分别查看一下这些层的内容:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /var/lib/docker/aufs/diff/72b0744e06247c7d0<span class="token punctuation">..</span>.
etc sbin usr var
$ <span class="token function">ls</span> /var/lib/docker/aufs/diff/32e8e20064858c0f2<span class="token punctuation">..</span>.
run
$ <span class="token function">ls</span> /var/lib/docker/aufs/diff/a524a729adadedb900<span class="token punctuation">..</span>.
bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, 这些层<strong>都以增量的方式分别包含了 Ubuntu 操作系统的一部分</strong>.</p> <p><mark><strong>第二部分, 可读写层</strong></mark>​ **. **</p> <p>它是这个容器的 rootfs <strong>最上面</strong>的一层(6e3be5d2ecccae7cc), 它的挂载方式为: rw, 即 read write. <strong>在没有写入文件之前, 这个目录是空的. 而一旦在容器里做了写操作, 你修改产生的内容就会以增量的方式出现在这个层中</strong>.</p> <p>可是, 你有没有想到这样一个问题: <strong>如果现在要做的, 是删除只读层里的一个文件呢</strong>?</p> <p>为了实现这样的删除操作, <strong>AuFS 会在可读写层创建一个 whiteout 文件, 把只读层里的文件&quot;遮挡&quot;起来</strong>.</p> <p>比如, 要删除只读层里一个名叫 foo 的文件, 那么这个删除操作实际上是在可读写层创建了一个名叫 .wh.foo 的文件. 这样, 当这两个层被联合挂载之后, foo 文件就会被 .wh.foo 文件&quot;遮挡&quot;起来, &quot;消失&quot;了. 这个功能, 就是 &quot;ro+wh&quot; 的挂载方式, 即只读 +whiteout 的含义. 可以把 whiteout 形象地翻译为: &quot;白障&quot;.</p> <p>**所以, 最上面这个可读写层的作用, 就是专门用来存放你修改 rootfs 后产生的增量, 无论是增, 删, 改, 都发生在这里. 而当我们使用完了这个被修改过的容器之后, 还可以使用 docker commit 和 push 指令, 保存这个被修改过的可读写层, 并上传到 Docker Hub 上, 供其他人使用; 而与此同时, 原先的只读层里的内容则不会有任何变化. 这就是增量 rootfs 的好处. **</p> <p><mark><strong>第三部分, Init 层</strong></mark>​ **. **</p> <p>它是一个以 &quot;-init&quot; 结尾的层, 夹在只读层和读写层之间. Init 层是 Docker 项目单独生成的一个内部层, 专门用来存放 /etc/hosts, /etc/resolv.conf 等信息.</p> <p>需要这样一层的原因是, <strong>这些文件本来属于只读的 Ubuntu 镜像的一部分, 但是用户往往需要在启动容器时写入一些指定的值比如 hostname, 所以就需要在可读写层对它们进行修改</strong>.</p> <p>可是, 这些修改往往<strong>只对当前的容器有效</strong>, 并不希望执行 docker commit 时, 把这些信息连同可读写层一起提交掉.</p> <p>所以 Docker 的做法是, <strong>在修改了这些文件之后, 以一个单独的层挂载了出来</strong>. 而<strong>用户执行 docker commit 只会提交可读写层</strong>, 所以是不包含这些内容的.</p> <p><mark>**最终, 这 7 个层都被联合挂载到 /var/lib/docker/aufs/mnt 目录下, 表现为一个完整的 Ubuntu 操作系统供容器使用. **</mark></p> <blockquote><p>总结</p></blockquote> <p>在今天的分享中, 着重介绍了 Linux <strong>容器文件系统的实现方式</strong>. 而这种机制, 正是经常提到的<strong>容器镜像</strong>, 也叫作: <strong>rootfs</strong>. 它只是<strong>一个操作系统的所有文件和目录, 并不包含内核</strong>, 最多也就几百兆. 而相比之下, 传统虚拟机的镜像大多是一个磁盘的&quot;快照&quot;, 磁盘有多大, 镜像就至少有多大.</p> <p>**通过结合使用 Mount Namespace 和 rootfs, 容器就能够为进程构建出一个完善的文件系统隔离环境. 当然, 这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力. **</p> <p><mark>**而在 rootfs 的基础上, Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案, 这就是容器镜像中&quot;层&quot;的概念. **</mark></p> <p>通过&quot;分层镜像&quot;的设计, 以 Docker 镜像为核心, 来自不同公司, 不同团队的技术人员被紧密地联系在了一起. 而且, 由于容器镜像的操作是<strong>增量式</strong>的, 这样每次镜像拉取, 推送的内容, 比原本多个完整的操作系统的大小要小得多; 而共享层的存在, 可以使得所有这些容器镜像需要的总空间, 也比每个镜像的总和要小. 这样就使得基于容器镜像的团队协作, 要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多.</p> <p>更重要的是, 一旦这个镜像被发布, 那么在全世界的任何一个地方下载这个镜像, 得到的内容都完全一致, 可以完全复现这个镜像制作者当初的完整环境. 这就是容器技术&quot;<strong>强一致性</strong>&quot;的重要体现.</p> <p>而这种价值正是支撑 Docker 公司在 2014~2016 年间迅猛发展的核心动力. 容器镜像的发明, 不仅打通了 &quot;开发-测试-部署&quot; 流程的每一个环节, 更重要的是:</p> <blockquote><p>容器镜像将会成为未来软件的主流发布方式.</p></blockquote> <p>下面是 Kubernetes 技术图谱.
<img src="/img/030dbcb984f327d2d99d41ebd4ff25a4-20230731162150-cfe804l.png" alt="">​</p> <h4 id="_08-白话容器基础-四-重新认识docker容器"><a href="#_08-白话容器基础-四-重新认识docker容器" class="header-anchor">#</a> 08 | 白话容器基础(四):重新认识Docker容器</h4> <p>前面三节分别从 <strong>Linux Namespace 的隔离能力, Linux Cgroups 的限制能力, 以及基于 rootfs 的文件系统</strong>三个角度, 为你剖析了一个 Linux 容器的核心实现原理.</p> <blockquote><p>备注: 之所以要强调 Linux 容器, 是因为比如 Docker on Mac, 以及 Windows Docker(Hyper-V 实现), 实际上是基于虚拟化技术实现的, 跟这个专栏着重介绍的 Linux 容器完全不同.</p></blockquote> <p>本节会通过一个实际案例, 对&quot;白话容器基础&quot;系列的所有内容做一次深入的总结和扩展. 希望通过这次的讲解, 能够让你更透彻地理解 Docker 容器的本质.</p> <p>在开始实践之前, 需要准备一台 Linux 机器, 并安装 Docker. 这个流程就不再赘述了. 本节要用 Docker 部署一个用 Python 编写的 Web 应用. 这个应用的代码部分(<a href="http://app.py" target="_blank" rel="noopener noreferrer">app.py<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>)非常简单:</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> flask <span class="token keyword">import</span> Flask
<span class="token keyword">import</span> socket
<span class="token keyword">import</span> os
 
app <span class="token operator">=</span> Flask<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>
 
<span class="token decorator annotation punctuation">@app<span class="token punctuation">.</span>route</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    html <span class="token operator">=</span> <span class="token string">&quot;&lt;h3&gt;Hello {name}!&lt;/h3&gt;&quot;</span> \
           <span class="token string">&quot;&lt;b&gt;Hostname:&lt;/b&gt; {hostname}&lt;br/&gt;&quot;</span>   
    <span class="token keyword">return</span> html<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>name<span class="token operator">=</span>os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">&quot;NAME&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;world&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hostname<span class="token operator">=</span>socket<span class="token punctuation">.</span>gethostname<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">&quot;__main__&quot;</span><span class="token punctuation">:</span>
    app<span class="token punctuation">.</span>run<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'0.0.0.0'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>在这段代码中, 使用 Flask 框架启动了一个 Web 服务器, 而它唯一的功能是: 如果当前环境中有 &quot;NAME&quot; 这个环境变量, 就把它打印在 &quot;Hello&quot; 后, 否则就打印 &quot;Hello world&quot;, 最后再打印出当前环境的 hostname.</p> <p>这个应用的<strong>依赖</strong>, 则被定义在了<strong>同目录下的 requirements.txt 文件</strong>里, 内容如下所示:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cat</span> requirements.txt
Flask
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><mark><strong>而将这样一个应用容器化的第一步, 是制作容器镜像.</strong></mark></p> <p>不过, 相较于之前介绍的制作 rootfs 的过程, Docker 提供了一种更便捷的方式, 叫作 <strong>Dockerfile</strong>, 如下所示.</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 使用官方提供的 Python 开发镜像作为基础镜像</span>
FROM python:2.7-slim
 
<span class="token comment"># 将工作目录切换为 /app</span>
WORKDIR /app
 
<span class="token comment"># 将当前目录下的所有内容复制到 /app 下</span>
ADD <span class="token builtin class-name">.</span> /app
 
<span class="token comment"># 使用 pip 命令安装这个应用所需要的依赖</span>
RUN pip <span class="token function">install</span> --trusted-host pypi.python.org <span class="token parameter variable">-r</span> requirements.txt
 
<span class="token comment"># 允许外界访问容器的 80 端口</span>
EXPOSE <span class="token number">80</span>
 
<span class="token comment"># 设置环境变量</span>
ENV NAME World
 
<span class="token comment"># 设置容器进程为: python app.py, 即: 这个 Python 应用的启动命令</span>
CMD <span class="token punctuation">[</span><span class="token string">&quot;python&quot;</span>, <span class="token string">&quot;app.py&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>通过这个文件的内容, 可以看到 <mark><strong>Dockerfile 的设计思想, 是使用一些标准的原语(即大写高亮的词语), 描述所要构建的 Docker 镜像. 并且这些原语, 都是按顺序处理的</strong></mark>​ **. **</p> <p>比如 FROM 原语, 指定了 &quot;python:2.7-slim&quot; 这个官方维护的基础镜像, 从而免去了安装 Python 等语言环境的操作. 否则, 这一段就得这么写了:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>FROM ubuntu:latest
RUN <span class="token function">apt-get</span> update <span class="token parameter variable">-yRUN</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> python-pip python-dev build-essential
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>其中, <strong>RUN 原语就是在容器里执行 shell 命令的意思</strong>.</p> <p>而 <strong>WORKDIR</strong>, 意思是在这一句之后, Dockerfile <strong>后面的操作都以这一句指定的 /app 目录作为当前目录</strong>.</p> <p>所以, 到了最后的 <strong>CMD</strong>, 意思是 Dockerfile <strong>指定 python app.py 为这个容器的进程</strong>. 这里, app.py 的实际路径是 /app/app.py. 所以, <strong>CMD [&quot;python&quot;, &quot;app.py&quot;] 等价于 &quot;docker run python app.py&quot;</strong> .</p> <p>另外, 在使用 Dockerfile 时, 可能还会看到一个叫作 <strong>ENTRYPOINT</strong> 的原语. 实际上, 它和 CMD 都是 Docker 容器进程启动所必需的参数, 完整执行格式是: &quot;ENTRYPOINT CMD&quot;.</p> <p>但默认情况下, Docker 会为你提供一个<strong>隐含的 ENTRYPOINT, 即: /bin/sh -c</strong>. 所以在不指定 ENTRYPOINT 时, 比如在这个例子里, 实际上运行在容器里的完整进程是:  <strong>/bin/sh -c &quot;python app.py&quot;</strong> , 即 <strong>CMD 的内容就是 ENTRYPOINT 的参数</strong>.</p> <blockquote><p>备注: 基于以上原因, 后面会统一称 Docker 容器的启动进程为 ENTRYPOINT, 而不是 CMD.</p></blockquote> <p>需要注意的是, Dockerfile 里的原语并不都是指对容器内部的操作. 就比如 ADD, 它指的是把当前目录(即 Dockerfile 所在的目录)里的文件, 复制到指定容器内的目录当中.</p> <p>读懂这个 Dockerfile 之后, 再把上述内容, 保存到当前目录里一个名叫 &quot;<strong>Dockerfile</strong>&quot; 的文件中:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span>
Dockerfile 
app.py
requirements.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>接下来就可以<strong>让 Docker 制作这个镜像</strong>了, 在当前目录执行:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> build <span class="token parameter variable">-t</span> helloworld <span class="token builtin class-name">.</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中, -t 的作用是给这个镜像加一个 Tag, 即: 起一个好听的名字. <mark><strong>docker build 会自动加载当前目录下的 Dockerfile 文件, 然后按照顺序, 执行文件中的原语. 而这个过程, 实际上可以等同于 Docker 使用基础镜像启动了一个容器, 然后在容器中依次执行 Dockerfile 中的原语</strong></mark>.</p> <p><mark><strong>需要注意的是, Dockerfile 中的每个原语执行后, 都会生成一个对应的镜像层</strong></mark>. 即使原语本身并没有明显地修改文件的操作(比如, ENV 原语), 它对应的层也会存在. 只不过在外界看来, 这个层是<strong>空的</strong>.</p> <p>docker build 操作完成后, 可以通过 docker images 命令查看结果:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> image <span class="token function">ls</span>

REPOSITORY            TAG                 IMAGE ID
helloworld         latest              653287cdf998
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>通过这个镜像 ID, 就可以使用前一节讲过的方法, 查看这些新增的层在 AuFS 路径下对应的文件和目录了.</p> <p>**接下来, 使用这个镜像, 通过 docker run 命令启动容器: **</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-p</span> <span class="token number">4000</span>:80 helloworld
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在这一句命令中, 镜像名 helloworld 后面, 什么都不用写, 因为在 Dockerfile 中已经<strong>指定了 CMD</strong>. 否则<strong>就得把进程的启动命令加在后面</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-p</span> <span class="token number">4000</span>:80 helloworld python app.py
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>容器启动之后, 可以使用 docker ps 命令看到:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> <span class="token function">ps</span>
CONTAINER ID        IMAGE               COMMAND             CREATED
4ddf4638572d        helloworld       <span class="token string">&quot;python app.py&quot;</span>     <span class="token number">10</span> seconds ago
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>同时已经通过 -p 4000:80 告诉了 Docker, 请<strong>把容器内的 80 端口映射在宿主机的 4000 端口</strong>上.</p> <p>这样做的目的是, 只要访问宿主机的 4000 端口, 就可以看到容器里应用返回的结果:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">curl</span> http://localhost:4000
<span class="token operator">&lt;</span>h<span class="token operator"><span class="token file-descriptor important">3</span>&gt;</span>Hello World<span class="token operator">!</span><span class="token operator">&lt;</span>/h<span class="token operator"><span class="token file-descriptor important">3</span>&gt;</span><span class="token operator">&lt;</span>b<span class="token operator">&gt;</span>Hostname:<span class="token operator">&lt;</span>/b<span class="token operator">&gt;</span> 4ddf4638572d<span class="token operator">&lt;</span>br/<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>否则, 就得先用 <strong>docker inspect 命令</strong>查看容器的 IP 地址, 然后访问 <code>http://&lt; 容器 IP 地址 &gt;:80</code>​ 才可以看到容器内应用的返回.</p> <p>至此, 已经使用容器完成了一个应用的开发与测试, 如果现在想要把这个容器的镜像上传到 DockerHub 上分享给更多的人, 要怎么做呢?</p> <p>为了能够上传镜像, 首先需要注册一个 Docker Hub 账号, 然后使用 docker login 命令登录.</p> <p>接下来, 要用 docker tag 命令给容器镜像起一个完整的名字:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> tag helloworld geektime/helloworld:v1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>注意: 自己做实验时, 请将 &quot;geektime&quot; 替换成自己的 Docker Hub 账户名称, 比如 zhangsan/helloworld:v1</p></blockquote> <p>其中, geektime 是我在 Docker Hub 上的用户名, 它的&quot;学名&quot;叫镜像仓库(Repository); &quot;/&quot;后面的 helloworld 是这个镜像的名字, 而 &quot;v1&quot; 则是这个镜像分配的版本号.</p> <p>然后执行 docker push:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> push geektime/helloworld:v1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样就可以把这个镜像上传到 Docker Hub 上了.</p> <p>此外还可以使用 docker commit 指令, 把一个<strong>正在运行的容器, 直接提交为一个镜像</strong>. 一般来说, 需要这么操作原因是: <strong>这个容器运行起来后, 又在里面做了一些操作, 并且要把操作结果保存到镜像里</strong>, 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> 4ddf4638572d /bin/sh
<span class="token comment"># 在容器内部新建了一个文件</span>
root@4ddf4638572d:/app<span class="token comment"># touch test.txt</span>
root@4ddf4638572d:/app<span class="token comment"># exit</span>
 
<span class="token comment"># 将这个新建的文件提交到镜像中保存</span>
$ <span class="token function">docker</span> commit 4ddf4638572d geektime/helloworld:v2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这里使用了 <strong>docker exec 命令进入到了容器</strong>当中. 在了解了 Linux Namespace 的隔离机制后, 你应该会很自然地想到一个问题: docker exec 是怎么做到进入容器里的呢?</p> <p>实际上, Linux Namespace 创建的隔离空间虽然看不见摸不着, <strong>但一个进程的 Namespace 信息在宿主机上是确确实实存在的, 并且是以一个文件的方式存在</strong>.</p> <p>比如, 通过如下指令, 可以看到当前正在运行的 Docker 容器的进程号(PID)是 25686:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> inspect <span class="token parameter variable">--format</span> <span class="token string">'{{ .State.Pid }}'</span>  4ddf4638572d
<span class="token number">25686</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这时, 可以通过<strong>查看宿主机的 proc 文件, 看到这个 25686 进程的所有 Namespace 对应的文件</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> <span class="token parameter variable">-l</span>  /proc/25686/ns
total <span class="token number">0</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 cgroup -<span class="token operator">&gt;</span> cgroup:<span class="token punctuation">[</span><span class="token number">4026531835</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 ipc -<span class="token operator">&gt;</span> ipc:<span class="token punctuation">[</span><span class="token number">4026532278</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 mnt -<span class="token operator">&gt;</span> mnt:<span class="token punctuation">[</span><span class="token number">4026532276</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 net -<span class="token operator">&gt;</span> net:<span class="token punctuation">[</span><span class="token number">4026532281</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 pid -<span class="token operator">&gt;</span> pid:<span class="token punctuation">[</span><span class="token number">4026532279</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 pid_for_children -<span class="token operator">&gt;</span> pid:<span class="token punctuation">[</span><span class="token number">4026532279</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 user -<span class="token operator">&gt;</span> user:<span class="token punctuation">[</span><span class="token number">4026531837</span><span class="token punctuation">]</span>
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 uts -<span class="token operator">&gt;</span> uts:<span class="token punctuation">[</span><span class="token number">4026532277</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到, <mark><strong>一个进程的每种 Linux Namespace, 都在它对应的 /proc/[进程号]/ns 下有一个对应的虚拟文件, 并且链接到一个真实的 Namespace 文件上</strong></mark>.</p> <p>有了这样一个可以 &quot;hold 住&quot; 所有 Linux Namespace 的文件, 就可以对 Namespace 做一些很有意义事情了, 比如: 加入到一个已经存在的 Namespace 当中.</p> <p>**这也就意味着: **​<mark><strong>一个进程, 可以选择加入到某个进程已有的 Namespace 当中, 从而达到&quot;进入&quot;这个进程所在容器的目的, 这正是 docker exec 的实现原理</strong></mark>​ **. **</p> <p>而这个操作所依赖的, 乃是一个名叫 <strong>setns()</strong>  的 Linux 系统调用. 它的调用方法, 可以用如下一段小程序来说明:</p> <div class="language-c line-numbers-mode"><pre class="language-c"><code><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">_GNU_SOURCE</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;fcntl.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sched.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdlib.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span>
 
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">errExit</span><span class="token expression"><span class="token punctuation">(</span>msg<span class="token punctuation">)</span> <span class="token keyword">do</span> <span class="token punctuation">{</span> <span class="token function">perror</span><span class="token punctuation">(</span>msg<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span> <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></span></span>
 
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">int</span> fd<span class="token punctuation">;</span>
  
    fd <span class="token operator">=</span> <span class="token function">open</span><span class="token punctuation">(</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> O_RDONLY<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">setns</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token function">errExit</span><span class="token punctuation">(</span><span class="token string">&quot;setns&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token function">execvp</span><span class="token punctuation">(</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
    <span class="token function">errExit</span><span class="token punctuation">(</span><span class="token string">&quot;execvp&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>这段代码功能非常简单: 它一共接收两个参数, 第一个参数是 argv[1], 即<strong>当前进程要加入的 Namespace 文件的路径</strong>, 比如 /proc/25686/ns/net; 而第二个参数, 则是<strong>要在这个 Namespace 里运行的进程</strong>, 比如 /bin/bash.</p> <p>这段代码的的核心操作, 则是<strong>通过 open() 系统调用打开了指定的 Namespace 文件, 并把这个文件的描述符 fd 交给 setns() 使用. 在 setns() 执行后, 当前进程就加入了这个文件对应的 Linux Namespace 当中了</strong>.</p> <p>现在可以编译执行一下这个程序, 加入到容器进程(PID=25686)的 Network Namespace 中:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ gcc <span class="token parameter variable">-o</span> set_ns set_ns.c 
$ ./set_ns /proc/25686/ns/net /bin/bash 
$ <span class="token function">ifconfig</span>
eth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0
	   collisions:0 txqueuelen:0 
          RX bytes:976 <span class="token punctuation">(</span><span class="token number">976.0</span> B<span class="token punctuation">)</span>  TX bytes:796 <span class="token punctuation">(</span><span class="token number">796.0</span> B<span class="token punctuation">)</span>
 
lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
	  collisions:0 txqueuelen:1000 
          RX bytes:0 <span class="token punctuation">(</span><span class="token number">0.0</span> B<span class="token punctuation">)</span>  TX bytes:0 <span class="token punctuation">(</span><span class="token number">0.0</span> B<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>正如上所示, 当执行 ifconfig 命令查看<strong>网络设备</strong>时, 会发现能看到的网卡&quot;变少&quot;了: 只有两个. 而宿主机则至少有四个网卡. 这是怎么回事呢?</p> <p>实际上, 在 setns() 之后看到的这两个网卡, 正是<strong>在前面启动的 Docker 容器里的网卡</strong>. 也就是说, <strong>新创建的这个 /bin/bash 进程, 由于加入了该容器进程(PID=25686)的 Network Namepace, 它看到的网络设备与这个容器里是一样的, 即: /bin/bash 进程的网络设备视图, 也被修改了</strong>.</p> <p>而一旦一个进程加入到了另一个 Namespace 当中, 在宿主机的 Namespace 文件上, 也会有所体现.</p> <p>在宿主机上, 可以用 ps 指令找到这个 <strong>set_ns 程序执行的 /bin/bash 进程</strong>, 其真实的 PID 是 28499:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 在宿主机上</span>
<span class="token function">ps</span> aux <span class="token operator">|</span> <span class="token function">grep</span> /bin/bash
root     <span class="token number">28499</span>  <span class="token number">0.0</span>  <span class="token number">0.0</span> <span class="token number">19944</span>  <span class="token number">3612</span> pts/0    S    <span class="token number">14</span>:15   <span class="token number">0</span>:00 /bin/bash
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这时如果按照前面介绍过的方法, 查看一下这个 PID=28499 的进程的 <strong>Namespace</strong>, 就会发现这样一个事实:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> <span class="token parameter variable">-l</span> /proc/28499/ns/net
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:18 /proc/28499/ns/net -<span class="token operator">&gt;</span> net:<span class="token punctuation">[</span><span class="token number">4026532281</span><span class="token punctuation">]</span>
 
$ <span class="token function">ls</span> <span class="token parameter variable">-l</span>  /proc/25686/ns/net
lrwxrwxrwx <span class="token number">1</span> root root <span class="token number">0</span> Aug <span class="token number">13</span> <span class="token number">14</span>:05 /proc/25686/ns/net -<span class="token operator">&gt;</span> net:<span class="token punctuation">[</span><span class="token number">4026532281</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>在 /proc/[PID]/ns/net 目录下, 这个 PID=28499 进程, 与前面的 Docker 容器进程(PID=25686)指向的 Network Namespace 文件完全一样. 这说明<strong>这两个进程, 共享了这个名叫 net:[4026532281] 的 Network Namespace</strong>.</p> <p>此外, <strong>Docker 还专门提供了一个参数, 可以让你启动一个容器并 &quot;加入&quot; 到另一个容器的 Network Namespace 里, 这个参数就是 -net</strong>, 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token parameter variable">--net</span> container:4ddf4638572d busybox <span class="token function">ifconfig</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样, 新启动的这个容器, 就会直接加入到 ID=4ddf4638572d 的容器, 也就是前面的创建的 Python 应用容器(PID=25686)的 Network Namespace 中. 所以这里 ifconfig 返回的网卡信息, 跟前面那个小程序返回的结果一模一样, 你也可以尝试一下.</p> <p><strong>==而如果指定 –net=host, 就意味着这个容器不会为进程启用 Network Namespace==</strong>. 这就意味着, 这个容器拆除了 Network Namespace 的 &quot;隔离墙&quot;, 所以<strong>它会和宿主机上的其他普通进程一样, 直接共享宿主机的网络栈. 这就为容器直接操作和使用宿主机网络提供了一个渠道</strong>.</p> <p>**转了一个大圈子, 其实是为了详细解读 docker exec 这个操作背后, Linux Namespace 更具体的工作原理. 这种通过操作系统进程相关的知识, 逐步剖析 Docker 容器的方法, 是理解容器的一个关键思路, 希望你一定要掌握. **</p> <p>现在, 再一起回到前面提交镜像的操作 docker commit 上来吧.</p> <p>docker commit, 实际上就是在容器运行起来后, 把<strong>最上层的 &quot;可读写层&quot;, 加上原先容器镜像的只读层, 打包组成了一个新的镜像</strong>. 当然, 下面这些只读层在宿主机上是<strong>共享</strong>的, 不会占用额外的空间.</p> <p>而由于使用了联合文件系统, 你在容器里对镜像 rootfs 所做的任何修改, 都会被操作系统<strong>先复制到这个可读写层, 然后再修改</strong>. 这就是所谓的: Copy-on-Write.</p> <p>而正如前所说, <strong>Init 层的存在, 就是为了避免你执行 docker commit 时, 把 Docker 自己对 /etc/hosts 等文件做的修改, 也一起提交掉</strong>.</p> <p>有了新的镜像, 就可以把它推送到 Docker Hub 上了:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> push geektime/helloworld:v2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>你可能还会有这样的问题: 在企业内部, 能不能也搭建一个跟 Docker Hub 类似的镜像上传系统呢?</p> <p>当然可以, 这个统一存放镜像的系统, 就叫作 <strong>Docker Registry</strong>. 这可以查看<a href="https://docs.docker.com/registry/" target="_blank" rel="noopener noreferrer">Docker 的官方文档<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 以及<a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener noreferrer">VMware 的 Harbor 项目<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>最后再来讲解一下 Docker 项目另一个重要的内容: <strong>Volume(数据卷)</strong> .</p> <p>前面已经介绍过, 容器技术使用了 <strong>rootfs 机制和 Mount Namespace, 构建出了一个同宿主机完全隔离开的文件系统环境</strong>. 这时候, 就需要考虑这样两个问题:</p> <ol><li>容器里进程新建的文件, 怎么才能让<strong>宿主机</strong>获取到?</li> <li>宿主机上的文件和目录, 怎么才能让容器里的进程访问到?</li></ol> <p>这正是 Docker Volume 要解决的问题: <mark><strong>Volume 机制, 允许你将宿主机上指定的目录或者文件, 挂载到容器里面进行读取和修改操作</strong></mark>​ **. **</p> <p>在 Docker 项目里, 它支持<strong>两种 Volume 声明方式, 可以把宿主机目录挂载进容器的 /test 目录当中</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-v</span> /test <span class="token punctuation">..</span>.
$ <span class="token function">docker</span> run <span class="token parameter variable">-v</span> /home:/test <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>而这两种声明方式的本质, 实际上是相同的: <strong>都是把一个宿主机的目录挂载进了容器的 /test 目录</strong>.</p> <p>只不过在第一种情况下, 由于并没有显示声明宿主机目录, <strong>那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data, 然后把它挂载到容器的 /test 目录上. 而在第二种情况下, Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上</strong>.</p> <p>那么, Docker 又是如何做到把一个宿主机上的目录或者文件, 挂载到容器里面去呢? 难道又是 Mount Namespace 的黑科技吗? 实际上并不需要这么麻烦.</p> <p>前面已经介绍过, <strong>当容器进程被创建之后, 尽管开启了 Mount Namespace, 但是在它执行 chroot(或者 pivot_root)之前, 容器进程一直可以看到宿主机上的整个文件系统</strong>.</p> <p>而宿主机上的文件系统, 也自然包括了要使用的<strong>容器镜像</strong>. 这个镜像的各个层, 保存在  <strong>/var/lib/docker/aufs/diff 目录</strong>下, 在容器进程启动后, 它们会被<strong>联合挂载在 /var/lib/docker/aufs/mnt/ 目录中, 这样容器所需的 rootfs 就准备好了</strong>.</p> <p><strong>所以只需要在 rootfs 准备好之后, 在执行 chroot 之前, 把 Volume 指定的宿主机目录(比如 /home 目录), 挂载到指定的容器目录(比如 /test 目录)在宿主机上对应的目录(即 /var/lib/docker/aufs/mnt/[可读写层 ID]/test)上, 这个 Volume 的挂载工作就完成了.</strong></p> <p>更重要的是, 由于执行这个挂载操作时, &quot;容器进程&quot;已经创建了, 也就意味着此时 Mount Namespace 已经开启了. 所以, 这个挂载事件<strong>只在这个容器里可见</strong>. 因此在宿主机上, 是看不见容器内部的这个挂载点的. 这就<mark><strong>保证了容器的隔离性不会被 Volume 打破</strong></mark>.</p> <blockquote><p>注意: 这里提到的 &quot;容器进程&quot;, 是 Docker 创建的一个容器初始化进程 (dockerinit), 而不是应用进程 (ENTRYPOINT + CMD). dockerinit 会负责完成根目录的准备, 挂载设备和目录, 配置 hostname 等一系列需要在容器内进行的初始化操作. 最后, 它通过 execv() 系统调用, 让应用进程取代自己, 成为容器里的 PID=1 的进程.</p></blockquote> <p>而这里要使用到的<strong>挂载技术</strong>, 就是 Linux 的<mark><strong>绑定挂载(bind mount)机制</strong></mark>. 它的主要作用就是, 允许你<strong>将一个目录或者文件, 而不是整个设备, 挂载到一个指定的目录上</strong>. 并且, <strong>这时你在该挂载点上进行的任何操作, 只是发生在被挂载的目录或者文件上, 而原挂载点的内容则会被隐藏起来且不受影响</strong>.</p> <p>如果你了解 Linux 内核的话, 就会明白, <strong>绑定挂载实际上是一个 inode 替换的过程</strong>. 在 Linux 操作系统中, inode 可以理解为存放文件内容的&quot;对象&quot;, 而 dentry, 也叫目录项, 就是访问这个 inode 所使用的&quot;指针&quot;.</p> <p><img src="/img/ee090e202de9a17c13618d128f39b386-20230731162150-wh3nco6.png" alt=""></p> <p>正如上图所示, mount --bind /home /test, 会<strong>将 /home 挂载到 /test 上</strong>. 其实相当于<mark><strong>将 /test 的 dentry, 重定向到了 /home 的 inode</strong></mark>. 这样当修改 /test 目录时, 实际修改的是 /home 目录的 inode. 这也就是为何, 一旦执行 umount 命令, /test 目录原先的内容就会恢复: <strong>因为修改真正发生在的, 是 /home 目录里</strong>.</p> <p>**所以, 在一个正确的时机, 进行一次绑定挂载, Docker 就可以成功地将一个宿主机上的目录或文件, 不动声色地挂载到容器中. **</p> <p>这样, 进程在容器里对这个 /test 目录进行的所有操作, <strong>都实际发生在宿主机的对应目录</strong>(比如, /home, 或者 /var/lib/docker/volumes/[VOLUME_ID]/_data)里, 而不会影响容器镜像的内容.</p> <p>那么, 这个 /test 目录里的内容, 既然挂载在容器 rootfs 的可读写层, 它会不会被 docker commit 提交掉呢? <strong>也不会</strong>.</p> <p>这个原因其实前面已经提到过. <strong>容器的镜像操作, 比如 docker commit, 都是发生在宿主机空间的. 而由于 Mount Namespace 的隔离作用, 宿主机并不知道这个绑定挂载的存在</strong>. 所以在宿主机看来, 容器中可读写层的 /test 目录(/var/lib/docker/aufs/mnt/[可读写层 ID]/test), **始终是空的. **</p> <p>不过, 由于 Docker 一开始还是要创建 /test 这个目录作为挂载点, 所以执行了 docker commit 之后, 你会发现新产生的镜像里, 会<strong>多出来一个空的 /test 目录</strong>. 毕竟, 新建目录操作, 又不是挂载操作, Mount Namespace 对它可起不到&quot;障眼法&quot;的作用.</p> <p>结合以上的讲解, 现在来亲自验证一下:</p> <p>首先, 启动一个 helloworld 容器, 给它声明一个 Volume, 挂载在容器里的 /test 目录上:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-v</span> /test helloworld
cf53b766fa6f
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>容器启动之后, 来查看一下这个 Volume 的 ID:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> volume <span class="token function">ls</span>
DRIVER              VOLUME NAME
<span class="token builtin class-name">local</span>               cb1c2f7221fa9b0971cc35f68aa1034824755ac44a034c0c0a1dd318838d3a6d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后, 使用这个 ID, 可以找到<strong>它在 Docker 工作目录下的 volumes 路径</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /var/lib/docker/volumes/cb1c2f7221fa/_data/
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个 _data 文件夹, 就是这个<strong>容器的 Volume 在宿主机上对应的临时目录</strong>了.</p> <p>接下来, 在容器的 Volume 里, 添加一个文件 text.txt:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> cf53b766fa6f /bin/sh
<span class="token builtin class-name">cd</span> test/
<span class="token function">touch</span> text.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这时, 再回到宿主机, 就会发现 text.txt 已经<strong>出现在了宿主机上对应的临时目录</strong>里:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /var/lib/docker/volumes/cb1c2f7221fa/_data/
text.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可是, 如果你在宿主机上<strong>查看该容器的可读写层, 虽然可以看到这个 /test 目录, 但其内容是空的</strong>(关于如何找到这个 AuFS 文件系统的路径, 请参考前面的内容):</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /var/lib/docker/aufs/mnt/6780d0778b8a/test
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以确认, <strong>容器 Volume 里的信息, 并不会被 docker commit 提交掉; 但这个挂载点目录 /test 本身, 则会出现在新的镜像当中</strong>.</p> <p>以上内容, 就是 Docker Volume 的核心原理了.</p> <blockquote><p>总结</p></blockquote> <p>本节用了一个非常经典的 Python 应用作为案例, 讲解了 Docker 容器使用的主要场景. 熟悉了这些操作, 也就基本上摸清了 Docker 容器的核心功能.</p> <p>本节着重介绍了如何使用 Linux Namespace, Cgroups, 以及 rootfs 的知识, 对容器进行了一次庖丁解牛似的解读.</p> <p>借助这种思考问题的方法, 最后的 Docker 容器, 实际上就可以用下面这个 &quot;全景图&quot; 描述出来:</p> <p><img src="/img/3d75f6b9b1e9cd9f33d1e1e00cd29e66-20230731162150-6v4abts.png" alt=""></p> <p>这个容器进程 &quot;python app.py&quot;, 运行在由 Linux Namespace 和 Cgroups 构成的隔离环境里; 而它运行<strong>所需要的各种文件, 比如 python, app.py, 以及整个操作系统文件, 则由多个联合挂载在一起的 rootfs 层提供</strong>.</p> <p>这些 rootfs 层的最下层, 是来自 Docker 镜像的只读层.</p> <p>在只读层之上, 是 Docker 自己添加的 <strong>Init 层, 用来存放被临时修改过的 /etc/hosts 等文件</strong>.</p> <p>而 rootfs 的最上层是一个<strong>可读写层, 它以 Copy-on-Write 的方式存放任何对只读层的修改, 容器声明的 Volume 的挂载点, 也出现在这一层</strong>.</p> <p>通过这样的剖析, 对于容器技术, 是不是感觉清晰了很多呢?</p> <h4 id="_09-从容器到容器云-谈谈kubernetes的本质"><a href="#_09-从容器到容器云-谈谈kubernetes的本质" class="header-anchor">#</a> 09 | 从容器到容器云:谈谈Kubernetes的本质</h4> <p>本节的主题是: 从容器到容器云, 谈谈 Kubernetes 的本质.</p> <p>在前面的四节中, 以 Docker 项目为例, 一步步剖析了 Linux 容器的具体实现方式. 通过这些讲解应该能够明白: <mark><strong>一个 &quot;容器&quot;, 实际上是一个由 Linux Namespace, Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境</strong></mark>.</p> <p>从这个结构中不难看出, 一个正在运行的 Linux 容器, 其实可以被 &quot;一分为二&quot; 地看待:</p> <ol><li><mark>**一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs, 这一部分称为&quot;容器镜像&quot;(Container Image), 是容器的静态视图; **</mark></li> <li><mark>**一个由 Namespace + Cgroups 构成的隔离环境, 这一部分称为&quot;容器运行时&quot;(Container Runtime), 是容器的动态视图. **</mark></li></ol> <p>更进一步地说, 作为一名开发者, 我们并不关心容器运行时的差异. 因为, 在整个 &quot;开发-测试-发布&quot; 的流程中, 真正承载着容器信息进行传递的, <strong>是容器镜像, 而不是容器运行时</strong>.</p> <p>这个重要假设, 正是容器技术圈在 Docker 项目成功后不久, 就迅速走向了 &quot;容器编排&quot; 这个 &quot;上层建筑&quot; 的主要原因: <strong>作为一家云服务商或者基础设施提供商, 只要能够将用户提交的 Docker 镜像以容器的方式运行起来, 就能成为这个非常热闹的容器生态图上的一个承载点, 从而将整个容器技术栈上的价值, 沉淀在这个节点上</strong>.</p> <p>更重要的是, 只要从这个承载点向 Docker 镜像制作者和使用者方向回溯, 整条路径上的各个服务节点, 比如 CI/CD, 监控, 安全, 网络, 存储等等, 都有可以发挥和盈利的余地. 这个逻辑, 正是所有云计算提供商如此热衷于容器技术的重要原因: <strong>通过容器镜像, 它们可以和潜在用户(即, 开发者)直接关联起来</strong>.</p> <p>从一个开发者和单一的容器镜像, 到无数开发者和庞大的容器集群, 容器技术实现了从 &quot;容器&quot; 到 &quot;容器云&quot; 的飞跃, 标志着它真正得到了市场和生态的认可.</p> <p>这样, **容器就从一个开发者手里的小工具, 一跃成为了云计算领域的绝对主角; 而能够定义容器组织和管理规范的 &quot;容器编排&quot; 技术, 则当仁不让地坐上了容器技术领域的 &quot;头把交椅&quot;. **</p> <p>这其中, 最具代表性的容器编排工具, 当属 Docker 公司的 <strong>Compose+Swarm 组合</strong>, 以及 Google 与 RedHat 公司共同主导的 <strong>Kubernetes 项目</strong>. 前面的四篇预习文章中, 已经对这两个开源项目做了详细地剖析和评述. 所以本节会专注于本专栏的主角 <strong>Kubernetes 项目, 谈一谈它的设计与架构</strong>.</p> <p>跟很多基础设施领域先有工程实践, 后有方法论的发展路线不同, Kubernetes 项目的理论基础则要比工程实践走得靠前得多, 这当然要归功于 Google 公司在 2015 年 4 月发布的 <strong>Borg 论文</strong>了. Borg 系统一直以来都被誉为 Google 公司内部最强大的&quot;秘密武器&quot;. 虽然略显夸张, 但这个说法倒不算是吹牛. 因为相比于 Spanner, BigTable 等相对上层的项目, Borg 要承担的责任, 是<strong>承载 Google 公司整个基础设施的核心依赖</strong>. 在 Google 公司已经公开发表的基础设施体系论文中, Borg 项目当仁不让地位居整个基础设施技术栈的最底层.</p> <p><img src="/img/9ac1b118f8d83dfe4f888cd7728e6c91-20230731162150-3j8hkk2.png" alt=""></p> <p>图片来源: <a href="http://malteschwarzkopf.de/research/assets/google-stack.pdf" target="_blank" rel="noopener noreferrer">Malte Schwarzkopf. &quot;Operating system support for warehouse-scale computing&quot;. PhD thesis. University of Cambridge Computer Laboratory (to appear), 2015, Chapter 2.<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>上面这幅图, 来自于 Google Omega 论文的第一作者的博士毕业论文. 它描绘了当时 Google 已经公开发表的整个基础设施栈. 在这个图里, 你既可以找到 MapReduce, BigTable 等知名项目, 也能看到 Borg 和它的继任者 Omega 位于整个技术栈的最底层.</p> <p>正是由于这样的定位, Borg 可以说是 Google 最不可能开源的一个项目. 而幸运地是, 得益于 Docker 项目和容器技术的风靡, 它却终于得以以另一种方式与开源社区见面, 这个方式就是 <strong>Kubernetes 项目</strong>.</p> <p>所以, 相比于 &quot;小打小闹&quot; 的 Docker 公司, &quot;旧瓶装新酒&quot; 的 Mesos 社区, <strong>Kubernetes 项目从一开始就比较幸运地站上了一个他人难以企及的高度</strong>: 在它的成长阶段, 这个项目<strong>每一个核心特性的提出, 几乎都脱胎于 Borg/Omega 系统的设计与经验</strong>. 更重要的是, 这些特性在开源社区落地的过程中, 又在整个社区的合力之下得到了极大的改进, 修复了很多当年遗留在 Borg 体系中的缺陷和问题.</p> <p>所以, 尽管在发布之初被批评是 &quot;曲高和寡&quot;, 但是在逐渐觉察到 Docker 技术栈的 &quot;稚嫩&quot; 和 Mesos 社区的 &quot;老迈&quot; 之后, 这个社区很快就明白了: <strong>Kubernetes 项目在 Borg 体系的指导下, 体现出了一种独有的 &quot;先进性&quot; 与 &quot;完备性&quot;, 而这些特质才是一个基础设施领域开源项目赖以生存的核心价值</strong>.</p> <p>为了更好地理解这两种特质, 不妨从 Kubernetes 的<strong>顶层设计</strong>说起.</p> <p><mark><strong>首先, Kubernetes 项目要解决的问题是什么? 编排? 调度? 容器云? 还是集群管理?</strong></mark></p> <p>实际上, 这个问题到目前为止都没有固定的答案. 因为在不同的发展阶段, Kubernetes 需要着重解决的问题是不同的.</p> <p>但对于大多数用户来说, 他们希望 Kubernetes 项目带来的体验是确定的:  <strong>(1)现在我有了应用的容器镜像, 请帮我在一个给定的集群上把这个应用运行起来. (2) 更进一步地说, 我还希望 Kubernetes 能给我提供路由网关, 水平扩展, 监控, 备份, 灾难恢复等一系列运维能力</strong>.</p> <p>等一下, 这些功能听起来好像有些耳熟? <strong>这不就是经典 PaaS(比如, Cloud Foundry)项目的能力吗</strong>? 而且, 有了 Docker 之后, 根本不需要什么 Kubernetes, PaaS, 只要使用 Docker 公司的 Compose+Swarm 项目, 就完全可以很方便地 DIY 出这些功能了!</p> <p>所以说, <strong>如果 Kubernetes 项目只是停留在拉取用户镜像, 运行容器, 以及提供常见的运维功能的话, 那么别说跟 &quot;原生&quot; 的 Docker Swarm 项目竞争了, 哪怕跟经典的 PaaS 项目相比也难有什么优势可言</strong>.</p> <p>而实际上, 在定义核心功能的过程中, Kubernetes 项目正是依托着 Borg 项目的理论优势, 才在短短几个月内迅速站稳了脚跟, 进而确定了一个如下图所示的全局架构:</p> <p><img src="/img/8d7e299c32e52e58b8ded7fce3a4e76e-20230731162150-rnsnnu8.png" alt=""></p> <p>可以看到, Kubernetes 项目的架构, 跟它的原型项目 Borg 非常类似, <strong>都由 Master 和 Node 两种节点组成, 而这两种角色分别对应着控制节点和计算节点</strong>.</p> <p>**其中, 控制节点, 即 Master 节点, 由三个紧密协作的独立组件组合而成, 它们分别是负责 API 服务的 kube-apiserver, 负责调度的 kube-scheduler, 以及负责容器编排的 kube-controller-manager. 整个集群的持久化数据, 则由 kube-apiserver 处理后保存在 Etcd 中. **</p> <p>而计算节点上最核心的部分, 则是一个叫作 <strong>kubelet</strong> 的组件. **在 Kubernetes 项目中, **​<mark><strong>kubelet 主要负责同容器运行时(比如 Docker 项目)打交道</strong></mark>. 而这个交互所依赖的, 是一个称作 <strong>CRI(Container Runtime Interface)的远程调用接口, 这个接口定义了容器运行时的各项核心操作, 比如: 启动一个容器需要的所有参数</strong>.</p> <p><mark><strong>这也是为何 Kubernetes 项目并不关心你部署的是什么容器运行时, 使用的什么技术实现, 只要你的这个容器运行时能够运行标准的容器镜像, 它就可以通过实现 CRI 接入到 Kubernetes 项目当中.</strong></mark>​ ** 而具体的容器运行时, 比如 Docker 项目, 则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互, 即: 把 CRI 请求翻译成对 Linux 操作系统的调用(操作 Linux Namespace 和 Cgroups 等). **</p> <p><strong>此外, kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互</strong>. 这个插件是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件, 也是基于 Kubernetes 项目进行机器学习训练, 高性能作业支持等工作必须关注的功能.</p> <p>而 **kubelet 的另一个重要功能, **​<mark><strong>则是调用网络插件和存储插件为容器配置网络和持久化存储</strong></mark>. 这两个插件与 kubelet 进行交互的接口, 分别是 <strong>CNI</strong>(Container Networking Interface)和 <strong>CSI</strong>(Container Storage Interface).</p> <p>实际上, kubelet 这个奇怪的名字, 来自于 Borg 项目里的同源组件 Borglet. 不过, 如果你浏览过 Borg 论文的话就会发现, 这个命名方式可能是 kubelet 组件与 Borglet 组件的唯一相似之处. 因为 Borg 项目, 并不支持这里所讲的容器技术, 而只是简单地使用了 Linux Cgroups 对进程进行限制. 这就意味着, 像 Docker 这样的 &quot;容器镜像&quot; 在 Borg 中是不存在的, Borglet 组件也自然不需要像 kubelet 这样考虑如何同 Docker 进行交互, 如何对容器镜像进行管理的问题, 也不需要支持 CRI, CNI, CSI 等诸多容器技术接口.</p> <p>**可以说, kubelet 完全就是为了实现 Kubernetes 项目对容器的管理能力而重新实现的一个组件, 与 Borg 之间并没有直接的传承关系. **</p> <blockquote><p>备注: 虽然不使用 Docker, 但 Google 内部确实在使用一个包管理工具, 名叫 Midas Package Manager (MPM), 其实它可以部分取代 Docker 镜像的角色.</p></blockquote> <p>那么, Borg 对于 Kubernetes 项目的<strong>指导作用</strong>又体现在哪里呢?</p> <p>答案是 <strong>Master 节点</strong>. 虽然在 Master 节点的实现细节上 Borg 项目与 Kubernetes 项目不尽相同, 但它们的出发点却高度一致, 即: <strong>如何编排, 管理, 调度用户提交的作业</strong>? 所以 Borg 项目完全可以把 Docker 镜像看做是一种新的应用打包方式. 这样 Borg 团队过去在大规模作业管理与编排上的经验就可以直接&quot;套&quot;在 Kubernetes 项目上了.</p> <p>这些经验最主要的表现就是, **从一开始, Kubernetes 项目就没有像同时期的各种 &quot;容器云&quot; 项目那样, 把 Docker 作为整个架构的核心, **​<mark><strong>而仅仅把它作为最底层的一个容器运行时实现</strong></mark>​ **. **</p> <p>而 Kubernetes 项目<strong>要着重解决的问题</strong>, 则来自于 Borg 的研究人员在论文中提到的一个非常重要的观点:</p> <blockquote><p><mark>运行在大规模集群中的各种任务之间, 实际上存在着各种各样的关系. 这些关系的处理, 才是作业编排和管理系统最困难的地方. </mark></p></blockquote> <p>事实也正是如此. 其实, 这种任务与任务之间的关系, 在平常的各种技术场景中随处可见. 比如, <strong>一个 Web 应用与数据库之间的访问关系, 一个负载均衡器和它的后端服务之间的代理关系, 一个门户应用与授权组件之间的调用关系</strong>. 更进一步地说, 同属于一个服务单位的不同功能之间, 也完全可能存在这样的关系. 比如一个 Web 应用与日志搜集组件之间的文件交换关系.</p> <p>而在容器技术普及之前, 传统虚拟机环境对这种关系的处理方法都是比较 &quot;粗粒度&quot; 的. 你会经常发现<strong>很多功能并不相关的应用被一股脑儿地部署在同一台虚拟机中</strong>, 只是因为它们之间偶尔会互相发起几个 HTTP 请求. 更常见的情况则是, 一个应用被部署在虚拟机里之后, 你还得手动维护很多跟它协作的守护进程(Daemon), 用来处理它的日志搜集, 灾难恢复, 数据备份等辅助工作.</p> <p>但容器技术出现以后, 就不难发现, <strong>在&quot;功能单位&quot;的划分上, 容器有着独一无二的&quot;细粒度&quot;优势: 毕竟容器的本质, 只是一个进程而已</strong>. 也就是说只要你愿意, 那些<strong>原先拥挤在同一个虚拟机里的各个应用, 组件, 守护进程, 都可以被分别做成镜像, 然后运行在一个个专属的容器中. 它们之间互不干涉, 拥有各自的资源配额, 可以被调度在整个集群里的任何一台机器上</strong>. 而这正是一个 PaaS 系统最理想的工作状态, 也是所谓 &quot;微服务&quot; 思想得以落地的先决条件.</p> <p>当然, 如果只做到 &quot;<strong>封装微服务, 调度单容器</strong>&quot; 这一层次, Docker Swarm 项目就已经绰绰有余了. 如果再加上 Compose 项目, 甚至还具备了处理一些简单依赖关系的能力, 比如: 一个 &quot;Web 容器&quot; 和它要访问的数据库 &quot;DB 容器&quot;.</p> <p>在 Compose 项目中, 可以为这样的两个容器<strong>定义一个 &quot;link&quot;</strong> , 而 Docker 项目则会负责维护这个 &quot;link&quot; 关系, 其具体做法是: Docker 会在 Web 容器中, 将 DB 容器的 IP 地址, 端口等信息以<strong>环境变量</strong>的方式注入进去, 供应用进程使用, 比如:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token assign-left variable">DB_NAME</span><span class="token operator">=</span>/web/db
<span class="token assign-left variable">DB_PORT</span><span class="token operator">=</span>tcp://172.17.0.5:5432
<span class="token assign-left variable">DB_PORT_5432_TCP</span><span class="token operator">=</span>tcp://172.17.0.5:5432
<span class="token assign-left variable">DB_PORT_5432_TCP_PROTO</span><span class="token operator">=</span>tcp
<span class="token assign-left variable">DB_PORT_5432_TCP_PORT</span><span class="token operator">=</span><span class="token number">5432</span>
<span class="token assign-left variable">DB_PORT_5432_TCP_ADDR</span><span class="token operator">=</span><span class="token number">172.17</span>.0.5
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>而当 DB 容器发生变化时(比如镜像更新, 被迁移到其他宿主机上等等), 这些<strong>环境变量的值会由 Docker 项目自动更新</strong>. **这就是平台项目自动地处理容器间关系的典型例子. **</p> <p>可如果现在的需求是, 要求这个项目<strong>能够处理前面提到的所有类型的关系, 甚至还要能够支持未来可能出现的更多种类的关系</strong>呢?</p> <p>这时, &quot;link&quot; 这种单独针对一种案例设计的解决方案就太过简单了. 如果你做过架构方面的工作, 就会深有感触: 一旦要追求项目的普适性, 那就<strong>一定要从顶层开始做好设计</strong>.</p> <p><mark>所以, </mark>​<mark>**Kubernetes 项目最主要的设计思想是, 从更宏观的角度, 以统一的方式来定义任务之间的各种关系, 并且为将来支持更多种类的关系留有余地. **</mark></p> <p>比如, Kubernetes 项目<strong>对容器间的 &quot;访问&quot; 进行了分类, 首先总结出了一类非常常见的 &quot;紧密交互&quot; 的关系, 即: 这些应用之间需要非常频繁的交互和访问; 又或者它们会直接通过本地文件进行信息交换</strong>.</p> <p>在常规环境下, 这些应用往往会被直接部署在同一台机器上, 通过 Localhost 通信, 通过本地磁盘目录交换文件. 而在 Kubernetes 项目中, <strong>这些容器则会被划分为一个 &quot;Pod&quot;, Pod 里的容器共享同一个 Network Namespace, 同一组数据卷, 从而达到高效率交换信息的目的</strong>.</p> <p>Pod 是 Kubernetes 项目中<strong>最基础的一个对象</strong>, 源自于 Google Borg 论文中一个名叫 Alloc 的设计. 在后续的章节会对 Pod 做更进一步地阐述.</p> <p>而对于另外一种更为常见的需求, 比如 Web 应用与数据库之间的访问关系, Kubernetes 项目则提供了一种叫作 &quot;Service&quot; 的服务. 像这样的两个应用, 往往<strong>故意不部署在同一台机器上, 这样即使 Web 应用所在的机器宕机了, 数据库也完全不受影响</strong>. 可是对于一个容器来说, 它的 <strong>IP 地址等信息不是固定</strong>的, 那么 Web 应用又怎么找到数据库容器的 Pod 呢?</p> <p><mark><strong>所以 Kubernetes 项目的做法是给 Pod 绑定一个 Service 服务, 而 Service 服务声明的 IP 地址等信息是&quot;终生不变&quot;的. 这个 Service 服务的主要作用, 就是作为 Pod 的代理入口(Portal), 从而代替 Pod 对外暴露一个固定的网络地址.</strong></mark></p> <p>这样对于 Web 应用的 Pod 来说, 它需要关心的就是<strong>数据库 Pod 的 Service 信息</strong>. 不难想象, <strong>Service 后端真正代理的 Pod 的 IP 地址, 端口等信息的自动更新, 维护, 则是 Kubernetes 项目的职责</strong>.</p> <p>像这样, 围绕着容器和 Pod 不断向真实的技术场景扩展, 就能够摸索出一幅如下所示的 Kubernetes 项目核心功能的&quot;全景图&quot;.</p> <p><img src="/img/87b8382d04f8a4ef3a8f68a85e771b33-20230731162150-zbjtsrt.png" alt=""></p> <p>按照这幅图的线索, 我们从容器这个最基础的概念出发, <strong>首先遇到了容器间 &quot;紧密协作&quot; 关系的难题, 于是就扩展到了 Pod; 有了 Pod 之后, 我们希望能一次启动多个应用的实例, 这样就需要 Deployment 这个 Pod 的多实例管理器; 而有了这样一组相同的 Pod 后, 又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它, 于是就有了 Service</strong>.</p> <p>可是如果现在两个不同 Pod 之间不仅有 &quot;访问关系&quot;, 还要求在发起时加上<strong>授权信息</strong>. 最典型的例子就是 Web 应用对数据库访问时需要 <strong>Credential</strong>(数据库的用户名和密码)信息. 那么, 在 Kubernetes 中这样的关系又如何处理呢? Kubernetes 项目提供了一种叫作 <strong>Secret 的对象, 它其实是一个保存在 Etcd 里的键值对数据</strong>. 当你把 Credential 信息以 Secret 的方式存在 Etcd 里, Kubernetes 就会在你指定的 <strong>Pod(比如, Web 应用的 Pod)启动时, 自动把 Secret 里的数据以 Volume 的方式挂载到容器里</strong>. 这样这个 Web 应用就可以访问数据库了.</p> <p><mark>**除了应用与应用之间的关系外, 应用运行的形态是影响 &quot;如何容器化这个应用&quot; 的第二个重要因素. **</mark></p> <p>为此, Kubernetes 定义了新的, 基于 Pod 改进后的对象. 比如 <strong>Job</strong>, 用来描述一次性运行的 Pod(比如, 大数据任务); 再比如 <strong>DaemonSet</strong>, 用来描述每个宿主机上必须且只能运行一个副本的守护进程服务; 又比如 <strong>CronJob</strong>, 则用于描述定时任务等等. 如此种种, 正是 Kubernetes 项目<strong>定义容器间关系和形态</strong>的主要方法.</p> <p>可以看到, Kubernetes 项目并没有像其他项目那样, <strong>为每一个管理功能创建一个指令, 然后在项目中实现其中的逻辑. 这种做法, 的确可以解决当前的问题, 但是在更多的问题来临之后, 往往会力不从心</strong>.</p> <p>相比之下, 在 Kubernetes 项目中, 推崇的使用方法是:</p> <ul><li>**首先, 通过一个 &quot;编排对象&quot;, 比如 Pod, Job, CronJob 等, 来描述你试图管理的应用; **</li> <li>**然后, 再为它定义一些 &quot;服务对象&quot;, 比如 Service, Secret, Horizontal Pod Autoscaler(自动水平扩展器)等. 这些对象, 会负责具体的平台级功能. **</li></ul> <p><mark><strong>这种使用方法, 就是所谓的 &quot;声明式 API&quot;. 这种 API 对应的 &quot;编排对象&quot; 和 &quot;服务对象&quot;, 都是 Kubernetes 项目中的 API 对象(API Object).</strong></mark></p> <p>这就是 Kubernetes <strong>最核心的设计理念</strong>, 也是接下来会重点剖析的关键技术点.</p> <p>最后来回答一个更直接的问题: <strong>Kubernetes 项目如何启动一个容器化任务呢?</strong></p> <p>比如现在已经制作好了一个 Nginx 容器镜像, 希望让平台帮我启动这个镜像. 并且要求平台帮我<strong>运行两个完全相同的 Nginx 副本, 以负载均衡的方式共同对外提供服务</strong>.</p> <p>如果是自己 DIY 的话, 可能需要启动两台虚拟机, 分别安装两个 Nginx, 然后使用 keepalived 为这两个虚拟机做一个虚拟 IP. 而如果使用 Kubernetes 项目呢? 需要做的则是编写如下这样一个 <strong>YAML 文件</strong>(比如名叫 nginx-deployment.yaml):</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>在上面这个 YAML 文件中, 定义了一个 <strong>Deployment 对象</strong>, 它的主体部分(spec.template 部分)是一个<strong>使用 Nginx 镜像的 Pod, 而这个 Pod 的副本数是 2(replicas=2)</strong> .</p> <p>然后执行:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl create <span class="token punctuation">-</span>f nginx<span class="token punctuation">-</span>deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样两个完全相同的 Nginx 容器副本就被启动了.</p> <p>不过这么看来, 做同样一件事情, Kubernetes 用户要做的工作也不少嘛. 别急, 在后续的讲解中, 会陆续介绍 Kubernetes 项目这种 &quot;声明式 API&quot; 的种种好处, 以及基于它实现的强大的编排能力.</p> <blockquote><p>总结</p></blockquote> <p>本节首先回顾了容器的核心知识, 说明了容器其实可以分为两个部分: <strong>容器运行时和容器镜像</strong>.</p> <p>然后重点介绍了 Kubernetes 项目的架构, 详细讲解了它如何<strong>使用 &quot;声明式 API&quot; 来描述容器化业务和容器间关系的设计思想</strong>.</p> <p>实际上, 过去很多的集群管理项目(比如 Yarn, Mesos, 以及 Swarm)所擅长的, 都是把一个容器, 按照某种规则, 放置在某个最佳节点上运行起来. 这种功能, 我们称为&quot;调度&quot;. <mark>而 Kubernetes 项目所擅长的, 是按照用户的意愿和整个系统的规则, 完全自动化地处理好容器之间的各种关系. </mark>​<mark><strong>这种功能, 就是我们经常听到的一个概念: 编排</strong></mark>​ **. **</p> <p>所以说, <mark><strong>Kubernetes 项目的本质, 是为用户提供一个具有普遍意义的容器编排工具</strong></mark>.</p> <p>不过, 更重要的是, Kubernetes 项目为用户提供的不仅限于一个工具. 它真正的价值, 乃在于提供了一套基于容器构建分布式系统的基础依赖. 关于这一点, 相信你会在今后的学习中, 体会的越来越深.</p> <p>‍</p> <p>‍</p> <h3 id="kubernetes集群搭建与实践"><a href="#kubernetes集群搭建与实践" class="header-anchor">#</a> Kubernetes集群搭建与实践</h3> <h4 id="_10-kubernetes一键部署利器-kubeadm"><a href="#_10-kubernetes一键部署利器-kubeadm" class="header-anchor">#</a> 10 | Kubernetes一键部署利器:kubeadm</h4> <p>本节的主题是: Kubernetes 一键部署利器之 kubeadm.</p> <p>前面几章其实阐述了这样一个思想: **要真正发挥容器技术的实力, 就不能仅仅局限于对 Linux 容器本身的钻研和使用. ** 这些知识更适合作为你的技术储备, 以便在需要的时候可以帮你更快的定位问题, 并解决问题. 而更深入的学习容器技术的关键在于, **如何使用这些技术来&quot;容器化&quot;你的应用. **</p> <p>比如, 我们的应用既可能是 Java Web 和 MySQL 这样的组合, 也可能是 Cassandra 这样的分布式系统. 而要使用容器把后者运行起来, 单单通过 Docker 把一个 Cassandra 镜像跑起来是没用的.</p> <p>**要把 Cassandra 应用容器化的关键, 在于如何处理好这些 Cassandra **​<mark><strong>容器之间的编排关系</strong></mark>. 比如, <strong>哪些 Cassandra 容器是主, 哪些是从? 主从容器如何区分? 它们之间又如何进行自动发现和通信? Cassandra 容器的持久化数据又如何保持</strong>, 等等.</p> <p>这也是为什么要反复强调 Kubernetes 项目的主要原因: 这个项目体现出来的容器化&quot;表达能力&quot;, 具有独有的先进性和完备性. 这就使得它不仅能运行 Java Web 与 MySQL 这样的常规组合, 还能够处理 Cassandra 容器集群等复杂编排问题. 所以, 对这种编排能力的剖析, 解读和最佳实践, 将是本专栏最重要的一部分内容.</p> <p>不过, 万事开头难.</p> <p>作为一个典型的分布式项目, Kubernetes 的部署一直以来都是挡在初学者前面的一只&quot;拦路虎&quot;. 尤其是在 Kubernetes 项目发布初期, 它的部署完全要依靠一堆由社区维护的脚本.</p> <p>其实, Kubernetes 作为一个 Golang 项目, 已经免去了很多类似于 Python 项目要安装语言级别依赖的麻烦. 但除了将各个组件编译成二进制文件外, 用户还要负责为这些二进制文件编写对应的配置文件, 配置自启动脚本, 以及为 kube-apiserver 配置授权文件等等诸多运维工作.</p> <p>目前, 各大云厂商最常用的部署的方法, 是使用 SaltStack, Ansible 等<strong>运维工具自动化</strong>地执行这些步骤. 但即使这样, 这个部署过程依然非常繁琐. 因为, SaltStack 这类专业运维工具本身的学习成本, 就可能比 Kubernetes 项目还要高.</p> <p>**难道 Kubernetes 项目就没有简单的部署方法了吗? **</p> <p>这个问题, 在 Kubernetes 社区里一直没有得到足够重视. 直到 2017 年, 在志愿者的推动下, 社区才终于发起了一个独立的部署工具, 名叫: <a href="https://github.com/kubernetes/kubeadm" target="_blank" rel="noopener noreferrer">kubeadm<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>这个项目的目的, 就是要让用户能够通过这样<strong>两条指令完成一个 Kubernetes 集群的部署</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># 创建一个 Master 节点</span>
$ kubeadm init
 
<span class="token comment"># 将一个 Node 节点加入到当前集群中</span>
$ kubeadm <span class="token function">join</span> <span class="token operator">&lt;</span>Master 节点的 IP 和端口 <span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>是不是非常方便呢?</p> <p>不过, 你可能也会有所顾虑: **Kubernetes 的功能那么多, 这样一键部署出来的集群, 能用于生产环境吗? ** 为了回答这个问题, 本节就先介绍一下 kubeadm 的工作原理.</p> <h5 id="_1-kubeadm的工作原理"><a href="#_1-kubeadm的工作原理" class="header-anchor">#</a> 1.kubeadm的工作原理</h5> <p>在《从容器到容器云: 谈谈 Kubernetes 的本质》中, 已经详细介绍了 Kubernetes 的架构和它的组件. 在部署时, 它的每一个组件都是<strong>一个需要被执行的, 单独的二进制文件</strong>. 所以不难想象, SaltStack 这样的运维工具或者由社区维护的脚本的功能, 就是要把这些二进制文件传输到指定的机器当中, 然后编写控制脚本来启停这些组件.</p> <p>不过, 在理解了容器技术之后, 你可能已经萌生出了这样一个想法, **为什么不用容器部署 Kubernetes 呢? **</p> <p>这样, 只要<strong>给每个 Kubernetes 组件做一个容器镜像, 然后在每台宿主机上用 docker run 指令启动这些组件容器</strong>, 部署不就完成了吗?</p> <p>事实上, 在 Kubernetes 早期的部署脚本里, 确实有一个脚本就是用 Docker 部署 Kubernetes 项目的, 这个脚本相比于 SaltStack 等的部署方式, 也的确简单了不少.</p> <p>但**这样做会带来一个很麻烦的问题, 即: 如何容器化 kubelet. **</p> <p>在上一篇文章已经提到 kubelet 是 Kubernetes 项目用来操作 Docker 等容器运行时的<strong>核心组件</strong>. 可<strong>除了跟容器运行时打交道外, kubelet 在配置容器网络, 管理容器数据卷时, 都需要直接操作宿主机</strong>. 而如果现在 kubelet 本身就运行在一个容器里, 那么直接操作宿主机就会变得很麻烦. 对于网络配置来说还好, kubelet 容器可以通过不开启 Network Namespace(即 Docker 的 host network 模式)的方式, 直接共享宿主机的网络栈. 可是要让 kubelet 隔着容器的 Mount Namespace 和文件系统, 操作宿主机的文件系统, 就有点儿困难了.</p> <p>比如, 如果用户想要使用 NFS 做容器的持久化数据卷, 那么 kubelet 就需要在容器进行绑定挂载前, 在宿主机的指定目录上, 先挂载 NFS 的远程目录. 可是这时候问题来了. 由于现在 kubelet 是<strong>运行在容器里</strong>的, 这就意味着它要做的这个 &quot;mount -F nfs&quot; 命令, 被<strong>隔离</strong>在了一个单独的 Mount Namespace 中. 即, kubelet 做的挂载操作, 不能被 &quot;传播&quot; 到宿主机上.</p> <p>对于这个问题, 有人说, 可以使用 setns() 系统调用, 在宿主机的 Mount Namespace 中执行这些挂载操作; 也有人说, 应该让 Docker 支持一个 –mnt=host 的参数. 但是, 到目前为止, <strong>在容器里运行 kubelet, 依然没有很好的解决办法, 因此也不推荐用容器去部署 Kubernetes 项目</strong>.</p> <p>正因为如此, kubeadm 选择了一种妥协方案:</p> <blockquote><p><mark>把 kubelet 直接运行在宿主机上, 然后使用容器部署其他的 Kubernetes 组件. </mark></p></blockquote> <p>所以使用 kubeadm 的第一步, 是在机器上<strong>手动安装 kubeadm, kubelet 和 kubectl 这三个二进制文件</strong>. 当然, kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包, 所以只需要执行:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">apt-get</span> <span class="token function">install</span> kubeadm
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>就可以了.</p> <p>接下来, 就可以使用 &quot;<strong>kubeadm init</strong>&quot; 部署 Master 节点了.</p> <h5 id="_2-kubeadm-init的工作流程"><a href="#_2-kubeadm-init的工作流程" class="header-anchor">#</a> 2.kubeadm init的工作流程</h5> <p>当执行 kubeadm init 指令后, <strong>kubeadm 首先要做的, 是一系列的检查工作, 以确定这台机器可以用来部署 Kubernetes</strong>. 这一步检查称为 &quot;Preflight Checks&quot;, 它可以为你省掉很多后续的麻烦.</p> <p>其实, Preflight Checks 包括了很多方面, 比如:</p> <ul><li>Linux 内核的版本必须是否是 3.10 以上?</li> <li>Linux Cgroups 模块是否可用?</li> <li>机器的 hostname 是否标准? 在 Kubernetes 项目里, 机器的名字以及一切存储在 Etcd 中的 API 对象, 都必须使用标准的 DNS 命名(RFC 1123).</li> <li>用户安装的 kubeadm 和 kubelet 的版本是否匹配?</li> <li>机器上是不是已经安装了 Kubernetes 的二进制文件?</li> <li>Kubernetes 的工作端口 10250/10251/10252 端口是不是已经被占用?</li> <li>ip, mount 等 Linux 指令是否存在?</li> <li>Docker 是否已经安装?</li> <li>...</li></ul> <p>**在通过了 Preflight Checks 之后, kubeadm 会生成 Kubernetes 对外提供服务所需的各种证书和对应的目录. **</p> <p>Kubernetes 对外提供服务时, 除非专门开启&quot;不安全模式&quot;, 否则都要<strong>通过 HTTPS 才能访问 kube-apiserver</strong>. 这就需要为 Kubernetes 集群配置好证书文件.</p> <p>kubeadm 为 Kubernetes 项目生成的<strong>证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下</strong>. 在这个目录下, 最主要的证书文件是 ca.crt 和对应的私钥 ca.key.</p> <p>此外, 用户使用 kubectl 获取容器日志等 streaming 操作时, 需要通过 kube-apiserver 向 kubelet 发起请求, 这个连接也必须是安全的. <strong>kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件, 对应的私钥是 apiserver-kubelet-client.key</strong>.</p> <p>除此之外, Kubernetes 集群中还有 Aggregate APIServer 等特性, 也需要用到专门的证书, 这里就不再一一列举了. 需要指出的是, 可以选择不让 kubeadm 生成这些证书, 而是<strong>拷贝现有的证书</strong>到如下证书的目录里:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>/etc/kubernetes/pki/ca.<span class="token punctuation">{</span>crt,key<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时 kubeadm 就会跳过证书生成的步骤, 把它完全交给用户处理.</p> <p><strong>证书生成后, kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件</strong>. 这些文件的路径是:  <strong>/etc/kubernetes/xxx.conf</strong>:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">ls</span> /etc/kubernetes/
admin.conf  controller-manager.conf  kubelet.conf  scheduler.conf
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这些文件里面记录的是, <strong>当前这个 Master 节点的服务器地址, 监听端口, 证书目录</strong>等信息. 这样对应的客户端(比如 scheduler, kubelet 等), 可以直接加载相应的文件, 使用里面的信息与 kube-apiserver 建立安全连接.</p> <p><strong>接下来, kubeadm 会为 Master 组件生成 Pod 配置文件</strong>. 上一篇文章中已经介绍过 Kubernetes 有三个 Master 组件 <strong>kube-apiserver, kube-controller-manager, kube-scheduler</strong>, 而它们都会被<strong>使用 Pod 的方式部署</strong>起来.</p> <p>你可能会有些疑问: 这时 Kubernetes 集群<strong>尚不存在</strong>, 难道 kubeadm 会直接执行 docker run 来启动这些容器吗?</p> <p>当然不是.</p> <p>在 Kubernetes 中, 有一种<strong>特殊的容器启动方法叫做 &quot;Static Pod&quot;</strong> . <strong>它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里. 这样当这台机器上的 kubelet 启动时, 它会自动检查这个目录, 加载所有的 Pod YAML 文件, 然后在这台机器上启动它们</strong>.</p> <p>从这一点也可以看出, kubelet 在 Kubernetes 项目中的地位非常高, 在设计上它就是一个<strong>完全独立</strong>的组件, 而其他 Master 组件, 则更像是<strong>辅助性的系统容器</strong>.</p> <p>在 kubeadm 中, Master 组件的 YAML 文件会被生成在  <strong>/etc/kubernetes/manifests</strong> 路径下. 比如, kube-apiserver.yaml:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
    <span class="token key atrule">scheduler.alpha.kubernetes.io/critical-pod</span><span class="token punctuation">:</span> <span class="token string">&quot;&quot;</span>
  <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token null important">null</span>
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">component</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>apiserver
    <span class="token key atrule">tier</span><span class="token punctuation">:</span> control<span class="token punctuation">-</span>plane
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>apiserver
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> kube<span class="token punctuation">-</span>apiserver
    <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>authorization<span class="token punctuation">-</span>mode=Node<span class="token punctuation">,</span>RBAC
    <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>runtime<span class="token punctuation">-</span>config=api/all=true
    <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>advertise<span class="token punctuation">-</span>address=10.168.0.2
    <span class="token punctuation">...</span>
    <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>tls<span class="token punctuation">-</span>cert<span class="token punctuation">-</span>file=/etc/kubernetes/pki/apiserver.crt
    <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>tls<span class="token punctuation">-</span>private<span class="token punctuation">-</span>key<span class="token punctuation">-</span>file=/etc/kubernetes/pki/apiserver.key
    <span class="token key atrule">image</span><span class="token punctuation">:</span> k8s.gcr.io/kube<span class="token punctuation">-</span>apiserver<span class="token punctuation">-</span>amd64<span class="token punctuation">:</span>v1.11.1
    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent
    <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>
      <span class="token punctuation">...</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>apiserver
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 250m
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/ca<span class="token punctuation">-</span>certificates
      <span class="token key atrule">name</span><span class="token punctuation">:</span> usr<span class="token punctuation">-</span>share<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>certificates
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token punctuation">...</span>
  <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> system<span class="token punctuation">-</span>cluster<span class="token punctuation">-</span>critical
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
      <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/ca<span class="token punctuation">-</span>certificates
      <span class="token key atrule">type</span><span class="token punctuation">:</span> DirectoryOrCreate
    <span class="token key atrule">name</span><span class="token punctuation">:</span> etc<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>certificates
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div><p>关于一个 Pod 的 YAML 文件怎么写, 里面的字段如何解读, 会在后续专门的文章详细分析. 这里只需要关注这样几个信息:</p> <ol><li>这个 Pod 里只定义了一个容器, 它使用的镜像是: <code>k8s.gcr.io/kube-apiserver-amd64:v1.11.1</code>​. 这个镜像是 Kubernetes 官方维护的一个组件镜像.</li> <li>这个容器的启动命令(commands)是 kube-apiserver --authorization-mode=Node,RBAC..., 这样一句非常长的命令. 其实, 它就是<strong>容器里 kube-apiserver 这个二进制文件再加上指定的配置参数</strong>而已.</li> <li>如果要修改一个<strong>已有集群的 kube-apiserver 的配置, 需要修改这个 YAML 文件</strong>.</li> <li>这些组件的参数也可以在部署时指定, 后面就会讲解到.</li></ol> <p>在这一步完成后, kubeadm 还会再生成一个 <strong>Etcd 的 Pod YAML 文件</strong>, 用来通过同样的 Static Pod 的方式启动 Etcd. 所以, 最后 Master 组件的 Pod YAML 文件如下所示:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">ls</span> /etc/kubernetes/manifests/
etcd.yaml  
kube-apiserver.yaml  
kube-controller-manager.yaml  
kube-scheduler.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>**而一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下, kubelet 就会自动创建这些 YAML 文件中定义的 Pod, 即 Master 组件的容器. **</p> <p>Master 容器启动后, kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL, 等待 Master 组件完全运行起来.</p> <p><strong>然后, kubeadm 就会为集群生成一个 bootstrap token</strong>. 在后面, 只要持有这个 token, 任何一个安装了 kubelet 和 kubadm 的节点, 都可以通过 kubeadm join 加入到这个集群当中. 这个 token 的值和使用方法会在 kubeadm init 结束后被打印出来.</p> <p><strong>在 token 生成之后, kubeadm 会将 ca.crt 等 Master 节点的重要信息, 通过 ConfigMap 的方式保存在 Etcd 当中, 供后续部署 Node 节点使用</strong>. 这个 ConfigMap 的名字是 <strong>cluster-info</strong>.</p> <p><strong>kubeadm init 的最后一步, 就是安装默认插件</strong>. Kubernetes 默认 <strong>kube-proxy 和 DNS 这两个插件</strong>是必须安装的. 它们分别用来提供整个集群的<strong>服务发现和 DNS 功能</strong>. 其实, 这两个插件也只是两个容器镜像而已, 所以 kubeadm 只要用 Kubernetes 客户端<strong>创建两个 Pod</strong> 就可以了.</p> <h5 id="_3-kubeadm-join的工作流程"><a href="#_3-kubeadm-join的工作流程" class="header-anchor">#</a> 3.kubeadm join的工作流程</h5> <p>这个流程其实非常简单, kubeadm init 生成 bootstrap token 之后, 就可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 <strong>kubeadm join</strong> 了.</p> <p>可是, 为什么执行 kubeadm join 需要这样一个 token 呢?</p> <p>因为, 任何一台机器想要成为 Kubernetes 集群中的一个节点, 就必须<strong>在集群的 kube-apiserver 上注册</strong>. 可是, 要想跟 apiserver 打交道, 这台机器就必须要获取到<strong>相应的证书文件(CA 文件)</strong> . 可是, 为了能够一键安装, 就不能让用户去 Master 节点上手动拷贝这些文件.</p> <p><strong>所以, kubeadm 至少需要发起一次 &quot;不安全模式&quot; 的访问到 kube-apiserver, 从而拿到保存在 ConfigMap 中的 cluster-info(它保存了 APIServer 的授权信息). 而 bootstrap token, 扮演的就是这个过程中的安全验证的角色.</strong></p> <p>只要有了 cluster-info 里的 kube-apiserver 的地址, 端口, 证书, kubelet 就可以以 &quot;安全模式&quot; 连接到 apiserver 上, 这样一个新的节点就部署完成了.</p> <p>接下来只要在其他节点上重复这个指令就可以了.</p> <h5 id="_4-配置kubeadm的部署参数"><a href="#_4-配置kubeadm的部署参数" class="header-anchor">#</a> 4.配置kubeadm的部署参数</h5> <p>前面讲解了 kubeadm 部署 Kubernetes 集群最关键的两个步骤, <strong>kubeadm init 和 kubeadm join</strong>. 相信你一定会有这样的疑问: kubeadm 确实简单易用, 可是又该<strong>如何定制我的集群组件参数呢</strong>? 比如, 要指定 kube-apiserver 的启动参数, 该怎么办?</p> <p>在这里, 强烈推荐在使用 kubeadm init 部署 Master 节点时, 使用下面这条指令:</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kubeadm init <span class="token parameter variable">--config</span> kubeadm.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时就可以<strong>给 kubeadm 提供一个 YAML 文件(比如, kubeadm.yaml)</strong> , 它的内容如下所示(仅列举了主要部分):</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> kubeadm.k8s.io/v1alpha2
<span class="token key atrule">kind</span><span class="token punctuation">:</span> MasterConfiguration
<span class="token key atrule">kubernetesVersion</span><span class="token punctuation">:</span> v1.11.0
<span class="token key atrule">api</span><span class="token punctuation">:</span>
  <span class="token key atrule">advertiseAddress</span><span class="token punctuation">:</span> 192.168.0.102
  <span class="token key atrule">bindPort</span><span class="token punctuation">:</span> <span class="token number">6443</span>
  <span class="token punctuation">...</span>
<span class="token key atrule">etcd</span><span class="token punctuation">:</span>
  <span class="token key atrule">local</span><span class="token punctuation">:</span>
    <span class="token key atrule">dataDir</span><span class="token punctuation">:</span> /var/lib/etcd
    <span class="token key atrule">image</span><span class="token punctuation">:</span> <span class="token string">&quot;&quot;</span>
<span class="token key atrule">imageRepository</span><span class="token punctuation">:</span> k8s.gcr.io
<span class="token key atrule">kubeProxy</span><span class="token punctuation">:</span>
  <span class="token key atrule">config</span><span class="token punctuation">:</span>
    <span class="token key atrule">bindAddress</span><span class="token punctuation">:</span> 0.0.0.0
    <span class="token punctuation">...</span>
<span class="token key atrule">kubeletConfiguration</span><span class="token punctuation">:</span>
  <span class="token key atrule">baseConfig</span><span class="token punctuation">:</span>
    <span class="token key atrule">address</span><span class="token punctuation">:</span> 0.0.0.0
    <span class="token punctuation">...</span>
<span class="token key atrule">networking</span><span class="token punctuation">:</span>
  <span class="token key atrule">dnsDomain</span><span class="token punctuation">:</span> cluster.local
  <span class="token key atrule">podSubnet</span><span class="token punctuation">:</span> <span class="token string">&quot;&quot;</span>
  <span class="token key atrule">serviceSubnet</span><span class="token punctuation">:</span> 10.96.0.0/12
<span class="token key atrule">nodeRegistration</span><span class="token punctuation">:</span>
  <span class="token key atrule">criSocket</span><span class="token punctuation">:</span> /var/run/dockershim.sock
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p>通过<strong>制定这样一个部署参数配置文件</strong>, 就可以很方便地在这个文件里填写各种<strong>自定义的部署参数</strong>了. 比如现在要指定 kube-apiserver 的参数, 那么只要在这个文件里加上这样一段信息:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token key atrule">apiServerExtraArgs</span><span class="token punctuation">:</span>
  <span class="token key atrule">advertise-address</span><span class="token punctuation">:</span> 192.168.0.103
  <span class="token key atrule">anonymous-auth</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
  <span class="token key atrule">enable-admission-plugins</span><span class="token punctuation">:</span> AlwaysPullImages<span class="token punctuation">,</span>DefaultStorageClass
  <span class="token key atrule">audit-log-path</span><span class="token punctuation">:</span> /home/johndoe/audit.log
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>然后, <strong>kubeadm 就会使用上面这些信息替换 /etc/kubernetes/manifests/kube-apiserver.yaml 里的 command 字段里的参数了</strong>.</p> <p>而这个 YAML 文件提供的可配置项远不止这些. 比如, 还可以<strong>修改 kubelet 和 kube-proxy 的配置</strong>, 修改 Kubernetes 使用的基础镜像的 URL(默认的<code>k8s.gcr.io/xxx</code>​镜像 URL 在国内访问是有困难的), 指定自己的证书文件, 指定特殊的容器运行时等等. 这些配置项, 就留给你在后续实践中探索了.</p> <h5 id="_5-总结"><a href="#_5-总结" class="header-anchor">#</a> 5.总结</h5> <p>本节重点介绍了 kubeadm 这个部署工具的工作原理和使用方法. 下一篇文章会使用它一步步地部署一个完整的 Kubernetes 集群.</p> <p>可以看到 kubeadm 的设计非常简洁. 并且它在实现每一步部署功能时, <strong>都在最大程度地重用 Kubernetes 已有的功能</strong>, 这也就使得在使用 kubeadm 部署 Kubernetes 项目时, 非常有&quot;原生&quot;的感觉, 一点都不会感到突兀. 而 kubeadm 的源代码, 直接就在 kubernetes/cmd/kubeadm 目录下, 是 <strong>Kubernetes 项目的一部分</strong>. 其中, app/phases 文件夹下的代码, 对应的就是本节详细介绍的每一个具体步骤.</p> <p>所以开源社区的魅力也在于此: 一个成功的开源项目, 总能够吸引到全世界最厉害的贡献者参与其中. 尽管参与者的总体水平参差不齐, 而且频繁的开源活动又显得杂乱无章难以管控, 但一个有足够热度的社区最终的收敛方向, 却一定是代码越来越完善, Bug 越来越少, 功能越来越强大.</p> <p>最后再来回答一下开始提到的问题: <strong>kubeadm 能够用于生产环境吗</strong>? 到目前为止(2018 年 9 月), 这个问题的答案是: <strong>不能</strong>.</p> <p><strong>因为 kubeadm 目前最欠缺的是, 一键部署一个高可用的 Kubernetes 集群, 即: Etcd, Master 组件都应该是多节点集群, 而不是现在这样的单点. 这, 当然也正是 kubeadm 接下来发展的主要方向.</strong></p> <p>当然, 如果有部署规模化生产环境的需求, 推荐使用 <a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener noreferrer">kops<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 或者 SaltStack 这样更复杂的部署工具. 但接下来的讲解都会以 kubeadm 为依据进行讲述.</p> <ul><li>一方面, 作为 Kubernetes 项目的原生部署工具, kubeadm 对 Kubernetes 项目特性的使用和集成, 确实要比其他项目&quot;技高一筹&quot;, 非常值得我们学习和借鉴;</li> <li>另一方面, kubeadm 的部署方法, 不会涉及到太多的运维工作, 也不需要我们额外学习复杂的部署工具. 而它部署的 Kubernetes 集群, 跟一个<strong>完全使用二进制文件</strong>搭建起来的集群几乎没有任何区别.</li></ul> <p>因此, 使用 kubeadm 去部署一个 Kubernetes 集群, 对于你理解 Kubernetes 组件的工作方式和架构, 最好不过了.</p> <h4 id="_11-从0到1-搭建一个完整的kubernetes集群"><a href="#_11-从0到1-搭建一个完整的kubernetes集群" class="header-anchor">#</a> 11 | 从0到1:搭建一个完整的Kubernetes集群</h4> <p>上一节介绍了 kubeadm 这个 Kubernetes 半官方管理工具的工作原理. 既然 kubeadm 的初衷是让 Kubernetes 集群的部署不再让人头疼, 那么本节就来使用它部署一个完整的 Kubernetes 集群. 这里所说的 &quot;完整&quot;, <strong>指的是这个集群具备 Kubernetes 项目在 GitHub 上已经发布的所有功能, 并能够模拟生产环境的所有使用需求. 但并不代表这个集群是生产级别可用的: 类似于高可用, 授权, 多租户, 灾难备份等生产级别集群的功能暂时不在本节的讨论范围</strong>.</p> <p>本次部署不会依赖于任何公有云或私有云的能力, 而是完全在 Bare-metal 环境中完成. 这样的部署经验会更有普适性. 而在后续的讲解中, 如非特殊强调, 也都会以本次搭建的这个<strong>集群</strong>为基础.</p> <h5 id="_1-准备工作"><a href="#_1-准备工作" class="header-anchor">#</a> 1.准备工作</h5> <p>首先, 准备机器. 最直接的办法, 自然是到公有云上申请几个虚拟机. 当然, 如果条件允许的话, 拿几台本地的物理服务器来组集群是最好不过了. 这些机器只要满足如下几个条件即可:</p> <ol><li>满足安装 Docker 项目所需的要求, 比如 64 位的 Linux 操作系统, 3.10 及以上的内核版本;</li> <li>x86 或者 ARM 架构均可;</li> <li><strong>机器之间网络互通</strong>, 这是将来容器之间网络互通的前提;</li> <li>有外网访问权限, 因为需要拉取镜像;</li> <li>能够访问到 <code>gcr.io, quay.io</code>​ 这两个 docker registry, 因为有小部分镜像需要在这里拉取;</li> <li><strong>单机可用资源建议 2 核 CPU, 8 GB 内存或以上</strong>, 再小的话问题也不大, 但是能调度的 Pod 数量就比较有限了;</li> <li>30 GB 或以上的可用磁盘空间, 这主要是留给 Docker 镜像和日志文件用的.</li></ol> <p>在本次部署中, 准备的机器配置如下:</p> <ol><li>2 核 CPU, 7.5 GB 内存;</li> <li>30 GB 磁盘;</li> <li>Ubuntu 16.04;</li> <li>内网互通;</li> <li>外网访问权限不受限制.</li></ol> <blockquote><p>备注: 在开始部署前, 推荐先花几分钟时间, 回忆一下 Kubernetes 的架构.</p></blockquote> <p>本节实践的目标:</p> <ol><li>在所有节点上安装 Docker 和 kubeadm;</li> <li>部署 Kubernetes Master;</li> <li>部署容器网络插件;</li> <li>部署 Kubernetes Worker;</li> <li>部署 Dashboard 可视化插件;</li> <li>部署容器存储插件.</li></ol> <p>现在就来开始这次集群部署之旅!</p> <h5 id="_2-安装kubeadm和docker"><a href="#_2-安装kubeadm和docker" class="header-anchor">#</a> 2.安装kubeadm和Docker</h5> <p>上一篇已经介绍过 kubeadm 的基础用法, 它的一键安装非常方便, 只需要添加 kubeadm 的源, 然后直接使用 apt-get 安装即可, 具体流程如下所示:</p> <blockquote><p>备注: 为了方便讲解, 后续都直接会在 root 用户下进行操作</p></blockquote> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-s</span> https://packages.cloud.google.com/apt/doc/apt-key.gpg <span class="token operator">|</span> apt-key <span class="token function">add</span> -
$ <span class="token function">cat</span> <span class="token operator">&lt;&lt;</span><span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">&gt;</span> /etc/apt/sources.list.d/kubernetes.list</span>
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF</span>
$ <span class="token function">apt-get</span> update
$ <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> docker.io kubeadm
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>**在上述安装 kubeadm 的过程中, kubeadm 和 kubelet, kubectl, kubernetes-cni 这几个二进制文件都会被自动安装好. **</p> <p>另外, 这里直接使用 Ubuntu 的 docker.io 的安装源, 原因是 Docker 公司每次发布的最新的 Docker CE(社区版)产品往往还没有经过 Kubernetes 项目的验证, 可能会有兼容性方面的问题.</p> <h5 id="_3-部署kubernetes的master节点"><a href="#_3-部署kubernetes的master节点" class="header-anchor">#</a> 3.部署Kubernetes的Master节点</h5> <p>上一节已经介绍过 kubeadm 可以<strong>一键部署 Master 节点</strong>. 不过, 在本篇文章中既然要部署一个&quot;完整&quot;的 Kubernetes 集群, 那不妨稍微提高一下难度: <strong>通过配置文件来开启一些实验性功能</strong>.</p> <p>所以, 这里<strong>编写了一个给 kubeadm 用的 YAML 文件</strong>(名叫: kubeadm.yaml):</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> kubeadm.k8s.io/v1alpha1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> MasterConfiguration
<span class="token key atrule">controllerManagerExtraArgs</span><span class="token punctuation">:</span>
  <span class="token key atrule">horizontal-pod-autoscaler-use-rest-clients</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span>
  <span class="token key atrule">horizontal-pod-autoscaler-sync-period</span><span class="token punctuation">:</span> <span class="token string">&quot;10s&quot;</span>
  <span class="token key atrule">node-monitor-grace-period</span><span class="token punctuation">:</span> <span class="token string">&quot;10s&quot;</span>
<span class="token key atrule">apiServerExtraArgs</span><span class="token punctuation">:</span>
  <span class="token key atrule">runtime-config</span><span class="token punctuation">:</span> <span class="token string">&quot;api/all=true&quot;</span>
<span class="token key atrule">kubernetesVersion</span><span class="token punctuation">:</span> <span class="token string">&quot;stable-1.11&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>这个配置给 kube-controller-manager 设置了:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">horizontal-pod-autoscaler-use-rest-clients</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这意味着, 将来部署的 kube-controller-manager 能够<strong>使用自定义资源(Custom Metrics)进行自动水平扩展</strong>. 这是后面章节中会重点介绍的一个内容.</p> <p>其中, &quot;stable-1.11&quot; 就是 kubeadm 帮我们部署的 <strong>Kubernetes 版本号</strong>, 即: Kubernetes release 1.11 最新的稳定版, 在我的环境下, 它是 v1.11.1. 也可以直接指定这个版本, 比如: kubernetesVersion: &quot;v1.11.1&quot;.</p> <p>然后, 只需要执行一句指令:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubeadm init <span class="token punctuation">-</span><span class="token punctuation">-</span>config kubeadm.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>就可以完成 Kubernetes Master 的部署了, 这个过程只需要几分钟. 部署完成后, kubeadm 会生成一行指令(这里返回了 token):</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>kubeadm join 10.168.0.2<span class="token punctuation">:</span>6443 <span class="token punctuation">-</span><span class="token punctuation">-</span>token 00bwbx.uvnaa2ewjflwu1ry <span class="token punctuation">-</span><span class="token punctuation">-</span>discovery<span class="token punctuation">-</span>token<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>cert<span class="token punctuation">-</span>hash sha256<span class="token punctuation">:</span>00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个 <strong>kubeadm join 命令, 就是用来给这个 Master 节点添加更多工作节点(Worker)的命令</strong>. 在后面部署 Worker 节点的时候马上会用到它, 所以找一个地方把这条命令<strong>记录下来</strong>.</p> <p>此外, kubeadm 还会提示我们第一次使用 Kubernetes 集群<strong>所需要的配置命令</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> <span class="token environment constant">$HOME</span>/.kube
<span class="token function">sudo</span> <span class="token function">cp</span> <span class="token parameter variable">-i</span> /etc/kubernetes/admin.conf <span class="token environment constant">$HOME</span>/.kube/config
<span class="token function">sudo</span> <span class="token function">chown</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">id</span> <span class="token parameter variable">-u</span><span class="token variable">)</span></span><span class="token builtin class-name">:</span><span class="token variable"><span class="token variable">$(</span><span class="token function">id</span> <span class="token parameter variable">-g</span><span class="token variable">)</span></span> <span class="token environment constant">$HOME</span>/.kube/config
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>而需要这些配置命令的原因是: Kubernetes 集群默认<strong>需要加密方式</strong>访问. 所以这几条命令, 就是<strong>将刚刚部署生成的 Kubernetes 集群的安全配置文件, 保存到当前用户的 .kube 目录下, kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群</strong>.</p> <p><strong>如果不这么做的话, 每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置.</strong></p> <p>现在就可以使用 <strong>kubectl get 命令</strong>来查看当前唯一一个节点的状态了:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get nodes

NAME      STATUS     ROLES     AGE       VERSION
master    NotReady   master    1d        v1.11.1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 这个 get 指令输出的结果里, Master 节点的状态是 <strong>NotReady</strong>, 这是为什么呢?</p> <p>在调试 Kubernetes 集群时, 最重要的手段就是<strong>用 kubectl describe 来查看这个节点(Node)对象的详细信息, 状态和事件(Event)</strong> , 来试一下:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl describe <span class="token function">node</span> master
<span class="token punctuation">..</span>.
Conditions:
<span class="token punctuation">..</span>.
Ready   False <span class="token punctuation">..</span>. KubeletNotReady  runtime network not ready: <span class="token assign-left variable">NetworkReady</span><span class="token operator">=</span>false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>通过 kubectl describe 指令的输出, 可以看到 NodeNotReady 的原因在于, 我们<strong>尚未部署任何网络插件</strong>.</p> <p>另外, 还可以通过 kubectl 检查这个节点上<strong>各个系统 Pod 的状态</strong>, 其中, kube-system 是 Kubernetes 项目预留的系统 Pod 的工作空间(Namepsace, 注意它并不是 Linux Namespace, 它只是 Kubernetes 划分不同工作空间的单位):</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get pods <span class="token parameter variable">-n</span> kube-system
 
NAME               READY   STATUS   RESTARTS  AGE
coredns-78fcdf6894-j9s52     <span class="token number">0</span>/1    Pending  <span class="token number">0</span>     1h
coredns-78fcdf6894-jm4wf     <span class="token number">0</span>/1    Pending  <span class="token number">0</span>     1h
etcd-master           <span class="token number">1</span>/1    Running  <span class="token number">0</span>     2s
kube-apiserver-master      <span class="token number">1</span>/1    Running  <span class="token number">0</span>     1s
kube-controller-manager-master  <span class="token number">0</span>/1    Pending  <span class="token number">0</span>     1s
kube-proxy-xbd47         <span class="token number">1</span>/1    NodeLost  <span class="token number">0</span>     1h
kube-scheduler-master      <span class="token number">1</span>/1    Running  <span class="token number">0</span>     1s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到, <strong>CoreDNS, kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态, 即调度失败</strong>. 这当然是符合预期的: 因为这个 Master 节点的网络尚未就绪.</p> <h5 id="_4-部署网络插件"><a href="#_4-部署网络插件" class="header-anchor">#</a> 4.部署网络插件</h5> <p>在 Kubernetes 项目 &quot;一切皆容器&quot; 的设计理念指导下, 部署网络插件非常简单, 只需要执行一句 <strong>kubectl apply 指令</strong>, 以 Weave 为例:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://git.io/weave-kube-1.6
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>部署完成后, 可以通过 kubectl get 重新<strong>检查 Pod 的状态</strong>:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get pods <span class="token parameter variable">-n</span> kube-system
 
NAME                             READY     STATUS    RESTARTS   AGE
coredns-78fcdf6894-j9s52         <span class="token number">1</span>/1       Running   <span class="token number">0</span>          1d
coredns-78fcdf6894-jm4wf         <span class="token number">1</span>/1       Running   <span class="token number">0</span>          1d
etcd-master                      <span class="token number">1</span>/1       Running   <span class="token number">0</span>          9s
kube-apiserver-master            <span class="token number">1</span>/1       Running   <span class="token number">0</span>          9s
kube-controller-manager-master   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          9s
kube-proxy-xbd47                 <span class="token number">1</span>/1       Running   <span class="token number">0</span>          1d
kube-scheduler-master            <span class="token number">1</span>/1       Running   <span class="token number">0</span>          9s
weave-net-cmk27                  <span class="token number">2</span>/2       Running   <span class="token number">0</span>          19s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>可以看到, 所有的系统 Pod <strong>都成功启动</strong>了, 而刚刚部署的 Weave 网络插件则在 kube-system 下面新建了一个名叫 weave-net-cmk27 的 Pod, 一般来说, <strong>这些 Pod 就是容器网络插件在每个节点上的控制组件</strong>.</p> <p><strong>Kubernetes 支持容器网络插件, 使用的是一个名叫 CNI 的通用接口, 它也是当前容器网络的事实标准, 市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes, 比如 Flannel, Calico, Canal, Romana 等等</strong>, 它们的部署方式也都是类似的&quot;一键部署&quot;. 关于这些开源项目的实现细节和差异, 会在后续的网络部分详细介绍.</p> <p>至此, Kubernetes 的 Master 节点就部署完成了. 如果只需要一个单节点的 Kubernetes, 现在就可以使用了. 不过, 在默认情况下, <strong>Kubernetes 的 Master 节点是不能运行用户 Pod 的, 所以还需要额外做一个小操作</strong>. 在本篇的最后部分会介绍到它.</p> <h5 id="_5-部署kubernetes的worker节点"><a href="#_5-部署kubernetes的worker节点" class="header-anchor">#</a> 5.部署Kubernetes的Worker节点</h5> <p>Kubernetes 的 <strong>Worker 节点跟 Master 节点几乎是相同</strong>的, 它们运行着的<strong>都是一个 kubelet 组件</strong>. 唯一的区别在于, 在 kubeadm init 的过程中, kubelet 启动后, Master 节点上还会自动运行 <strong>kube-apiserver, kube-scheduler, kube-controller-manger</strong> 这三个系统 Pod.</p> <p>所以, 相比之下, 部署 Worker 节点反而是最简单的, 只需要两步即可完成.</p> <p>第一步, <strong>在所有 Worker 节点上执行 &quot;安装 kubeadm 和 Docker&quot; 一节的所有步骤</strong>.</p> <p>第二步, <strong>执行部署 Master 节点时生成的 kubeadm join 指令</strong>:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubeadm <span class="token function">join</span> <span class="token number">10.168</span>.0.2:6443 <span class="token parameter variable">--token</span> 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h5 id="_6-通过taint-toleration调整master执行pod的策略"><a href="#_6-通过taint-toleration调整master执行pod的策略" class="header-anchor">#</a> 6.通过Taint/Toleration调整Master执行Pod的策略</h5> <p>前面提到过, <strong>默认情况下 Master 节点是不允许运行用户 Pod 的</strong>. 而 Kubernetes 做到这一点, 依靠的是 Kubernetes 的 <strong>Taint/Toleration 机制</strong>.</p> <p>它的原理非常简单: 一旦某个节点被加上了一个 <strong>Taint</strong>, 即被 &quot;打上了污点&quot;, 那么所有 Pod 就都不能在这个节点上运行, 因为 Kubernetes 的 Pod 都有 &quot;洁癖&quot;. 除非有个别的 Pod 声明自己能 &quot;容忍&quot; 这个 &quot;污点&quot;, 即声明了 <strong>Toleration</strong>, 它才可以在这个节点上运行.</p> <p>其中, 为节点打上 &quot;污点&quot;(Taint)的命令是:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl taint nodes node1 <span class="token assign-left variable">foo</span><span class="token operator">=</span>bar:NoSchedule
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时, 该 node1 节点上就会增加<strong>一个键值对格式的 Taint</strong>, 即: foo=bar:NoSchedule. 其中值里面的 NoSchedule, 意味着这个 Taint 只会在调度新 Pod 时产生作用, 而不会影响已经在 node1 上运行的 Pod, 哪怕它们没有 Toleration.</p> <p>那么 Pod 又如何声明 Toleration 呢?</p> <p>只要在 Pod 的 .yaml 文件中的 spec 部分, 加入 <strong>tolerations</strong> 字段即可:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">&quot;foo&quot;</span>
    <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">&quot;Equal&quot;</span>
    <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;bar&quot;</span>
    <span class="token key atrule">effect</span><span class="token punctuation">:</span> <span class="token string">&quot;NoSchedule&quot;</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这个 Toleration 的含义是, 这个 Pod 能 &quot;容忍&quot; 所有键值对为 foo=bar 的 <strong>Taint</strong>( operator: &quot;Equal&quot;, &quot;等于&quot;操作).</p> <p>现在回到已经搭建的集群上来. 这时, 如果通过 kubectl describe 检查一下 Master 节点的 Taint 字段, 就会有所发现了:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl describe <span class="token function">node</span> master
 
Name:               master
Roles:              master
Taints:             node-role.kubernetes.io/master:NoSchedule
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, Master 节点默认被加上了 <code>node-role.kubernetes.io/master:NoSchedule</code>​ 这样一个 &quot;污点&quot;, 其中 &quot;键&quot; 是 <code>node-role.kubernetes.io/master</code>​, 而没有提供 &quot;值&quot;.</p> <p>此时, 就需要像下面这样用 &quot;Exists&quot; 操作符(operator: &quot;Exists&quot;, &quot;存在&quot;即可)来说明, <strong>该 Pod 能够容忍所有以 foo 为键的 Taint, 才能让这个 Pod 运行在该 Master 节点上</strong>:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">&quot;foo&quot;</span>
    <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">&quot;Exists&quot;</span>
    <span class="token key atrule">effect</span><span class="token punctuation">:</span> <span class="token string">&quot;NoSchedule&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>当然, 如果就是想要一个单节点的 Kubernetes, 删除这个 Taint 才是正确的选择:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl taint nodes <span class="token parameter variable">--all</span> node-role.kubernetes.io/master-
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>如上所示, 在 &quot;<code>node-role.kubernetes.io/master</code>​&quot; 这个键后面加上了一个短横线 &quot;-&quot;, 这个格式就意味着<strong>移除</strong>所有以 &quot;<code>node-role.kubernetes.io/master</code>​&quot; 为键的 Taint.</p> <p>到了这一步, 一个基本完整的 Kubernetes 集群就部署完毕了. 是不是很简单呢?</p> <p>有了 kubeadm 这样的原生管理工具, Kubernetes 的部署已经被大大简化. 更重要的是, <strong>像证书, 授权, 各个组件的配置等部署中最麻烦的操作, kubeadm 都已经帮你完成了</strong>.</p> <p>接下来, 再在这个 Kubernetes 集群上安装一些其他的<strong>辅助插件, 比如 Dashboard 和存储插件</strong>.</p> <h5 id="_7-部署dashboard可视化插件"><a href="#_7-部署dashboard可视化插件" class="header-anchor">#</a> 7.部署Dashboard可视化插件</h5> <p>在 Kubernetes 社区中, 有一个很受欢迎的 Dashboard 项目, 它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息. 毫不意外, 它的部署也相当简单:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>部署完成之后, 就可以查看 Dashboard 对应的 Pod 的状态了:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get pods <span class="token parameter variable">-n</span> kube-system
kubernetes-dashboard-6948bdb78-f67xk   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>需要注意的是, 由于 Dashboard 是一个 <strong>Web Server</strong>, 很多人经常会在自己的公有云上无意地暴露 Dashboard 的端口, 从而造成安全隐患. 所以, 1.7 版本之后的 Dashboard 项目部署完成后, <strong>默认只能通过 Proxy 的方式在本地访问</strong>. 具体的操作, 你可以查看 Dashboard 项目的<a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener noreferrer">官方文档<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p><strong>而如果想从集群外访问这个 Dashboard 的话, 就需要用到 Ingress</strong>, 后面的文章会专门介绍这部分内容.</p> <h5 id="_8-部署容器存储插件"><a href="#_8-部署容器存储插件" class="header-anchor">#</a> 8.部署容器存储插件</h5> <p>接下来完成这个 Kubernetes 集群的最后一块拼图: <strong>容器持久化存储</strong>.</p> <p>前面介绍容器原理时已经提到过, <strong>很多时候需要用数据卷(Volume)把外面宿主机上的目录或者文件挂载进容器的 Mount Namespace 中, 从而达到容器和宿主机共享这些目录或者文件的目的. 容器里的应用, 也就可以在这些数据卷中新建和写入文件</strong>.</p> <p>可是, 如果你在某一台机器上启动的一个容器, 显然无法看到其他机器上的容器在它们的数据卷里写入的文件. **这是容器最典型的特征之一: **​<mark><strong>无状态</strong></mark>​ **. **</p> <p>**而容器的持久化存储, 就是用来保存容器存储状态的重要手段: 存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷, 使得在容器里创建的文件, 实际上是保存在远程存储服务器上, 或者以分布式的方式保存在多个节点上, 而与当前宿主机没有任何绑定关系. 这样, 无论你在其他哪个宿主机上启动新的容器, 都可以请求挂载指定的持久化存储卷, 从而访问到数据卷里保存的内容. 这就是&quot;持久化&quot;的含义. **</p> <p>由于 Kubernetes 本身的松耦合设计, 绝大多数存储项目, 比如 Ceph, GlusterFS, NFS 等, 都可以为 Kubernetes 提供持久化存储能力. 在这次的部署实战选择部署一个很重要的 Kubernetes 存储插件项目: <strong>Rook</strong>.</p> <p>Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件(它后期也在加入对更多存储实现的支持). 不过, 不同于对 Ceph 的简单封装, Rook 在自己的实现中加入了水平扩展, 迁移, 灾难备份, 监控等大量的企业级功能, 使得这个项目变成了一个完整的, 生产级别可用的容器存储插件.</p> <p>得益于容器化技术, 用两条指令, Rook 就可以把复杂的 Ceph 存储后端部署起来:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在部署完成后, 就可以看到 Rook 项目会<strong>将自己的 Pod 放置在由它自己管理的两个 Namespace 当中</strong>:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get pods <span class="token parameter variable">-n</span> rook-ceph-system
NAME                                  READY     STATUS    RESTARTS   AGE
rook-ceph-agent-7cv62                 <span class="token number">1</span>/1       Running   <span class="token number">0</span>          15s
rook-ceph-operator-78d498c68c-7fj72   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          44s
rook-discover-2ctcv                   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          15s
 
$ kubectl get pods <span class="token parameter variable">-n</span> rook-ceph
NAME                   READY     STATUS    RESTARTS   AGE
rook-ceph-mon0-kxnzh   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          13s
rook-ceph-mon1-7dn2t   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          2s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这样, 一个基于 Rook 持久化存储集群就以容器的方式运行起来了, 而接下来<strong>在 Kubernetes 项目上创建的所有 Pod 就能够通过 Persistent Volume(PV)和 Persistent Volume Claim(PVC)的方式, 在容器里挂载由 Ceph 提供的数据卷了</strong>. 而 Rook 项目, 则会负责这些数据卷的生命周期管理, 灾难备份等运维工作. 关于这些容器持久化存储的知识, 会在后续章节中专门讲解.</p> <p>你可能会有个疑问: 为什么要选择 Rook 项目呢?</p> <p>其实, 是因为这个项目很有前途. 如果你去研究一下 Rook 项目的实现, 就会发现它巧妙地<strong>依赖了 Kubernetes 提供的编排能力</strong>, 合理的使用了很多诸如 Operator, CRD 等重要的扩展特性(这些特性都会在后面逐一讲解到). 这使得 Rook 项目, 成为了目前社区中基于 Kubernetes API 构建的最完善也最成熟的容器存储插件.</p> <p>备注: 其实在很多时候, 大家说的所谓 &quot;云原生&quot;, 就是 &quot;<strong>Kubernetes 原生</strong>&quot; 的意思. 而像 Rook, Istio 这样的项目, 正是<strong>贯彻这个思路的典范</strong>. 在后面讲解了声明式 API 之后, 相信你对这些项目的设计思想会有更深刻的体会.</p> <h5 id="_9-总结"><a href="#_9-总结" class="header-anchor">#</a> 9.总结</h5> <p>本节完全从 0 开始, 在 Bare-metal 环境下使用 kubeadm 工具部署了一个完整的 Kubernetes 集群: <strong>这个集群有一个 Master 节点和多个 Worker 节点; 使用 Weave 作为容器网络插件; 使用 Rook 作为容器持久化存储插件; 使用 Dashboard 插件提供了可视化的 Web 界面</strong>. 这个集群, 也将会是进行后续讲解所依赖的集群环境, 并且在后面的讲解中, 还会给它安装更多的插件, 添加更多的新能力.</p> <p>另外, 这个集群的部署过程并不像传说中那么繁琐, 这主要得益于:</p> <ol><li>kubeadm 项目大大简化了部署 Kubernetes 的准备工作, 尤其是配置文件, 证书, 二进制文件的准备和制作, 以及集群版本管理等操作, 都被 kubeadm 接管了.</li> <li>Kubernetes 本身 &quot;一切皆容器&quot; 的设计思想, 加上良好的可扩展机制, 使得插件的部署非常简便.</li></ol> <p>上述思想, 也是开发和使用 Kubernetes 的重要指导思想, 即: 基于 Kubernetes 开展工作时, 一定要优先考虑这两个问题:</p> <ol><li>**我的工作是不是可以容器化? **</li> <li>**我的工作是不是可以借助 Kubernetes API 和可扩展机制来完成? **</li></ol> <p>而一旦这项工作能够基于 Kubernetes 实现容器化, 就很有可能像上面的部署过程一样, 大幅简化原本复杂的运维工作. 对于时间宝贵的技术人员来说, 这个变化的重要性是不言而喻的.</p> <h4 id="_12-牛刀小试-我的第一个容器化应用"><a href="#_12-牛刀小试-我的第一个容器化应用" class="header-anchor">#</a> 12 | 牛刀小试:我的第一个容器化应用</h4> <p>前一节部署了一套完整的 Kubernetes 集群. 这个集群虽然离生产环境的要求还有一定差距(比如没有一键高可用部署), 但也可以当作是一个准生产级别的 Kubernetes 集群了. 本节就来扮演一个应用开发者的角色, 使用这个 Kubernetes 集群<strong>发布第一个容器化应用</strong>.</p> <p>在开始实践之前, 先讲解一下 Kubernetes 里面与开发者关系最密切的几个概念.</p> <p>作为一个应用开发者, 首先要做的是<strong>制作容器的镜像</strong>. 这一部分内容, 已经在前面重点讲解过了.</p> <p>而有了容器镜像之后, 需要按照 Kubernetes 项目的规范和要求, 将镜像组织为它能够 &quot;认识&quot; 的方式, 然后<strong>提交</strong>上去. 那什么才是 Kubernetes 项目能 &quot;认识&quot; 的方式呢?</p> <p>这就是使用 Kubernetes 的必备技能: <strong>编写配置文件</strong>. 这些配置文件可以是 YAML 或者 JSON 格式的. 为方便阅读与理解, 在后面的讲解中, 会统一使用 YAML 文件来指代它们.</p> <p>Kubernetes 跟 Docker 等很多项目最大的不同, 就在于它<strong>不推荐你使用命令行的方式直接运行容器</strong>(虽然 Kubernetes 项目也支持这种方式, 比如: kubectl run), 而是<strong>希望你用 YAML 文件的方式, 即: 把容器的定义, 参数, 配置, 统统记录在一个 YAML 文件中, 然后用这样一句指令把它运行起来</strong>:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl create <span class="token parameter variable">-f</span> 我的配置文件
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这么做最直接的好处是, 你会有一个文件能记录下 Kubernetes 到底 &quot;run&quot; 了什么. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>像这样的<strong>一个 YAML 文件, 对应到 Kubernetes 中, 就是一个 API Object(API 对象)</strong> . 当<strong>为这个对象的各个字段填好值并提交给 Kubernetes 之后, Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源</strong>.</p> <p>可以看到, 这个 YAML 文件中的 <strong>Kind 字段, 指定了这个 API 对象的类型(Type), 是一个 Deployment</strong>. 所谓 Deployment, 是一个<strong>定义多副本应用(即多个副本 Pod)的对象</strong>, 前面的章节中也曾经简单提到过它的用法. 此外, Deployment 还负责在 Pod 定义发生变化时, 对<strong>每个副本进行滚动更新</strong>(Rolling Update).</p> <p>在上面这个 YAML 文件中, 给它定义的 <strong>Pod 副本个数 (spec.replicas)</strong>  是: 2.</p> <p>而这些 Pod 具体的又长什么样子呢?</p> <p>为此, 定义了一个 <strong>Pod 模版(spec.template)</strong> , 这个模版描述了想要创建的 Pod 的细节. 在上面的例子里, 这个 Pod 里<strong>只有一个容器, 这个容器的镜像(spec.containers.image)是 nginx:1.7.9, 这个容器监听端口(containerPort)是 80</strong>.</p> <p>需要记住这样一句话:</p> <blockquote><p>Pod 就是 Kubernetes 世界里的&quot;应用&quot;; 而一个应用, 可以由多个容器组成.</p></blockquote> <p>需要注意的是, <strong>像这样使用一种 API 对象(Deployment)管理另一种 API 对象(Pod)的方法, 在 Kubernetes 中, 叫作 &quot;控制器&quot; 模式(controller pattern)</strong> . 在上面的例子中, Deployment 扮演的正是 Pod 的控制器的角色. 关于 Pod 和控制器模式的更多细节, 会在后续编排部分做进一步讲解.</p> <p>你可能还注意到, 这样的<strong>每一个 API 对象都有一个叫作 Metadata 的字段, 这个字段就是 API 对象的 &quot;标识&quot;, 即元数据, 它也是我们从 Kubernetes 里找到这个对象的主要依据. 这其中最主要使用到的字段是 Labels</strong>. 顾名思义, Labels 就是一组 key-value 格式的标签. 而像 Deployment 这样的控制器对象, 就可以<strong>通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象</strong>.</p> <p>比如在上面这个 YAML 文件中, Deployment 会把所有<strong>正在运行的, 携带 &quot;app: nginx&quot; 标签的 Pod 识别为被管理的对象, 并确保这些 Pod 的总数严格等于两个</strong>.</p> <p>而这个过滤规则的定义, 是在 Deployment 的 &quot;<strong>spec.selector.matchLabels</strong>&quot; 字段. 我们一般称之为: <strong>Label Selector</strong>.</p> <p>另外, 在 Metadata 中, 还有一个<strong>与 Labels 格式, 层级完全相同的字段叫 Annotations</strong>, 它专门用来携带 key-value 格式的内部信息. 所谓内部信息, 指的是对这些信息感兴趣的, 是 Kubernetes 组件本身, 而不是用户. 所以大多数 Annotations, 都是在 Kubernetes 运行过程中, 被自动加在这个 API 对象上.</p> <p>一个 Kubernetes 的 API 对象的定义, 大多可以<strong>分为 Metadata 和 Spec 两个部分. 前者存放的是这个对象的元数据, 对所有 API 对象来说, 这一部分的字段和格式基本上是一样的; 而后者存放的, 则是属于这个对象独有的定义, 用来描述它所要表达的功能</strong>.</p> <p>在了解了上述 Kubernetes 配置文件的基本知识之后, 现在就可以把这个 YAML 文件 &quot;运行&quot; 起来. 正如前所述, 可以使用 kubectl create 指令完成这个操作:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl create <span class="token parameter variable">-f</span> nginx-deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后, 通过 kubectl get 命令检查这个 YAML 运行起来的状态是不是与预期的一致:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get pods <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-67594d6bf6-9gdvr   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          10m
nginx-deployment-67594d6bf6-v6j7w   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          10m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><strong>kubectl get 指令的作用, 就是从 Kubernetes 里面获取(GET)指定的 API 对象</strong>. 可以看到, 在这里还加上了一个 -l 参数, 即获取所有<strong>匹配 app: nginx 标签的 Pod</strong>. 需要注意的是, **在命令行中, 所有 key-value 格式的参数, 都使用 &quot;=&quot; 而非 &quot;:&quot; 表示. **</p> <p>从这条指令返回的结果中, 可以看到现在有两个 Pod 处于 Running 状态, 也就意味着这个 Deployment 所管理的 Pod 都处于预期的状态.</p> <p>此外, 还可以<strong>使用 kubectl describe 命令, 查看一个 API 对象的细节</strong>, 比如:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl describe pod nginx-deployment-67594d6bf6-9gdvr
Name:               nginx-deployment-67594d6bf6-9gdvr
Namespace:          default
Priority:           <span class="token number">0</span>
PriorityClassName:  <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Node:               node-1/10.168.0.3
Start Time:         Thu, <span class="token number">16</span> Aug <span class="token number">2018</span> 08:48:42 +0000
Labels:             <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx
                    pod-template-hash<span class="token operator">=</span><span class="token number">2315082692</span>
Annotations:        <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Status:             Running
IP:                 <span class="token number">10.32</span>.0.23
Controlled By:      ReplicaSet/nginx-deployment-67594d6bf6
<span class="token punctuation">..</span>.
Events:
 
  Type     Reason                  Age                From               Message
 
  ----     ------                  ----               ----               -------
  
  Normal   Scheduled               1m                 default-scheduler  Successfully assigned default/nginx-deployment-67594d6bf6-9gdvr to node-1
  Normal   Pulling                 25s                kubelet, node-1    pulling image <span class="token string">&quot;nginx:1.7.9&quot;</span>
  Normal   Pulled                  17s                kubelet, node-1    Successfully pulled image <span class="token string">&quot;nginx:1.7.9&quot;</span>
  Normal   Created                 17s                kubelet, node-1    Created container
  Normal   Started                 17s                kubelet, node-1    Started container
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>在 kubectl describe 命令返回的结果中, 可以清楚地看到这个 Pod 的详细信息, 比如它的 IP 地址等等. 其中, 有一个部分值得你特别关注, 它就是 **Events(事件). ** 在 Kubernetes 执行的过程中, <strong>对 API 对象的所有重要操作, 都会被记录在这个对象的 Events 里</strong>, 并且显示在 kubectl describe 指令返回的结果中. 比如, 对于这个 Pod, 可以看到它被创建之后, 被调度器调度(Successfully assigned)到了 node-1, 拉取了指定的镜像(pulling image), 然后启动了 Pod 里定义的容器(Started container). 所以, 这个部分正是将来进行 Debug 的重要依据. <strong>如果有异常发生, 一定要第一时间查看这些 Events</strong>, 往往可以看到非常详细的错误信息.</p> <p>接下来, 如果要<strong>对这个 Nginx 服务进行升级</strong>, 把它的镜像版本从 1.7.9 升级为 1.8, 要怎么做呢?</p> <p>很简单, 只要修改这个 YAML 文件即可.</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>  
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span><span class="token number">1.8</span> <span class="token comment"># 这里被从 1.7.9 修改为 1.8</span>
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可是, 这个修改目前<strong>只发生在本地</strong>, 如何让这个更新在 Kubernetes 里也生效呢?</p> <p>可以使用 kubectl replace 指令来完成这个更新:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl replace <span class="token parameter variable">-f</span> nginx-deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>不过本专栏里还是推荐使用 <strong>kubectl apply</strong> 命令, 来统一进行 Kubernetes 对象的创建和更新操作, 具体做法如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> nginx-deployment.yaml
<span class="token comment"># 修改 nginx-deployment.yaml 的内容</span>
$ kubectl apply <span class="token parameter variable">-f</span> nginx-deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>**这样的操作方法, 是 Kubernetes &quot;声明式 API&quot; 所推荐的使用方法. 也就是说, 作为用户, 你不必关心当前的操作是创建, 还是更新, 你执行的命令始终是 kubectl apply, 而 Kubernetes 则会根据 YAML 文件的内容变化, 自动进行具体的处理. ** 这个流程的好处是, 它有助于帮助开发和运维人员, <strong>围绕着可以版本化管理的 YAML 文件</strong>, 而不是 &quot;行踪不定&quot; 的命令行进行协作, 从而大大降低开发人员和运维人员之间的沟通成本.</p> <p>举个例子, 一位开发人员开发好一个应用, 制作好了容器镜像. 那么他就可以<strong>在应用的发布目录里附带上一个 Deployment 的 YAML 文件</strong>. 而运维人员拿到这个应用的发布目录后, 就可以<strong>直接用这个 YAML 文件执行 kubectl apply 操作把它运行起来</strong>. 这时候, 如果开发人员修改了应用, 生成了新的发布内容, 那么这个 YAML 文件, 也就需要被修改, 并且成为这次<strong>变更的一部分</strong>. 而接下来, 运维人员可以使用 git diff 命令查看到这个 YAML 文件本身的变化, 然后<strong>继续用 kubectl apply 命令更新这个应用</strong>.</p> <p>所以说, 如果通过容器镜像, 我们能够保证应用本身在开发与部署环境里的一致性的话, 那么现在, <strong>Kubernetes 项目通过这些 YAML 文件, 就保证了应用的&quot;部署参数&quot;在开发与部署环境中的一致性</strong>.</p> <p><mark><strong>而当应用本身发生变化时, 开发人员和运维人员可以依靠容器镜像来进行同步; 当应用部署参数发生变化时, 这些 YAML 文件就是他们相互沟通和信任的媒介. 大家都通过 YML 文件进行管理.</strong></mark></p> <p>以上就是 Kubernetes 发布应用的最基本操作了.</p> <p>接下来, 再在这个 Deployment 中尝试声明一个 Volume.</p> <p>在 Kubernetes 中, Volume 是属于 <strong>Pod 对象</strong>的一部分. 所以就需要修改这个 YAML 文件里的 template.spec 字段, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span><span class="token number">1.8</span>
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&quot;/usr/share/nginx/html&quot;</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>vol
      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>vol
        <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>可以看到, 在 Deployment 的 <strong>Pod 模板部分添加了一个 volumes 字段, 定义了这个 Pod 声明的所有 Volume</strong>. 它的名字叫作 nginx-vol, 类型是 <strong>emptyDir</strong>.</p> <p>那什么是 emptyDir 类型呢? 它其实就等同于之前讲过的 Docker 的隐式 Volume 参数, 即: <strong>不显式声明宿主机目录的 Volume. 所以, Kubernetes 也会在宿主机上创建一个临时目录, 这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上</strong>.</p> <p>不难看到, Kubernetes 的 emptyDir 类型, 只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录, 交给了 Docker. 这么做的原因, 是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录.</p> <p><strong>而 Pod 中的容器, 使用的是 volumeMounts 字段来声明自己要挂载哪个 Volume, 并通过 mountPath 字段来定义容器内的 Volume 目录</strong>, 比如: /usr/share/nginx/html.</p> <p>当然, Kubernetes 也提供了显式的 Volume 定义, 它叫做 <strong>hostPath</strong>. 比如下面的这个 YAML 文件:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>   
    <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>vol
        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span> 
          <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/data
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这样, 容器 Volume 挂载的<strong>宿主机目录</strong>, 就变成了 /var/data.</p> <p>在上述修改完成后, 还是使用 kubectl apply 指令, 更新这个 Deployment:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> nginx-deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>接下来, 可以通过 kubectl get 指令, 查看<strong>两个 Pod 被逐一更新</strong>的过程:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl get pods
NAME                                READY     STATUS              RESTARTS   AGE
nginx-deployment-5c678cfb6d-v5dlh   <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>          4s
nginx-deployment-67594d6bf6-9gdvr   <span class="token number">1</span>/1       Running             <span class="token number">0</span>          10m
nginx-deployment-67594d6bf6-v6j7w   <span class="token number">1</span>/1       Running             <span class="token number">0</span>          10m
$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-5c678cfb6d-lg9lw   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          8s
nginx-deployment-5c678cfb6d-v5dlh   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          19s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>从返回结果中可以看到, <strong>新旧两个 Pod, 被交替创建, 删除, 最后剩下的就是新版本的 Pod</strong>. 这个滚动更新的过程, 也会在后续进行详细的讲解.</p> <p>然后, 可以使用 kubectl describe 查看一下最新的 Pod, 就会发现 Volume 的信息已经出现在了 Container 描述部分:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token punctuation">..</span>.
Containers:
  nginx:
    Container ID:   docker://07b4f89248791c2aa47787e3da3cc94b48576cd173018356a6ec8db2b6041343
    Image:          nginx:1.8
    <span class="token punctuation">..</span>.
    Environment:    <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Mounts:
      /usr/share/nginx/html from nginx-vol <span class="token punctuation">(</span>rw<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.
Volumes:
  nginx-vol:
    Type:    EmptyDir <span class="token punctuation">(</span>a temporary directory that shares a pod's lifetime<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>作为一个完整的容器化平台项目, Kubernetes 提供的 Volume 类型远远不止这些, 在容器存储章节里将会详细介绍这部分内容.</p> <p>最后, 还可以使用 <strong>kubectl exec 指令, 进入到这个 Pod 当中(即容器的 Namespace 中)查看这个 Volume 目录</strong>:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash
<span class="token comment"># ls /usr/share/nginx/html</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>此外, 如果想要从 Kubernetes 集群中删除这个 Nginx Deployment 的话, 直接执行下面命令即可:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ kubectl delete <span class="token parameter variable">-f</span> nginx-deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>总结</p></blockquote> <p>本节通过一个小案例体验了 Kubernetes 的使用方法.</p> <p>可以看到, <mark><strong>Kubernetes 推荐的使用方式, 是用一个 YAML 文件来描述所要部署的 API 对象. 然后统一使用 kubectl apply 命令完成对这个对象的创建和更新操作</strong></mark>.</p> <p><strong>而 Kubernetes 里 &quot;最小&quot; 的 API 对象是 Pod. Pod 可以等价为一个应用, 所以, Pod 可以由多个紧密协作的容器组成</strong>.</p> <p><strong>在 Kubernetes 中, 经常会看到它通过一种 API 对象来管理另一种 API 对象, 比如 Deployment 和 Pod 之间的关系; 而由于 Pod 是 &quot;最小&quot;的对象, 所以它往往都是被其他对象控制的. 这种组合方式, 正是 Kubernetes 进行容器编排的重要模式</strong>.</p> <p>而像这样的 Kubernetes API 对象, 往往由 <strong>Metadata 和 Spec 两部分组成</strong>, 其中 Metadata 里的 Labels 字段是 Kubernetes 过滤对象的主要手段.</p> <p>在这些字段里面, 容器想要使用的数据卷, 也就是 Volume, 正是 Pod 的 Spec 字段的一部分. 而 Pod 里的每个容器, 则需要显式的声明自己要挂载哪个 Volume.</p> <p>上面这些<mark><strong>基于 YAML 文件的容器管理方式</strong></mark>, 跟 Docker, Mesos 的使用习惯都是不一样的, 而从 docker run 这样的命令行操作, 向 kubectl apply YAML 文件这样的声明式 API 的转变, 是每一个容器技术学习者, 必须要跨过的第一道门槛.</p> <p>所以, 如果想要快速熟悉 Kubernetes, 请按照下面的流程进行练习:</p> <ul><li>**首先, 在本地通过 Docker 测试代码, 制作镜像; **</li> <li>**然后, 选择合适的 Kubernetes API 对象, 编写对应 YAML 文件(比如, Pod, Deployment); **</li> <li>**最后, 在 Kubernetes 上部署这个 YAML 文件. **</li></ul> <p>更重要的是, 在部署到 Kubernetes 之后, 接下来的所有操作, 要么通过 kubectl 来执行, 要么<strong>通过修改 YAML 文件</strong>来实现, <strong>就尽量不要再碰 Docker 的命令行了</strong>.</p> <h3 id="容器编排与kubernetes作业管理"><a href="#容器编排与kubernetes作业管理" class="header-anchor">#</a> 容器编排与Kubernetes作业管理</h3> <h4 id="_13-为什么需要pod"><a href="#_13-为什么需要pod" class="header-anchor">#</a> 13 | 为什么需要Pod?</h4> <p>前面详细介绍了在 Kubernetes 里部署一个应用的过程并提到了这样一个知识点: <strong>Pod, 是 Kubernetes 项目中最小的 API 对象</strong>. 如果换一个更专业的说法就是: <strong>Pod, 是 Kubernetes 项目的原子调度单位</strong>.</p> <p>可能你会问: 为什么我们会需要 Pod? 前面已经花了很多精力去解读 Linux 容器的原理, 分析了 Docker 容器的本质, 终于, &quot;<mark><strong>Namespace 做隔离, Cgroups 做限制, rootfs 做文件系统</strong></mark>&quot; 这样的 &quot;三句箴言&quot; 可以朗朗上口了, **为什么 Kubernetes 项目又突然搞出一个 Pod 来呢? **</p> <p>要回答这个问题, 还是要一起回忆一下我曾经反复强调的一个问题: <strong>容器的本质到底是什么</strong>? 你现在应该可以不假思索地回答出来: <strong>容器的本质是进程</strong>. 没错. 容器就是未来云计算系统中的进程; 容器镜像就是这个系统里的 &quot;.exe&quot; 安装包. 那么 Kubernetes 呢? 你应该也能立刻回答上来: <strong>Kubernetes 就是操作系统</strong>! 非常正确.</p> <p>现在登录到一台 Linux 机器里, 执行一条如下所示的命令:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ pstree <span class="token parameter variable">-g</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这条命令的作用, 是展示当前系统中<strong>正在运行的进程</strong>的树状结构. 它的返回结果如下所示:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>systemd<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>-+-accounts-daemon<span class="token punctuation">(</span><span class="token number">1984</span><span class="token punctuation">)</span>-+-<span class="token punctuation">{</span>gdbus<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1984</span><span class="token punctuation">)</span>
           <span class="token operator">|</span> <span class="token variable"><span class="token variable">`</span>-<span class="token punctuation">{</span>gmain<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1984</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>-acpid<span class="token punctuation">(</span><span class="token number">2044</span><span class="token punctuation">)</span>
          <span class="token punctuation">..</span>.    
           <span class="token operator">|</span>-lxcfs<span class="token punctuation">(</span><span class="token number">1936</span><span class="token punctuation">)</span>-+-<span class="token punctuation">{</span>lxcfs<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1936</span><span class="token punctuation">)</span>
           <span class="token operator">|</span> <span class="token variable">`</span></span>-<span class="token punctuation">{</span>lxcfs<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1936</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>-mdadm<span class="token punctuation">(</span><span class="token number">2135</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>-ntpd<span class="token punctuation">(</span><span class="token number">2358</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>-polkitd<span class="token punctuation">(</span><span class="token number">2128</span><span class="token punctuation">)</span>-+-<span class="token punctuation">{</span>gdbus<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">2128</span><span class="token punctuation">)</span>
           <span class="token operator">|</span> <span class="token variable"><span class="token variable">`</span>-<span class="token punctuation">{</span>gmain<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">2128</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>-rsyslogd<span class="token punctuation">(</span><span class="token number">1632</span><span class="token punctuation">)</span>-+-<span class="token punctuation">{</span>in:imklog<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1632</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>  <span class="token operator">|</span>-<span class="token punctuation">{</span>in:imuxsock<span class="token punctuation">)</span> S <span class="token number">1</span><span class="token punctuation">(</span><span class="token number">1632</span><span class="token punctuation">)</span>
           <span class="token operator">|</span> <span class="token variable">`</span></span>-<span class="token punctuation">{</span>rs:main Q:Reg<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1632</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>-snapd<span class="token punctuation">(</span><span class="token number">1942</span><span class="token punctuation">)</span>-+-<span class="token punctuation">{</span>snapd<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1942</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>  <span class="token operator">|</span>-<span class="token punctuation">{</span>snapd<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1942</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>  <span class="token operator">|</span>-<span class="token punctuation">{</span>snapd<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1942</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>  <span class="token operator">|</span>-<span class="token punctuation">{</span>snapd<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1942</span><span class="token punctuation">)</span>
           <span class="token operator">|</span>  <span class="token operator">|</span>-<span class="token punctuation">{</span>snapd<span class="token punctuation">}</span><span class="token punctuation">(</span><span class="token number">1942</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>不难发现, 在一个真正的操作系统里, 进程并不是 &quot;孤苦伶仃&quot; 地独自运行的, 而是以<strong>进程组</strong>的方式, &quot;有原则地&quot; 组织在一起. 比如, 这里有一个叫作 rsyslogd 的程序, 它负责的是 Linux 操作系统里的日志处理. 可以看到, rsyslogd 的主程序 main, 和它要用到的内核日志模块 imklog 等, <strong>同属于 1632 进程组. 这些进程相互协作, 共同完成 rsyslogd 程序的职责</strong>.</p> <p>注意: 本篇中提到的&quot;进程&quot;, 比如, rsyslogd 对应的 imklog, imuxsock 和 main, 严格意义上来说, 其实是 Linux 操作系统语境下的 &quot;<strong>线程</strong>&quot;. 这些线程, 或者说, 轻量级进程之间, <strong>可以共享文件, 信号, 数据内存, 甚至部分代码, 从而紧密协作共同完成一个程序的职责</strong>. 所以同理, 本节提到的 &quot;进程组&quot;, 对应的也是 Linux 操作系统语境下的 &quot;线程组&quot;. 这种命名关系与实际情况的不一致, 是 Linux 发展历史中的一个遗留问题. 具体可以阅读<a href="https://www.ibm.com/developerworks/cn/linux/kernel/l-thread/index.html" target="_blank" rel="noopener noreferrer">这篇技术文章<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>来了解一下.</p> <p>**而 Kubernetes 项目所做的, 其实就是将&quot;进程组&quot;的概念映射到了容器技术中, 并使其成为了这个云计算&quot;操作系统&quot;里的&quot;一等公民&quot;. **</p> <p>Kubernetes 项目之所以要这么做的原因, 在前面介绍 Kubernetes 和 Borg 的关系时曾经提到过: <strong>在 Borg 项目的开发和实践过程中, Google 公司的工程师们发现, 他们部署的应用, 往往都存在着类似于 &quot;进程和进程组&quot; 的关系. 更具体地说, 就是这些应用之间有着密切的协作关系, 使得它们必须部署在同一台机器上</strong>.</p> <p>而如果事先没有 &quot;组&quot; 的概念, 像这样的运维关系就会非常难以处理.</p> <p>还是以前面的 rsyslogd 为例子. 已知 rsyslogd 由三个进程组成: 一个 imklog 模块, 一个 imuxsock 模块, 一个 rsyslogd 自己的 main 函数主进程. <strong>这三个进程一定要运行在同一台机器上</strong>, 否则它们之间基于 Socket 的通信和文件交换, 都会出现问题.</p> <p>现在, 要把 rsyslogd 这个应用给容器化, 由于<strong>受限于容器的 &quot;单进程模型&quot;, 这三个模块必须被分别制作成三个不同的容器</strong>. 而在这三个容器运行的时候, 它们设置的内存配额都是 1 GB.</p> <p>再次强调一下: <mark><strong>容器的 &quot;单进程模型&quot;, 并不是指容器里只能运行 &quot;一个 &quot;进程, 而是指容器没有管理多个进程的能力</strong></mark>. <strong>这是因为容器里 PID=1 的进程就是应用本身, 其他的进程都是这个 PID=1 进程的子进程. 可是, 用户编写的应用, 并不能够像正常操作系统里的 init 进程或者 systemd 那样拥有进程管理的功能</strong>. 比如, 你的应用是一个 Java Web 程序(PID=1), 然后执行 docker exec 在后台启动了一个 Nginx 进程(PID=3). 可是当这个 Nginx 进程异常退出的时候, 该怎么知道呢? 这个进程退出后的垃圾收集工作, 又应该由谁去做呢?</p> <p>假设我们的 Kubernetes 集群上有<strong>两个节点</strong>: node-1 上有 3 GB 可用内存, node-2 有 2.5 GB 可用内存.</p> <p>这时, 假设要用 Docker Swarm 来运行这个 rsyslogd 程序. 为了能够让这三个容器都运行在同一台机器上, 就必须在另外两个容器上设置一个 <strong>affinity=main</strong>(与 main 容器有亲密性)的约束, 即: <strong>它们俩必须和 main 容器运行在同一台机器上</strong>.</p> <p>然后, 顺序执行: &quot;docker run main&quot;, &quot;docker run imklog&quot;, &quot;docker run imuxsock&quot;, 创建这三个容器.</p> <p>这样, 这三个容器都会进入 Swarm 的待调度队列. 然后, main 容器和 imklog 容器都先后出队并被调度到了 node-2 上(这个情况是完全有可能的). 可是, 当 imuxsock 容器出队开始被调度时, Swarm 就有点懵了: node-2 上的可用资源只有 0.5 GB 了, 并不足以运行 imuxsock 容器; 可是, 根据 affinity=main 的约束, imuxsock 容器又只能运行在 node-2 上. 这就是一个典型的<strong>成组调度(gang scheduling)没有被妥善处理</strong>的例子.</p> <p>在工业界和学术界, 关于这个问题的讨论可谓旷日持久, 也产生了很多可供选择的解决方案.</p> <p>比如, Mesos 中就有一个<strong>资源囤积</strong>(resource hoarding)的机制, 会在所有设置了 Affinity 约束的任务都达到时, 才开始对它们统一进行调度. 而在 Google Omega 论文中, 则提出了使用乐观调度处理冲突的方法, 即: <strong>先不管这些冲突, 而是通过精心设计的回滚机制在出现了冲突之后解决问题</strong>. 可是这些方法都谈不上完美. 资源囤积带来了不可避免的调度效率损失和死锁的可能性; 而乐观调度的复杂程度, 则不是常规技术团队所能驾驭的.</p> <p>但是, 到了 Kubernetes 项目里, 这样的问题就迎刃而解了: <strong>Pod 是 Kubernetes 里的原子调度单位. 这就意味着, Kubernetes 项目的调度器, 是统一按照 Pod 而非容器的资源需求进行计算的</strong>.</p> <p>所以, 像 imklog, imuxsock 和 main 函数主进程这样的三个容器, 正是<strong>一个典型的</strong>​<mark><strong>由三个容器组成的 Pod</strong></mark>​ <strong>. Kubernetes 项目在调度时, 自然就会去选择可用内存等于 3 GB 的 node-1 节点进行绑定, 而根本不会考虑 node-2</strong>.</p> <p>像这样<strong>容器间的紧密协作</strong>, 我们可以称为 &quot;<mark><strong>超亲密关系</strong></mark>&quot;. 这些具有 &quot;超亲密关系&quot; 容器的典型特征包括但不限于: <mark><strong>互相之间会发生直接的文件交换, 使用 localhost 或者 Socket 文件进行本地通信, 会发生非常频繁的远程调用, 需要共享某些 Linux Namespace(比如, 一个容器要加入另一个容器的 Network Namespace)等等</strong></mark>.</p> <p>这也就意味着, 并不是所有有 &quot;关系&quot; 的容器都属于同一个 Pod. 比如, PHP 应用容器和 MySQL 虽然会发生访问关系, 但<strong>并没有必要, 也不应该部署在同一台机器上, 它们更适合做成两个 Pod</strong>.</p> <p>不过此时你可能会有<strong>第二个疑问: ** 对于初学者来说, 一般都是先学会了用 Docker 这种单容器的工具, 才会开始接触 Pod. 而如果 Pod 的设计只是出于</strong>调度上的考虑**, 那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为 &quot;一等公民&quot; 吧? 这不是故意增加用户的学习门槛吗?</p> <p>没错, 如果只是处理 &quot;超亲密关系&quot; 这样的调度问题, 有 Borg 和 Omega 论文珠玉在前, Kubernetes 项目肯定可以在调度器层面给它解决掉. 不过, **Pod 在 Kubernetes 项目里还有更重要的意义, 那就是: **​<mark><strong>容器设计模式</strong></mark>.</p> <p>为了理解这一层含义, 就必须先介绍一下Pod 的实现原理.</p> <p>**首先, 关于 Pod 最重要的一个事实是: 它只是一个逻辑概念. ** 也就是说, <strong>Kubernetes 真正处理的, 还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups, 而并不存在一个所谓的 Pod 的边界或者隔离环境</strong>.</p> <p>那么, Pod 又是怎么被&quot;创建&quot;出来的呢? 答案是: <mark><strong>Pod 其实是一组共享了某些资源的容器</strong></mark>. 具体的说: <mark><strong>Pod 里的所有容器, 共享的是同一个 Network Namespace, 并且可以声明共享同一个 Volume</strong></mark>​ **. **</p> <p>那这么来看的话, <strong>一个有 A, B 两个容器的 Pod, 不就是等同于一个容器(容器 A)共享另外一个容器(容器 B)的网络和 Volume 的玩儿法么</strong>? 这好像通过 docker run --net --volumes-from 这样的命令就能实现嘛, 比如:</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">--net</span><span class="token operator">=</span>B --volumes-from<span class="token operator">=</span>B <span class="token parameter variable">--name</span><span class="token operator">=</span>A image-A <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>但你有没有考虑过, 如果真这样做的话, <strong>容器 B 就必须比容器 A 先启动, 这样一个 Pod 里的多个容器就不是对等关系, 而是拓扑关系了</strong>.</p> <p>所以, 在 Kubernetes 项目里, <mark><strong>Pod 的实现需要使用一个中间容器, 这个容器叫作 Infra 容器</strong></mark>. 在这个 Pod 中, <mark><strong>Infra 容器永远都是第一个被创建的容器, 而其他用户定义的容器, 则通过 Join Network Namespace 的方式, 与 Infra 容器关联在一起</strong></mark>. 这样的组织关系, 可以用下面这样一个示意图来表达:</p> <p><img src="/img/add08656550069dc4d93e50a21b45b10-20230731162150-ntids5t.png" alt=""></p> <p>如上图所示, 这个 Pod 里有<strong>两个用户容器 A 和 B, 还有一个 Infra 容器</strong>. 很容易理解, 在 Kubernetes 项目里, Infra 容器一定要占用<strong>极少的资源</strong>, 所以它使用的是一个非常特殊的镜像, 叫作: <code>k8s.gcr.io/pause</code>​. 这个镜像是一个用汇编语言编写的, 永远<strong>处于 &quot;暂停&quot; 状态的容器</strong>, 解压后的大小也只有 100~200 KB 左右.</p> <p><strong>而在 Infra 容器 &quot;Hold 住&quot; Network Namespace 后, 用户容器就可以加入到 Infra 容器的 Network Namespace 当中了</strong>. 所以如果查看这些容器在宿主机上的 Namespace 文件(这个 Namespace 文件的路径, 前面的内容中介绍过), 它们指向的值一定是<strong>完全一样</strong>的.</p> <p>这也就意味着, 对于 Pod 里的容器 A 和容器 B 来说:</p> <ul><li>它们可以直接使用 <strong>localhost</strong> 进行通信;</li> <li>它们看到的<strong>网络设备</strong>跟 Infra 容器看到的完全一样;</li> <li>**一个 Pod 只有一个 IP 地址, 也就是这个 Pod 的 Network Namespace 对应的 IP 地址; **</li> <li>当然, 其他的所有网络资源, 都是一个 Pod 一份, 并且被该 Pod 中的所有容器共享;</li> <li>**Pod 的生命周期只跟 Infra 容器一致, 而与容器 A 和 B 无关. **</li></ul> <p>而对于同一个 Pod 里面的所有用户容器来说, 它们的<strong>进出流量</strong>, 也可以认为都是<strong>通过 Infra 容器</strong>完成的. 这一点很重要, 因为<mark><strong>如果要为 Kubernetes 开发一个网络插件时, 应该重点考虑的是如何配置这个 Pod 的 Network Namespace, 而不是每一个用户容器如何使用你的网络配置, 这是没有意义的</strong></mark>​ <strong>. ** 这就意味着, 如果你的网络插件</strong>需要在容器里安装某些包或者配置才能完成的话, 是不可取的**: Infra 容器镜像的 rootfs 里<strong>几乎什么都没有, 没有你随意发挥的空间</strong>. 当然, 这同时也意味着你的<strong>网络插件完全不必关心用户容器的启动与否, 而只需要关注如何配置 Pod, 也就是 Infra 容器的 Network Namespace 即可</strong>.</p> <p>有了这个设计之后, <strong>共享 Volume</strong> 就简单多了: <mark><strong>Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可</strong></mark>. 这样, 一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个, <strong>Pod 里的容器只要声明挂载这个 Volume, 就一定可以共享这个 Volume 对应的宿主机目录</strong>. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> two<span class="token punctuation">-</span>containers
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shared<span class="token punctuation">-</span>data
    <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>    
      <span class="token key atrule">path</span><span class="token punctuation">:</span> /data
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shared<span class="token punctuation">-</span>data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> debian<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> debian
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shared<span class="token punctuation">-</span>data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /pod<span class="token punctuation">-</span>data
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sh&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;echo Hello from the debian container &gt; /pod-data/index.html&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><p>在这个例子中, <strong>debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume</strong>. 而 shared-data 是 hostPath 类型. 所以, 它对应在宿主机上的目录就是:  <strong>/data</strong>. 而这个目录, 其实就被同时绑定挂载进了上述两个容器当中.</p> <p>这就是为什么, <strong>nginx-container 可以从它的 /usr/share/nginx/html 目录中, 读取到 debian-container 生成的 index.html 文件的原因</strong>.</p> <p>明白了 Pod 的实现原理后, 再来讨论 &quot;<strong>容器设计模式</strong>&quot;, 就容易多了. Pod 这种 &quot;超亲密关系&quot; 容器的设计思想, 实际上就是希望, <mark><strong>当用户想在一个容器里跑多个功能并不相关的应用时, 应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器</strong></mark>. 为了能够掌握这种思考方式, 就应该尽量尝试<strong>使用它来描述一些用单个容器难以解决的问题</strong>.</p> <p>**第一个最典型的例子是: WAR 包与 Web 服务器. **</p> <p>假设现在有一个 Java Web 应用的 WAR 包, 它需要被放在 <strong>Tomcat</strong> 的 webapps 目录下运行起来. 假如现在只能用 Docker 来做这件事情, 那该如何处理这个组合关系呢?</p> <ul><li>一种方法是, 把 WAR 包直接放在 Tomcat 镜像的 webapps 目录下, 做成一个<strong>新的镜像</strong>运行起来. 可如果你要更新 WAR 包的内容, 或者要升级 Tomcat 镜像, 就要重新制作一个新的发布镜像, 非常麻烦.</li> <li>另一种方法是, 压根儿不管 WAR 包, <strong>永远只发布一个 Tomcat 容器</strong>. 不过这个容器的 webapps 目录, 就<strong>必须声明一个 hostPath 类型的 Volume, 从而把宿主机上的 WAR 包挂载进 Tomcat 容器当中运行起来</strong>. 不过这样就必须要解决一个问题, 即: <strong>如何让每一台宿主机, 都预先准备好这个存储有 WAR 包的目录呢? 这样来看, 你只能独立维护一套分布式存储系统了</strong>.</li></ul> <p>实际上, 有了 Pod 之后, 这样的问题就很容易解决了. <strong>可以把 WAR 包和 Tomcat 分别做成镜像, 然后把它们作为一个 Pod 里的两个容器 &quot;组合&quot; 在一起</strong>. 这个 Pod 的配置文件如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> javaweb<span class="token punctuation">-</span><span class="token number">2</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">initContainers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> geektime/sample<span class="token punctuation">:</span>v2
    <span class="token key atrule">name</span><span class="token punctuation">:</span> war
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;cp&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;/sample.war&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;/app&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /app
      <span class="token key atrule">name</span><span class="token punctuation">:</span> app<span class="token punctuation">-</span>volume
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> geektime/tomcat<span class="token punctuation">:</span><span class="token number">7.0</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> tomcat
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sh&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;/root/apache-tomcat-7.0.42-v2/bin/start.sh&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /root/apache<span class="token punctuation">-</span>tomcat<span class="token punctuation">-</span>7.0.42<span class="token punctuation">-</span>v2/webapps
      <span class="token key atrule">name</span><span class="token punctuation">:</span> app<span class="token punctuation">-</span>volume
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
      <span class="token key atrule">hostPort</span><span class="token punctuation">:</span> <span class="token number">8001</span> 
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> app<span class="token punctuation">-</span>volume
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>在这个 Pod 中, 定义了两个容器, 第一个容器使用的镜像是 <strong>geektime/sample:v2</strong>, 这个镜像里<strong>只有一个 WAR 包</strong>(sample.war)放在根目录下. 而第二个容器则使用的是一个标准的 <strong>Tomcat 镜像</strong>.</p> <p>你可能已经注意到, WAR 包容器的类型<strong>不再是一个普通容器, 而是一个 Init Container 类型的容器</strong>. <strong>在 Pod 中, 所有 Init Container 定义的容器, 都会比 spec.containers 定义的用户容器先启动. 并且 Init Container 容器会按顺序逐一启动, 而直到它们都启动并且退出了, 用户容器才会启动</strong>.</p> <p>所以这个 Init Container 类型的 WAR 包容器启动后, 执行了一句 &quot;cp /sample.war /app&quot;, <strong>把应用的 WAR 包拷贝到 /app 目录下</strong>, 然后退出. 而后这个 /app 目录, <strong>就挂载了一个名叫 app-volume 的 Volume</strong>.</p> <p>接下来就很关键了. <strong>Tomcat 容器, 同样声明了挂载 app-volume 到自己的 webapps 目录下</strong>. 所以, 等 Tomcat 容器启动时, 它的 webapps 目录下就一定会存在 sample.war 文件: <strong>这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的, 而这个 Volume 是被这两个容器共享的</strong>.</p> <p>**这样就用一种 &quot;组合&quot; 方式, 解决了 WAR 包与 Tomcat 容器之间耦合关系的问题. **</p> <p>实际上, 这个所谓的 &quot;组合&quot; 操作, 正是容器设计模式里最常用的一种模式, **它的名字叫: **​<mark><strong>sidecar</strong></mark>. 顾名思义, sidecar 指的就是可以在一个 Pod 中, <strong>启动一个辅助容器, 来完成一些独立于主进程(主容器)之外的工作</strong>.</p> <p>在上面的应用 Pod 中, Tomcat 容器是要使用的主容器, 而 WAR 包容器的存在, 只是为了给它提供一个 WAR 包而已. 所以用 Init Container 的方式优先运行 WAR 包容器, 扮演了一个 sidecar 的角色.</p> <p>**第二个例子, 则是容器的日志收集. **</p> <p>比如现在有一个应用, 需要不断地<strong>把日志文件输出到容器的 /var/log 目录</strong>中. 这时就可以<strong>把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录</strong>上. 然后在这个 Pod 里同时运行一个 sidecar 容器, 它也声明挂载同一个 Volume 到自己的 /var/log 目录上. 接下来 sidecar 容器就只需要做一件事儿, 那就是<strong>不断地从自己的 /var/log 目录里读取日志文件, 转发到 MongoDB 或者 Elasticsearch 中存储起来</strong>. 这样一个最基本的日志收集工作就完成了. 跟第一个例子一样, 这个例子中的 sidecar 的主要工作也是<strong>使用共享的 Volume 来完成对文件的操作</strong>.</p> <p>但不要忘记, Pod 的另一个重要特性是, <mark><strong>它的所有容器都共享同一个 Network Namespace</strong></mark>. 这就使得<strong>很多与 Pod 网络相关的配置和管理, 也都可以交给 sidecar 完成, 而完全无须干涉用户容器</strong>. 这里最典型的例子莫过于 <strong>Istio 这个微服务治理项目</strong>了. Istio 项目使用 sidecar 容器完成微服务治理的原理, 在后面会讲解到.</p> <p>Kubernetes 社区曾经把&quot;容器设计模式&quot;这个理论, 整理成了<a href="https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns" target="_blank" rel="noopener noreferrer">一篇小论文<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 可以参考下.</p> <blockquote><p>总结</p></blockquote> <p>本节重点分享了 Kubernetes 项目中 Pod 的实现原理.</p> <p>Pod 是 Kubernetes 项目与其他单容器项目相比最大的不同, 也是一位容器技术初学者需要面对的第一个与常规认知不一致的知识点. 事实上, 直到现在, 仍有很多人把容器跟虚拟机相提并论, 他们把容器当做性能更好的虚拟机, 喜欢讨论如何把应用从虚拟机无缝地迁移到容器中. 但实际上, <strong>无论是从具体的实现原理, 还是从使用方法, 特性, 功能等方面, 容器与虚拟机几乎没有任何相似的地方</strong>; 也不存在一种普遍的方法, 能够把虚拟机里的应用无缝迁移到容器中. 因为, 容器的性能优势, 必然伴随着相应缺陷, 即: <strong>它不能像虚拟机那样, 完全模拟本地物理机环境中的部署方法</strong>.</p> <p>所以, 这个 &quot;上云&quot; 工作的完成, 最终还是要靠深入<strong>理解容器的本质, 即: 进程</strong>.</p> <p>实际上, 一个运行在虚拟机里的应用, 哪怕再简单, 也是被管理在 systemd 或者 supervisord 之下的<strong>一组进程, 而不是一个进程</strong>. 这跟本地物理机上应用的运行方式其实是一样的. 这也是为什么, 从物理机到虚拟机之间的应用迁移, 往往并不困难.</p> <p>**可是对于容器来说, 一个容器永远只能管理一个进程. 更确切地说, 一个容器, 就是一个进程. 这是容器技术的&quot;天性&quot;, 不可能被修改. 所以, 将一个原本运行在虚拟机里的应用, &quot;无缝迁移&quot;到容器中的想法, 实际上跟容器的本质是相悖的. ** 这也是当初 Swarm 项目无法成长起来的重要原因之一: 一旦到了真正的生产环境上, <strong>Swarm 这种单容器的工作方式, 就难以描述真实世界里复杂的应用架构</strong>了.</p> <p>所以, 你现在可以这么理解 Pod 的本质: <strong>Pod 实际上是在扮演传统基础设施里&quot;虚拟机&quot;的角色; 而容器, 则是这个虚拟机里运行的用户程序</strong>.</p> <p>所以下一次, 当需要把一个运行在虚拟机里的应用迁移到 Docker 容器中时, 一定要仔细分析到底有哪些进程(组件)运行在这个虚拟机里.</p> <p><strong>然后就可以把整个虚拟机想象成为一个 Pod, 把这些进程分别做成容器镜像, 把有顺序关系的容器, 定义为 Init Container. 这才是更加合理的, 松耦合的容器编排诀窍, 也是从传统应用架构, 到&quot;微服务架构&quot;最自然的过渡方式.</strong></p> <p><mark><strong>Pod 这个概念, 提供的是一种编排思想</strong></mark>, 而不是具体的技术方案. 所以, 如果愿意的话, <strong>完全可以使用虚拟机来作为 Pod 的实现</strong>, 然后把用户容器都运行在这个虚拟机里. 比如, Mirantis 公司的 <a href="https://github.com/Mirantis/virtlet" target="_blank" rel="noopener noreferrer">virtlet 项目<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>就在干这个事情. 甚至可以去实现一个带有 Init 进程的容器项目, 来模拟传统应用的运行方式. 这些工作, 在 Kubernetes 中都是非常轻松的, 也是后面讲解 CRI 时会提到的内容. 相反的, 如果强行把整个应用塞到一个容器里, 甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案, 恐怕最后往往会得不偿失.</p> <h4 id="_14-深入解析pod对象-一-基本概念"><a href="#_14-深入解析pod对象-一-基本概念" class="header-anchor">#</a> 14 | 深入解析Pod对象(一):基本概念</h4> <p>上一节详细介绍了 Pod 这个 Kubernetes 项目中最重要的概念. 本节会介绍 Pod 对象的更多细节. 现在已经非常清楚: <strong>Pod, 而不是容器, 才是 Kubernetes 项目中的最小编排单位</strong>. 将<strong>这个设计落实到 API 对象上, 容器(Container)就成了 Pod 属性里的一个普通的字段</strong>.</p> <p>那么一个很自然的问题就是: <strong>到底哪些属性属于 Pod 对象, 而又有哪些属性属于 Container 呢</strong>?</p> <p>要彻底理解这个问题, 就一定要牢记我在前一节中提到的一个结论: <strong>Pod 扮演的是传统部署环境里&quot;虚拟机&quot;的角色. 这样的设计, 是为了使用户从传统环境(虚拟机环境)向 Kubernetes(容器环境)的迁移, 更加平滑</strong>.</p> <p>**而如果你能把 Pod 看成传统环境里的 &quot;机器&quot;, 把容器看作是运行在这个 &quot;机器&quot; 里的 &quot;用户程序&quot;, 那么很多关于 Pod 对象的设计就非常容易理解了. 比如, **​<mark><strong>凡是调度, 网络, 存储, 以及安全相关的属性, 基本上是 Pod 级别的</strong></mark>​ <strong>. 这些属性的共同特征是, 它们描述的是 &quot;机器&quot; 这个整体, 而不是里面运行的&quot;程序&quot;. 比如, 配置这个 &quot;机器&quot; 的网卡(即: Pod 的网络定义), 配置这个 &quot;机器&quot; 的磁盘(即: Pod 的存储定义), 配置这个 &quot;机器&quot; 的防火墙(即: Pod 的安全定义). 更不用说, 这台 &quot;机器&quot; 运行在哪个服务器之上(即: Pod 的调度).</strong></p> <p>接下来就先介绍 Pod 中几个重要字段的含义和用法.</p> <p><strong>NodeSelector: 是一个供用户</strong>​<mark><strong>将 Pod 与 Node 进行绑定</strong></mark>​<strong>的字段</strong>, 用法如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
 <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>
   <span class="token key atrule">disktype</span><span class="token punctuation">:</span> ssd
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这样的一个配置, 意味着这个 Pod 永远只能<strong>运行</strong>在携带了 &quot;disktype: ssd&quot; 标签(Label)的节点上; 否则, 它将调度失败.</p> <p><strong>NodeName</strong>: 一旦 Pod 的这个字段被赋值, Kubernetes 项目就会被认为<strong>这个 Pod 已经经过了调度</strong>, 调度的结果就是<strong>赋值的节点名字</strong>. 所以这个字段一般<strong>由调度器负责设置</strong>, 但用户也可以设置它来&quot;骗过&quot;调度器, 当然这个做法一般是在测试或者调试的时候才会用到.</p> <p><strong>HostAliases: 定义了 Pod 的 hosts 文件(比如 /etc/hosts)里的内容</strong>, 用法如下:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hostAliases</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">ip</span><span class="token punctuation">:</span> <span class="token string">&quot;10.1.2.3&quot;</span>
    <span class="token key atrule">hostnames</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">&quot;foo.remote&quot;</span>
    <span class="token punctuation">-</span> <span class="token string">&quot;bar.remote&quot;</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>在这个 Pod 的 YAML 文件中, 设置了一组 <strong>IP 和 hostname</strong> 的数据. 这样这个 Pod 启动后,  <strong>/etc/hosts 文件的内容</strong>将如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>cat /etc/hosts
<span class="token comment"># Kubernetes-managed hosts file.</span>
127.0.0.1 localhost
<span class="token punctuation">...</span>
10.244.135.10 hostaliases<span class="token punctuation">-</span>pod
10.1.2.3 foo.remote
10.1.2.3 bar.remote
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>其中最下面两行记录, 就是通过 <strong>HostAliases</strong> 字段为 Pod 设置的. 需要指出的是, 在 Kubernetes 项目中, <mark><strong>如果要设置 hosts 文件里的内容, 一定要通过这种方法. 否则如果直接修改了 hosts 文件的话, 在 Pod 被删除重建之后, kubelet 会自动覆盖掉被修改的内容</strong></mark>.</p> <p>除了上述跟 &quot;机器&quot; 相关的配置外, 你可能也会发现, <mark><strong>凡是跟容器的 Linux Namespace 相关的属性, 也一定是 Pod 级别的</strong></mark>. 这个原因也很容易理解: <mark><strong>Pod 的设计, 就是要让它里面的容器尽可能多地共享 Linux Namespace, 仅保留必要的隔离和限制能力. 这样, Pod 模拟出的效果, 就跟虚拟机里程序间的关系非常类似了</strong></mark>.</p> <p>举个例子, 在下面这个 Pod 的 YAML 文件中, 定义了 <strong>shareProcessNamespace=true</strong>:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">shareProcessNamespace</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shell
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">stdin</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token key atrule">tty</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>这就意味着这个 Pod 里的容器要<strong>共享 PID Namespace</strong>. 而在这个 YAML 文件中, 还定义了两个容器: 一个是 nginx 容器, 一个是开启了 tty 和 stdin 的 shell 容器. 在前面介绍容器基础时, 曾经讲解过什么是 tty 和 stdin. <strong>而在 Pod 的 YAML 文件里声明开启它们俩, 其实等同于设置了 docker run 里的 -it(-i 即 stdin, -t 即 tty)参数</strong>. 如果你还是不太理解它们俩的作用的话, 可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序, 用于<strong>接收用户的标准输入</strong>, 返回操作系统的标准输出. 当然, 为了能够在 tty 中输入信息, 还需要<strong>同时开启 stdin(标准输入流)</strong> .</p> <p>于是, 这个 Pod 被创建后, 就可以使用 shell 容器的 tty 跟这个容器进行交互了. 一起实践一下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> nginx.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>接下来, 使用 kubectl attach 命令, <strong>连接到 shell 容器的 tty 上</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl attach <span class="token parameter variable">-it</span> nginx <span class="token parameter variable">-c</span> shell
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样就可以在 shell 容器里执行 ps 指令, 查看所有正在运行的进程:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl attach <span class="token parameter variable">-it</span> nginx <span class="token parameter variable">-c</span> shell
/ <span class="token comment"># ps ax</span>
PID   <span class="token environment constant">USER</span>     TIME  COMMAND
    <span class="token number">1</span> root      <span class="token number">0</span>:00 /pause
    <span class="token number">8</span> root      <span class="token number">0</span>:00 nginx: master process nginx <span class="token parameter variable">-g</span> daemon off<span class="token punctuation">;</span>
   <span class="token number">14</span> <span class="token number">101</span>       <span class="token number">0</span>:00 nginx: worker process
   <span class="token number">15</span> root      <span class="token number">0</span>:00 <span class="token function">sh</span>
   <span class="token number">21</span> root      <span class="token number">0</span>:00 <span class="token function">ps</span> ax
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 在这个容器里, 不仅可以看到它<strong>本身的 ps ax 指令, 还可以看到 nginx 容器的进程, 以及 Infra 容器的 /pause 进程</strong>. 这就意味着, <mark><strong>整个 Pod 里的每个容器的进程, 对于所有容器来说都是可见的: 它们共享了同一个 PID Namespace</strong></mark>.</p> <p>类似地, <mark><strong>凡是 Pod 中的容器要共享宿主机的 Namespace, 也一定是 Pod 级别的定义</strong></mark>, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  hostNetwork: <span class="token boolean">true</span>
  hostIPC: <span class="token boolean">true</span>
  hostPID: <span class="token boolean">true</span>
  containers:
  - name: nginx
    image: nginx
  - name: shell
    image: busybox
    stdin: <span class="token boolean">true</span>
    tty: <span class="token boolean">true</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>在这个 Pod 中, 定义了<strong>共享宿主机的 Network, IPC 和 PID Namespace</strong>. 这就意味着, <strong>这个 Pod 里的所有容器, 会直接使用宿主机的网络, 直接与宿主机进行 IPC 通信, 看到宿主机里正在运行的所有进程</strong>.</p> <p>当然, 除了这些属性, Pod 里最重要的字段当属 &quot;<strong>Containers</strong>&quot; 了. 而在上一节还介绍过 &quot;Init Containers&quot;. 其实, 这两个字段都属于 Pod 对容器的定义, 内容也完全相同, <strong>只是 Init Containers 的生命周期, 会先于所有的 Containers, 并且严格按照定义的顺序执行</strong>.</p> <p>Kubernetes 项目中对 Container 的定义, 和 Docker 相比并没有什么太大区别. 在前面的容器技术概念入门系列中介绍的 Image(镜像), Command(启动命令), workingDir(容器的工作目录), Ports(容器要开发的端口), 以及 volumeMounts(容器要挂载的 Volume)都是<strong>构成 Kubernetes 项目中 Container 的主要字段</strong>. 不过在这里, 还有这么几个属性值得你额外关注.</p> <p><strong>首先, 是 ImagePullPolicy 字段</strong>. 它定义了镜像拉取的策略. 而它之所以是一个 Container 级别的属性, 是因为容器镜像本来就是 Container 定义中的一部分. ImagePullPolicy 的值默认是 Always, 即<strong>每次创建 Pod 都重新拉取一次镜像</strong>. 另外, 当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时, ImagePullPolicy 也会被认为 Always. 而如果它的值被定义为 Never 或者 IfNotPresent, 则意味着 Pod 永远不会主动拉取这个镜像, 或者只在宿主机上不存在这个镜像时才拉取.</p> <p><strong>其次, 是 Lifecycle 字段</strong>. 它定义的是 Container Lifecycle Hooks. 顾名思义, Container Lifecycle Hooks 的作用, 是<strong>在容器状态发生变化时触发一系列&quot;钩子&quot;</strong> . 来看这样一个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> lifecycle<span class="token punctuation">-</span>demo
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> lifecycle<span class="token punctuation">-</span>demo<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>
      <span class="token key atrule">postStart</span><span class="token punctuation">:</span>
        <span class="token key atrule">exec</span><span class="token punctuation">:</span>
          <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sh&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">preStop</span><span class="token punctuation">:</span>
        <span class="token key atrule">exec</span><span class="token punctuation">:</span>
          <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/usr/sbin/nginx&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;-s&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;quit&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>这是一个来自 Kubernetes 官方文档的 Pod 的 YAML 文件. 它其实非常简单, 只是定义了一个 nginx 镜像的容器. 不过, 在这个 YAML 文件的容器(Containers)部分, 可以看到这个容器分别设置了一个 <strong>postStart 和 preStop 参数</strong>. 这是什么意思呢?</p> <p>先说 postStart 吧. 它指的是, <strong>在容器启动后, 立刻执行一个指定的操作</strong>. 需要明确的是, postStart 定义的操作, 虽然是在 Docker 容器 ENTRYPOINT 执行之后, 但它<strong>并不严格保证顺序</strong>. 也就是说, 在 postStart 启动时, ENTRYPOINT 有可能还没有结束. 当然, 如果 postStart 执行超时或者错误, Kubernetes 会在该 Pod 的 Events 中报出该容器启动失败的错误信息, 导致 Pod 也处于失败的状态.</p> <p>而类似地, preStop 发生的时机, 则是<strong>容器被杀死之前</strong>(比如收到了 SIGKILL 信号). 而需要明确的是, preStop 操作的执行, 是<strong>同步</strong>的. 所以它会<strong>阻塞当前的容器杀死流程</strong>, 直到这个 Hook 定义操作完成之后, 才允许容器被杀死, 这跟 postStart 不一样.</p> <p>所以, 在这个例子中, 在容器成功启动之后, 在 /usr/share/message 里写入了一句 &quot;欢迎信息&quot;(即 postStart 定义的操作). 而在这个容器被删除之前, 则先调用了 nginx 的退出指令(即 preStop 定义的操作), 从而<strong>实现了容器的 &quot;优雅退出&quot;</strong> .</p> <p>在熟悉了 Pod 以及它的 Container 部分的主要字段之后, 再分享一下<strong>这样一个的 Pod 对象在 Kubernetes 中的生命周期</strong>.</p> <p>Pod <strong>生命周期</strong>的变化, 主要体现在 Pod API 对象的 <strong>Status 部分</strong>, 这是它除了 Metadata 和 Spec 之外的第三个重要字段. 其中, <strong>pod.status.phase, 就是 Pod 的当前状态</strong>, 它有如下几种可能的情况:</p> <ol><li><strong>Pending</strong>. 这个状态意味着, Pod 的 YAML 文件已经提交给了 Kubernetes, API 对象已经被创建并保存在 Etcd 当中. 但是, 这个 Pod 里有些容器因为某种原因而不能被顺利创建. 比如调度不成功.</li> <li><strong>Running</strong>. 这个状态下, Pod 已经<strong>调度成功</strong>, 跟一个具体的节点绑定. 它<strong>包含的容器都已经创建成功, 并且至少有一个正在运行中</strong>.</li> <li><strong>Succeeded</strong>. 这个状态意味着, Pod 里的所有容器都<strong>正常运行完毕, 并且已经退出了</strong>. 这种情况在运行一次性任务时最为常见.</li> <li><strong>Failed</strong>. 这个状态下, Pod 里<strong>至少有一个容器以不正常的状态(非 0 的返回码)退出</strong>. 这个状态的出现, 意味着得想办法 Debug 这个容器的应用, 比如查看 Pod 的 Events 和日志.</li> <li><strong>Unknown</strong>. 这是一个异常状态, 意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver, 这很有可能是主从节点(Master 和 Kubelet)间的通信出现了问题.</li></ol> <p>更进一步地, Pod 对象的 Status 字段, 还可以再细分出一组 <strong>Conditions</strong>. 这些细分状态的值包括: <strong>PodScheduled, Ready, Initialized, 以及 Unschedulable</strong>. 它们主要用于<strong>描述造成当前 Status 的具体原因</strong>是什么.</p> <p>比如, Pod 当前的 Status 是 Pending, 对应的 Condition 是 Unschedulable, 这就意味着它的调度出现了问题. 而其中, <strong>Ready</strong> 这个细分状态非常值得关注: <strong>它意味着 Pod 不仅已经正常启动(Running 状态), 而且已经可以对外提供服务了</strong>. 这两者之间(Running 和 Ready)是有区别的, 你不妨仔细思考一下.</p> <p>Pod 的这些状态信息, 是判断应用<strong>运行情况的重要标准</strong>, 尤其是 Pod 进入了非 &quot;Running&quot; 状态后, 一定要能迅速做出反应, 根据它所代表的异常情况开始跟踪和定位, 而不是去手忙脚乱地查阅文档.</p> <blockquote><p>总结</p></blockquote> <p>本节详细讲解了 Pod API 对象, 介绍了 Pod 的核心使用方法, 并分析了 Pod 和 Container 在字段上的异同. 希望这些讲解能够帮你更好地理解和记忆 Pod YAML 中的核心字段, 以及这些字段的准确含义.</p> <p>实际上, <strong>Pod API 对象是整个 Kubernetes 体系中最核心的一个概念</strong>, 也是后面讲解各种控制器时都要用到的.</p> <p>在学习完本节后, 希望你能仔细阅读 $GOPATH/src/k8s.io/kubernetes/vendor/k8s.io/api/core/v1/types.go 里, type Pod struct, 尤其是 <strong>PodSpec 部分的内容</strong>. 争取做到下次看到一个 Pod 的 YAML 文件时, 不再需要查阅文档, 就能做到把常用字段及其作用信手拈来.</p> <p>下一节会通过大量的实践, 来巩固和进阶关于 Pod API 对象核心字段的使用方法.</p> <h4 id="_15-深入解析pod对象-二-使用进阶"><a href="#_15-深入解析pod对象-二-使用进阶" class="header-anchor">#</a> 15 | 深入解析Pod对象(二):使用进阶</h4> <p>上一节深入解析了 Pod 的 API 对象, 讲解了 Pod 和 Container 的关系. <strong>作为 Kubernetes 项目里最核心的编排对象, Pod 携带的信息非常丰富. 其中, 资源定义(比如 CPU, 内存等), 以及调度相关的字段</strong>, 会在后面专门讲解调度器时再进行深入的分析.</p> <p>本节就先从一种特殊的 Volume 开始, 来更加深入地理解 Pod 对象各个重要字段的含义. 这种特殊的 Volume, 叫作 <strong>Projected Volume</strong>, 可以把它翻译为 &quot;投射数据卷&quot;. Projected Volume 是 Kubernetes v1.11 之后的新特性.</p> <p>这是什么意思呢?</p> <p>在 Kubernetes 中, 有几种特殊的 Volume, 它们<strong>存在的意义不是为了存放容器里的数据, 也不是用来进行容器和宿主机之间的数据交换</strong>. 这些特殊 Volume 的作用, 是<mark><strong>为容器提供预先定义好的数据</strong></mark>. 所以从容器的角度来看, <mark><strong>这些 Volume 里的信息就是仿佛是被 Kubernetes &quot;投射&quot; (Project)进入容器当中的</strong></mark>. 这正是 Projected Volume 的含义.</p> <p>到目前为止, Kubernetes 支持的 Projected Volume 一共有四种:</p> <ol><li>**Secret; **</li> <li>**ConfigMap; **</li> <li>**Downward API; **</li> <li>**ServiceAccountToken. **</li></ol> <p><mark><strong>本节首先介绍的是 Secret. 它的作用是把 Pod 想要访问的加密数据, 存放到 Etcd 中. 然后就可以通过在 Pod 的容器里挂载 Volume 的方式, 访问到这些 Secret 里保存的信息了.</strong></mark></p> <p>Secret 最典型的使用场景, 莫过于<strong>存放数据库的 Credential 信息</strong>, 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>projected<span class="token punctuation">-</span>volume 
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>secret<span class="token punctuation">-</span>volume
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> sleep
    <span class="token punctuation">-</span> <span class="token string">&quot;86400&quot;</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql<span class="token punctuation">-</span>cred
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&quot;/projected-volume&quot;</span>
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql<span class="token punctuation">-</span>cred
    <span class="token key atrule">projected</span><span class="token punctuation">:</span>
      <span class="token key atrule">sources</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">secret</span><span class="token punctuation">:</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> user
      <span class="token punctuation">-</span> <span class="token key atrule">secret</span><span class="token punctuation">:</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> pass
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><p>在这个 Pod 中, 定义了一个简单的容器. 它声明挂载的 Volume, 并不是常见的 emptyDir 或者 hostPath 类型, 而是 <strong>projected 类型</strong>. 而这个 Volume 的<strong>数据来源(sources), 则是名为 user 和 pass 的 Secret 对象, 分别对应的是数据库的用户名和密码</strong>.</p> <p>这里用到的数据库的用户名, 密码, 正是<strong>以 Secret 对象的方式交给 Kubernetes 保存的</strong>. 完成这个操作的指令, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">cat</span> ./username.txt
admin
$ <span class="token function">cat</span> ./password.txt
c1oudc0w<span class="token operator">!</span>
 
$ kubectl create secret generic user --from-file<span class="token operator">=</span>./username.txt
$ kubectl create secret generic pass --from-file<span class="token operator">=</span>./password.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>其中, username.txt 和 password.txt 文件里, 存放的就是用户名和密码; 而 user 和 pass, 则是为 Secret 对象指定的名字. 而想要查看这些 Secret 对象的话, 只要执行一条 <strong>kubectl get 命令</strong>就可以了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get secrets
NAME           TYPE                                DATA      AGE
user          Opaque                                <span class="token number">1</span>         51s
pass          Opaque                                <span class="token number">1</span>         51s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>当然, 除了使用 kubectl create secret 指令外, 也可以<strong>直接通过编写 YAML 文件的方式</strong>来创建这个 Secret 对象, 比如:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mysecret
<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">user</span><span class="token punctuation">:</span> YWRtaW4=
  <span class="token key atrule">pass</span><span class="token punctuation">:</span> MWYyZDFlMmU2N2Rm
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 通过编写 YAML 文件创建出来的 <strong>Secret 对象只有一个</strong>. 但它的 data 字段, 却以 Key-Value 的格式保存了两份 Secret 数据. 其中, &quot;user&quot; 就是第一份数据的 <strong>Key</strong>, &quot;pass&quot; 是第二份数据的 <strong>Key</strong>.</p> <p>需要注意的是, Secret 对象要求这些数据必须是<strong>经过 Base64 转码</strong>的, 以免出现明文密码的安全隐患. 这个转码操作也很简单, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token builtin class-name">echo</span> <span class="token parameter variable">-n</span> <span class="token string">'admin'</span> <span class="token operator">|</span> base64
<span class="token assign-left variable">YWRtaW4</span><span class="token operator">=</span>
$ <span class="token builtin class-name">echo</span> <span class="token parameter variable">-n</span> <span class="token string">'1f2d1e2e67df'</span> <span class="token operator">|</span> base64
MWYyZDFlMmU2N2Rm
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这里需要注意的是, 像这样创建的 Secret 对象, 它里面的内容仅仅是经过了转码, 而<strong>并没有被加密</strong>. 在真正的生产环境中, 需要在 Kubernetes 中<strong>开启 Secret 的加密插件, 增强数据的安全性</strong>. 关于开启 Secret 加密插件的内容, 会在后续专门讲解 Secret 的时候, 再做进一步说明.</p> <p>接下来, 尝试一下创建这个 Pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> test-projected-volume.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>当 Pod 变成 <strong>Running</strong> 状态之后, 再验证一下这些 Secret 对象是不是已经在容器里了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> test-projected-volume -- /bin/sh
$ <span class="token function">ls</span> /projected-volume/
user
pass
$ <span class="token function">cat</span> /projected-volume/user
root
$ <span class="token function">cat</span> /projected-volume/pass
1f2d1e2e67df
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>从返回结果可以看到, <strong>保存在 Etcd 里的用户名和密码信息, 已经以文件的形式出现在了容器的 Volume 目录里</strong>. 而这个文件的名字, 就是 <strong>kubectl create secret 指定的 Key, 或者说是 Secret 对象的 data 字段指定的 Key</strong>.</p> <p>更重要的是, 像这样通过挂载方式进入到容器里的 Secret, <strong>一旦其对应的 Etcd 里的数据被更新, 这些 Volume 里的文件内容, 同样也会被更新</strong>. 其实<strong>这是 kubelet 组件在定时维护这些 Volume. ** 需要注意的是, 这个更新可能会有一定的</strong>延时**. 所以**在编写应用程序时, 在发起数据库连接的代码处写好重试和超时的逻辑, 绝对是个好习惯. **</p> <p><strong>与 Secret 类似的是 ConfigMap</strong>, 它与 Secret 的区别在于, ConfigMap 保存的是<strong>不需要加密的, 应用所需的配置信息</strong>. 而 ConfigMap 的用法几乎与 Secret 完全相同: 可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap, 也可以直接编写 ConfigMap 对象的 YAML 文件.</p> <p>比如, 一个 Java 应用所需的配置文件(.properties 文件), 就可以通过下面这样的方式保存在 ConfigMap 里:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token comment"># .properties 文件的内容</span>
$ cat example/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
 
<span class="token comment"># 从.properties 文件创建 ConfigMap</span>
$ kubectl create configmap ui<span class="token punctuation">-</span>config <span class="token punctuation">-</span><span class="token punctuation">-</span>from<span class="token punctuation">-</span>file=example/ui.properties
 
<span class="token comment"># 查看这个 ConfigMap 里保存的信息 (data)</span>
$ kubectl get configmaps ui<span class="token punctuation">-</span>config <span class="token punctuation">-</span>o yaml
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">ui.properties</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> ui<span class="token punctuation">-</span>config
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><blockquote><p>备注: kubectl get -o yaml 这样的参数, 会将指定的 Pod API 对象以 YAML 的方式展示出来.</p></blockquote> <p><strong>接下来是 Downward API</strong>, 它的作用是: <strong>让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息</strong>.</p> <p>举个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>downwardapi<span class="token punctuation">-</span>volume
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">zone</span><span class="token punctuation">:</span> us<span class="token punctuation">-</span>est<span class="token punctuation">-</span>coast
    <span class="token key atrule">cluster</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>cluster1
    <span class="token key atrule">rack</span><span class="token punctuation">:</span> rack<span class="token punctuation">-</span><span class="token number">22</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> client<span class="token punctuation">-</span>container
      <span class="token key atrule">image</span><span class="token punctuation">:</span> k8s.gcr.io/busybox
      <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sh&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-c&quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">args</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> while true; do
          if <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token punctuation">-</span>e /etc/podinfo/labels <span class="token punctuation">]</span><span class="token punctuation">]</span>; then
            echo <span class="token punctuation">-</span>en '\n\n'; cat /etc/podinfo/labels; fi;
          sleep 5;
        done;
      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> podinfo
          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/podinfo
          <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> podinfo
      <span class="token key atrule">projected</span><span class="token punctuation">:</span>
        <span class="token key atrule">sources</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">downwardAPI</span><span class="token punctuation">:</span>
            <span class="token key atrule">items</span><span class="token punctuation">:</span>
              <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;labels&quot;</span>
                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.labels
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><p>在这个 Pod 的 YAML 文件中, 定义了一个简单的容器, 声明了一个 <strong>projected 类型的 Volume</strong>. 只不过这次 Volume 的数据来源, 变成了 Downward API. 而这个 Downward API Volume, 则<strong>声明了要暴露 Pod 的 metadata.labels 信息给容器</strong>.</p> <p>通过这样的声明方式, <strong>当前 Pod 的 Labels 字段的值, 就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件</strong>.</p> <p>而这个容器的启动命令, 则是不断打印出 /etc/podinfo/labels 里的内容. 所以, 当创建了这个 Pod 之后, 就可以通过 <strong>kubectl logs</strong> 指令, 查看到这些 Labels 字段被打印出来, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> dapi-volume.yaml
$ kubectl logs test-downwardapi-volume
<span class="token assign-left variable">cluster</span><span class="token operator">=</span><span class="token string">&quot;test-cluster1&quot;</span>
<span class="token assign-left variable">rack</span><span class="token operator">=</span><span class="token string">&quot;rack-22&quot;</span>
<span class="token assign-left variable">zone</span><span class="token operator">=</span><span class="token string">&quot;us-est-coast&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>目前, Downward API 支持的<strong>字段</strong>已经非常丰富了, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">1</span>. 使用 fieldRef 可以声明使用:
spec.nodeName - 宿主机名字
status.hostIP - 宿主机 IP
metadata.name - Pod 的名字
metadata.namespace - Pod 的 Namespace
status.podIP - Pod 的 IP
spec.serviceAccountName - Pod 的 Service Account 的名字
metadata.uid - Pod 的 <span class="token environment constant">UID</span>
metadata.labels<span class="token punctuation">[</span><span class="token string">'&lt;KEY&gt;'</span><span class="token punctuation">]</span> - 指定 <span class="token operator">&lt;</span>KEY<span class="token operator">&gt;</span> 的 Label 值
metadata.annotations<span class="token punctuation">[</span><span class="token string">'&lt;KEY&gt;'</span><span class="token punctuation">]</span> - 指定 <span class="token operator">&lt;</span>KEY<span class="token operator">&gt;</span> 的 Annotation 值
metadata.labels - Pod 的所有 Label
metadata.annotations - Pod 的所有 Annotation
 
<span class="token number">2</span>. 使用 resourceFieldRef 可以声明使用:
容器的 CPU limit
容器的 CPU request
容器的 memory limit
容器的 memory request
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>上面这个列表的内容, 随着 Kubernetes 项目的发展肯定还会不断增加. 所以这里列出来的信息仅供参考, 在使用 Downward API 时, 还是要记得去查阅一下官方文档.</p> <p>需要注意的是, Downward API <strong>能够获取到的信息, 一定</strong>​<mark><strong>是 Pod 里的容器进程启动之前就能够确定下来的信息</strong></mark>. 而如果想要获取 Pod 容器<strong>运行后才会出现的信息</strong>, 比如容器进程的 PID, 那就肯定不能使用 Downward API 了, 而应该考虑在 Pod 里定义一个 <strong>sidecar 容器</strong>.</p> <p>其实, Secret, ConfigMap, 以及 Downward API 这三种 Projected Volume 定义的信息, 大多还可以通过<strong>环境变量</strong>的方式出现在容器里. <strong>但是通过环境变量获取这些信息的方式, 不具备自动更新的能力</strong>. 所以一般情况下, <strong>都建议你使用 Volume 文件的方式获取这些信息</strong>.</p> <p>明白了 Secret 之后, 再来讲解 Pod 中一个与它密切相关的概念: <strong>Service Account</strong>.</p> <p>相信你一定有过这样的想法: 我现在有了一个 Pod, 能不能<strong>在这个 Pod 里安装一个 Kubernetes 的 Client, 这样就可以从容器里直接访问并且操作这个 Kubernetes 的 API 了</strong>呢?</p> <p>这当然是可以的. 不过首先要解决 <strong>API Server 的授权</strong>问题.</p> <p><strong>Service Account 对象的作用, 就是 Kubernetes 系统内置的一种 &quot;服务账户&quot;, 它是 Kubernetes 进行权限分配的对象</strong>. 比如, Service Account A, 可以只被允许对 Kubernetes API 进行 GET 操作, 而 Service Account B, 则可以有 Kubernetes API 的所有操作的权限.</p> <p>像这样的 <strong>Service Account 的授权信息和文件, 实际上保存在它所绑定的一个特殊的 Secret 对象里</strong>的. 这个特殊的 Secret 对象, 就叫作 <strong>ServiceAccountToken</strong>. 任何<mark><strong>运行在 Kubernetes 集群上的应用, 都必须使用这个 ServiceAccountToken 里保存的授权信息, 也就是 Token, 才可以合法地访问 API Server</strong></mark>. 所以说, Kubernetes 项目的 Projected Volume 其实只有三种, 因为第四种 ServiceAccountToken, 只是一种特殊的 <strong>Secret</strong> 而已.</p> <p>另外为了方便使用, Kubernetes 已经为你提供了一个的默认 &quot;服务账户&quot;(default Service Account). 并且任何一个运行在 Kubernetes 里的 Pod, 都可以<strong>直接使用这个默认的 Service Account, 而无需显示地声明挂载它</strong>.</p> <p>**这是如何做到的呢? **</p> <p>当然还是<strong>靠 Projected Volume 机制</strong>.</p> <p>如果查看一下<mark><strong>任意一个运行在 Kubernetes 集群里的 Pod, 就会发现每一个 Pod 都已经自动声明一个类型是 Secret, 名为 default-token-xxxx 的 Volume, 然后自动挂载在每个容器的一个固定目录上</strong></mark>. 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw
Containers:
<span class="token punctuation">..</span>.
  Mounts:
    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq <span class="token punctuation">(</span>ro<span class="token punctuation">)</span>
Volumes:
  default-token-s8rbq:
  Type:       Secret <span class="token punctuation">(</span>a volume populated by a Secret<span class="token punctuation">)</span>
  SecretName:  default-token-s8rbq
  Optional:    <span class="token boolean">false</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这个 Secret 类型的 Volume, 正是<strong>默认 Service Account 对应的 ServiceAccountToken</strong>. 所以说, Kubernetes 其实在每个 Pod 创建的时候, <strong>自动在它的 spec.volumes 部分添加上了默认 ServiceAccountToken 的定义, 然后自动给每个容器加上了对应的 volumeMounts 字段</strong>. 这个过程对于用户来说是完全<strong>透明</strong>的.</p> <p>这样, 一旦 Pod 创建完成, <strong>容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件</strong>. 这个容器内的路径在 Kubernetes 里是固定的, 即:  <strong>/var/run/secrets/kubernetes.io/serviceaccount</strong>, 而这个 Secret 类型的 Volume 里面的内容如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">ls</span> /var/run/secrets/kubernetes.io/serviceaccount 
ca.crt namespace  token
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>所以, 你的应用程序只要<strong>直接加载这些授权文件</strong>, 就可以访问并操作 Kubernetes API 了. 而且如果你使用的是 Kubernetes 官方的 Client 包(<code>k8s.io/client-go</code>​)的话, 它还可以<strong>自动加载</strong>这个目录下的文件, 你不需要做任何配置或者编码操作.</p> <p><mark><strong>这种把 Kubernetes 客户端以容器的方式运行在集群里, 然后使用 default Service Account 自动授权的方式, 被称作 &quot;InClusterConfig&quot;, 也是我最推荐的进行 Kubernetes API 编程的授权方式.</strong></mark></p> <p>当然, 考虑到自动挂载默认 ServiceAccountToken 的潜在风险, Kubernetes 允许你设置默认不为 Pod 里的容器自动挂载这个 Volume.</p> <p>除了这个默认的 Service Account 外, 很多时候还需要创建一些<strong>自定义的 Service Account</strong>, 来对应不同的权限设置. 这样, <strong>Pod 里的容器就可以通过挂载这些 Service Account 对应的 ServiceAccountToken, 来使用这些自定义的授权信息</strong>. 在后面讲解为 Kubernetes 开发插件的时候, 将会实践到这个操作.</p> <p>接下来再来看 Pod 的另一个重要的配置: <strong>容器健康检查和恢复机制</strong>.</p> <p>在 Kubernetes 中, 可以为 Pod 里的容器定义一个健康检查 &quot;<strong>探针</strong>&quot;(Probe). 这样, kubelet 就会根据这个 Probe 的返回值决定这个容器的状态, 而不是直接以容器进行是否运行(来自 Docker 返回的信息)作为依据. <strong>这种机制是生产环境中保证应用健康存活的重要手段</strong>.</p> <p>一起来看一个 Kubernetes 文档中的例子.</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">test</span><span class="token punctuation">:</span> liveness
  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>liveness<span class="token punctuation">-</span>exec
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> liveness
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> /bin/sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
    <span class="token punctuation">-</span> touch /tmp/healthy; sleep 30; rm <span class="token punctuation">-</span>rf /tmp/healthy; sleep 600
    <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>
      <span class="token key atrule">exec</span><span class="token punctuation">:</span>
        <span class="token key atrule">command</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> cat
        <span class="token punctuation">-</span> /tmp/healthy
      <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>
      <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>在这个 Pod 中定义了一个有趣的容器. 它在启动之后做的第一件事, 就是在 /tmp 目录下创建了一个 healthy 文件, 以此作为自己已经正常运行的标志. 而 30 s 过后, 它会把这个文件删除掉.</p> <p>与此同时, 定义了一个这样的 <strong>livenessProbe</strong>(健康检查). 它的类型是 exec, 这意味着, 它会在容器启动后, 在容器里面执行一句指定的命令, 比如: &quot;cat /tmp/healthy&quot;. 这时如果这个文件存在, 这条命令的返回值就是 0, Pod 就会认为这个容器不仅已经启动, 而且是健康的. 这个健康检查, 在容器启动 5 s 后开始执行(initialDelaySeconds: 5), 每 5 s 执行一次(periodSeconds: 5).</p> <p>现在来<strong>具体实践一下这个过程</strong>.</p> <p>首先创建这个 Pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> test-liveness-exec.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后查看这个 Pod 的状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod
NAME                READY     STATUS    RESTARTS   AGE
test-liveness-exec   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          10s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到, 由于已经通过了健康检查, 这个 Pod 就进入了 <strong>Running</strong> 状态.</p> <p>而 30 s 之后, 再查看一下 Pod 的 Events:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pod test-liveness-exec
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>你会发现这个 Pod 在 Events 报告了一个异常:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
2s        2s      <span class="token number">1</span>   <span class="token punctuation">{</span>kubelet worker0<span class="token punctuation">}</span>   spec.containers<span class="token punctuation">{</span>liveness<span class="token punctuation">}</span>   Warning     Unhealthy   Liveness probe failed: cat: can<span class="token string">'t open '</span>/tmp/healthy': No such <span class="token function">file</span> or directory
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>显然, 这个健康检查探查到 /tmp/healthy 已经<strong>不存在</strong>了, 所以它报告容器是不健康的. 那么接下来会发生什么呢?</p> <p>不妨再次查看一下这个 Pod 的状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod test-liveness-exec
NAME           READY     STATUS    RESTARTS   AGE
liveness-exec   <span class="token number">1</span>/1       Running   <span class="token number">1</span>          1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这时发现, Pod <strong>并没有进入 Failed 状态, 而是保持了 Running 状态</strong>. 这是为什么呢? 其实, 如果你注意到 <strong>RESTARTS</strong> 字段从 0 到 1 的变化, 就明白原因了: <strong>这个异常的容器已经被 Kubernetes 重启了. 在这个过程中, Pod 保持 Running 状态不变</strong>.</p> <p>需要注意的是: Kubernetes 中并没有 Docker 的 Stop 语义. 所以<strong>虽然是 Restart(重启), 但实际却是重新创建了容器</strong>.</p> <p>这个功能就是 Kubernetes 里的 <strong>Pod 恢复机制</strong>, 也叫 <strong>restartPolicy</strong>. 它是 Pod 的 Spec 部分的一个标准字段(pod.spec.restartPolicy), 默认值是 <strong>Always</strong>, 即: <strong>任何时候这个容器发生了异常, 它一定会被重新创建</strong>.</p> <p>但一定要强调的是, Pod 的恢复过程, <strong>永远都是发生在当前节点上, 而不会跑到别的节点上去</strong>. 事实上, 一旦一个 Pod 与一个节点(Node)绑定, 除非这个绑定发生了变化(pod.spec.node 字段被修改), 否则它永远都不会离开这个节点. 这也就意味着, 如果这个宿主机宕机了, 这个 Pod 也不会主动迁移到其他节点上去.</p> <p>而作为用户, 还可以通过设置 restartPolicy, <strong>改变 Pod 的恢复策略</strong>. 除了 Always, 它还有 <strong>OnFailure</strong> 和 <strong>Never</strong> 两种情况:</p> <ul><li><strong>Always</strong>: 在任何情况下, 只要容器不在运行状态, 就自动重启容器;</li> <li><strong>OnFailure</strong>: 只在容器异常时才自动重启容器;</li> <li><strong>Never</strong>: 从来不重启容器.</li></ul> <p>在实际使用时, 需要根据应用运行的特性, 合理设置这三种恢复策略.</p> <p>比如, 一个 Pod, 它只计算 1+1=2, 计算完成输出结果后退出, 变成 Succeeded 状态. 这时, 如果再用 restartPolicy=Always 强制重启这个 Pod 的容器, 就没有任何意义了.</p> <p>而如果要关心这个容器退出后的<strong>上下文环境</strong>, 比如容器退出后的日志, 文件和目录, 就需要将 restartPolicy 设置为 Never. 因为<strong>一旦容器被自动重新创建, 这些内容就有可能丢失掉了(被垃圾回收了)</strong> .</p> <p>值得一提的是, Kubernetes 的官方文档, 把 restartPolicy 和 Pod 里容器的状态, 以及 Pod 状态的对应关系, 总结了非常复杂的一大堆情况. 实际上, 根本不需要死记硬背这些对应关系, 只要记住如下两个基本的设计原理即可:</p> <ol><li><mark><strong>只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器(比如: Always), 那么这个 Pod 就会保持 Running 状态, 并进行容器重启</strong></mark>. 否则, Pod 就会进入 Failed 状态 .</li> <li><mark><strong>对于包含多个容器的 Pod, 只有它里面所有的容器都进入异常状态后, Pod 才会进入 Failed 状态</strong></mark>. 在此之前, Pod 都是 Running 状态. 此时 Pod 的 READY 字段会显示正常容器的个数, 比如:</li></ol> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod test-liveness-exec
NAME           READY     STATUS    RESTARTS   AGE
liveness-exec   <span class="token number">0</span>/1       Running   <span class="token number">1</span>          1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>所以, 假如一个 Pod 里<strong>只有一个容器</strong>, 然后这个容器异常退出了. 那么只有当 restartPolicy=Never 时, 这个 Pod 才会进入 Failed 状态. 而其他情况下, 由于 Kubernetes 都可以<strong>重启</strong>这个容器, 所以 Pod 的状态保持 <strong>Running</strong> 不变.</p> <p>而如果这个 Pod <strong>有多个容器</strong>, 仅有一个容器异常退出, 它就始终保持 Running 状态, 哪怕即使 restartPolicy=Never. 只有当<strong>所有容器也异常退出</strong>之后, 这个 Pod 才会进入 <strong>Failed</strong> 状态.</p> <p>其他情况, 都可以以此类推出来.</p> <p>现在一起回到前面提到的 <strong>livenessProbe</strong> 上来. 除了在容器中执行命令外, livenessProbe 也可以<strong>定义为发起 HTTP 或者 TCP 请求的方式</strong>, 定义格式如下:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>
     <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>
       <span class="token key atrule">path</span><span class="token punctuation">:</span> /healthz
       <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>
       <span class="token key atrule">httpHeaders</span><span class="token punctuation">:</span>
       <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> X<span class="token punctuation">-</span>Custom<span class="token punctuation">-</span>Header
         <span class="token key atrule">value</span><span class="token punctuation">:</span> Awesome
       <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">3</span>
       <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">3</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
   <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>
     <span class="token key atrule">tcpSocket</span><span class="token punctuation">:</span>
       <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>
     <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">15</span>
     <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">20</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>所以, 你的 Pod 其实<strong>可以暴露一个健康检查 URL(比如 /healthz), 或者直接让健康检查去检测应用的监听端口</strong>. 这两种配置方法, 在 Web 服务类的应用中非常常用.</p> <p>在 Kubernetes 的 Pod 中, 还有一个叫 <strong>readinessProbe</strong> 的字段. 虽然它的用法与 livenessProbe 类似, 但作用却大不一样. readinessProbe 检查结果的成功与否, 决定的这个 Pod 是不是能被通过 Service 的方式访问到, 而并不影响 Pod 的生命周期. 这部分内容, 会留在讲解 Service 时再重点介绍.</p> <p>在讲解了这么多字段之后, 想必你对 Pod 对象的语义和描述能力, 已经有了一个初步的感觉. 这时你有没有产生这样一个想法: Pod 的字段这么多, 又不可能全记住, <strong>Kubernetes 能不能自动给 Pod 填充某些字段</strong>呢?</p> <p>这个需求实际上非常实用. 比如, 开发人员只需要提交一个基本的, 非常简单的 Pod YAML, Kubernetes 就可以自动给对应的 Pod 对象加上其他必要的信息, 比如 labels, annotations, volumes 等等. 而这些信息, 可以是运维人员事先定义好的. 这么一来, 开发人员编写 Pod YAML 的门槛, 就被大大降低了.</p> <p>所以, 这个叫作 <strong>PodPreset</strong>(Pod 预设置)的功能 已经出现在了 v1.11 版本的 Kubernetes 中.</p> <p>举个例子, 现在开发人员编写了如下一个 pod.yaml 文件:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> website
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> website
    <span class="token key atrule">role</span><span class="token punctuation">:</span> frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> website
      <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
      <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>作为 Kubernetes 的初学者, 你肯定眼前一亮: 这不就是我最擅长编写的, 最简单的 Pod 嘛. 没错, 这个 YAML 文件里的字段, 想必你现在闭着眼睛也能写出来. 可如果运维人员看到了这个 Pod, 他一定会连连摇头: <strong>这种 Pod 在生产环境里根本不能用啊</strong>!</p> <p>所以, 这个时候运维人员就可以<strong>定义一个 PodPreset 对象</strong>. 在这个对象中, <strong>凡是他想在开发人员编写的 Pod 里追加的字段, 都可以预先定义好</strong>. 比如这个 preset.yaml:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> settings.k8s.io/v1alpha1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PodPreset
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> allow<span class="token punctuation">-</span>database
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">role</span><span class="token punctuation">:</span> frontend
  <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> DB_PORT
      <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;6379&quot;</span>
  <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /cache
      <span class="token key atrule">name</span><span class="token punctuation">:</span> cache<span class="token punctuation">-</span>volume
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> cache<span class="token punctuation">-</span>volume
      <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>在这个 PodPreset 的定义中, 首先是一个 <strong>selector</strong>. 这就意味着<strong>后面这些追加的定义, 只会作用于 selector 所定义的, 带有 &quot;role: frontend&quot; 标签的 Pod 对象, 这就可以防止 &quot;误伤&quot;</strong> . 然后定义了一组 Pod 的 Spec 里的<strong>标准字段, 以及对应的值</strong>. 比如, env 里定义了 DB_PORT 这个环境变量, volumeMounts 定义了容器 Volume 的挂载目录, volumes 定义了一个 emptyDir 的 Volume.</p> <p>接下来, 假定运维人员<strong>先创建了这个 PodPreset, 然后开发人员才创建 Pod</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> preset.yaml
$ kubectl create <span class="token parameter variable">-f</span> pod.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这时, Pod 运行起来之后, 查看一下这个 Pod 的 API 对象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod website <span class="token parameter variable">-o</span> yaml
apiVersion: v1
kind: Pod
metadata:
  name: website
  labels:
    app: website
    role: frontend
  annotations:
    podpreset.admission.kubernetes.io/podpreset-allow-database: <span class="token string">&quot;resource version&quot;</span>
spec:
  containers:
    - name: website
      image: nginx
      volumeMounts:
        - mountPath: /cache
          name: cache-volume
      ports:
        - containerPort: <span class="token number">80</span>
      env:
        - name: DB_PORT
          value: <span class="token string">&quot;6379&quot;</span>
  volumes:
    - name: cache-volume
      emptyDir: <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>这个时候, 就可以清楚地看到, 这个 Pod 里<strong>多了新添加的 labels, env, volumes 和 volumeMount 的定义, 它们的配置跟 PodPreset 的内容一样</strong>. 此外, 这个 Pod 还被<strong>自动加上了一个 annotation 表示这个 Pod 对象被 PodPreset 改动过</strong>.</p> <p>需要说明的是, <mark><strong>PodPreset 里定义的内容, 只会在 Pod API 对象被创建之前追加在这个对象本身上, 而不会影响任何 Pod 的控制器的定义</strong></mark>​ **. **</p> <p>比如, 现在提交的是一个 nginx-deployment, 那么<mark><strong>这个 Deployment 对象本身是永远不会被 PodPreset 改变的, 被修改的只是这个 Deployment 创建出来的所有 Pod</strong></mark>. 这一点请务必区分清楚.</p> <p>这里有一个问题: <strong>如果定义了同时作用于一个 Pod 对象的多个 PodPreset</strong>, 会发生什么呢? 实际上, Kubernetes 项目会帮你<strong>合并(Merge)这两个 PodPreset 要做的修改</strong>. 而如果它们要做的修改有冲突的话, 这些<strong>冲突字段就不会被修改</strong>.</p> <blockquote><p>总结</p></blockquote> <p>本节详细介绍了 Pod 对象更高阶的使用方法, 通过对这些实例的讲解, 可以更深入地理解 Pod API 对象的各个字段.</p> <p>而在学习这些字段的同时, 还应该认真体会一下 <strong>Kubernetes &quot;一切皆对象&quot; 的设计思想: 比如应用是 Pod 对象, 应用的配置是 ConfigMap 对象, 应用要访问的密码则是 Secret 对象</strong>. 所以, 也就自然而然地有了 <strong>PodPreset 这样专门用来对 Pod 进行批量化, 自动化修改的工具对象</strong>. 在后面的内容中, 会讲解更多的这种对象, 还会介绍 Kubernetes 项目如何围绕着这些对象进行容器编排.</p> <h4 id="_16-编排其实很简单-谈谈-控制器-模型"><a href="#_16-编排其实很简单-谈谈-控制器-模型" class="header-anchor">#</a> 16 | 编排其实很简单:谈谈&quot;控制器&quot;模型</h4> <p>上一节详细<strong>介绍了 Pod 的用法, 讲解了 Pod 这个 API 对象的各个字段</strong>. 本节就来看看 &quot;编排&quot; 这个 Kubernetes 项目最核心的功能.</p> <p>其实你可能已经有所感悟: <mark><strong>Pod 这个看似复杂的 API 对象, 实际上就是对容器的进一步抽象和封装而已</strong></mark>​ **. **</p> <p>说得更形象些, &quot;容器&quot;镜像虽然好用, 但是容器这样一个&quot;沙盒&quot;的概念, 对于描述应用来说, 还是太过简单了. 这就好比集装箱固然好用, 但是如果它四面都光秃秃的, 吊车还怎么把这个集装箱吊起来并摆放好呢?</p> <p>**所以 Pod 对象, 其实就是容器的升级版. 它对容器进行了组合, 添加了更多的属性和字段. 这就好比给集装箱四面安装了吊环, 使得 Kubernetes 这架&quot;吊车&quot;, 可以更轻松地操作它. ** 而 Kubernetes <strong>操作这些&quot;集装箱&quot;的逻辑, 都由控制器(Controller)完成</strong>. 在前面曾经使用过 <strong>Deployment</strong> 这个最基本的控制器对象.</p> <p>来回顾一下这个名叫 nginx-deployment 的例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>这个 Deployment 定义的<strong>编排动作</strong>非常简单, 即: <strong>确保携带了 app=nginx 标签的 Pod 的个数, 永远等于 spec.replicas 指定的个数, 即 2 个</strong>.</p> <p>这就意味着, 如果在这个集群中, 携带 app=nginx 标签的 Pod 的个数大于 2 的时候, 就会有旧的 Pod 被<strong>删除</strong>; 反之, 就会有新的 Pod 被<strong>创建</strong>.</p> <p>这时你也许就会好奇: <strong>究竟是 Kubernetes 项目中的哪个组件, 在执行这些操作呢</strong>?</p> <p>在前面介绍 Kubernetes 架构的时候, 曾经提到过一个叫作 <strong>kube-controller-manager</strong> 的组件. 实际上, 这个组件, 就是一系列控制器的集合. 可以查看一下 Kubernetes 项目的 pkg/controller 目录:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token builtin class-name">cd</span> kubernetes/pkg/controller/
$ <span class="token function">ls</span> <span class="token parameter variable">-d</span> */            
deployment/             job/                    podautoscaler/        
cloud/                  disruption/             namespace/            
replicaset/             serviceaccount/         volume/
cronjob/                garbagecollector/       nodelifecycle/          replication/            statefulset/            daemon/
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>这个目录下面的每一个控制器, 都以独有的方式负责某种</strong>​<mark><strong>编排功能</strong></mark>. 而 Deployment 正是这些控制器中的一种. 实际上, 这些控制器之所以被统一放在 pkg/controller 目录下, 就是因为**它们都遵循 Kubernetes 项目中的一个通用编排模式, 即: **​<mark><strong>控制循环</strong></mark>​ <strong>(control loop)</strong> .</p> <p>比如, 现在有一种待编排的对象 X, 它有一个对应的控制器. 那么就可以用一段 Go 语言风格的伪代码, 来描述这个<strong>控制循环</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token keyword">for</span> <span class="token punctuation">{</span>
  实际状态 :<span class="token operator">=</span> 获取集群中对象 X 的实际状态<span class="token punctuation">(</span>Actual State<span class="token punctuation">)</span>
  期望状态 :<span class="token operator">=</span> 获取集群中对象 X 的期望状态<span class="token punctuation">(</span>Desired State<span class="token punctuation">)</span>
  <span class="token keyword">if</span> 实际状态 <span class="token operator">==</span> 期望状态<span class="token punctuation">{</span>
    什么都不做
  <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
    执行编排动作, 将实际状态调整为期望状态
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p><mark><strong>在具体实现中, 实际状态往往来自于 Kubernetes 集群本身</strong></mark>​<mark>.</mark> 比如, kubelet 通过心跳汇报的容器状态和节点状态, 或者监控系统中保存的应用监控数据, 或者控制器主动收集的它自己感兴趣的信息, 这些都是常见的实际状态的来源.</p> <p><mark><strong>而期望状态, 一般来自于用户提交的 YAML 文件</strong></mark>. 比如, Deployment 对象中 Replicas 字段的值. 很明显这些信息往往都保存在 <strong>Etcd</strong> 中.</p> <p>接下来, 以 Deployment 为例来简单描述一下它对控制器模型的实现:</p> <ol><li><strong>Deployment 控制器从 Etcd 中获取到所有携带了 &quot;app: nginx&quot; 标签的 Pod, 然后统计它们的数量, 这就是实际状态</strong>;</li> <li>**Deployment 对象的 Replicas 字段的值就是期望状态; **</li> <li>**Deployment 控制器将两个状态做比较, 然后根据比较结果, 确定是创建 Pod, 还是删除已有的 Pod(具体如何操作 Pod 对象, 会在下一节详细介绍). **</li></ol> <p>可以看到, 一个 Kubernetes 对象的主要编排逻辑, 实际上是在第三步的  <strong>&quot;对比&quot; 阶段</strong>完成的.</p> <p>这个操作, 通常被叫作<mark><strong>调谐</strong></mark>(Reconcile). 这个调谐的过程, 则被称作 &quot;Reconcile Loop&quot;(<strong>调谐循环</strong>) 或者&quot;Sync Loop&quot;(<strong>同步循环</strong>). 所以, 如果以后在文档或者社区中碰到这些词, 都不要担心, 它们其实指的都是同一个东西: <mark><strong>控制循环</strong></mark>.</p> <p><mark><strong>而调谐的最终结果, 往往都是对被控制对象的某种写操作</strong></mark>. 比如增加 Pod, 删除已有的 Pod, 或者更新 Pod 的某个字段. **这也是 Kubernetes 项目&quot;面向 API 对象编程&quot;的一个直观体现. **</p> <p>其实, 像 Deployment 这种控制器的设计原理, 就是前面提到过的, &quot;用一种对象管理另一种对象&quot; 的 &quot;艺术&quot;. 其中, <strong>这个控制器对象本身, 负责定义被管理对象的期望状态</strong>. 比如, Deployment 里的 replicas=2 这个字段. 而<strong>被控制对象的定义, 则来自于一个&quot;模板&quot;</strong> . 比如, Deployment 里的 template 字段.</p> <p>可以看到, Deployment 这个 <strong>template 字段里的内容, 跟一个标准的 Pod 对象的 API 定义, 丝毫不差</strong>. 而所有被这个 Deployment 管理的 Pod 实例, 其实都是<strong>根据这个 template 字段的内容创建</strong>出来的.</p> <p>像 Deployment 定义的 template 字段, 在 Kubernetes 项目中有一个专有的名字, 叫作 <strong>PodTemplate</strong>(Pod 模板).</p> <p>这个概念非常重要, 因为后面要讲解到的<strong>大多数控制器, 都会使用 PodTemplate 来统一定义它所要管理的 Pod</strong>. 更有意思的是, 还可以看到其他类型的对象模板, 比如 Volume 的模板.</p> <p>至此就可以对 Deployment 以及其他类似的<strong>控制器</strong>, 做一个简单总结了:</p> <p><img src="/img/276d18cbf5c3d728acc9994d11e559ac-20230731162150-t2uacji.png" alt=""></p> <p>如上图所示, <mark><strong>类似 Deployment 这样的一个控制器, 实际上都是由上半部分的控制器定义(包括期望状态), 加上下半部分的被控制对象的模板组成的</strong></mark>​ <strong>. ** 这就是为什么, 在所有 API 对象的 Metadata 里, 都有一个字段叫作 <strong>ownerReference</strong>, 用于</strong>保存当前这个 API 对象的拥有者(Owner)的信息**.</p> <p>那么, 对于上面这个 nginx-deployment 来说, 它创建出来的 Pod 的 ownerReference 就是 nginx-deployment 吗? 或者说, nginx-deployment 所直接控制的, 就是 Pod 对象么? 这个问题的答案, 就留到下一节再做详细解释.</p> <blockquote><p>总结</p></blockquote> <p>本节以 Deployment 为例, 详细分享了 Kubernetes 项目如何通过一个称作 &quot;<strong>控制器模式</strong>&quot;(controller pattern)的设计方法, 来<strong>统一地实现对各种不同的对象或者资源进行的编排操作</strong>.</p> <p>在后面的讲解中, 还会讲到<strong>很多不同类型的容器编排功能, 比如 StatefulSet, DaemonSet 等等, 它们无一例外地都有这样一个甚至多个控制器的存在, 并遵循控制循环(control loop)的流程, 完成各自的编排逻辑</strong>. 实际上, 跟 Deployment 相似, 这些<strong>控制循环最后的执行结果, 要么就是创建, 更新一些 Pod(或者其他的 API 对象, 资源), 要么就是删除一些已经存在的 Pod(或者其他的 API 对象, 资源)</strong> .</p> <p>**但也正是在这个统一的编排框架下, 不同的控制器可以在具体执行过程中, 设计不同的业务逻辑, 从而达到不同的编排效果. **</p> <p><strong>这个实现思路, 正是 Kubernetes 项目进行容器编排的核心原理</strong>. 在此后讲解 Kubernetes 编排功能的文章中, 都会遵循这个逻辑展开, 并且带你逐步领悟控制器模式在不同的容器化作业中的实现方式.</p> <h4 id="_17-经典paas的记忆-作业副本与水平扩展"><a href="#_17-经典paas的记忆-作业副本与水平扩展" class="header-anchor">#</a> 17 | 经典PaaS的记忆:作业副本与水平扩展</h4> <p>上一节详细介绍了 Kubernetes 项目中第一个重要的设计思想: <strong>控制器模式</strong>. 本节详细讲解一下, Kubernetes 里第一个控制器模式的完整实现: <strong>Deployment</strong>.</p> <p>Deployment 看似简单, 但实际上它实现了 Kubernetes 项目中一个非常重要的功能: <mark><strong>Pod 的 &quot;水平扩展 / 收缩&quot;(horizontal scaling out/in)</strong></mark> . 这个功能是从 PaaS 时代开始, 一个平台级项目就必须具备的编排能力.</p> <p>举个例子, <strong>如果你更新了 Deployment 的 Pod 模板(比如修改了容器的镜像), 那么 Deployment 就需要遵循一种叫作 &quot;滚动更新&quot;(rolling update)的方式, 来升级现有的容器</strong>. 而这个能力的实现, 依赖的是 Kubernetes 项目中的一个<mark><strong>非常重要的概念(API 对象): ReplicaSet</strong></mark>.</p> <p>ReplicaSet 的结构非常简单, 可以通过这个 YAML 文件查看一下:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ReplicaSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>set
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>从这个 YAML 文件中可以看到, <mark><strong>一个 ReplicaSet 对象, 其实就是由副本数目的定义和一个 Pod 模板组成的</strong></mark>​<mark>. 不难发现, 它的定义其实是 Deployment 的一个子集</mark>.</p> <p><mark><strong>更重要的是, Deployment 控制器实际操纵的, 正是这样的 ReplicaSet 对象, 而不是 Pod 对象.</strong></mark></p> <p>上一节曾经提出过这样一个问题: 对于一个 Deployment 所管理的 Pod, 它的 ownerReference 是谁? 所以这个问题的答案就是: <strong>ReplicaSet</strong>.</p> <p>明白了这个原理, 再来分析一个如下所示的 Deployment:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>可以看到, 这就是一个常用的 <strong>nginx-deployment, 它定义的 Pod 副本个数是 3</strong>(spec.replicas=3).</p> <p>那么, 在具体的实现上, 这个 <strong>Deployment, 与 ReplicaSet, 以及 Pod 的关系</strong>是怎样的呢? 可以用一张图把它描述出来:</p> <p><img src="/img/eaced430430452fb4243b9459464fbd7-20230731162150-xbgnafe.png" alt=""></p> <p>通过这张图就很清楚的看到, **一个定义了 replicas=3 的 Deployment, 与它的 ReplicaSet, 以及 Pod 的关系, 实际上是一种 &quot;**​<mark><strong>层层控制</strong></mark>​ <strong>&quot; 的关系</strong>.</p> <p>其中, <strong>ReplicaSet 负责通过 &quot;控制器模式&quot;, 保证系统中 Pod 的个数永远等于指定的个数(比如 3 个)</strong> . 这也正是 Deployment <strong>只允许容器的 restartPolicy=Always</strong> 的主要原因: <mark><strong>只有在容器能保证自己始终是 Running 状态的前提下, ReplicaSet 调整 Pod 的个数才有意义</strong></mark>.</p> <p>而在此基础上, <strong>Deployment 同样通过 &quot;控制器模式&quot;, 来操作 ReplicaSet 的个数和属性, 进而实现 &quot;水平扩展/收缩&quot; 和 &quot;滚动更新&quot; 这两个编排动作</strong>.</p> <p>其中 &quot;水平扩展/收缩&quot; 非常容易实现, Deployment Controller 只需要修改它所控制的 <strong>ReplicaSet 的 Pod 副本个数</strong>就可以了. 比如, 把这个值从 3 改成 4, 那么 Deployment 所对应的 ReplicaSet, 就会根据修改后的值<strong>自动创建</strong>一个新的 Pod. 这就是 &quot;水平扩展&quot; 了; &quot;水平收缩&quot; 则反之.</p> <p>而用户想要执行这个操作的指令也非常简单, 就是 kubectl scale, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl scale deployment nginx-deployment <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">4</span>
deployment.apps/nginx-deployment scaled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>那么  **&quot;滚动更新&quot; ** 又是什么意思, 是如何实现的呢?</p> <p>接下来还以这个 Deployment 为例来讲解 &quot;滚动更新&quot; 的过程. 首先来创建这个 nginx-deployment:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> nginx-deployment.yaml <span class="token parameter variable">--record</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>注意这里额外加了一个–record 参数. 它的作用是<strong>记录下你每次操作所执行的命令, 以方便后面查看</strong>. 然后来检查一下 nginx-deployment 创建后的状态信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   <span class="token number">3</span>         <span class="token number">0</span>         <span class="token number">0</span>            <span class="token number">0</span>           1s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在返回结果中, 可以看到四个状态字段, 它们的含义如下所示.</p> <ol><li><strong>DESIRED</strong>: 用户期望的 Pod 副本个数(spec.replicas 的值);</li> <li><strong>CURRENT</strong>: 当前处于 <strong>Running</strong> 状态的 Pod 的个数;</li> <li><strong>UP-TO-DATE</strong>: 当前处于<strong>最新版本的 Pod 的个数</strong>, 所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致;</li> <li><strong>AVAILABLE</strong>: 当前已经可用的 Pod 的个数, 即: <strong>既是 Running 状态, 又是最新版本, 并且已经处于 Ready(健康检查正确)状态的 Pod 的个数</strong>.</li></ol> <p>可以看到, 只有这个 <strong>AVAILABLE 字段, 描述的才是用户所期望的最终状态</strong>.</p> <p>而 Kubernetes 项目还提供了一条指令, 可以实时<strong>查看 Deployment 对象的状态变化</strong>. 这个指令就是 <strong>kubectl rollout status</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout status deployment/nginx-deployment
Waiting <span class="token keyword">for</span> rollout to finish: <span class="token number">2</span> out of <span class="token number">3</span> new replicas have been updated<span class="token punctuation">..</span>.
deployment.apps/nginx-deployment successfully rolled out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在这个返回结果中, &quot;2 out of 3 new replicas have been updated&quot; 意味着已经有 2 个 Pod 进入了 <strong>UP-TO-DATE</strong> 状态. 继续等待一会儿, 就能看到这个 Deployment 的 3 个 Pod, 就进入到了 AVAILABLE 状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   <span class="token number">3</span>         <span class="token number">3</span>         <span class="token number">3</span>            <span class="token number">3</span>           20s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>此时可以尝试查看一下这个 Deployment 所控制的 <strong>ReplicaSet</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-3167673210   <span class="token number">3</span>         <span class="token number">3</span>         <span class="token number">3</span>       20s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>如上所示, 在用户提交了一个 Deployment 对象后, <strong>Deployment Controller 就会立即创建一个 Pod 副本个数为 3 的 ReplicaSet. 这个 ReplicaSet 的名字, 则是由 Deployment 的名字和一个随机字符串共同组成</strong>. 这个随机字符串叫作 <strong>pod-template-hash</strong>, 在这个例子里就是: 3167673210. <strong>ReplicaSet 会把这个随机字符串加在它所控制的所有 Pod 的标签里, 从而保证这些 Pod 不会与集群里的其他 Pod 混淆</strong>.</p> <p>而 ReplicaSet 的 DESIRED, CURRENT 和 READY 字段的含义, 和 Deployment 中是<strong>一致</strong>的. 所以, **相比之下, Deployment 只是在 ReplicaSet 的基础上, 添加了 UP-TO-DATE 这个跟版本有关的状态字段. **</p> <p>这个时候, 如果<mark><strong>修改了 Deployment 的 Pod 模板, &quot;滚动更新&quot; 就会被自动触发</strong></mark>.</p> <p>修改 Deployment 有很多方法. 比如可以直接使用 <strong>kubectl edit 指令编辑 Etcd 里的 API 对象</strong>.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit deployment/nginx-deployment
<span class="token punctuation">..</span>. 
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1 <span class="token comment"># 1.7.9 -&gt; 1.9.1</span>
        ports:
        - containerPort: <span class="token number">80</span>
<span class="token punctuation">..</span>.
deployment.extensions/nginx-deployment edited
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这个 kubectl edit 指令, 会帮<strong>直接打开 nginx-deployment 的 API 对象</strong>. 然后就可以修改这里的 Pod 模板部分了. 比如这里将 nginx 镜像的版本<strong>升级到了 1.9.1</strong>. kubectl edit 其实并不神秘, 它不过是把 API 对象的内容下载到了本地文件, 让你<strong>修改完成后再提交</strong>上去.</p> <p><strong>kubectl edit 指令编辑完成后, 保存退出, Kubernetes 就会立刻触发 &quot;滚动更新&quot; 的过程</strong>. 还可以通过 <strong>kubectl rollout status</strong> 指令查看 nginx-deployment 的状态变化:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout status deployment/nginx-deployment
Waiting <span class="token keyword">for</span> rollout to finish: <span class="token number">2</span> out of <span class="token number">3</span> new replicas have been updated<span class="token punctuation">..</span>.
deployment.extensions/nginx-deployment successfully rolled out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这时可以通过查看 Deployment 的 <strong>Events</strong>, 看到这个 &quot;滚动更新&quot; 的流程:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe deployment nginx-deployment
<span class="token punctuation">..</span>.
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
<span class="token punctuation">..</span>.
  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica <span class="token builtin class-name">set</span> nginx-deployment-1764197365 to <span class="token number">1</span>
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica <span class="token builtin class-name">set</span> nginx-deployment-3167673210 to <span class="token number">2</span>
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica <span class="token builtin class-name">set</span> nginx-deployment-1764197365 to <span class="token number">2</span>
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica <span class="token builtin class-name">set</span> nginx-deployment-3167673210 to <span class="token number">1</span>
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica <span class="token builtin class-name">set</span> nginx-deployment-1764197365 to <span class="token number">3</span>
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica <span class="token builtin class-name">set</span> nginx-deployment-3167673210 to <span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>可以看到, 首先, <strong>当修改了 Deployment 里的 Pod 定义之后, <strong>​</strong>==Deployment Controller 会使用这个修改后的 Pod 模板, 创建一个新的 ReplicaSet(hash=1764197365), 这个新的 ReplicaSet 的初始 Pod 副本数是: 0==</strong>.</p> <p>然后, 在 Age=24 s 的位置, <strong>Deployment Controller 开始将这个新的 ReplicaSet 所控制的 Pod 副本数从 0 个变成 1 个, 即: &quot;水平扩展&quot;出一个副本</strong>.</p> <p>紧接着, 在 Age=22 s 的位置, <strong>Deployment Controller 又将旧的 ReplicaSet(hash=3167673210)所控制的旧 Pod 副本数减少一个, 即: &quot;水平收缩&quot;成两个副本</strong>.</p> <p>如此<mark><strong>交替进行</strong></mark>, 新 ReplicaSet 管理的 Pod 副本数, 从 0 个变成 1 个, 再变成 2 个, 最后变成 3 个. 而旧的 ReplicaSet 管理的 Pod 副本数则从 3 个变成 2 个, 再变成 1 个, 最后变成 0 个. 这样就完成了这一组 Pod 的版本升级过程.</p> <p>像这样, <mark><strong>将一个集群中正在运行的多个 Pod 版本, 交替地逐一升级的过程, 就是 &quot;滚动更新&quot;</strong></mark>​ **. **</p> <p>在这个 &quot;滚动更新&quot; 过程完成之后, 可以查看一下新, 旧两个 ReplicaSet 的<strong>最终状态</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1764197365   <span class="token number">3</span>         <span class="token number">3</span>         <span class="token number">3</span>       6s
nginx-deployment-3167673210   <span class="token number">0</span>         <span class="token number">0</span>         <span class="token number">0</span>       30s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>其中, 旧 ReplicaSet(hash=3167673210)已经被 &quot;水平收缩&quot; 成了 0 个副本.</p> <p><strong>这种&quot;滚动更新&quot;的好处是显而易见的. ** 比如, 在升级刚开始的时候, 集群里只有 1 个新版本的 Pod. 如果这时, 新版本 Pod 有问题启动不起来, 那么 &quot;滚动更新&quot; 就会停止, 从而允许开发和运维人员介入. 而在这个过程中, 由于应用本身还有两个旧版本的 Pod 在线, 所以服务并不会受到太大的影响. 当然, 这也就要求你一定要</strong>使用 Pod 的 Health Check 机制检查应用的运行状态**, 而不是简单地依赖于容器的 Running 状态. 要不然的话, 虽然容器已经变成 Running 了, 但服务很有可能尚未启动, &quot;滚动更新&quot; 的效果也就达不到了.</p> <p>而为了进一步<strong>保证服务的连续性</strong>, Deployment Controller 还会<strong>确保在任何时间窗口内, 只有指定比例的 Pod 处于离线状态</strong>. 同时, 它也会确保在任何时间窗口内, 只有<strong>指定比例的新 Pod 被创建</strong>出来. 这两个比例的值都是可以配置的, 默认都是 DESIRED 值的 25%.</p> <p>所以, 在上面这个 Deployment 的例子中, 它有 3 个 Pod 副本, 那么控制器在 &quot;滚动更新&quot; 的过程中永远都会确保<strong>至少有 2 个 Pod 处于可用状态</strong>, 至多只有 4 个 Pod 同时存在于集群中. 这个策略是 Deployment 对象的一个字段, 名叫 <strong>RollingUpdateStrategy</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
<span class="token punctuation">...</span>
  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>
    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate
    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>
      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>
      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>在上面这个 RollingUpdateStrategy 的配置中, <strong>maxSurge</strong> 指定的是除了 DESIRED 数量之外, 在一次 &quot;滚动&quot; 中, Deployment 控制器还<strong>可以创建多少个新 Pod</strong>; 而 <strong>maxUnavailable</strong> 指的是, 在一次 &quot;滚动&quot; 中, Deployment 控制器<strong>可以删除多少个旧 Pod</strong>. 同时这两个配置还可以用前面介绍的百分比形式来表示, 比如: maxUnavailable=50%, 指的是最多可以一次删除 &quot;50% * DESIRED 数量&quot; 个 Pod.</p> <p>结合以上讲述, 现在可以<strong>扩展一下 Deployment, ReplicaSet 和 Pod 的关系图</strong>了.</p> <p><img src="/img/b717f2805f427f993e209991a1b1809e-20230731162150-23q23fx.png" alt=""></p> <p><mark><strong>如上所示, Deployment 的控制器, 实际上控制的是 ReplicaSet 的数目, 以及每个 ReplicaSet 的属性. 而一个应用的版本, 对应的正是一个 ReplicaSet; 这个版本应用的 Pod 数量, 则由 ReplicaSet 通过它自己的控制器(ReplicaSet Controller)来保证. 通过这样的多个 ReplicaSet 对象, Kubernetes 项目就实现了对多个 &quot;应用版本&quot; 的描述.</strong></mark></p> <p>而明白了 &quot;<strong>应用版本和 ReplicaSet 一一对应</strong>&quot; 的设计思想之后, 就可以继续讲解 Deployment 对应用进行<strong>版本控制</strong>的具体原理了.</p> <p>这一次, 会使用一个叫 <strong>kubectl set image ** 的指令, 直接修改 nginx-deployment 所使用的</strong>镜像**. 这个命令的好处就是, 可以不用像 kubectl edit 那样需要打开编辑器. 不过这一次, 把这个镜像名字修改成为了一个<strong>错误的名字</strong>, 比如: nginx:1.91. 这样这个 Deployment 就会出现一个<strong>升级失败</strong>的版本.</p> <p>一起来实践一下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">set</span> image deployment/nginx-deployment <span class="token assign-left variable">nginx</span><span class="token operator">=</span>nginx:1.91
deployment.extensions/nginx-deployment image updated
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>由于这个 nginx:1.91 镜像在 Docker Hub 中<strong>并不存在</strong>, 所以这个 Deployment 的 &quot;滚动更新&quot; 被触发后, 会<strong>立刻报错并停止</strong>.</p> <p>这时来检查一下 <strong>ReplicaSet 的状态</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1764197365   <span class="token number">2</span>         <span class="token number">2</span>         <span class="token number">2</span>       24s
nginx-deployment-3167673210   <span class="token number">0</span>         <span class="token number">0</span>         <span class="token number">0</span>       35s
nginx-deployment-2156724341   <span class="token number">2</span>         <span class="token number">2</span>         <span class="token number">0</span>       7s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>通过这个返回结果可以看到, 新版本的 ReplicaSet(hash=2156724341)的  <strong>&quot;水平扩展&quot; 已经停止</strong>. 而且此时它<strong>已经创建了两个 Pod, 但是它们都没有进入 READY 状态. 这当然是因为这两个 Pod 都拉取不到有效的镜像</strong>.</p> <p>与此同时, 旧版本的 ReplicaSet(hash=1764197365)的  <strong>&quot;水平收缩&quot;, 也自动停止</strong>了. 此时, 已经有一个旧 Pod 被删除, 还剩下两个旧 Pod.</p> <p>那么问题来了, 如何让这个 Deployment 的 3 个 Pod, <strong>都回滚到以前的旧版本</strong>呢?</p> <p>只需要执行一条 <strong>kubectl rollout undo 命令</strong>, 就能把整个 Deployment 回滚到上一个版本:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout undo deployment/nginx-deployment
deployment.extensions/nginx-deployment
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>很容易想到, 在具体操作上, Deployment 的控制器, 其实就是让这个旧 ReplicaSet(hash=1764197365)<strong>再次 &quot;扩展&quot;</strong>  成 3 个 Pod, 而让新的 ReplicaSet(hash=2156724341)<strong>重新 &quot;收缩&quot;</strong>  到 0 个 Pod.</p> <p>更进一步地, 如果想<strong>回滚到更早之前的版本</strong>, 要怎么办呢?</p> <p><strong>首先需要使用 kubectl rollout history 命令, 查看每次 Deployment 变更对应的版本</strong>. 而由于在创建这个 Deployment 的时候, <strong>指定了 –record 参数</strong>, 所以创建这些版本时执行的 kubectl 命令, 都会被记录下来. 这个操作的输出如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout <span class="token function">history</span> deployment/nginx-deployment
deployments <span class="token string">&quot;nginx-deployment&quot;</span>
REVISION    CHANGE-CAUSE
<span class="token number">1</span>           kubectl create <span class="token parameter variable">-f</span> nginx-deployment.yaml <span class="token parameter variable">--record</span>
<span class="token number">2</span>           kubectl edit deployment/nginx-deployment
<span class="token number">3</span>           kubectl <span class="token builtin class-name">set</span> image deployment/nginx-deployment <span class="token assign-left variable">nginx</span><span class="token operator">=</span>nginx:1.91
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, 前面执行的创建和更新操作, 分别对应了<strong>版本 1 和版本 2</strong>, 而那次失败的更新操作, 则对应的是版本 3.</p> <p>当然还可以通过这个 <strong>kubectl rollout history</strong> 指令, 看到每个版本对应的 Deployment 的 API 对象的细节, 具体命令如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout <span class="token function">history</span> deployment/nginx-deployment <span class="token parameter variable">--revision</span><span class="token operator">=</span><span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><strong>然后就可以在 kubectl rollout undo 命令行最后, 加上要回滚到的指定版本的版本号, 就可以回滚到指定版本了</strong>. 这个指令的用法如下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout undo deployment/nginx-deployment --to-revision<span class="token operator">=</span><span class="token number">2</span>
deployment.extensions/nginx-deployment
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这样, Deployment Controller 还会按照 &quot;<strong>滚动更新</strong>&quot; 的方式, 完成对 Deployment 的降级操作.</p> <p>不过, 你可能已经想到了一个问题: <strong>对 Deployment 进行的每一次更新操作, 都会生成一个新的 ReplicaSet 对象, 是不是有些多余, 甚至浪费资源呢</strong>?</p> <p>没错.</p> <p>所以, Kubernetes 项目还提供了一个指令, <strong>使得对 Deployment 的多次更新操作, 最后只生成一个 ReplicaSet</strong>. 具体的做法是, 在更新 Deployment 前, 要<strong>先执行一条 kubectl rollout pause 指令</strong>. 它的用法如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout pause deployment/nginx-deployment
deployment.extensions/nginx-deployment paused
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个 kubectl rollout pause 的作用, 是让这个 Deployment 进入了一个 &quot;<strong>暂停</strong>&quot; 状态. 所以接下来就可以随意使用 kubectl edit 或者 kubectl set image 指令, <strong>修改</strong>这个 Deployment 的内容了. 由于此时 Deployment 正处于 &quot;<strong>暂停</strong>&quot; 状态, 所以对 Deployment 的所有修改, <strong>都不会触发新的&quot;滚动更新&quot;, 也不会创建新的 ReplicaSet</strong>.</p> <p>而等到对 Deployment 修改操作都完成之后, 只需要再执行一条 <strong>kubectl rollout resume</strong> 指令, 就可以把这个 Deployment &quot;恢复&quot;回来, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout resume deploy/nginx-deployment
deployment.extensions/nginx-deployment resumed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>而在这个 kubectl rollout resume 指令执行之前, 在 kubectl rollout pause 指令之后的这段时间里, 对 Deployment 进行的<strong>所有修改</strong>, <strong>最后只会触发一次 &quot;滚动更新&quot;</strong> .</p> <p>当然, 可以通过检查 ReplicaSet 状态的变化, 来验证一下 kubectl rollout pause 和 kubectl rollout resume 指令的执行效果, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-1764197365   <span class="token number">0</span>         <span class="token number">0</span>         <span class="token number">0</span>         2m
nginx-3196763511   <span class="token number">3</span>         <span class="token number">3</span>         <span class="token number">3</span>         28s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>通过返回结果可以看到, 只有一个 hash=3196763511 的 ReplicaSet 被创建了出来.</p> <p>不过, 即使你像上面这样小心翼翼地控制了 ReplicaSet 的生成数量, 随着应用版本的不断增加, Kubernetes 中还是会为同一个 Deployment 保存很多很多不同的 ReplicaSet.</p> <p><strong>那又该如何控制这些 &quot;历史&quot; ReplicaSet 的数量呢</strong>?</p> <p>很简单, Deployment 对象有一个字段, 叫作 <strong>spec.revisionHistoryLimit</strong>, 就是 Kubernetes 为 Deployment <strong>保留的 &quot;历史版本&quot; 个数</strong>. 所以, 如果把它设置为 0, 就再也不能做回滚操作了.</p> <blockquote><p>总结</p></blockquote> <p>本节详细讲解了 Deployment 这个 Kubernetes 项目中最基本的编排控制器的实现原理和使用方法. 应该了解到: Deployment 实际上是一个<strong>两层控制器</strong>. <mark><strong>首先它通过 ReplicaSet 的个数来描述应用的版本; 然后它再通过 ReplicaSet 的属性(比如 replicas 的值), 来保证 Pod 的副本数量</strong></mark>.</p> <blockquote><p>备注: Deployment 控制 ReplicaSet(版本), ReplicaSet 控制 Pod(副本数). 这个两层控制关系一定要牢记.</p></blockquote> <p>不过, 相信你也能够感受到, Kubernetes 项目对 Deployment 的设计, 实际上是<strong>代替我们完成了对 &quot;应用&quot; 的抽象, 使得我们可以使用这个 Deployment 对象来描述应用, 使用 kubectl rollout 命令控制应用的版本</strong>.</p> <p>可是, 在实际使用场景中, 应用发布的流程往往千差万别, 也可能有很多的定制化需求. 比如, 应用可能有会话黏连(session sticky), 这就意味着 &quot;滚动更新&quot; 的时候, <strong>哪个 Pod 能下线, 是不能随便选择的</strong>. 这种场景光靠 Deployment 自己就很难应对了. 对于这种需求, 后续文章中重点介绍的 &quot;<strong>自定义控制器</strong>&quot;, 就可以实现一个功能更加强大的 Deployment Controller.</p> <p>当然, Kubernetes 项目本身, 也提供了另外一种抽象方式, 帮我们应对其他一些用 Deployment 无法处理的应用编排场景. 这个设计, 就是对<strong>有状态应用的管理</strong>, 也是下一节中要重点讲解的内容.</p> <h4 id="_18-深入理解statefulset-一-拓扑状态"><a href="#_18-深入理解statefulset-一-拓扑状态" class="header-anchor">#</a> 18 | 深入理解StatefulSet(一):拓扑状态</h4> <p>本节的主题是: 深入理解 StatefulSet 之拓扑状态. 上一节的结尾处讨论到了 Deployment 实际上<strong>并不足以覆盖所有的应用编排问题</strong>. 造成这个问题的根本原因, 在于 Deployment 对应用做了一个<strong>简单化假设</strong>.</p> <p><strong>它认为一个应用的所有 Pod, 是完全一样的. 所以, 它们互相之间没有顺序, 也无所谓运行在哪台宿主机上. 需要的时候, Deployment 就可以通过 Pod 模板创建新的 Pod; 不需要的时候, Deployment 就可以&quot;杀掉&quot;任意一个 Pod</strong>.</p> <p>但在实际的场景中, 并不是所有的应用都可以满足这样的要求. 尤其是分布式应用, 它的多个实例之间, 往往有<strong>依赖关系, 比如: 主从关系, 主备关系</strong>. 还有就是数据存储类应用, 它的多个实例, 往往都会在本地磁盘上保存一份数据. 而这些实例一旦被杀掉, 即便重建出来, 实例与数据之间的对应关系也已经丢失, 从而导致应用失败.</p> <p><strong>所以, 这种实例之间有不对等关系, 以及实例对外部数据有依赖关系的应用, 就被称为 &quot;有状态应用&quot;(Stateful Application).</strong></p> <p>容器技术诞生后, 大家很快发现, 它用来封装 &quot;无状态应用&quot;(Stateless Application), 尤其是 Web 服务, 非常好用. 但一旦想要用容器运行 &quot;有状态应用&quot;, 其困难程度就会直线上升. 而且这个问题解决起来, <strong>单纯依靠容器技术本身已经无能为力</strong>, 这也就导致了很长一段时间内, &quot;有状态应用&quot; 几乎成了容器技术圈子的 &quot;忌讳&quot;, 大家一听到这个词, 就纷纷摇头.</p> <p>不过, Kubernetes 项目还是成为了 &quot;第一个吃螃蟹的人&quot;.</p> <p>得益于 &quot;控制器模式&quot; 的设计思想, Kubernetes 项目很早就<mark><strong>在 Deployment 的基础上, 扩展出了对 &quot;有状态应用&quot; 的初步支持. 这个编排功能, 就是: StatefulSet</strong></mark>.</p> <p>StatefulSet 的设计其实非常容易理解. 它把真实世界里的<strong>应用状态, 抽象为了两种情况</strong>:</p> <ol><li><mark><strong>拓扑状态</strong></mark>. 这种情况意味着, <strong>应用的多个实例之间不是完全对等的关系</strong>. 这些应用实例必须<strong>按照某些顺序启动</strong>, 比如应用的主节点 A 要先于从节点 B 启动. 而如果把 A 和 B 两个 Pod 删除掉, 它们再次被创建出来时也必须严格按照这个顺序才行. 并且新创建出来的 Pod, 必须和原来 Pod 的网络标识一样, 这样原先的访问者才能使用同样的方法, 访问到这个新 Pod.</li> <li><mark><strong>存储状态</strong></mark>. 这种情况意味着, <strong>应用的多个实例分别绑定了不同的存储数据</strong>. 对于这些应用实例来说, Pod A 第一次读取到的数据, 和隔了十分钟之后再次读取到的数据, 应该是<strong>同一份</strong>, 哪怕在此期间 Pod A 被重新创建过. 这种情况最典型的例子, 就是一个数据库应用的多个存储实例.</li></ol> <p>所以 <mark><strong>StatefulSet 的核心功能, 就是通过某种方式记录这些状态, 然后在 Pod 被重新创建时, 能够为新 Pod 恢复这些状态</strong></mark>​ **. **</p> <p>在开始讲述 StatefulSet 的工作原理之前, 必须先讲一个 Kubernetes 项目中非常实用的概念: <strong>Headless Service</strong>. 前面讨论 Kubernetes 架构的时候就曾介绍过, <strong>Service 是 Kubernetes 项目中用来将一组 Pod 暴露给外界访问的一种机制</strong>. 比如, 一个 Deployment 有 3 个 Pod, 那么就可以定义一个 Service. 然后用户只要能访问到这个 Service, 它就能访问到某个具体的 Pod.</p> <p>那么这个 Service 又是如何被访问的呢?</p> <ul><li><strong>第一种方式, 是以 Service 的 VIP(Virtual IP, 即: 虚拟 IP)方式</strong>. 比如: 当访问 10.0.23.1 这个 Service 的 IP 地址时, 10.0.23.1 其实就是一个 VIP, 它会把请求转发到该 Service 所<strong>代理的某一个 Pod 上</strong>. 这里的具体原理, 会在后续的 Service 章节中进行详细介绍.</li> <li><strong>第二种方式, 就是以 Service 的 DNS 方式</strong>. 比如: 这时候, 只要访问 &quot;my-svc.my-namespace.svc.cluster.local&quot; 这条 DNS 记录, 就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod.</li></ul> <p>而在第二种 Service DNS 的方式下, 具体还可以分为两种处理方法:</p> <ul><li>第一种处理方法, 是 <strong>Normal Service</strong>. 这种情况下, 访问 &quot;my-svc.my-namespace.svc.cluster.local&quot; 解析到的, 正是 my-svc 这个 Service 的 VIP, 后面的流程就跟 VIP 方式一致了.</li> <li>而第二种处理方法, 正是 <strong>Headless Service</strong>. 这种情况下, 访问 &quot;my-svc.my-namespace.svc.cluster.local&quot; 解析到的, 直接就是 my-svc 代理的某一个 Pod 的 IP 地址. <strong>可以看到这里的</strong>​<mark><strong>区别在于, Headless Service 不需要分配一个 VIP, 而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址</strong></mark>​ **. **</li></ul> <p>那这样的设计又有什么作用呢? 想要回答这个问题, 需要从 Headless Service 的定义方式看起.</p> <p>下面是一个标准的 Headless Service 对应的 YAML 文件:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> web
  <span class="token key atrule">clusterIP</span><span class="token punctuation">:</span> None
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>可以看到, 所谓的 <strong>Headless Service, 其实仍是一个标准 Service 的 YAML 文件</strong>. 只不过, 它的 clusterIP 字段的值是: None, 即: <mark><strong>这个 Service, 没有一个 VIP 作为&quot;头&quot;</strong></mark> . 这也就是 Headless 的含义. 所以<strong>这个 Service 被创建后并不会被分配一个 VIP, 而是会以 DNS 记录的方式暴露出它所代理的 Pod</strong>. 而它所代理的 Pod, 依然是采用前面提到的 <strong>Label Selector 机制</strong>选择出来的, 即: <strong>所有携带了 app=nginx 标签的 Pod, 都会被这个 Service 代理起来</strong>.</p> <p>然后关键来了.</p> <p>当按照这样的方式创建了一个 Headless Service 之后, <strong>它所代理的所有 Pod 的 IP 地址, 都会被绑定一个这样格式的 DNS 记录</strong>, 如下所示:</p> <div class="language-xml line-numbers-mode"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pod-name</span><span class="token punctuation">&gt;</span></span>.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>svc-name</span><span class="token punctuation">&gt;</span></span>.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>namespace</span><span class="token punctuation">&gt;</span></span>.svc.cluster.local
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><strong>这个 DNS 记录, 正是 Kubernetes 项目为 Pod 分配的唯一的 &quot;可解析身份&quot;(Resolvable Identity). 有了这个 &quot;可解析身份&quot;, 只要知道了一个 Pod 的名字, 以及它对应的 Service 的名字, 就可以非常确定地通过这条 DNS 记录访问到 Pod 的 IP 地址.</strong></p> <p>那么, <mark><strong>StatefulSet 又是如何使用这个 DNS 记录来维持 Pod 的拓扑状态的</strong></mark>呢?</p> <p>为了回答这个问题, 现在就来编写一个 StatefulSet 的 YAML 文件, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> web
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> <span class="token string">&quot;nginx&quot;</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.9.1
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> web
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>这个 YAML 文件, 和在前面文章中用到的 nginx-deployment 的唯一区别, 就是<strong>多了一个 serviceName=nginx 字段</strong>. 这个字段的作用, <strong>就是告诉 StatefulSet 控制器, 在执行控制循环(Control Loop)的时候, 请使用 nginx 这个 Headless Service 来保证 Pod 的 &quot;可解析身份&quot;</strong> .</p> <p>所以, 当通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后, 就会看到如下<strong>两个对象</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> svc.yaml
$ kubectl get <span class="token function">service</span> nginx
NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>   AGE
nginx     ClusterIP    None         <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">80</span>/TCP    10s
 
$ kubectl create <span class="token parameter variable">-f</span> statefulset.yaml
$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       <span class="token number">2</span>         <span class="token number">1</span>         19s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>这时候, 如果手比较快的话, 还可以通过 kubectl 的 -w 参数, 即: Watch 功能, 实时查看 StatefulSet <strong>创建两个有状态实例</strong>的过程.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods <span class="token parameter variable">-w</span> <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     <span class="token number">0</span>/1       Pending   <span class="token number">0</span>          0s
web-0     <span class="token number">0</span>/1       Pending   <span class="token number">0</span>         0s
web-0     <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>         0s
web-0     <span class="token number">1</span>/1       Running   <span class="token number">0</span>         19s
web-1     <span class="token number">0</span>/1       Pending   <span class="token number">0</span>         0s
web-1     <span class="token number">0</span>/1       Pending   <span class="token number">0</span>         0s
web-1     <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>         0s
web-1     <span class="token number">1</span>/1       Running   <span class="token number">0</span>         20s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>如果手不够快的话, Pod 很快就创建完了. 不过依然可以通过这个 StatefulSet 的 Events 看到这些信息.</p> <p>通过上面这个 Pod 的创建过程, 不难看到, <strong>StatefulSet 给它所管理的所有 Pod 的名字, 进行了编号, 编号规则是: -</strong> . 而且这些编号都是从 0 开始累加, 与 StatefulSet 的每个 Pod 实例一一对应, 绝不重复. 更重要的是, 这些 <strong>Pod 的创建, 也是严格按照编号顺序进行的</strong>. 比如在 web-0 进入到 Running 状态, 并且细分状态(Conditions)成为 Ready 之前, web-1 会一直处于 Pending 状态.</p> <blockquote><p>备注: Ready 状态再一次提醒了我们, 为 Pod 设置 livenessProbe 和 readinessProbe 的重要性.</p></blockquote> <p>当这两个 Pod 都进入了 <strong>Running</strong> 状态之后, 就可以查看到它们各自唯一的&quot;网络身份&quot;了.</p> <p>使用 kubectl exec 命令进入到容器中查看它们的 hostname:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> web-0 -- <span class="token function">sh</span> <span class="token parameter variable">-c</span> <span class="token string">'hostname'</span>
web-0
$ kubectl <span class="token builtin class-name">exec</span> web-1 -- <span class="token function">sh</span> <span class="token parameter variable">-c</span> <span class="token string">'hostname'</span>
web-1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 这两个 Pod 的 <strong>hostname 与 Pod 名字是一致的, 都被分配了对应的编号</strong>. 接下来, 再试着以 DNS 的方式, 访问一下这个 Headless Service:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span> <span class="token parameter variable">--image</span> busybox dns-test <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never <span class="token parameter variable">--rm</span> /bin/sh 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>通过这条命令, 启动了一个<strong>一次性的 Pod</strong>, 因为 –rm 意味着 Pod 退出后就会被删除掉. 然后在这个 Pod 的容器里面, 尝试用 <strong>nslookup 命令, 解析一下 Pod 对应的 Headless Service</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span> <span class="token parameter variable">--image</span> busybox dns-test <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never <span class="token parameter variable">--rm</span> /bin/sh
$ <span class="token function">nslookup</span> web-0.nginx
Server:    <span class="token number">10.0</span>.0.10
Address <span class="token number">1</span>: <span class="token number">10.0</span>.0.10 kube-dns.kube-system.svc.cluster.local
 
Name:      web-0.nginx
Address <span class="token number">1</span>: <span class="token number">10.244</span>.1.7
 
$ <span class="token function">nslookup</span> web-1.nginx
Server:    <span class="token number">10.0</span>.0.10
Address <span class="token number">1</span>: <span class="token number">10.0</span>.0.10 kube-dns.kube-system.svc.cluster.local
 
Name:      web-1.nginx
Address <span class="token number">1</span>: <span class="token number">10.244</span>.2.7
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>从 nslookup 命令的输出结果可以看到, <strong>在访问 web-0.nginx 的时候, 最后解析到的, 正是 web-0 这个 Pod 的 IP 地址; 而当访问 web-1.nginx 的时候, 解析到的则是 web-1 的 IP 地址</strong>.</p> <p>这时候, 如果在另外一个 Terminal 里把这两个 &quot;有状态应用&quot; 的 Pod 删掉:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete pod <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx
pod <span class="token string">&quot;web-0&quot;</span> deleted
pod <span class="token string">&quot;web-1&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后再在当前 Terminal 里 <strong>Watch 一下这两个 Pod 的状态变化</strong>, 就会发现一个有趣的现象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod <span class="token parameter variable">-w</span> <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     <span class="token number">1</span>/1       Running   <span class="token number">0</span>          2s
web-1     <span class="token number">0</span>/1       Pending   <span class="token number">0</span>         0s
web-1     <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>         0s
web-1     <span class="token number">1</span>/1       Running   <span class="token number">0</span>         32s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 当把这两个 Pod 删除之后, Kubernetes 会<strong>按照原先编号的顺序, 创建出了两个新的 Pod</strong>. 并且 Kubernetes 依然为它们<strong>分配了与原来相同的 &quot;网络身份&quot;: web-0.nginx 和 web-1.nginx</strong>.</p> <p>通过这种严格的对应规则, <mark><strong>StatefulSet 就保证了 Pod 网络标识的稳定性</strong></mark>.</p> <p>比如, 如果 web-0 是一个需要先启动的主节点, web-1 是一个后启动的从节点, 那么<strong>只要这个 StatefulSet 不被删除, 你访问 web-0.nginx 时始终都会落在主节点上, 访问 web-1.nginx 时, 则始终都会落在从节点上, 这个关系绝对不会发生任何变化</strong>.</p> <p>所以, 如果再用 nslookup 命令, 查看一下这个新 Pod 对应的 Headless Service 的话:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span> <span class="token parameter variable">--image</span> busybox dns-test <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never <span class="token parameter variable">--rm</span> /bin/sh 
$ <span class="token function">nslookup</span> web-0.nginx
Server:    <span class="token number">10.0</span>.0.10
Address <span class="token number">1</span>: <span class="token number">10.0</span>.0.10 kube-dns.kube-system.svc.cluster.local
 
Name:      web-0.nginx
Address <span class="token number">1</span>: <span class="token number">10.244</span>.1.8
 
$ <span class="token function">nslookup</span> web-1.nginx
Server:    <span class="token number">10.0</span>.0.10
Address <span class="token number">1</span>: <span class="token number">10.0</span>.0.10 kube-dns.kube-system.svc.cluster.local
 
Name:      web-1.nginx
Address <span class="token number">1</span>: <span class="token number">10.244</span>.2.8
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>可以看到, 在这个 StatefulSet 中, 这两个新 Pod 的 &quot;网络标识&quot;(比如: web-0.nginx 和 web-1.nginx), <strong>再次解析到了正确的 IP 地址</strong>(比如: web-0 Pod 的 IP 地址 10.244.1.8).</p> <p>通过这种方法, <mark><strong>Kubernetes 就成功地将 Pod 的拓扑状态(比如: 哪个节点先启动, 哪个节点后启动), 按照 Pod 的 &quot;名字 + 编号&quot; 的方式固定了下来</strong></mark>. 此外 Kubernetes 还<strong>为每一个 Pod 提供了一个固定并且唯一的访问入口, 即: 这个 Pod 对应的 DNS 记录</strong>. 这些状态, 在 StatefulSet 的<strong>整个生命周期里都会保持不变, 绝不会因为对应 Pod 的删除或者重新创建而失效</strong>.</p> <p>不过, 相信你也已经注意到了, 尽管 web-0.nginx 这条记录本身不会变, 但它解析到的 Pod 的 IP 地址, 并<strong>不是固定</strong>的. 这就意味着, <strong>对于 &quot;有状态应用&quot; 实例的访问, 必须使用 DNS 记录或者 hostname 的方式, 而绝不应该直接访问这些 Pod 的 IP 地址</strong>.</p> <blockquote><p>总结</p></blockquote> <p>本节首先分享了 StatefulSet 的基本概念, 解释了什么是应用的 &quot;状态&quot;. 接着分析了 StatefulSet 如何保证应用实例之间 &quot;拓扑状态&quot; 的稳定性. 如果用一句话来总结的话, 可以这么理解这个过程:</p> <p><mark><strong>StatefulSet 这个控制器的主要作用之一, 就是使用 Pod 模板创建 Pod 的时候, 对它们进行编号, 并且按照编号顺序逐一完成创建工作. 而当 StatefulSet 的 &quot;控制循环&quot; 发现 Pod 的 &quot;实际状态&quot; 与 &quot;期望状态&quot; 不一致, 需要新建或者删除 Pod 进行 &quot;调谐&quot; 的时候, 它会严格按照这些 Pod 编号的顺序, 逐一完成这些操作.</strong></mark></p> <p>所以, StatefulSet 其实可以认为是<strong>对 Deployment 的改良</strong>.</p> <p>与此同时, <strong>通过 Headless Service 的方式, StatefulSet 为每个 Pod 创建了一个固定并且稳定的 DNS 记录, 来作为它的访问入口.</strong></p> <p>实际上, <strong>在部署 &quot;有状态应用&quot; 的时候, 应用的每个实例拥有唯一并且稳定的 &quot;网络标识&quot;</strong> , 是一个非常重要的假设.</p> <p>下一接将会继续剖析 StatefulSet 如何处理存储状态.</p> <h4 id="_19-深入理解statefulset-二-存储状态"><a href="#_19-深入理解statefulset-二-存储状态" class="header-anchor">#</a> 19 | 深入理解StatefulSet(二):存储状态</h4> <p>上一节分享了 StatefulSet 如何保证应用实例的拓扑状态, 在 Pod 删除和再创建的过程中保持稳定.</p> <p>本节的主题是深入理解 StatefulSet 之存储状态. 本节将继续<strong>解读 StatefulSet 对存储状态的管理机制</strong>. 这个机制, 主要使用的是一个叫作 <mark><strong>Persistent Volume Claim</strong></mark> 的功能.</p> <p>在前面介绍 Pod 的时候, 曾提到过, 要在一个 Pod 里声明 Volume, <strong>只要在 Pod 里加上 spec.volumes 字段即可</strong>. 然后就可以在这个字段里定义一个具体类型的 Volume 了, 比如: hostPath.</p> <p>可你有没有想过这样一个场景: <strong>如果你并不知道有哪些 Volume 类型可以用, 要怎么办呢</strong>?</p> <p>更具体地说, 作为一个应用开发者, 可能对<strong>持久化存储项目</strong>(比如 Ceph, GlusterFS 等)一窍不通, 也不知道公司的 Kubernetes 集群里到底是怎么搭建出来的, 也自然不会编写它们对应的 Volume 定义文件. 所谓 &quot;术业有专攻&quot;, 这些关于 Volume 的管理和远程持久化存储的知识, 不仅超越了开发者的知识储备, 还会有暴露公司基础设施秘密的风险.</p> <p>比如, 下面这个例子, 就是一个声明了 Ceph RBD 类型 Volume 的 Pod:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> rbd
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> kubernetes/pause
      <span class="token key atrule">name</span><span class="token punctuation">:</span> rbd<span class="token punctuation">-</span>rw
      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> rbdpd
        <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt/rbd
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> rbdpd
      <span class="token key atrule">rbd</span><span class="token punctuation">:</span>
        <span class="token key atrule">monitors</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token string">'10.16.154.78:6789'</span>
        <span class="token punctuation">-</span> <span class="token string">'10.16.154.82:6789'</span>
        <span class="token punctuation">-</span> <span class="token string">'10.16.154.83:6789'</span>
        <span class="token key atrule">pool</span><span class="token punctuation">:</span> kube
        <span class="token key atrule">image</span><span class="token punctuation">:</span> foo
        <span class="token key atrule">fsType</span><span class="token punctuation">:</span> ext4
        <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
        <span class="token key atrule">user</span><span class="token punctuation">:</span> admin
        <span class="token key atrule">keyring</span><span class="token punctuation">:</span> /etc/ceph/keyring
        <span class="token key atrule">imageformat</span><span class="token punctuation">:</span> <span class="token string">&quot;2&quot;</span>
        <span class="token key atrule">imagefeatures</span><span class="token punctuation">:</span> <span class="token string">&quot;layering&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>其一, 如果不懂得 Ceph RBD 的使用方法, 那么这个 Pod 里 Volumes 字段, 你十有八九也完全看不懂. 其二, 这个 Ceph RBD <strong>对应的存储服务器的地址, 用户名, 授权文件的位置, 也都被轻易地暴露给了全公司的所有开发人员</strong>, 这是一个典型的信息被 &quot;过度暴露&quot; 的例子.</p> <p>这也是为什么, 在后来的演化中, <mark><strong>Kubernetes 项目引入了一组叫作 Persistent Volume Claim(PVC)和 Persistent Volume(PV)的 API 对象, 大大降低了用户声明和使用持久化 Volume 的门槛</strong></mark>​ **. **</p> <p>举个例子, 有了 PVC 之后, 一个开发人员想要<strong>使用一个 Volume</strong>, 只需要简单的两步即可.</p> <p>**第一步: 定义一个 PVC, 声明想要的 Volume 的属性: **</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>claim
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到, 在这个 PVC 对象里, <strong>不需要任何关于 Volume 细节的字段, 只有描述性的属性和定义</strong>. 比如, storage: 1Gi, 表示想要的 Volume 大小至少是 1 GiB; accessModes: ReadWriteOnce, 表示这个 Volume 的挂载方式是<strong>可读写</strong>, 并且只能被挂载在一个节点上而非被多个节点共享. 关于哪种类型的 Volume 支持哪种类型的 AccessMode, 可以查看 Kubernetes 项目官方文档中的详细列表.</p> <p>**第二步: 在应用的 Pod 中, 声明使用这个 PVC: **</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>container
      <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
      <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">&quot;http-server&quot;</span>
      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&quot;/usr/share/nginx/html&quot;</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage
      <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span>
        <span class="token key atrule">claimName</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>claim
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>可以看到, 在这个 Pod 的 Volumes 定义中, 只需要声明它的<strong>类型是 persistentVolumeClaim</strong>, 然后指定 PVC 的名字, 而完全不必关心 Volume 本身的定义. 这时候, <strong>只要创建这个 PVC 对象, Kubernetes 就会自动为它绑定一个符合条件的 Volume</strong>. 可是这些符合条件的 Volume 又是从哪里来的呢?</p> <p>答案是, 它们<strong>来自于由运维人员维护的 PV(Persistent Volume)对象</strong>. 一起看一个常见的 <strong>PV 对象的 YAML 文件</strong>:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>volume
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">type</span><span class="token punctuation">:</span> local
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>
    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 10Gi
  <span class="token key atrule">rbd</span><span class="token punctuation">:</span>
    <span class="token key atrule">monitors</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token string">'10.16.154.78:6789'</span>
    <span class="token punctuation">-</span> <span class="token string">'10.16.154.82:6789'</span>
    <span class="token punctuation">-</span> <span class="token string">'10.16.154.83:6789'</span>
    <span class="token key atrule">pool</span><span class="token punctuation">:</span> kube
    <span class="token key atrule">image</span><span class="token punctuation">:</span> foo
    <span class="token key atrule">fsType</span><span class="token punctuation">:</span> ext4
    <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token key atrule">user</span><span class="token punctuation">:</span> admin
    <span class="token key atrule">keyring</span><span class="token punctuation">:</span> /etc/ceph/keyring
    <span class="token key atrule">imageformat</span><span class="token punctuation">:</span> <span class="token string">&quot;2&quot;</span>
    <span class="token key atrule">imagefeatures</span><span class="token punctuation">:</span> <span class="token string">&quot;layering&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>可以看到, <strong>这个 PV 对象的 spec.rbd 字段</strong>, 正是前面介绍过的 Ceph RBD Volume 的详细定义. 而且它还声明了这个 PV 的容量是 10 GiB. 这样, Kubernetes 就会<strong>为刚刚创建的 PVC 对象绑定这个 PV</strong>.</p> <p><mark><strong>所以, Kubernetes 中 PVC 和 PV 的设计, 实际上类似于 &quot;接口&quot; 和 &quot;实现&quot; 的思想. 开发者只要知道并会使用 &quot;接口&quot;, 即: PVC; 而运维人员则负责给 &quot;接口&quot; 绑定具体的实现, 即: PV</strong></mark>.</p> <p>这种解耦, <strong>就避免了因为向开发者暴露过多的存储系统细节而带来的隐患</strong>. 此外, 这种职责的分离, 往往也意味着出现事故时可以更容易定位问题和明确责任, 从而避免 &quot;扯皮&quot; 现象的出现.</p> <p>而 PVC, PV 的设计, 也使得 <strong>StatefulSet 对存储状态的管理</strong>成为了可能. 还是以上一节中用到的 StatefulSet 为例:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> web
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> <span class="token string">&quot;nginx&quot;</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.9.1
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> web
        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> www
          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html
  <span class="token key atrule">volumeClaimTemplates</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> www
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> ReadWriteOnce
      <span class="token key atrule">resources</span><span class="token punctuation">:</span>
        <span class="token key atrule">requests</span><span class="token punctuation">:</span>
          <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br></div></div><p>这次, 为这个 StatefulSet <strong>额外添加了一个 volumeClaimTemplates 字段</strong>. 从名字就可以看出来, 它<strong>跟 Deployment 里 Pod 模板(PodTemplate)的作用类似</strong>. 也就是说, <mark><strong>凡是被这个 StatefulSet 管理的 Pod, 都会声明一个对应的 PVC; 而这个 PVC 的定义, 就来自于 volumeClaimTemplates 这个模板字段. 更重要的是, 这个 PVC 的名字, 会被分配一个与这个 Pod 完全一致的编号</strong></mark>. <strong>这个自动创建的 PVC, 与 PV 绑定成功后, 就会进入 Bound 状态, 这就意味着这个 Pod 可以挂载并使用这个 PV 了</strong>.</p> <p>如果还是不太理解 PVC 的话, 可以先记住这样一个结论: <mark><strong>PVC 其实就是一种特殊的 Volume</strong></mark>. 只不过一个 PVC 具体是什么类型的 Volume, 要在<strong>跟某个 PV 绑定之后才知道</strong>. 关于 PV, PVC 更详细的知识, 会在容器存储部分做进一步解读.</p> <p>当然, PVC 与 PV 的绑定得以实现的前提是, 运维人员<strong>已经在系统里创建好了符合条件的 PV</strong>(比如在前面用到的 pv-volume); 或者, 你的 Kubernetes 集群运行在公有云上, 这样 Kubernetes 就会通过 <strong>Dynamic Provisioning</strong> 的方式, 自动为你创建与 PVC 匹配的 PV.</p> <p>所以在使用 kubectl create 创建了 StatefulSet 之后, 就会看到 Kubernetes 集群里出现了两个 PVC:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl create <span class="token punctuation">-</span>f statefulset.yaml
$ kubectl get pvc <span class="token punctuation">-</span>l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www<span class="token punctuation">-</span>web<span class="token punctuation">-</span>0   Bound     pvc<span class="token punctuation">-</span>15c268c7<span class="token punctuation">-</span>b507<span class="token punctuation">-</span>11e6<span class="token punctuation">-</span>932f<span class="token punctuation">-</span>42010a800002   1Gi        RWO           48s
www<span class="token punctuation">-</span>web<span class="token punctuation">-</span>1   Bound     pvc<span class="token punctuation">-</span>15c79307<span class="token punctuation">-</span>b507<span class="token punctuation">-</span>11e6<span class="token punctuation">-</span>932f<span class="token punctuation">-</span>42010a800002   1Gi        RWO           48s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, 这些 PVC, 都以 &quot;&lt;PVC 名字 &gt;-&lt;StatefulSet 名字 &gt;-&lt; 编号 &gt;&quot; 的方式命名, 并且处于 <strong>Bound</strong> 状态.</p> <p>前面已经讲到过, <strong>这个 StatefulSet 创建出来的所有 Pod, 都会声明使用编号的 PVC</strong>. 比如在名叫 web-0 的 Pod 的 volumes 字段, 它会声明使用名叫 www-web-0 的 PVC, 从而挂载到这个 PVC 所绑定的 PV.</p> <p>所以就可以使用如下所示的指令, <strong>在 Pod 的 Volume 目录里写入一个文</strong>件, 来验证一下<strong>上述 Volume 的分配情况</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> <span class="token number">0</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token keyword">do</span> kubectl <span class="token builtin class-name">exec</span> web-<span class="token variable">$i</span> -- <span class="token function">sh</span> <span class="token parameter variable">-c</span> <span class="token string">'echo hello $(hostname) &gt; /usr/share/nginx/html/index.html'</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>如上所示, 通过 kubectl exec 指令, 在每个 Pod 的 Volume 目录里, 写入了一个 <strong>index.html</strong> 文件. 这个文件的内容, 正是 Pod 的 <strong>hostname</strong>. 比如在 web-0 的 index.html 里写入的内容就是 &quot;hello web-0&quot;.</p> <p>此时如果在这个 Pod 容器里访问 <code>&quot;http://localhost&quot;</code>​, 实际访问到的就是 Pod 里 <strong>Nginx 服务器进程</strong>, 而它会为你返回 /usr/share/nginx/html/index.html 里的内容. 这个操作的执行方法如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> <span class="token number">0</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token keyword">do</span> kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> web-<span class="token variable">$i</span> -- <span class="token function">curl</span> localhost<span class="token punctuation">;</span> <span class="token keyword">done</span>
hello web-0
hello web-1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>现在, 关键来了. 如果使用 kubectl delete 命令<strong>删除这两个 Pod</strong>, 这些 Volume 里的文件会不会丢失呢?</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete pod <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx
pod <span class="token string">&quot;web-0&quot;</span> deleted
pod <span class="token string">&quot;web-1&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到, 正如前面介绍过的, 在被删除之后, <strong>这两个 Pod 会被按照编号的顺序被重新创建出来</strong>. 而这时候, 如果在新创建的容器里通过访问 <code>&quot;http://localhost&quot;</code>​ 的方式去访问 web-0 里的 Nginx 服务:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在被重新创建出来的 Pod 容器里访问 http://localhost</span>
$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> web-0 -- <span class="token function">curl</span> localhost
hello web-0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>就会发现, 这个请求依然会返回: hello web-0. 也就是说, <strong>原先与名叫 web-0 的 Pod 绑定的 PV, 在这个 Pod 被重新创建之后, 依然同新的名叫 web-0 的 Pod 绑定在了一起</strong>. 对于 Pod web-1 来说, 也是完全一样的情况.</p> <p>**这是怎么做到的呢? **</p> <p>其实, 分析一下 StatefulSet 控制器<strong>恢复这个 Pod 的过程</strong>, 就可以很容易理解了.</p> <p>首先, 当<strong>把一个 Pod, 比如 web-0, 删除之后, 这个 Pod 对应的 PVC 和 PV, 并不会被删除, 而这个 Volume 里已经写入的数据, 也依然会保存在远程存储服务里</strong>(比如在这个例子里用到的 Ceph 服务器). 此时 StatefulSet 控制器发现, 一个名叫 web-0 的 Pod 消失了. 所以控制器就会<strong>重新创建一个新的</strong>, 名字还是叫作 web-0 的 Pod 来, &quot;纠正&quot; 这个不一致的情况.</p> <p>需要注意的是, 在这个新的 Pod 对象的定义里, 它声明使用的 PVC 的名字, 还是叫作: www-web-0. 这个 PVC 的定义, 还是来<strong>自于 PVC 模板(volumeClaimTemplates), 这是 StatefulSet 创建 Pod 的标准流程</strong>.</p> <p>所以, 在这个<strong>新的 web-0 Pod 被创建出来</strong>之后, Kubernetes 为它查找名叫 www-web-0 的 PVC 时, 就会直接<strong>找到旧 Pod 遗留下来的同名的 PVC, 进而找到跟这个 PVC 绑定在一起的 PV</strong>. 这样, <strong>新的 Pod 就可以挂载到旧 Pod 对应的那个 Volume, 并且获取到保存在 Volume 里的数据</strong>.</p> <p><mark><strong>通过这种方式, Kubernetes 的 StatefulSet 就实现了对应用存储状态的管理.</strong></mark></p> <p>看到这里, 你是不是已经大致理解了 StatefulSet 的工作原理呢? 现在再详细梳理一下.</p> <p><strong>首先, StatefulSet 的控制器直接管理的是 Pod</strong>. 这是因为, StatefulSet 里的不同 Pod 实例, 不再像 ReplicaSet 中那样都是完全一样的, 而是有了<strong>细微区别</strong>的. 比如每个 Pod 的 hostname, 名字等都是不同的, 携带了<strong>编号</strong>的. 而 StatefulSet 区分这些实例的方式, 就是<strong>通过在 Pod 的名字里加上事先约定好的编号</strong>.</p> <p><strong>其次, Kubernetes 通过 Headless Service, 为这些有编号的 Pod, 在 DNS 服务器中生成带有同样编号的 DNS 记录</strong>. 只要 StatefulSet 能够保证这些 Pod 名字里的<strong>编号不变</strong>, 那么 Service 里类似于 web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变, 而这条记录解析出来的 Pod 的 IP 地址, 则<strong>会随着后端 Pod 的删除和再创建而自动更新</strong>. 这当然是 Service 机制本身的能力, 不需要 StatefulSet 操心.</p> <p><strong>最后, StatefulSet 还为每一个 Pod 分配并创建一个同样编号的 PVC</strong>. 这样, Kubernetes 就可以通过 <strong>Persistent Volume 机制为这个 PVC 绑定上对应的 PV, 从而保证了每一个 Pod 都拥有一个独立的 Volume</strong>. 在这种情况下, 即使 Pod 被删除, 它所对应的 <strong>PVC 和 PV 依然会保留</strong>下来. 所以当这个 Pod 被重新创建出来之后, Kubernetes 会为它找到同样编号的 PVC, 挂载这个 PVC 对应的 Volume, 从而获取到以前保存在 Volume 里的数据.</p> <p>这么一看, 原本非常复杂的 StatefulSet, 是不是也很容易理解了呢?</p> <blockquote><p>总结</p></blockquote> <p>本节详细介绍了 StatefulSet <strong>处理存储状态</strong>的方法. 然后以此为基础, 梳理了 StatefulSet 控制器的工作原理.</p> <p>从这些讲述中不难看出 StatefulSet 的设计思想: <mark><strong>StatefulSet 其实就是一种特殊的 Deployment, 而其独特之处在于, 它的每个 Pod 都被编号了. 而且这个编号会体现在 Pod 的名字和 hostname 等标识信息上, 这不仅代表了 Pod 的创建顺序, 也是 Pod 的重要网络标识(即: 在整个集群里唯一的, 可被的访问身份)</strong></mark> .</p> <p>有了这个编号后, StatefulSet 就使用 Kubernetes 里的两个标准功能: <mark><strong>Headless Service 和 PV/PVC, 实现了对 Pod 的拓扑状态和存储状态的维护</strong></mark>.</p> <p>实际上, 在下一节的 &quot;有状态应用&quot; 实践环节, 以及后续的讲解中, 就会逐渐意识到, StatefulSet 可以说是 Kubernetes 中作业编排的 &quot;集大成者&quot;. 因为几乎每一种 Kubernetes 的编排功能, 都可以在编写 StatefulSet 的 YAML 文件时被用到.</p> <h4 id="_20-深入理解statefulset-三-有状态应用实践"><a href="#_20-深入理解statefulset-三-有状态应用实践" class="header-anchor">#</a> 20 | 深入理解StatefulSet(三):有状态应用实践</h4> <p>本节的主题是深入理解 StatefulSet 之有状态应用实践. 前面详细讲解了 StatefulSet 的工作原理, 以及处理拓扑状态和存储状态的方法. 本节将通过一个实际的例子, 再次深入解读一下部署一个 StatefulSet 的完整流程.</p> <p>今天选择的实例是<strong>部署一个 MySQL 集群</strong>, 这也是 Kubernetes 官方文档里的一个经典案例. 但是很多人都曾吐槽这个例子 &quot;完全看不懂&quot;. 这样的吐槽也可以理解: <strong>相比于 Etcd, Cassandra 等 &quot;原生&quot; 就考虑了分布式需求的项目, MySQL 以及很多其他的数据库项目, 在分布式集群的搭建上并不友好, 甚至有点 &quot;原始&quot;</strong> . 所以本节就直接选择了这个具有挑战性的例子, 来看看如何使用 <strong>StatefulSet</strong> 将它的集群搭建过程 &quot;容器化&quot;. 在开始实践之前, 请确保之前一起部署的那个 Kubernetes 集群还是可用的, 并且网络插件和存储插件都能正常运行.</p> <p>首先, 用自然语言来描述一下我们想要部署的 &quot;<strong>有状态应用</strong>&quot;.</p> <ol><li>是一个 &quot;<strong>主从复制</strong>&quot;(Maser-Slave Replication)的 MySQL 集群;</li> <li>**有 1 个主节点(Master); **</li> <li>**有多个从节点(Slave); **</li> <li><strong>从节点需要能水平扩展</strong>;</li> <li>**所有的写操作, 只能在主节点上执行; **</li> <li>**读操作可以在所有节点上执行. **</li></ol> <p>这就是一个非常典型的主从模式的 MySQL 集群了. 可以把上面描述的 &quot;有状态应用&quot; 的需求, 通过一张图来表示.</p> <p><img src="/img/58591947d87c6dad82a054737ec4ef54-20230731162150-952l0dh.png" alt="">
在常规环境里, 部署这样一个主从模式的 MySQL 集群的主要难点在于: <strong>如何让从节点能够拥有主节点的数据, 即如何配置主(Master)从(Slave)节点的复制与同步</strong>.</p> <p>所以, 在安装好 MySQL 的 Master 节点之后, 需要做的第一步工作, 就是**通过 XtraBackup 将 Master 节点的数据备份到指定目录. ** XtraBackup 是业界主要使用的开源 MySQL 备份和恢复工具. 这一步会自动在目标目录里生成一个备份信息文件, 名叫: <strong>xtrabackup_binlog_info</strong>. 这个文件一般会包含如下两个信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">cat</span> xtrabackup_binlog_info
TheMaster-bin.000001     <span class="token number">481</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这两个信息会在接下来配置 Slave 节点的时候用到.</p> <p><strong>第二步: 配置 Slave 节点</strong>. Slave 节点在第一次启动前, 需要先把 Master 节点的备份数据, 连同备份信息文件, 一起拷贝到自己的数据目录(/var/lib/mysql)下. 然后执行这样一句 SQL:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>TheSlave<span class="token operator">|</span>mysql<span class="token operator">&gt;</span> CHANGE MASTER TO
                <span class="token assign-left variable">MASTER_HOST</span><span class="token operator">=</span><span class="token string">'$masterip'</span>,
                <span class="token assign-left variable">MASTER_USER</span><span class="token operator">=</span><span class="token string">'xxx'</span>,
                <span class="token assign-left variable">MASTER_PASSWORD</span><span class="token operator">=</span><span class="token string">'xxx'</span>,
                <span class="token assign-left variable">MASTER_LOG_FILE</span><span class="token operator">=</span><span class="token string">'TheMaster-bin.000001'</span>,
                <span class="token assign-left variable">MASTER_LOG_POS</span><span class="token operator">=</span><span class="token number">481</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>其中 <strong>MASTER_LOG_FILE</strong> 和 <strong>MASTER_LOG_POS</strong>, 就是该备份对应的<strong>二进制日志</strong>(Binary Log)文件的名称和开始的位置(偏移量), 也正是 xtrabackup_binlog_info 文件里的那两部分内容(即: TheMaster-bin.000001 和 481).</p> <p><strong>第三步, 启动 Slave 节点</strong>. 在这一步需要执行这样一句 SQL:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>TheSlave<span class="token operator">|</span>mysql<span class="token operator">&gt;</span> START SLAVE<span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样 Slave 节点就启动了. 它会使用备份信息文件中的二进制日志文件和偏移量, 与主节点进行数据同步.</p> <p><strong>第四步, 在这个集群中添加更多的 Slave 节点</strong>.</p> <p>需要注意的是, 新添加的 Slave 节点的<strong>备份数据</strong>, 来自于已经存在的 Slave 节点. 所以在这一步需要将 Slave 节点的数据备份在指定目录. 而这个备份操作会自动生成另一种备份信息文件, 名叫: xtrabackup_slave_info. 同样地, 这个文件也包含了 MASTER_LOG_FILE 和 MASTER_LOG_POS 两个字段.</p> <p>然后就可以执行跟前面一样的 &quot;CHANGE MASTER TO&quot; 和 &quot;START SLAVE&quot; 指令, 来<strong>初始化并启动这个新的 Slave 节点</strong>了.</p> <p>通过上面的叙述, 不难看到, **将部署 MySQL 集群的流程迁移到 Kubernetes 项目上, 需要能够 &quot;容器化&quot; 地解决下面的 &quot;三座大山&quot;: **</p> <ol><li><strong>Master 节点和 Slave 节点需要有不同的配置文件(即: 不同的 my.cnf)</strong> ;</li> <li>**Master 节点和 Salve 节点需要能够传输备份信息文件; **</li> <li>**在 Slave 节点第一次启动之前, 需要执行一些初始化 SQL 操作; **</li></ol> <p>而由于 MySQL 本身同时拥有<strong>拓扑状态</strong>(主从节点的区别)和<strong>存储状态</strong>(MySQL 保存在本地的数据), 自然要通过 <strong>StatefulSet</strong> 来解决这 &quot;三座大山&quot; 的问题.</p> <p><strong>其中, &quot;第一座大山: Master 节点和 Slave 节点需要有不同的配置文件&quot;, 很容易处理</strong>: 只需要<strong>给主从节点分别准备两份不同的 MySQL 配置文件, 然后根据 Pod 的序号(Index)挂载</strong>进去即可. 正如前面介绍过的, 这样的配置文件信息, 应该保存在 <strong>ConfigMap 里供 Pod 使用</strong>. 它的定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> mysql
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">master.cnf</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    # 主节点 MySQL 的配置文件
    [mysqld]
    log-bin</span>
  <span class="token key atrule">slave.cnf</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    # 从节点 MySQL 的配置文件
    [mysqld]
    super-read-only</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>在这里定义了 <strong>master.cnf 和 slave.cnf</strong> 两个 MySQL 的配置文件.</p> <ul><li>master.cnf 开启了 log-bin, 即: 使用二进制日志文件的方式进行<strong>主从复制</strong>, 这是一个标准的设置.</li> <li>slave.cnf 的开启了 super-read-only, 代表的是从节点会<strong>拒绝除了主节点的数据同步操作之外的所有写操作</strong>, 即: 它对用户是只读的.</li></ul> <p>而上述 ConfigMap 定义里的 data 部分, 是 Key-Value 格式的. 比如, master.cnf 就是这份配置数据的 Key, 而 &quot;|&quot; 后面的内容, 就是<strong>这份配置数据的 Value</strong>. 这份数据将来挂载进 Master 节点对应的 Pod 后, 就会在 Volume 目录里生成一个叫作 <strong>master.cnf</strong> 的文件.</p> <p>如果你对 ConfigMap 的用法感到陌生的话, 可以稍微复习一下前面 Secret 对象部分的内容. 因为 ConfigMap 跟 Secret, 无论是使用方法还是实现原理, 几乎都是一样的.</p> <p>接下来需要<strong>创建两个 Service 来供 StatefulSet 以及用户使用</strong>. 这两个 Service 的定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> mysql
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">3306</span>
  <span class="token key atrule">clusterIP</span><span class="token punctuation">:</span> None
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> mysql
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql<span class="token punctuation">-</span>read
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> mysql
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">3306</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> mysql
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>可以看到, 这两个 Service 都代理了<strong>所有携带 app=mysql 标签的 Pod</strong>, 也就是所有的 MySQL Pod. 端口映射都是用 Service 的 3306 端口对应 Pod 的 3306 端口.</p> <p>不同的是, 第一个名叫 &quot;mysql&quot; 的 Service 是一个 <strong>Headless Service(即: clusterIP= None)</strong> . 所以它的作用, 是<strong>通过为 Pod 分配 DNS 记录来固定它的拓扑状态</strong>, 比如 &quot;mysql-0.mysql&quot; 和 &quot;mysql-1.mysql&quot; 这样的 DNS 名字. 其中编号为 0 的节点就是主节点.</p> <p>而第二个名叫 &quot;mysql-read&quot; 的 Service, 则是一个<strong>常规的 Service</strong>.</p> <p>并且这里规定, 所有<strong>用户的读请求, 都必须访问第二个 Service 被自动分配的 DNS 记录</strong>, 即: &quot;mysql-read&quot; (当然, 也可以访问这个 Service 的 VIP). 这样<strong>读请求就可以被转发到任意一个 MySQL 的主节点或者从节点</strong>上. 注意 Kubernetes 中的所有 Service, Pod 对象, 都会被<strong>自动分配同名的 DNS 记录</strong>. 具体细节会在后面 Service 部分做重点讲解.</p> <p>而所有<strong>用户的写请求</strong>, 则必须直接<strong>以 DNS 记录的方式访问到 MySQL 的主节点</strong>, 也就是 &quot;mysql-0.mysql&quot; 这条 DNS 记录.</p> <p>接下来再一起解决 &quot;第二座大山: Master 节点和 Salve 节点需要能够传输备份文件&quot; 的问题.</p> <p>**翻越这座大山的思路比较推荐的做法是: 先搭建框架, 再完善细节. 其中 Pod 部分如何定义, 是完善细节时的重点. 所以首先为 StatefulSet 对象规划一个大致的框架, 如下图所示: **</p> <p><img src="/img/0123b66331625dbd787b6aa5bf44dbc5-20230731162150-wgjhzij.png" alt=""></p> <p>在这一步可以先为 StatefulSet 定义一些<strong>通用的字段</strong>.</p> <p>比如: selector 表示, 这个 StatefulSet <strong>要管理的 Pod 必须携带 app=mysql 标签</strong>; 它声明要使用的 Headless Service 的名字是: mysql.</p> <p>这个 StatefulSet 的 replicas 值是 3, 表示它定义的 MySQL 集群有<strong>三个节点</strong>: 一个 Master 节点, 两个 Slave 节点.</p> <p>可以看到, StatefulSet 管理的 &quot;有状态应用&quot; 的多个实例, 也都是通过<strong>同一份 Pod 模板创建出来</strong>的, 使用的是同一个 Docker 镜像. 这也就意味着: <mark><strong>如果你的应用要求不同节点的镜像不一样, 那就不能再使用 StatefulSet 了. 对于这种情况, 应该考虑后面会讲解到的 Operator</strong></mark>.</p> <p>除了这些基本的字段外, 作为一个<strong>有存储状态</strong>的 MySQL 集群, <strong>StatefulSet 还需要管理存储状态</strong>. 所以需要<strong>通过 volumeClaimTemplate(PVC 模板)来为每个 Pod 定义 PVC</strong>. 比如, 这个 PVC 模板的 resources.requests.strorage 指定了存储的大小为 10 GiB; ReadWriteOnce 指定了该存储的属性为<strong>可读写</strong>, 并且一个 PV 只允许挂载在一个宿主机上. 将来, 这个 PV 对应的的 <strong>Volume</strong> 就会充当 MySQL Pod 的存储数据目录.</p> <p>**然后来重点设计一下这个 StatefulSet 的 Pod 模板, 也就是 template 字段. **</p> <p>由于 StatefulSet 管理的 <strong>Pod 都来自于同一个镜像</strong>, 这就要求在编写 Pod 时, 一定要保持清醒, 用 &quot;人格分裂&quot;的方式进行思考:</p> <ol><li>如果这个 Pod 是 <strong>Master</strong> 节点, 要怎么做;</li> <li>如果这个 Pod 是 <strong>Slave</strong> 节点, 又要怎么做.</li></ol> <p>想清楚这两个问题, 就可以<strong>按照 Pod 的启动过程</strong>来一步步定义它们了.</p> <p>**第一步: 从 ConfigMap 中, 获取 MySQL 的 Pod 对应的配置文件. **</p> <p>为此需要进行一个<strong>初始化</strong>操作, 根据节点的角色是 Master 还是 Slave 节点, <strong>为 Pod 分配对应的配置文件</strong>. 此外, MySQL 还要求集群里的<strong>每个节点都有一个唯一的 ID 文件, 名叫 server-id.cnf</strong>.</p> <p>而根据已经掌握的 Pod 知识, 这些初始化操作显然适合通过 <strong>InitContainer</strong> 来完成. 所以, 首先定义了一个 InitContainer, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token comment"># template.spec</span>
<span class="token key atrule">initContainers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> init<span class="token punctuation">-</span>mysql
    <span class="token key atrule">image</span><span class="token punctuation">:</span> mysql<span class="token punctuation">:</span><span class="token number">5.7</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> bash
    <span class="token punctuation">-</span> <span class="token string">&quot;-c&quot;</span>
    <span class="token punctuation">-</span> <span class="token punctuation">|</span><span class="token scalar string">
      set -ex
      # 从 Pod 的序号, 生成 server-id
      [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
      ordinal=${BASH_REMATCH[1]}
      echo [mysqld] &gt; /mnt/conf.d/server-id.cnf
      # 由于 server-id=0 有特殊含义, 我们给 ID 加一个 100 来避开它
      echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf
      # 如果 Pod 序号是 0, 说明它是 Master 节点, 从 ConfigMap 里把 Master 的配置文件拷贝到 /mnt/conf.d/ 目录; 
      # 否则, 拷贝 Slave 的配置文件
      if [[ $ordinal -eq 0 ]]; then
        cp /mnt/config-map/master.cnf /mnt/conf.d/
      else
        cp /mnt/config-map/slave.cnf /mnt/conf.d/
      fi</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> conf
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt/conf.d
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>map
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt/config<span class="token punctuation">-</span>map
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><p>在这个名叫 init-mysql 的 InitContainer 的配置中, <strong>它从 Pod 的 hostname 里, 读取到了 Pod 的序号, 以此作为 MySQL 节点的 server-id</strong>.</p> <p>然后, init-mysql 通过这个序号, <strong>判断当前 Pod 到底是 Master 节点</strong>(即: 序号为 0)还是 Slave 节点(即: 序号不为 0), 从而把对应的配置文件从 /mnt/config-map 目录拷贝到 /mnt/conf.d/ 目录下.</p> <p>其中, 文件拷贝的源目录 /mnt/config-map, 正是 ConfigMap 在这个 Pod 的 Volume, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token comment"># template.spec</span>
<span class="token key atrule">volumes</span><span class="token punctuation">:</span>
	<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> conf
	<span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
	<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>map
<span class="token key atrule">configMap</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>通过这个定义, init-mysql 在声明了挂载 config-map 这个 Volume 之后, ConfigMap 里保存的内容, 就会以文件的方式出现在它的 /mnt/config-map 目录当中.</p> <p>而<strong>文件拷贝的目标目录, 即容器里的 /mnt/conf.d/ 目录, 对应的则是一个名叫 conf 的, emptyDir 类型的 Volume</strong>. 基于 Pod Volume 共享的原理, 当 InitContainer 复制完配置文件退出后, 后面启动的 MySQL 容器只需要直接声明挂载这个名叫 conf 的 Volume, 它所需要的 .cnf 配置文件已经出现在里面了. 这跟之前介绍的 Tomcat 和 WAR 包的处理方法是完全一样的.</p> <p>**第二步: 在 Slave Pod 启动前, 从 Master 或者其他 Slave Pod 里拷贝数据库数据到自己的目录下. **</p> <p>为了实现这个操作, 就需要再定义<strong>第二个 InitContainer</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token comment"># template.spec.initContainers</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> clone<span class="token punctuation">-</span>mysql
    <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/google<span class="token punctuation">-</span>samples/xtrabackup<span class="token punctuation">:</span><span class="token number">1.0</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> bash
    <span class="token punctuation">-</span> <span class="token string">&quot;-c&quot;</span>
    <span class="token punctuation">-</span> <span class="token punctuation">|</span><span class="token scalar string">
      set -ex
      # 拷贝操作只需要在第一次启动时进行, 所以如果数据已经存在, 跳过
      [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0
      # Master 节点 (序号为 0) 不需要做这个操作
      [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
      ordinal=${BASH_REMATCH[1]}
      [[ $ordinal -eq 0 ]] &amp;&amp; exit 0
      # 使用 ncat 指令, 远程地从前一个节点拷贝数据到本地
      ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
      # 执行 --prepare, 这样拷贝来的数据就可以用作恢复了
      xtrabackup --prepare --target-dir=/var/lib/mysql</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/mysql
      <span class="token key atrule">subPath</span><span class="token punctuation">:</span> mysql
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> conf
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/mysql/conf.d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>在这个名叫 clone-mysql 的 InitContainer 里, 使用的是 xtrabackup 镜像(它里面安装了 xtrabackup 工具). 而在它的启动命令里, 首先做了一个判断. 即: <strong>当初始化所需的数据(/var/lib/mysql/mysql 目录)已经存在, 或者当前 Pod 是 Master 节点的时候, 不需要做拷贝操作</strong>.</p> <p>接下来, clone-mysql 会使用 Linux 自带的 <strong>ncat</strong> 指令, 向 DNS 记录为 &quot;mysql-当前序号-.mysql&quot; 的 Pod, 也就是当前 Pod 的前一个 Pod, 发起<strong>数据传输请求</strong>, 并且直接用 xbstream 指令将收到的备份数据保存在 /var/lib/mysql 目录下. 注意 3307 是一个特殊端口, 运行着一个专门负责备份 MySQL 数据的辅助进程. 这一步可以随意选择用自己喜欢的方法来传输数据. 比如用 scp 或者 rsync, 都没问题.</p> <p>你可能已经注意到, 这个容器里的  <strong>/var/lib/mysql</strong> 目录, <strong>实际上正是一个名为 data 的 PVC</strong>, 即: 在前面<strong>声明的持久化存储</strong>.</p> <p>这就可以保证, <strong>哪怕宿主机宕机了, 数据库的数据也不会丢失</strong>. 更重要的是, 由于 Pod Volume 是被 Pod 里的<strong>容器共享</strong>的, 所以后面启动的 MySQL 容器, <strong>就可以把这个 Volume 挂载到自己的 /var/lib/mysql 目录下, 直接使用里面的备份数据进行恢复操作</strong>.</p> <p>不过, clone-mysql 容器还要对 /var/lib/mysql 目录, 执行一句 xtrabackup --prepare 操作, 目的是让拷贝来的数据进入一致性状态, 这样这些数据才能被用作数据恢复.</p> <p>至此就通过 InitContainer 完成了对 &quot;主, 从节点间<strong>备份文件传输</strong>&quot; 操作的处理过程, 也就是翻越了&quot;第二座大山&quot;.</p> <p>接下来可以开始定义 MySQL 容器, 启动 MySQL 服务了. 由于 StatefulSet 里的所有 Pod 都来自用同一个 Pod 模板, 所以还要 &quot;人格分裂&quot; 地去思考: 这个 MySQL 容器的<strong>启动命令, 在 Master 和 Slave 两种情况下有什么不同</strong>.</p> <p>有了 Docker 镜像, 在 Pod 里声明一个 Master 角色的 MySQL 容器并不是什么困难的事情: 直接执行 MySQL 启动命令即可. 但是如果这个 Pod 是一个第一次启动的 Slave 节点, 在执行 MySQL 启动命令之前, 它就<strong>需要使用前面 InitContainer 拷贝来的备份数据进行初始化</strong>.</p> <p>可是别忘了, **容器是一个单进程模型. ** 所以, 一个 Slave 角色的 MySQL 容器启动之前, <strong>谁能负责给它执行初始化的 SQL 语句</strong>呢?</p> <p>这就是需要解决的 &quot;第三座大山&quot; 的问题, 即: <strong>如何在 Slave 节点的 MySQL 容器第一次启动之前, 执行初始化 SQL</strong>.</p> <p>你可能已经想到了, 可以为这个 MySQL 容器<strong>额外定义一个 sidecar 容器</strong>, 来完成这个操作, 它的定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
  <span class="token comment"># template.spec.containers</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> xtrabackup
    <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/google<span class="token punctuation">-</span>samples/xtrabackup<span class="token punctuation">:</span><span class="token number">1.0</span>
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> xtrabackup
      <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">3307</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> bash
    <span class="token punctuation">-</span> <span class="token string">&quot;-c&quot;</span>
    <span class="token punctuation">-</span> <span class="token punctuation">|</span><span class="token scalar string">
      set -ex
      cd /var/lib/mysql</span>
    
      <span class="token comment"># 从备份信息文件里读取 MASTER_LOG_FILEM 和 MASTER_LOG_POS 这两个字段的值, 用来拼装集群初始化 SQL</span>
      if <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token punctuation">-</span>f xtrabackup_slave_info <span class="token punctuation">]</span><span class="token punctuation">]</span>; then
        <span class="token comment"># 如果 xtrabackup_slave_info 文件存在, 说明这个备份数据来自于另一个 Slave 节点. 这种情况下, XtraBackup 工具在备份的时候, 就已经在这个文件里自动生成了 &quot;CHANGE MASTER TO&quot; SQL 语句. 所以, 我们只需要把这个文件重命名为 change_master_to.sql.in, 后面直接使用即可</span>
        mv xtrabackup_slave_info change_master_to.sql.in
        <span class="token comment"># 所以, 也就用不着 xtrabackup_binlog_info 了</span>
        rm <span class="token punctuation">-</span>f xtrabackup_binlog_info
      elif <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token punctuation">-</span>f xtrabackup_binlog_info <span class="token punctuation">]</span><span class="token punctuation">]</span>; then
        <span class="token comment"># 如果只存在 xtrabackup_binlog_inf 文件, 那说明备份来自于 Master 节点, 我们就需要解析这个备份信息文件, 读取所需的两个字段的值</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> `cat xtrabackup_binlog_info` =~ ^(.<span class="token important">*?)</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">:</span>space<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">]</span>+(.<span class="token important">*?)$</span> <span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token punctuation">|</span><span class="token punctuation">|</span> exit 1
        rm xtrabackup_binlog_info
        <span class="token comment"># 把两个字段的值拼装成 SQL, 写入 change_master_to.sql.in 文件</span>
        echo &quot;CHANGE MASTER TO MASTER_LOG_FILE='$<span class="token punctuation">{</span>BASH_REMATCH<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>'<span class="token punctuation">,</span>\
              MASTER_LOG_POS=$<span class="token punctuation">{</span>BASH_REMATCH<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">}</span>&quot; <span class="token punctuation">&gt;</span> change_master_to.sql.in
      fi
    
      <span class="token comment"># 如果 change_master_to.sql.in, 就意味着需要做集群初始化工作</span>
      if <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token punctuation">-</span>f change_master_to.sql.in <span class="token punctuation">]</span><span class="token punctuation">]</span>; then
        <span class="token comment"># 但一定要先等 MySQL 容器启动之后才能进行下一步连接 MySQL 的操作</span>
        echo &quot;Waiting for mysqld to be ready (accepting connections)&quot;
        until mysql <span class="token punctuation">-</span>h 127.0.0.1 <span class="token punctuation">-</span>e &quot;SELECT 1&quot;; do sleep 1; done
      
        echo &quot;Initializing replication from clone position&quot;
        <span class="token comment"># 将文件 change_master_to.sql.in 改个名字, 防止这个 Container 重启的时候, 因为又找到了 change_master_to.sql.in, 从而重复执行一遍这个初始化流程</span>
        mv change_master_to.sql.in change_master_to.sql.orig
        <span class="token comment"># 使用 change_master_to.sql.orig 的内容, 也是就是前面拼装的 SQL, 组成一个完整的初始化和启动 Slave 的 SQL 语句</span>
        mysql <span class="token punctuation">-</span>h 127.0.0.1 &lt;&lt;EOF
      $(&lt;change_master_to.sql.orig)<span class="token punctuation">,</span>
        MASTER_HOST='mysql<span class="token punctuation">-</span>0.mysql'<span class="token punctuation">,</span>
        MASTER_USER='root'<span class="token punctuation">,</span>
        MASTER_PASSWORD=''<span class="token punctuation">,</span>
        MASTER_CONNECT_RETRY=10;
      START SLAVE;
      EOF
      fi
    
      <span class="token comment"># 使用 ncat 监听 3307 端口. 它的作用是, 在收到传输请求的时候, 直接执行 &quot;xtrabackup --backup&quot; 命令, 备份 MySQL 的数据并发送给请求者</span>
      exec ncat <span class="token punctuation">-</span><span class="token punctuation">-</span>listen <span class="token punctuation">-</span><span class="token punctuation">-</span>keep<span class="token punctuation">-</span>open <span class="token punctuation">-</span><span class="token punctuation">-</span>send<span class="token punctuation">-</span>only <span class="token punctuation">-</span><span class="token punctuation">-</span>max<span class="token punctuation">-</span>conns=1 3307 <span class="token punctuation">-</span>c \
        &quot;xtrabackup <span class="token punctuation">-</span><span class="token punctuation">-</span>backup <span class="token punctuation">-</span><span class="token punctuation">-</span>slave<span class="token punctuation">-</span>info <span class="token punctuation">-</span><span class="token punctuation">-</span>stream=xbstream <span class="token punctuation">-</span><span class="token punctuation">-</span>host=127.0.0.1 <span class="token punctuation">-</span><span class="token punctuation">-</span>user=root&quot;
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/mysql
      <span class="token key atrule">subPath</span><span class="token punctuation">:</span> mysql
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> conf
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/mysql/conf.d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br></div></div><p>可以看到, **在这个名叫 xtrabackup 的 sidecar 容器的启动命令里, 其实实现了两部分工作. **</p> <p><strong>第一部分工作, 当然是 MySQL 节点的初始化工作</strong>. 这个初始化需要使用的 SQL, <strong>是 sidecar 容器拼装出来</strong>, 保存在一个名为 change_master_to.sql.in 的文件里的, 具体过程如下所示:</p> <p>sidecar 容器首先会判断当前 Pod 的 /var/lib/mysql 目录下, 是否有 xtrabackup_slave_info 这个备份信息文件.</p> <ul><li>如果有, 则说明这个目录下的备份数据是<strong>由一个 Slave 节点生成</strong>的. 这种情况下, XtraBackup 工具在备份的时候, 就已经在这个文件里自动生成了 &quot;CHANGE MASTER TO&quot; SQL 语句. 所以只需要把这个文件重命名为 change_master_to.sql.in, 后面直接使用即可.</li> <li>如果没有 xtrabackup_slave_info 文件, 但是存在 xtrabackup_binlog_info 文件, 那就说明备份数据来自于 Master 节点. 这种情况下, sidecar 容器就需要<strong>解析这个备份信息文件</strong>, 读取 MASTER_LOG_FILE 和 MASTER_LOG_POS 这两个字段的值, 用它们拼装出初始化 SQL 语句, 然后把这句 SQL 写入到 change_master_to.sql.in 文件中.</li></ul> <p>接下来, sidecar 容器就可以执行初始化了. 从上面的叙述中可以看到, 只要这个 change_master_to.sql.in 文件存在, 那就说明接下来需要进行集群初始化操作.</p> <p>所以这时候, sidecar 容器只需要读取并执行 change_master_to.sql.in 里面的 &quot;CHANGE MASTER TO&quot; 指令, 再执行一句 START SLAVE 命令, 一个 Slave 节点就被成功启动了. 需要注意的是: <strong>Pod 里的容器并没有先后顺序, 所以在执行初始化 SQL 之前, 必须先执行一句 SQL(select 1)来检查一下 MySQL 服务是否已经可用</strong>.</p> <p>当然, 上述这些初始化操作完成后, 还要<strong>删除掉前面用到的这些备份信息文件</strong>. 否则下次这个容器重启时, 就会发现这些文件存在, 所以又会重新执行一次数据恢复和集群初始化的操作, 这是不对的. 同理, change_master_to.sql.in 在使用后也要被重命名, 以免容器重启时因为发现这个文件存在又执行一遍初始化.</p> <p>**在完成 MySQL 节点的初始化后, 这个 sidecar 容器的第二个工作, 则是启动一个数据传输服务. **</p> <p>具体做法是: sidecar 容器会使用 ncat 命令启动一个工作在 3307 端口上的<strong>网络发送服务</strong>. 一旦收到数据传输请求时, <strong>sidecar 容器就会调用 xtrabackup --backup 指令备份当前 MySQL 的数据, 然后把这些备份数据返回给请求者</strong>. 这就是为什么前面在 InitContainer 里定义数据拷贝的时候, 访问的是 &quot;上一个 MySQL 节点&quot; 的 3307 端口.</p> <p>值得一提的是, 由于 sidecar 容器和 MySQL 容器同处于一个 Pod 里, 所以它是<strong>直接通过 Localhost 来访问和备份 MySQL 容器里的数据</strong>的, 非常方便. 同样地, 在这里举例用的只是一种<strong>备份方法</strong>而已, 你完全可以选择其他自己喜欢的方案. 比如可以使用 innobackupex 命令做数据备份和准备, 它的使用方法几乎与本文的备份方法一样.</p> <p>至此也就翻越了 &quot;第三座大山&quot;, 完成了 Slave 节点第一次启动前的初始化工作.</p> <p>扳倒了这 &quot;三座大山&quot; 后, 终于可以<strong>定义 Pod 里的主角, MySQL 容器</strong>了. 有了前面这些定义和初始化工作, MySQL 容器本身的定义就非常简单了, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>  <span class="token punctuation">...</span>
  <span class="token comment"># template.spec</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
    <span class="token key atrule">image</span><span class="token punctuation">:</span> mysql<span class="token punctuation">:</span><span class="token number">5.7</span>
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> MYSQL_ALLOW_EMPTY_PASSWORD
      <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;1&quot;</span>
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mysql
      <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">3306</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/mysql
      <span class="token key atrule">subPath</span><span class="token punctuation">:</span> mysql
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> conf
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/mysql/conf.d
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 500m
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi
    <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>
      <span class="token key atrule">exec</span><span class="token punctuation">:</span>
        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;mysqladmin&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;ping&quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>
      <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>
      <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>
    <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>
      <span class="token key atrule">exec</span><span class="token punctuation">:</span>
        <span class="token comment"># 通过 TCP 连接的方式进行健康检查</span>
        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;mysql&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-h&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;127.0.0.1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-e&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;SELECT 1&quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>
      <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">2</span>
      <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">1</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><p>在这个容器的定义里, 使用了一个标准的 MySQL 5.7 的官方镜像. 它的<strong>数据目录是 /var/lib/mysql, 配置文件目录是 /etc/mysql/conf.d</strong>.</p> <p>这时候, 你应该能够明白, <strong>如果 MySQL 容器是 Slave 节点的话, 它的数据目录里的数据, 就来自于 InitContainer 从其他节点里拷贝而来的备份. 它的配置文件目录 /etc/mysql/conf.d 里的内容, 则来自于 ConfigMap 对应的 Volume. 而它的初始化工作, 则是由同一个 Pod 里的 sidecar 容器完成的</strong>. 这些操作, 正是前面讲述的大部分内容.</p> <p>另外, 我们为它定义了一个 <strong>livenessProbe</strong>, 通过 mysqladmin ping 命令来检查它是否健康; 还定义了一个 <strong>readinessProbe</strong>, 通过查询 SQL(select 1)来检查 MySQL 服务是否可用. 当然, 凡是 readinessProbe 检查失败的 MySQL Pod, 都会从 Service 里被<strong>摘除</strong>掉.</p> <p>至此一个完整的主从复制模式的 MySQL 集群就定义完了. 现在就可以使用 kubectl 命令, 尝试运行一下这个 StatefulSet 了.</p> <p><strong>首先需要在 Kubernetes 集群里创建满足条件的 PV</strong>. 如果使用的是前面部署的 Kubernetes 集群的话, 可以按照如下方式使用存储插件 Rook:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> rook-storage.yaml
$ <span class="token function">cat</span> rook-storage.yaml
apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: <span class="token number">3</span>
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  clusterNamespace: rook-ceph
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>在这里用到了 <strong>StorageClass</strong> 来完成这个操作. 它的作用是自动地为集群里存在的每一个 PVC, 调用存储插件(Rook)创建对应的 PV, 从而省去了手动创建 PV 的机械劳动. 后续讲解容器存储的时候, 会再详细介绍这个机制. 注意在使用 Rook 的情况下, mysql-statefulset.yaml 里的 volumeClaimTemplates 字段需要加上声明 storageClassName=rook-ceph-block, 才能使用到这个 Rook 提供的持久化存储.</p> <p><strong>然后, 就可以创建这个 StatefulSet 了</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> mysql-statefulset.yaml
$ kubectl get pod <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>mysql
NAME      READY     STATUS    RESTARTS   AGE
mysql-0   <span class="token number">2</span>/2       Running   <span class="token number">0</span>          2m
mysql-1   <span class="token number">2</span>/2       Running   <span class="token number">0</span>          1m
mysql-2   <span class="token number">2</span>/2       Running   <span class="token number">0</span>          1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, StatefulSet 启动成功后, 会有三个 Pod 运行.</p> <p>**接下来, 可以尝试向这个 MySQL 集群发起请求, 执行一些 SQL 操作来验证它是否正常: **</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run mysql-client <span class="token parameter variable">--image</span><span class="token operator">=</span>mysql:5.7 <span class="token parameter variable">-i</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never --<span class="token punctuation">\</span>
  mysql <span class="token parameter variable">-h</span> mysql-0.mysql <span class="token operator">&lt;&lt;</span><span class="token string">EOF
CREATE DATABASE test;
CREATE TABLE test.messages (message VARCHAR(250));
INSERT INTO test.messages VALUES ('hello');
EOF</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>如上所示, 通过启动一个容器, 使用 MySQL client 执行了创建数据库和表, 以及插入数据的操作. 需要注意的是, 连接的 MySQL 的地址必须是 mysql-0.mysql(即: <strong>Master</strong> 节点的 DNS 记录). 因为<strong>只有 Master 节点才能处理写操作</strong>.</p> <p>而通过连接 mysql-read 这个 Service, 就可以用 SQL 进行读操作, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run mysql-client <span class="token parameter variable">--image</span><span class="token operator">=</span>mysql:5.7 <span class="token parameter variable">-i</span> <span class="token parameter variable">-t</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never --<span class="token punctuation">\</span>
 mysql <span class="token parameter variable">-h</span> mysql-read <span class="token parameter variable">-e</span> <span class="token string">&quot;SELECT * FROM test.messages&quot;</span>
Waiting <span class="token keyword">for</span> pod default/mysql-client to be running, status is Pending, pod ready: <span class="token boolean">false</span>
+---------+
<span class="token operator">|</span> message <span class="token operator">|</span>
+---------+
<span class="token operator">|</span> hello   <span class="token operator">|</span>
+---------+
pod <span class="token string">&quot;mysql-client&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>在有了 StatefulSet 以后, 就可以像 Deployment 那样, 非常方便地<strong>扩展</strong>这个 MySQL 集群, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl scale statefulset mysql  <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">5</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时候就会发现新的 Slave Pod mysql-3 和 mysql-4 被自动创建了出来.</p> <p>而如果像如下所示的这样, 直接连接 mysql-3.mysql, 即 mysql-3 这个 Pod 的 DNS 名字来进行查询操作:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run mysql-client <span class="token parameter variable">--image</span><span class="token operator">=</span>mysql:5.7 <span class="token parameter variable">-i</span> <span class="token parameter variable">-t</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never --<span class="token punctuation">\</span>
  mysql <span class="token parameter variable">-h</span> mysql-3.mysql <span class="token parameter variable">-e</span> <span class="token string">&quot;SELECT * FROM test.messages&quot;</span>
Waiting <span class="token keyword">for</span> pod default/mysql-client to be running, status is Pending, pod ready: <span class="token boolean">false</span>
+---------+
<span class="token operator">|</span> message <span class="token operator">|</span>
+---------+
<span class="token operator">|</span> hello   <span class="token operator">|</span>
+---------+
pod <span class="token string">&quot;mysql-client&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>就会看到, 从 StatefulSet 新创建的 mysql-3 上, 同样可以读取到之前插入的记录. 也就是说集群的数据备份和恢复, 都是有效的.</p> <blockquote><p>总结</p></blockquote> <p>本节以 MySQL 集群为例, 详细分享了一个实际的 StatefulSet 的编写过程. 在这个过程中, 有以下几个关键点(坑)特别值得注意和体会.</p> <ol><li>&quot;人格分裂&quot;: <strong>在解决需求的过程中, 一定要记得思考, 该 Pod 在扮演不同角色时的不同操作</strong>.</li> <li>&quot;阅后即焚&quot;: 很多  <strong>&quot;有状态应用&quot;</strong>  的节点, 只是在第一次启动的时候才需要做额外处理. 所以在编写 YAML 文件时, 一定要考虑 &quot;容器重启&quot; 的情况, 不要让这一次的操作干扰到下一次的容器启动.</li> <li>&quot;容器之间平等无序&quot;: <strong>除非是 InitContainer, 否则一个 Pod 里的多个容器之间, 是完全平等的.</strong>  所以, 精心设计的 <strong>sidecar, 绝不能对容器的顺序做出假设, 否则就需要进行前置检查</strong>.</li></ol> <p><mark><strong>StatefulSet 其实是一种特殊的 Deployment, 只不过这个 &quot;Deployment&quot; 的每个 Pod 实例的名字里, 都携带了一个唯一并且固定的编号. 这个编号的顺序, 固定了 Pod 的拓扑关系; 这个编号对应的 DNS 记录, 固定了 Pod 的访问方式; 这个编号对应的 PV, 绑定了 Pod 与持久化存储的关系. 所以, 当 Pod 被删除重建时, 这些 &quot;状态&quot; 都会保持不变.</strong></mark></p> <p>而一旦你的应用没办法通过上述方式进行状态的管理, 那就代表了 StatefulSet 已经不能解决它的部署问题了. 这时候后面讲到的 <strong>Operator</strong>, 可能才是一个更好的选择.</p> <h4 id="_21-容器化守护进程的意义-daemonset"><a href="#_21-容器化守护进程的意义-daemonset" class="header-anchor">#</a> 21 | 容器化守护进程的意义:DaemonSet</h4> <p>本节的主题是容器化守护进程的意义之 DaemonSet.</p> <p>上一节分享了使用 StatefulSet 编排 &quot;有状态应用&quot; 的过程. 从中不难看出, <strong>StatefulSet 其实就是对现有典型运维业务的容器化抽象</strong>. 也就是说, 你一定有方法在不使用 Kubernetes, 甚至不使用容器的情况下, 自己 DIY 一个类似的方案出来. 但是一旦涉及到升级, 版本管理等更工程化的能力, Kubernetes 的好处, 才会更加凸现.</p> <p>比如, 如何对 StatefulSet 进行 &quot;<strong>滚动更新</strong>&quot;(rolling update)? 很简单. 只要<strong>修改 StatefulSet 的 Pod 模板, 就会自动触发 &quot;滚动更新&quot;</strong> :</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl patch statefulset mysql <span class="token parameter variable">--type</span><span class="token operator">=</span><span class="token string">'json'</span> <span class="token parameter variable">-p</span><span class="token operator">=</span><span class="token string">'[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;mysql:5.7.23&quot;}]'</span>
statefulset.apps/mysql patched
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在这里使用了 <strong>kubectl patch</strong> 命令. 它的意思是, 以 &quot;补丁&quot; 的方式(JSON 格式的)修改一个 API 对象的指定字段, 也就是后面指定的 &quot;<code>spec/template/spec/containers/0/image</code>​&quot;.</p> <p>这样, <strong>StatefulSet Controller 就会按照与 Pod 编号相反的顺序, 从最后一个 Pod 开始, 逐一更新这个 StatefulSet 管理的每个 Pod</strong>. 而如果更新发生了错误, 这次 &quot;滚动更新&quot; 就会停止. 此外, StatefulSet 的 &quot;滚动更新&quot; 还可以进行更精细的控制, 比如<mark><strong>金丝雀发布(Canary Deploy)或者灰度发布</strong></mark>, **这意味着应用的多个实例中被指定的一部分不会被更新到最新的版本. ** 这个字段, 正是 StatefulSet 的 <strong>spec.updateStrategy.rollingUpdate 的 partition 字段</strong>.</p> <p>比如, 现在将前面这个 StatefulSet 的 partition 字段设置为 2:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl patch statefulset mysql <span class="token parameter variable">-p</span> <span class="token string">'{&quot;spec&quot;:{&quot;updateStrategy&quot;:{&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:{&quot;partition&quot;:2}}}}'</span>
statefulset.apps/mysql patched
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>其中, kubectl patch 命令后面的参数(JSON 格式的), <strong>就是 partition 字段在 API 对象里的路径</strong>. 所以, 上述操作等同于直接<strong>使用 kubectl edit 命令, 打开这个对象, 把 partition 字段修改为 2</strong>.</p> <p>这样就指定了当 Pod 模板发生变化的时候, 比如 MySQL 镜像更新到 5.7.23, 那么只有序号大于或者等于 2 的 Pod 会被更新到这个版本. 并且如果删除或者重启了序号小于 2 的 Pod, 等它再次启动后, 也会保持原先的 5.7.2 版本, 绝不会被升级到 5.7.23 版本.</p> <p><mark><strong>StatefulSet 可以说是 Kubernetes 项目中最为复杂的编排对象</strong></mark>, 一定要认真消化, 动手实践一下这个例子.</p> <p>本节将会为重点讲解一个相对轻松的知识点: <strong>DaemonSet</strong>. 顾名思义, DaemonSet 的主要作用, 是让你在 Kubernetes 集群里, 运行一个 <strong>Daemon Pod</strong>. 所以这个 Pod 有如下三个特征:</p> <ol><li>**这个 Pod 运行在 Kubernetes 集群里的每一个节点(Node)上; **</li> <li>**每个节点上只有一个这样的 Pod 实例; **</li> <li>**当有新的节点加入 Kubernetes 集群后, 该 Pod 会自动地在新节点上被创建出来; 而当旧节点被删除后, 它上面的 Pod 也相应地会被回收掉. **</li></ol> <p>这个机制听起来很简单, 但 Daemon Pod 的意义确实是非常重要的. 随便给列举几个例子:</p> <ol><li>各种<strong>网络插件的 Agent 组件</strong>, 都必须<strong>运行在每一个节点</strong>上, 用来处理这个节点上的容器网络;</li> <li>各种<strong>存储插件的 Agent 组件</strong>, 也必须<strong>运行在每一个节点</strong>上, 用来在这个节点上挂载远程存储目录, 操作容器的 Volume 目录;</li> <li>各种<strong>监控组件和日志组件, 也必须运行在每一个节点上</strong>, 负责这个节点上的监控信息和日志搜集.</li></ol> <p>更重要的是, 跟其他编排对象不一样, <mark><strong>DaemonSet 开始运行的时机, 很多时候比整个 Kubernetes 集群出现的时机都要早</strong></mark>.</p> <p>这个乍一听起来可能有点儿奇怪. 但其实你来想一下: 如果这个 DaemonSet 正是一个网络插件的 Agent 组件呢? 这个时候, 整个 Kubernetes 集群里还没有可用的容器网络, 所有 Worker 节点的状态都是 <strong>NotReady</strong>(NetworkReady=false). 这种情况下, 普通的 Pod 肯定不能运行在这个集群上. 所以这也就意味着 DaemonSet 的设计, 必须要有某种 &quot;过人之处&quot; 才行.</p> <p>为了弄清楚 DaemonSet 的工作原理, 还是按照老规矩, 先从它的 <strong>API 对象的定义</strong>说起.</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>logging
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master
        <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch
        <span class="token key atrule">image</span><span class="token punctuation">:</span> k8s.gcr.io/fluentd<span class="token punctuation">-</span>elasticsearch<span class="token punctuation">:</span><span class="token number">1.20</span>
        <span class="token key atrule">resources</span><span class="token punctuation">:</span>
          <span class="token key atrule">limits</span><span class="token punctuation">:</span>
            <span class="token key atrule">memory</span><span class="token punctuation">:</span> 200Mi
          <span class="token key atrule">requests</span><span class="token punctuation">:</span>
            <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m
            <span class="token key atrule">memory</span><span class="token punctuation">:</span> 200Mi
        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlibdockercontainers
          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/docker/containers
          <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>
      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
          <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/log
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlibdockercontainers
        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
          <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/lib/docker/containers
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div><p>这个 DaemonSet, 管理的是一个 fluentd-elasticsearch 镜像的 Pod. 这个镜像的功能非常实用: <strong>通过 fluentd 将 Docker 容器里的日志转发到 ElasticSearch 中</strong>.</p> <p>可以看到, DaemonSet 跟 Deployment 其实非常相似, 只不过是<strong>没有 replicas 字段</strong>; 它也<strong>使用 selector 选择管理所有携带了 name=fluentd-elasticsearch 标签的 Pod</strong>. 而这些 Pod 的模板, 也是用 <strong>template</strong> 字段定义的. 在这个字段中, 定义了一个使用 fluentd-elasticsearch:1.20 镜像的容器, 而且这个容器<strong>挂载了两个 hostPath 类型的 Volume, 分别对应宿主机的 /var/log 目录和 /var/lib/docker/containers 目录</strong>. 显然, fluentd 启动之后, 它会从<strong>这两个目录里搜集日志信息</strong>, 并转发给 ElasticSearch 保存. 这样通过 ElasticSearch 就可以很方便地检索这些日志了.</p> <p>需要注意的是, Docker 容器里应用的日志, 默认会保存在宿主机的 <code>/var/lib/docker/containers/容器ID/容器ID-json.log</code>​ 文件里, 所以这个目录正是 fluentd 的搜集目标.</p> <p>那么, <mark><strong>DaemonSet 又是如何保证每个 Node 上有且只有一个被管理的 Pod 呢</strong></mark>​ **? ** 显然, 这是一个典型的 &quot;<mark><strong>控制器模型</strong></mark>&quot; 能够处理的问题.</p> <p>DaemonSet Controller, 首先从 Etcd 里获取所有的 Node 列表, 然后遍历所有的 Node. 这时它就可以很容易地去检查, 当前这个 Node 上<strong>是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行</strong>.</p> <p>而检查的结果, 可能有这么三种情况:</p> <ol><li><strong>没有</strong>这种 Pod, 那么就意味着要在这个 Node 上创建这样一个 Pod;</li> <li><strong>有</strong>这种 Pod, 但是数量大于 1, 那就说明要把多余的 Pod 从这个 Node 上删除掉;</li> <li><strong>正好只有一个</strong>这种 Pod, 那说明这个节点是正常的.</li></ol> <p>其中, 删除节点(Node)上多余的 Pod 非常简单, 直接调用 Kubernetes API 就可以了.</p> <p>但是, **如何在指定的 Node 上创建新 Pod 呢? ** 如果熟悉 Pod API 对象的话, 那一定可以立刻说出答案: <strong>用 nodeSelector, 选择 Node 的名字即可</strong>.</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> &lt;Node 名字<span class="token punctuation">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>没错.</p> <p>不过, 在 Kubernetes 项目里, <strong>nodeSelector 其实已经是一个将要被废弃的字段了</strong>. 因为现在有了一个新的, 功能更完善的字段可以代替它, 即: <mark><strong>nodeAffinity</strong></mark>. 举个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> with<span class="token punctuation">-</span>node<span class="token punctuation">-</span>affinity
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
    <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>
      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>
        <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> metadata.name
            <span class="token key atrule">operator</span><span class="token punctuation">:</span> In
            <span class="token key atrule">values</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> node<span class="token punctuation">-</span>geektime
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>在这个 Pod 里, 声明了一个 <strong>spec.affinity 字段</strong>, 然后定义了一个 nodeAffinity. 其中 spec.affinity 字段, 是 Pod 里跟<strong>调度相关</strong>的一个字段. 关于它的完整内容, 会在讲解调度策略的时候再详细阐述.</p> <p>而在这里定义的 nodeAffinity 的含义是:</p> <ol><li><strong>requiredDuringSchedulingIgnoredDuringExecution</strong>: 它的意思是说, 这个 nodeAffinity <strong>必须在每次调度的时候予以考虑</strong>. 同时, 这也意味着可以设置在某些情况下不考虑这个 nodeAffinity;</li> <li>这个 Pod, 将来只允许运行在 &quot;<code>metadata.name</code>​&quot; 是 &quot;node-geektime&quot; 的节点上.</li></ol> <p>在这里应该注意到 nodeAffinity 的定义, 可以支持更加<strong>丰富的语法</strong>, 比如 operator: In(即: 部分匹配; 如果定义 operator: Equal, 就是完全匹配), 这也正是 nodeAffinity 会取代 nodeSelector 的原因之一. 其实在大多数时候, 这些 Operator 语义没啥用处. 所以在<mark><strong>学习开源项目的时候, 一定要学会抓住 &quot;主线&quot;. 不要顾此失彼</strong></mark>.</p> <p>所以 ** **​<mark><strong>DaemonSet Controller 会在创建 Pod 的时候, 自动在这个 Pod 的 API 对象里, 加上这样一个 nodeAffinity 定义</strong></mark>. 其中, 需要绑定的节点名字, 正是当前正在遍历的这个 Node.</p> <p>当然, <strong>DaemonSet 并不需要修改用户提交的 YAML 文件里的 Pod 模板, 而是在向 Kubernetes 发起请求之前, 直接修改根据模板生成的 Pod 对象</strong>. 这个思路也正是在前面讲解 Pod 对象时介绍过的.</p> <p>此外, <strong>DaemonSet 还会给这个 Pod 自动加上另外一个与调度相关的字段, 叫作 tolerations</strong>. 这个字段意味着这个 Pod, 会 &quot;容忍&quot;(Toleration)某些 Node 的 &quot;污点&quot;(Taint). 而 DaemonSet 自动加上的 tolerations 字段, 格式如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> with<span class="token punctuation">-</span>toleration
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node.kubernetes.io/unschedulable
    <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists
    <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>这个 Toleration 的含义是: &quot;容忍&quot; 所有被标记为 unschedulable &quot;污点&quot; 的 Node;  <strong>&quot;容忍&quot; 的效果是允许调度</strong>. 关于如何给一个 Node 标记上 &quot;污点&quot;, 以及这里具体的语法定义, 会在后面介绍调度器的时候做详细介绍. 这里可以简单地把 &quot;污点&quot; 理解为一种特殊的 <strong>Label</strong>.</p> <p>而在正常情况下, 被标记了 unschedulable &quot;污点&quot; 的 Node, <strong>是不会有任何 Pod 被调度上去</strong>的(effect: NoSchedule). 可是, DaemonSet 自动地给被管理的 Pod 加上了这个特殊的 Toleration, 就<strong>使得这些 Pod 可以忽略这个限制, 继而保证每个节点上都会被调度一个 Pod</strong>. 当然如果这个节点有故障的话, 这个 Pod 可能会启动失败, 而 DaemonSet 则会始终尝试下去, 直到 Pod 启动成功.</p> <p>这时应该可以猜到, 在前面介绍到的 <strong>DaemonSet 的&quot;过人之处&quot;, 其实就是</strong>​<mark><strong>依靠 Toleration 实现</strong></mark>​**的. **</p> <p>假如当前 DaemonSet 管理的, 是一个<strong>网络插件的 Agent Pod</strong>, 那么就必须在这个 DaemonSet 的 <strong>YAML</strong> 文件里, 给它的 <strong>Pod 模板</strong>加上一个能够 &quot;容忍&quot; <code>node.kubernetes.io/network-unavailable</code>​ &quot;污点&quot; 的 Toleration. 正如下面这个例子所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> network<span class="token punctuation">-</span>plugin<span class="token punctuation">-</span>agent
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node.kubernetes.io/network<span class="token punctuation">-</span>unavailable
        <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists
        <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>在 Kubernetes 项目中, 当一个节点的网络插件<strong>尚未安装</strong>时, 这个节点就会被<strong>自动加上</strong>名为 <code>node.kubernetes.io/network-unavailable</code>​ 的&quot;污点&quot;. <mark><strong>而通过这样一个 Toleration, 调度器在调度这个 Pod 的时候, 就会忽略当前节点上的 &quot;污点&quot;, 从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来</strong></mark>​ **. **</p> <p>这种机制, 正是在部署 Kubernetes 集群的时候, 能够<strong>先部署 Kubernetes 本身, 再部署网络插件的根本原因</strong>: 因为当时所创建的 Weave 的 YAML, 实际上就是一个 DaemonSet.</p> <p><strong>通过上面这些内容应该能够明白, DaemonSet 其实是一个非常简单的控制器. 在它的控制循环中, 只需要遍历所有节点, 然后根据节点上是否有被管理 Pod 的情况, 来决定是否要创建或者删除一个 Pod. 只不过, 在创建每个 Pod 的时候, DaemonSet 会自动给这个 Pod 加上一个 nodeAffinity, 从而保证这个 Pod 只会在指定节点上启动. 同时它还会自动给这个 Pod 加上一个 Toleration, 从而忽略节点的 unschedulable &quot;污点&quot;.</strong></p> <p>当然, <strong>也可以在 Pod 模板里加上更多种类的 Toleration, 从而利用 DaemonSet 实现自己的目的</strong>. 比如在这个 fluentd-elasticsearch DaemonSet 里, 可以给它加上了这样的 Toleration:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master
  <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这是因为在默认情况下, Kubernetes 集群<strong>不允许用户在 Master 节点部署 Pod</strong>. 因为 Master 节点默认携带了一个叫作 <code>node-role.kubernetes.io/master</code>​ 的&quot;污点&quot;. 所以为了能在 Master 节点上部署 DaemonSet 的 Pod, 就必须让这个 Pod &quot;容忍&quot; 这个 &quot;污点&quot;.</p> <p>在理解了 DaemonSet 的工作原理之后, 接下来就通过一个具体的实践来深入地掌握 DaemonSet 的使用方法. 需要注意的是, 在 Kubernetes v1.11 之前, 由于调度器尚不完善, DaemonSet 是由 DaemonSet Controller 自行调度的, 即它会直接设置 Pod 的 spec.nodename 字段, 这样就可以跳过调度器了. 但这样的做法很快就会被废除, 所以在这里也不推荐再花时间学习这个流程了.</p> <p>**首先, 创建这个 DaemonSet 对象: **</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl create <span class="token punctuation">-</span>f fluentd<span class="token punctuation">-</span>elasticsearch.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>需要注意的是, <strong>在 DaemonSet 上, 一般都应该加上 resources 字段, 来限制它的 CPU 和内存使用, 防止它占用过多的宿主机资源</strong>.</p> <p>而创建成功后就能看到, 如果有 N 个节点, 就会有 N 个 fluentd-elasticsearch Pod 在运行. 比如在这个例子里, 会有两个 Pod, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod <span class="token parameter variable">-n</span> kube-system <span class="token parameter variable">-l</span> <span class="token assign-left variable">name</span><span class="token operator">=</span>fluentd-elasticsearch
NAME                          READY     STATUS    RESTARTS   AGE
fluentd-elasticsearch-dqfv9   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          53m
fluentd-elasticsearch-pf9z5   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          53m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>而如果此时通过 <strong>kubectl get</strong> 查看一下 Kubernetes 集群里的 DaemonSet 对象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get ds <span class="token parameter variable">-n</span> kube-system fluentd-elasticsearch
NAME                    DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
fluentd-elasticsearch   <span class="token number">2</span>         <span class="token number">2</span>         <span class="token number">2</span>         <span class="token number">2</span>            <span class="token number">2</span>           <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>          1h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意 Kubernetes 里比较长的 API 对象都有<strong>短名字</strong>, 比如 DaemonSet 对应的是 ds, Deployment 对应的是 deploy.</p> <p>可以发现 <strong>DaemonSet</strong> 和 Deployment 一样, 也有 <strong>DESIRED, CURRENT 等多个状态字段</strong>. 这也就意味着, DaemonSet 可以像 Deployment 那样, 进行<strong>版本管理</strong>. 这个版本可以使用 kubectl rollout history 看到:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout <span class="token function">history</span> daemonset fluentd-elasticsearch <span class="token parameter variable">-n</span> kube-system
daemonsets <span class="token string">&quot;fluentd-elasticsearch&quot;</span>
REVISION  CHANGE-CAUSE
<span class="token number">1</span>         <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>**接下来, 来把这个 DaemonSet 的容器镜像版本到 v2.2.0: **</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">set</span> image ds/fluentd-elasticsearch fluentd-elasticsearch<span class="token operator">=</span>k8s.gcr.io/fluentd-elasticsearch:v2.2.0 <span class="token parameter variable">--record</span> <span class="token parameter variable">-n</span><span class="token operator">=</span>kube-system
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个 kubectl set image 命令里, 第一个 fluentd-elasticsearch 是 <strong>DaemonSet</strong> 的名字, 第二个 fluentd-elasticsearch 是<strong>容器</strong>的名字.</p> <p>这时候可以使用 kubectl rollout status 命令看到这个 &quot;滚动更新&quot; 的过程, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout status ds/fluentd-elasticsearch <span class="token parameter variable">-n</span> kube-system
Waiting <span class="token keyword">for</span> daemon <span class="token builtin class-name">set</span> <span class="token string">&quot;fluentd-elasticsearch&quot;</span> rollout to finish: <span class="token number">0</span> out of <span class="token number">2</span> new pods have been updated<span class="token punctuation">..</span>.
Waiting <span class="token keyword">for</span> daemon <span class="token builtin class-name">set</span> <span class="token string">&quot;fluentd-elasticsearch&quot;</span> rollout to finish: <span class="token number">0</span> out of <span class="token number">2</span> new pods have been updated<span class="token punctuation">..</span>.
Waiting <span class="token keyword">for</span> daemon <span class="token builtin class-name">set</span> <span class="token string">&quot;fluentd-elasticsearch&quot;</span> rollout to finish: <span class="token number">1</span> of <span class="token number">2</span> updated pods are available<span class="token punctuation">..</span>.
daemon <span class="token builtin class-name">set</span> <span class="token string">&quot;fluentd-elasticsearch&quot;</span> successfully rolled out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>注意由于这一次在升级命令后面加上了  <strong>–record 参数</strong>, 所以这次升级使用到的指令就会自动出现在 DaemonSet 的 <strong>rollout history</strong> 里面, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout <span class="token function">history</span> daemonset fluentd-elasticsearch <span class="token parameter variable">-n</span> kube-system
daemonsets <span class="token string">&quot;fluentd-elasticsearch&quot;</span>
REVISION  CHANGE-CAUSE
<span class="token number">1</span>         <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token number">2</span>         kubectl <span class="token builtin class-name">set</span> image ds/fluentd-elasticsearch fluentd-elasticsearch<span class="token operator">=</span>k8s.gcr.io/fluentd-elasticsearch:v2.2.0 <span class="token parameter variable">--namespace</span><span class="token operator">=</span>kube-system <span class="token parameter variable">--record</span><span class="token operator">=</span>true
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>有了版本号, 也就可以像 Deployment 一样, 将 DaemonSet <strong>回滚</strong>到某个指定的历史版本了.</p> <p>而在讲解 Deployment 对象的时候曾经提到过, Deployment 管理这些版本, 靠的是 &quot;<strong>一个版本对应一个 ReplicaSet 对象</strong>&quot;. 可是, DaemonSet 控制器<strong>操作的直接就是 Pod</strong>, 不可能有 ReplicaSet 这样的对象参与其中. **那么它的这些版本又是如何维护的呢? **</p> <p>**所谓, 一切皆对象! ** 在 Kubernetes 项目中, 任何你觉得<mark><strong>需要记录下来的状态, 都可以被用 API 对象的方式实现</strong></mark>. 当然, &quot;<strong>版本</strong>&quot; 也不例外.</p> <p>Kubernetes v1.7 之后添加了一个 API 对象, 名叫 <mark><strong>ControllerRevision</strong></mark>, 专门用来<strong>记录某种 Controller 对象的版本</strong>. 比如可以通过如下命令查看 fluentd-elasticsearch 对应的 ControllerRevision:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get controllerrevision <span class="token parameter variable">-n</span> kube-system <span class="token parameter variable">-l</span> <span class="token assign-left variable">name</span><span class="token operator">=</span>fluentd-elasticsearch
NAME                               CONTROLLER                             REVISION   AGE
fluentd-elasticsearch-64dc6799c9   daemonset.apps/fluentd-elasticsearch   <span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>而如果使用 kubectl describe 查看这个 ControllerRevision 对象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 <span class="token parameter variable">-n</span> kube-system
Name:         fluentd-elasticsearch-64dc6799c9
Namespace:    kube-system
Labels:       controller-revision-hash<span class="token operator">=</span><span class="token number">2087235575</span>
              <span class="token assign-left variable">name</span><span class="token operator">=</span>fluentd-elasticsearch
Annotations:  <span class="token assign-left variable">deprecated.daemonset.template.generation</span><span class="token operator">=</span><span class="token number">2</span>
              kubernetes.io/change-cause<span class="token operator">=</span>kubectl <span class="token builtin class-name">set</span> image ds/fluentd-elasticsearch fluentd-elasticsearch<span class="token operator">=</span>k8s.gcr.io/fluentd-elasticsearch:v2.2.0 <span class="token parameter variable">--record</span><span class="token operator">=</span>true <span class="token parameter variable">--namespace</span><span class="token operator">=</span>kube-system
API Version:  apps/v1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <span class="token operator">&lt;</span>nil<span class="token operator">&gt;</span>
        Labels:
          Name:  fluentd-elasticsearch
      Spec:
        Containers:
          Image:              k8s.gcr.io/fluentd-elasticsearch:v2.2.0
          Image Pull Policy:  IfNotPresent
          Name:               fluentd-elasticsearch
<span class="token punctuation">..</span>.
Revision:                  <span class="token number">2</span>
Events:                    <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p>就会看到, 这个 ControllerRevision 对象, <strong>实际上是在 Data 字段保存了该版本对应的完整的 DaemonSet 的 API 对象</strong>. 并且在 Annotation 字段保存了创建这个对象所使用的 kubectl 命令.</p> <p>接下来可以尝试将这个 DaemonSet 回滚到 Revision=1 时的状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision<span class="token operator">=</span><span class="token number">1</span> <span class="token parameter variable">-n</span> kube-system
daemonset.extensions/fluentd-elasticsearch rolled back
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个 kubectl rollout undo 操作, 实际上相当于读取到了 <strong>Revision=1 的 ControllerRevision 对象保存的 Data 字段</strong>. 而这个 Data 字段里保存的信息, 就是 Revision=1 时这个 DaemonSet 的<strong>完整 API 对象</strong>.</p> <p>所以, 现在 DaemonSet Controller 就可以使用这个历史 API 对象, <strong>对现有的 DaemonSet 做一次 PATCH 操作(等价于执行一次 kubectl apply -f &quot;旧的 DaemonSet 对象&quot;), 从而把这个 DaemonSet &quot;更新&quot; 到一个旧版本</strong>.</p> <p>这也是为什么在执行完这次回滚完成后会发现, DaemonSet 的 Revision 并不会从 Revision=2 退回到 1, 而是会<strong>增加成 Revision=3</strong>. 这是因为一个<strong>新的 ControllerRevision 被创建</strong>了出来.</p> <blockquote><p>总结</p></blockquote> <p>本节首先简单介绍了 StatefulSet 的 &quot;滚动更新&quot;, 然后重点讲解了第三个重要编排对象: <strong>DaemonSet</strong>. 相比于 Deployment, <strong>DaemonSet 只管理 Pod 对象, 然后通过 nodeAffinity 和 Toleration 这两个调度器的小功能, 保证了每个节点上有且只有一个 Pod.</strong>  这个控制器的实现原理简单易懂, 希望你能够快速掌握.</p> <p>与此同时, DaemonSet 使用 <strong>ControllerRevision</strong>, 来保存和管理自己对应的 &quot;版本&quot;. 这种 &quot;面向 API 对象&quot; 的设计思路, 大大简化了控制器本身的逻辑, 也正是 Kubernetes 项目 &quot;声明式 API&quot; 的优势所在.</p> <p>而且相信聪明的你此时已经想到了, <strong>StatefulSet 也是直接控制 Pod 对象的</strong>, 那么它是不是也在使用 ControllerRevision 进行版本管理呢? 没错. 在 Kubernetes 项目里, <strong>ControllerRevision 其实是一个通用的版本管理对象</strong>. 这样 Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题.</p> <h4 id="_22-撬动离线业务-job与cronjob"><a href="#_22-撬动离线业务-job与cronjob" class="header-anchor">#</a> 22 | 撬动离线业务:Job与CronJob</h4> <p>前面几节详细分享了 Deployment, StatefulSet, 以及 DaemonSet 这三个编排概念. 你有没有发现它们的共同之处呢?</p> <p>实际上, 它们<strong>主要编排的对象, 都是 &quot;在线业务&quot;, 即: Long Running Task(长作业)</strong> . 比如前面举例时常用的 Nginx, Tomcat, 以及 MySQL 等等. 这些应用一旦运行起来, 除非出错或者停止, 它的容器进程会<strong>一直保持在 Running 状态</strong>.</p> <p>但是有一类作业显然不满足这样的条件, 这就是 &quot;<strong>离线业务</strong>&quot;, 或者叫作 <strong>Batch Job</strong>(计算业务). 这种业务在<strong>计算完成后就直接退出</strong>了, 而此时如果依然用 Deployment 来管理这种业务的话, 就会发现 Pod 会在计算结束后退出, 然后被 Deployment Controller <strong>不断地重启</strong>; 而像 &quot;滚动更新&quot; 这样的编排功能, 更无从谈起了.</p> <p>所以, 早在 Borg 项目中, Google 就已经<strong>对作业进行了分类处理</strong>, 提出了 <strong>LRS</strong>(Long Running Service)和 <strong>Batch Jobs</strong> 两种作业形态, 对它们进行 &quot;分别管理&quot; 和 &quot;混合调度&quot;. 不过, 在 2015 年 Borg 论文刚刚发布的时候, Kubernetes 项目并不支持对 Batch Job 的管理. 直到 v1.4 版本之后, 社区才逐步设计出了一个用来描述离线业务的 API 对象, 它的名字就是: <strong>Job</strong>.</p> <p>Job API 对象的定义非常简单, 举个例子, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pi
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pi
        <span class="token key atrule">image</span><span class="token punctuation">:</span> resouer/ubuntu<span class="token punctuation">-</span>bc 
        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sh&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;echo 'scale=10000; 4*a(1)' | bc -l &quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never
  <span class="token key atrule">backoffLimit</span><span class="token punctuation">:</span> <span class="token number">4</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>此时, 相信你对 Kubernetes 的 API 对象已经不再陌生了. 在这个 Job 的 YAML 文件里, 肯定一眼就会看到一位&quot;老熟人&quot;: <strong>Pod 模板, 即 spec.template 字段</strong>.</p> <p>在这个 Pod 模板中, 定义了一个 Ubuntu 镜像的容器(准确地说是一个安装了 bc 命令的 Ubuntu 镜像), 它运行的程序是:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token builtin class-name">echo</span> <span class="token string">&quot;scale=10000; 4*a(1)&quot;</span> <span class="token operator">|</span> <span class="token function">bc</span> <span class="token parameter variable">-l</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中, bc 命令是 Linux 里的 &quot;计算器&quot;; -l 表示要使用标准数学库; 而 a(1), 则是调用数学库中的 arctangent 函数, 计算 atan(1). 这是什么意思呢? <code>tan(π/4) = 1</code>​. 所以, <code>4*atan(1)</code>​ 正好就是 π, 也就是 3.1415926... 这其实就是一个计算 π 值的容器. 而通过 scale=10000, 指定了输出的小数点后的位数是 10000. 在实验的计算机上, 这个计算大概用时 2 分钟.</p> <p>跟其他控制器不同的是, <strong>Job 对象并不要求定义一个 spec.selector 来描述要控制哪些 Pod</strong>. 具体原因马上会讲解到.</p> <p>现在就可以创建这个 Job 了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> job.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在成功创建后来查看一下这个 <strong>Job 对象</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe jobs/pi
Name:             pi
Namespace:        default
Selector:         controller-uid<span class="token operator">=</span>c2db599a-2c9d-11e6-b324-0209dc45a495
Labels:           controller-uid<span class="token operator">=</span>c2db599a-2c9d-11e6-b324-0209dc45a495
                  job-name<span class="token operator">=</span>pi
Annotations:      <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Parallelism:      <span class="token number">1</span>
Completions:      <span class="token number">1</span>
<span class="token punctuation">..</span>
Pods Statuses:    <span class="token number">0</span> Running / <span class="token number">1</span> Succeeded / <span class="token number">0</span> Failed
Pod Template:
  Labels:       controller-uid<span class="token operator">=</span>c2db599a-2c9d-11e6-b324-0209dc45a495
                job-name<span class="token operator">=</span>pi
  Containers:
   <span class="token punctuation">..</span>.
  Volumes:              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Events:
  FirstSeen    LastSeen    Count    From            SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----            -------------    --------    ------            -------
  1m           1m          <span class="token number">1</span>        <span class="token punctuation">{</span>job-controller <span class="token punctuation">}</span>                Normal      SuccessfulCreate  Created pod: pi-rq5rl
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>可以看到, 这个 Job 对象在创建后, 它的 Pod 模板, 被<strong>自动</strong>加上了一个 <code>controller-uid=&lt;一个随机字符串&gt;</code>​ 这样的 Label. <strong>而这个 Job 对象本身, 则被自动加上了这个 Label 对应的 Selector, 从而保证了 Job 与它所管理的 Pod 之间的匹配关系</strong>.</p> <p>而 Job Controller 之所以要使用这种携带了 UID 的 Label, 就是为了<strong>避免不同 Job 对象所管理的 Pod 发生重合</strong>. 需要注意的是, **这种自动生成的 Label 对用户来说并不友好, 所以不太适合推广到 Deployment 等长作业编排对象上. **</p> <p>接下来可以看到这个 Job 创建的 Pod 进入了 Running 状态, 这意味着它正在计算 Pi 的值.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
pi-rq5rl                            <span class="token number">1</span>/1       Running   <span class="token number">0</span>          10s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>而几分钟后计算结束, 这个 Pod 就会进入 <strong>Completed</strong> 状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME                                READY     STATUS      RESTARTS   AGE
pi-rq5rl                            <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          4m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这也是需要在 Pod 模板中定义 <strong>restartPolicy=Never</strong> 的原因: <strong>离线计算的 Pod 永远都不应该被重启, 否则它们会再重新计算一遍</strong>.</p> <blockquote><p>事实上, restartPolicy 在 Job 对象里只允许被设置为 Never 和 OnFailure; 而在 Deployment 对象里, restartPolicy 则只允许被设置为 Always.</p></blockquote> <p>此时通过 kubectl logs 查看一下这个 Pod 的日志, 就可以看到计算得到的 Pi 值已经被打印了出来:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs pi-rq5rl
<span class="token number">3.141592653589793238462643383279</span><span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这时候你一定会想到这样一个问题, **如果这个离线作业失败了要怎么办? **</p> <p>比如在这个例子中<strong>定义了 restartPolicy=Never, 那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME                                READY     STATUS              RESTARTS   AGE
pi-55h89                            <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>          2s
pi-tqbcz                            <span class="token number">0</span>/1       Error               <span class="token number">0</span>          5s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 这时候会不断地有新 Pod 被创建出来.</p> <p>当然, 这个尝试肯定不能无限进行下去. 所以就在 Job 对象的 <strong>spec.backoffLimit 字段里定义了重试次数</strong>为 4(即, backoffLimit=4), 而这个字段的默认值是 6.</p> <p>需要注意的是, Job Controller 重新创建 Pod 的间隔是呈<strong>指数增加</strong>的, 即下一次重新创建 Pod 的动作会分别发生在 10 s, 20 s, 40 s... 后.</p> <p>而如果<strong>定义的 restartPolicy=OnFailure, 那么离线作业失败后, Job Controller 就不会去尝试创建新的 Pod. 但是它会不断地尝试重启 Pod 里的容器</strong>. 这也正好对应了 restartPolicy 的含义.</p> <p>如前所述, 当一个 Job 的 Pod 运行结束后, 它会进入 <strong>Completed</strong> 状态. 但是如果这个 Pod 因为某种原因<strong>一直不肯结束</strong>呢?</p> <p>在 Job 的 API 对象里, 有一个 <strong>spec.activeDeadlineSeconds 字段可以设置最长运行时间</strong>, 比如:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
 <span class="token key atrule">backoffLimit</span><span class="token punctuation">:</span> <span class="token number">5</span>
 <span class="token key atrule">activeDeadlineSeconds</span><span class="token punctuation">:</span> <span class="token number">100</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>一旦运行超过了 100 s, 这个 Job 的所有 Pod 都会被终止. 并且可以在 Pod 的状态里看到终止的原因是 reason: <strong>DeadlineExceeded</strong>.</p> <p>以上就是一个 Job API 对象最主要的概念和用法了. 不过, 离线业务之所以被称为 Batch Job, 当然是因为它们可以以 &quot;Batch&quot;, 也就是<strong>并行的方式去运行</strong>.</p> <p>接下来就来讲解一下 <strong>Job Controller 对并行作业的控制方法</strong>.</p> <p>在 Job 对象中, 负责并行控制的参数有两个:</p> <ol><li><strong>spec.parallelism: 它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行;</strong></li> <li><strong>spec.completions: 它定义的是 Job 至少要完成的 Pod 数目, 即 Job 的最小完成数.</strong></li></ol> <p>这两个参数听起来有点儿抽象, 下面通过一个例子来帮助理解. 现在在之前计算 Pi 值的 Job 里, 添加这两个参数:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pi
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">parallelism</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">completions</span><span class="token punctuation">:</span> <span class="token number">4</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pi
        <span class="token key atrule">image</span><span class="token punctuation">:</span> resouer/ubuntu<span class="token punctuation">-</span>bc
        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sh&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;echo 'scale=5000; 4*a(1)' | bc -l &quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never
  <span class="token key atrule">backoffLimit</span><span class="token punctuation">:</span> <span class="token number">4</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>这样就指定了这个 Job 最大的并行数是 2, 而最小的完成数是 4.</p> <p>接下来创建这个 Job 对象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> job.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到, 这个 Job 其实也<strong>维护了两个状态字段, 即 DESIRED 和 SUCCESSFUL</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get job
NAME      DESIRED   SUCCESSFUL   AGE
pi        <span class="token number">4</span>         <span class="token number">0</span>            3s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>其中, DESIRED 的值, 正是 completions 定义的<strong>最小完成数</strong>.</p> <p>然后可以看到, 这个 Job 首先创建了<strong>两个并行运行的 Pod</strong> 来计算 Pi:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get job
NAME      DESIRED   SUCCESSFUL   AGE
pi        <span class="token number">4</span>         <span class="token number">0</span>            3s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>而在 40 s 后, 这两个 Pod 相继<strong>完成计算</strong>.</p> <p>这时可以看到, 每当有一个 Pod 完成计算进入 Completed 状态时, 就会<strong>有一个新的 Pod 被自动创建出来, 并且快速地从 Pending 状态进入到 ContainerCreating 状态</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME       READY     STATUS    RESTARTS   AGE
pi-gmcq5   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>         40s
pi-84ww8   <span class="token number">0</span>/1       Pending   <span class="token number">0</span>         0s
pi-5mt88   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>         41s
pi-62rbt   <span class="token number">0</span>/1       Pending   <span class="token number">0</span>         0s
 
$ kubectl get pods
NAME       READY     STATUS    RESTARTS   AGE
pi-gmcq5   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>         40s
pi-84ww8   <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>         0s
pi-5mt88   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>         41s
pi-62rbt   <span class="token number">0</span>/1       ContainerCreating   <span class="token number">0</span>         0s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>紧接着, Job Controller 第二次创建出来的两个并行的 Pod 也进入了 Running 状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods 
NAME       READY     STATUS      RESTARTS   AGE
pi-5mt88   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          54s
pi-62rbt   <span class="token number">1</span>/1       Running     <span class="token number">0</span>          13s
pi-84ww8   <span class="token number">1</span>/1       Running     <span class="token number">0</span>          14s
pi-gmcq5   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          54s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>最终后面创建的这两个 Pod 也完成了计算, 进入了 Completed 状态.</p> <p>这时, 由于所有的 Pod 均已经成功退出, 这个 Job 也就<strong>执行完了</strong>, 所以可以看到它的 SUCCESSFUL 字段的值变成了 4:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods 
NAME       READY     STATUS      RESTARTS   AGE
pi-5mt88   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          5m
pi-62rbt   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          4m
pi-84ww8   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          4m
pi-gmcq5   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          5m
 
$ kubectl get job
NAME      DESIRED   SUCCESSFUL   AGE
pi        <span class="token number">4</span>         <span class="token number">4</span>            5m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>通过上述 Job 的 <strong>DESIRED 和 SUCCESSFUL 字段</strong>的关系, 就可以很容易地理解 Job Controller 的工作原理了.</p> <p>**首先, Job Controller 控制的对象, 直接就是 Pod. **</p> <p>**其次, Job Controller 在控制循环中进行的调谐(Reconcile)操作, 是根据实际在 Running 状态 Pod 的数目, 已经成功退出的 Pod 的数目, 以及 parallelism, completions 参数的值共同计算出在这个周期里, 应该创建或者删除的 Pod 数目, 然后调用 Kubernetes API 来执行这个操作. **</p> <p>以创建 Pod 为例. 在上面计算 Pi 值的这个例子中, 当 Job 一开始创建出来时, 实际处于 Running 状态的 Pod 数目 = 0, 已经成功退出的 Pod 数目 = 0, 而用户定义的 completions, 也就是最终用户需要的 Pod 数目 = 4.</p> <p>所以, 在这个时刻, 需要<strong>创建的 Pod 数目 = 最终需要的 Pod 数目 - 实际在 Running 状态 Pod 数目 - 已经成功退出的 Pod 数目 = 4 - 0 - 0 = 4</strong>. 也就是说, Job Controller 需要创建 4 个 Pod 来纠正这个不一致状态.</p> <p>可是又定义了这个 Job 的 parallelism=2. 也就是规定了每次并发创建的 Pod 个数不能超过 2 个. 所以, Job Controller 会对前面的计算结果做一个修正, 修正后的<strong>期望创建的 Pod 数目应该是: 2 个</strong>.</p> <p>这时候, Job Controller 就会并发地向 kube-apiserver 发起两个创建 Pod 的请求.</p> <p>类似地, 如果在这次<strong>调谐周期</strong>里, Job Controller 发现实际在 Running 状态的 Pod 数目, 比 parallelism 还大, 那么它就会删除一些 Pod, 使两者相等.</p> <p>综上所述, Job Controller 实际上控制了, <strong>作业执行的</strong>​<mark><strong>并行度</strong></mark>​ <strong>, 以及总共</strong>​<mark><strong>需要完成的任务数</strong></mark>​<strong>这两个重要参数</strong>. 而在实际使用时, <strong>需要根据作业的特性, 来决定并行度(parallelism)和任务数(completions)的合理取值</strong>.</p> <p>接下来再和分享三种常用的, 使用 Job 对象的方法.</p> <p>**第一种用法, 也是最简单粗暴的用法: 外部管理器 + Job 模板. **</p> <p>这种模式的特定用法是: 把 Job 的 YAML 文件定义为一个 &quot;模板&quot;, 然后<strong>用一个外部工具控制这些 &quot;模板&quot; 来生成 Job</strong>. 这时, Job 的定义方式如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> process<span class="token punctuation">-</span>item<span class="token punctuation">-</span>$ITEM
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">jobgroup</span><span class="token punctuation">:</span> jobexample
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> jobexample
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">jobgroup</span><span class="token punctuation">:</span> jobexample
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> c
        <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sh&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;echo Processing item $ITEM &amp;&amp; sleep 5&quot;</span><span class="token punctuation">]</span>
      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>可以看到, 在这个 Job 的 YAML 里, 定义了 $ITEM 这样的 &quot;变量&quot;.</p> <p>所以, 在控制这种 Job 时, 只要注意如下两个方面即可:</p> <ol><li>创建 Job 时, 替换掉 $ITEM 这样的变量;</li> <li>所有来自于同一个模板的 Job, 都有一个 jobgroup: jobexample 标签, 也就是说这一组 Job 使用这样一个<strong>相同的标识</strong>.</li></ol> <p>而做到第一点非常简单. 比如可以通过这样一句 shell 把 $ITEM 替换掉:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">mkdir</span> ./jobs
$ <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> apple banana cherry
<span class="token keyword">do</span>
  <span class="token function">cat</span> job-tmpl.yaml <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">&quot;s/\<span class="token variable">$ITEM</span>/<span class="token variable">$i</span>/&quot;</span> <span class="token operator">&gt;</span> ./jobs/job-<span class="token variable">$i</span>.yaml
<span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这样, 一组来自于同一个模板的不同 Job 的 yaml 就生成了. 接下来就可以通过一句 kubectl create 指令创建这些 Job 了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> ./jobs
$ kubectl get pods <span class="token parameter variable">-l</span> <span class="token assign-left variable">jobgroup</span><span class="token operator">=</span>jobexample
NAME                        READY     STATUS      RESTARTS   AGE
process-item-apple-kixwv    <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          4m
process-item-banana-wrsf7   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          4m
process-item-cherry-dnfu9   <span class="token number">0</span>/1       Completed   <span class="token number">0</span>          4m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这个模式看起来虽然很&quot;傻&quot;, 但却是 Kubernetes 社区里使用 Job 的一个很普遍的模式.</p> <p>原因很简单: <strong>大多数用户在需要管理 Batch Job 的时候, 都已经有了一套自己的方案, 需要做的往往就是集成工作</strong>. 这时候, Kubernetes 项目对这些方案来说最有价值的, 就是 Job 这个 API 对象. 所以, 只需要编写一个外部工具(等同于这里的 for 循环)来管理这些 Job 即可.</p> <p>这种模式最典型的应用, 就是 TensorFlow 社区的 KubeFlow 项目. 很容易理解, 在这种模式下使用 Job 对象, completions 和 parallelism 这两个字段都应该使用默认值 1, 而不应该由我们自行设置. 而作业 Pod 的并行控制, 应该完全交由外部工具来进行管理(比如, KubeFlow).</p> <p><strong>第二种用法: 拥有固定任务数目的并行 Job</strong>.</p> <p>这种模式下, 只关心最后是否有指定数目(spec.completions)个任务成功退出. 至于执行时的并行度是多少, 则并不关心.</p> <p>比如这个计算 Pi 值的例子, 就是这样一个典型的, 拥有<strong>固定任务数目</strong>(completions=4)的应用场景. 它的 parallelism 值是 2; 或者可以干脆不指定 parallelism, 直接使用默认的并行度(即: 1).</p> <p>此外还可以使用一个<strong>工作队列</strong>(Work Queue)进行任务分发. 这时, Job 的 YAML 文件定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> job<span class="token punctuation">-</span>wq<span class="token punctuation">-</span><span class="token number">1</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">completions</span><span class="token punctuation">:</span> <span class="token number">8</span>
  <span class="token key atrule">parallelism</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> job<span class="token punctuation">-</span>wq<span class="token punctuation">-</span><span class="token number">1</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> c
        <span class="token key atrule">image</span><span class="token punctuation">:</span> myrepo/job<span class="token punctuation">-</span>wq<span class="token punctuation">-</span><span class="token number">1</span>
        <span class="token key atrule">env</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> BROKER_URL
          <span class="token key atrule">value</span><span class="token punctuation">:</span> amqp<span class="token punctuation">:</span>//guest<span class="token punctuation">:</span>guest@rabbitmq<span class="token punctuation">-</span>service<span class="token punctuation">:</span><span class="token number">5672</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> QUEUE
          <span class="token key atrule">value</span><span class="token punctuation">:</span> job1
      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>可以看到, 它的 completions 的值是: 8, 这意味着总共要处理的任务数目是 8 个. 也就是说, 总共会有 8 个任务会被逐一放入工作队列里(可以运行一个外部小程序作为生产者来提交任务).</p> <p>在这个实例中, 选择充当工作队列的是一个运行在 Kubernetes 里的 RabbitMQ. 所以, 需要在 Pod 模板里定义 BROKER_URL, 来作为消费者.</p> <p>所以, 一旦用 kubectl create 创建了这个 Job, 它就会以并发度为 2 的方式, 每两个 Pod 一组, 创建出 8 个 Pod. 每个 Pod 都会去<strong>连接 BROKER_URL, 从 RabbitMQ 里读取任务, 然后各自进行处理</strong>. 这个 Pod 里的执行逻辑, 可以用这样一段伪代码来表示:</p> <div class="language-c line-numbers-mode"><pre class="language-c"><code><span class="token comment">/* job-wq-1 的伪代码 */</span>
queue <span class="token operator">:</span><span class="token operator">=</span> <span class="token function">newQueue</span><span class="token punctuation">(</span>$BROKER_URL<span class="token punctuation">,</span> $QUEUE<span class="token punctuation">)</span>
task <span class="token operator">:</span><span class="token operator">=</span> queue<span class="token punctuation">.</span><span class="token function">Pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token function">process</span><span class="token punctuation">(</span>task<span class="token punctuation">)</span>
exit
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, 每个 Pod 只需要将任务信息读取出来, 处理完成, 然后退出即可. 而作为用户, 只关心最终一共有 8 个计算任务启动并且退出, 只要这个目标达到, 就认为整个 Job 处理完成了. 所以这种用法, 对应的就是&quot;任务总数固定&quot;的场景.</p> <p>**第三种用法, 也是很常用的一个用法: 指定并行度(parallelism), 但不设置固定的 completions 的值. **</p> <p>此时就必须自己想办法, 来决定什么时候启动新 Pod, 什么时候 Job 才算执行完成. 在这种情况下, <strong>任务的总数是未知的</strong>, 所以<strong>不仅需要一个工作队列来负责任务分发, 还需要能够判断工作队列已经为空</strong>(即: 所有的工作已经结束了).</p> <p>这时候, Job 的定义基本上没变化, 只不过是不再需要定义 completions 的值了而已:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> job<span class="token punctuation">-</span>wq<span class="token punctuation">-</span><span class="token number">2</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">parallelism</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> job<span class="token punctuation">-</span>wq<span class="token punctuation">-</span><span class="token number">2</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> c
        <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/myproject/job<span class="token punctuation">-</span>wq<span class="token punctuation">-</span><span class="token number">2</span>
        <span class="token key atrule">env</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> BROKER_URL
          <span class="token key atrule">value</span><span class="token punctuation">:</span> amqp<span class="token punctuation">:</span>//guest<span class="token punctuation">:</span>guest@rabbitmq<span class="token punctuation">-</span>service<span class="token punctuation">:</span><span class="token number">5672</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> QUEUE
          <span class="token key atrule">value</span><span class="token punctuation">:</span> job2
      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>而对应的 Pod 的逻辑会稍微复杂一些, 可以用这样一段伪代码来描述:</p> <div class="language-c line-numbers-mode"><pre class="language-c"><code><span class="token comment">/* job-wq-2 的伪代码 */</span>
<span class="token keyword">for</span> <span class="token operator">!</span>queue<span class="token punctuation">.</span><span class="token function">IsEmpty</span><span class="token punctuation">(</span>$BROKER_URL<span class="token punctuation">,</span> $QUEUE<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  task <span class="token operator">:</span><span class="token operator">=</span> queue<span class="token punctuation">.</span><span class="token function">Pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token function">process</span><span class="token punctuation">(</span>task<span class="token punctuation">)</span>
<span class="token punctuation">}</span>
<span class="token function">print</span><span class="token punctuation">(</span><span class="token string">&quot;Queue empty, exiting&quot;</span><span class="token punctuation">)</span>
exit
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>由于任务数目的总数不固定, 所以每一个 Pod 必须能够知道, 自己什么时候可以退出. 比如, 在这个例子中, 简单地以 &quot;队列为空&quot;, 作为任务全部完成的标志. 所以这种用法, 对应的是&quot;任务总数不固定&quot;的场景.</p> <p>不过在实际的应用中, 需要处理的条件往往会非常复杂. 比如任务完成后的输出, 每个任务 Pod 之间是不是有资源的竞争和协同等等.所以本节就不再展开 Job 的用法了. 因为在实际场景里, 要么干脆就用第一种用法来自己管理作业; 要么这些任务 Pod 之间的关系就不那么 &quot;单纯&quot;, 甚至还是 &quot;有状态应用&quot;(比如, 任务的输入/输出是在持久化数据卷里). 在这种情况下, 在后面要重点讲解的 <strong>Operator, 加上 Job 对象一起, 可能才能更好的满足实际离线任务的编排需求</strong>.</p> <p>最后, 再来分享一个非常有用的 Job 对象, 叫作: <strong>CronJob</strong>. 顾名思义, CronJob 描述的正是<strong>定时任务</strong>. 它的 API 对象, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> hello
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">schedule</span><span class="token punctuation">:</span> <span class="token string">&quot;*/1 * * * *&quot;</span>
  <span class="token key atrule">jobTemplate</span><span class="token punctuation">:</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">template</span><span class="token punctuation">:</span>
        <span class="token key atrule">spec</span><span class="token punctuation">:</span>
          <span class="token key atrule">containers</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> hello
            <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
            <span class="token key atrule">args</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> /bin/sh
            <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
            <span class="token punctuation">-</span> date; echo Hello from the Kubernetes cluster
          <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>在这个 YAML 文件中, 最重要的关键词就是 <strong>jobTemplate</strong>. 看到它, 你一定恍然大悟, 原来 <strong>CronJob 是一个 Job 对象的控制器</strong>(Controller)!</p> <p>没错, CronJob 与 Job 的关系, 正如同 Deployment 与 Pod 的关系一样. <strong>CronJob 是一个专门用来管理 Job 对象的控制器</strong>. 只不过它创建和删除 Job 的依据, 是 <strong>schedule</strong> 字段定义的, 一个标准的 <a href="https://en.wikipedia.org/wiki/Cron" target="_blank" rel="noopener noreferrer">Unix Cron<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 格式的表达式.</p> <p>比如 &quot;*/1 * * * *&quot; 这个 Cron 表达式里 */1 中的 * 表示从 0 开始, / 表示&quot;每&quot;, 1 表示偏移量. 所以, 它的意思就是: 从 0 开始, 每 1 个时间单位执行一次.</p> <p>而这里要执行的内容, 就是 <strong>jobTemplate 定义的 Job</strong> 了.</p> <p>所以这个 CronJob 对象在创建 1 分钟后, 就会有一个 Job 产生了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> ./cronjob.yaml
cronjob <span class="token string">&quot;hello&quot;</span> created
 
<span class="token comment"># 一分钟后</span>
$ kubectl get <span class="token function">jobs</span>
NAME               DESIRED   SUCCESSFUL   AGE
hello-4111706356   <span class="token number">1</span>         <span class="token number">1</span>         2s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>此时, CronJob 对象会记录下这次 Job 执行的时间:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get cronjob hello
NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE
hello     */1 * * * *   False     <span class="token number">0</span>         Thu, <span class="token number">6</span> Sep <span class="token number">2018</span> <span class="token number">14</span>:34:00 <span class="token parameter variable">-070</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>需要注意的是, 由于定时任务的特殊性, 很可能某个 Job 还<strong>没有执行完</strong>, 另外一个新 Job 就产生了. 这时候可以通过 <strong>spec.concurrencyPolicy</strong> 字段来定义具体的<strong>处理策略</strong>. 比如:</p> <ol><li>concurrencyPolicy=<strong>Allow</strong>, 这也是默认情况, 这意味着这些 Job 可以<strong>同时存在</strong>;</li> <li>concurrencyPolicy=<strong>Forbid</strong>, 这意味着不会创建新的 Pod, 该创建周期被<strong>跳过</strong>;</li> <li>concurrencyPolicy=<strong>Replace</strong>, 这意味着新产生的 Job 会<strong>替换</strong>旧的, 没有执行完的 Job.</li></ol> <p>而如果某一次 Job 创建失败, 这次创建就会被标记为 &quot;miss&quot;. 当在指定的时间窗口内, miss 的数目达到 100 时, 那么 CronJob 会<strong>停止</strong>再创建这个 Job. 这个时间窗口, 可以由 <strong>spec.startingDeadlineSeconds</strong> 字段指定. 比如 startingDeadlineSeconds=200, 意味着在过去 200s 里, 如果 miss 的数目达到了 100 次, 那么这个 Job 就不会被创建执行了.</p> <blockquote><p>总结</p></blockquote> <p>在本节分享了 Job 这个离线业务的编排方法, 讲解了 completions 和 parallelism 字段的含义, 以及 Job Controller 的执行原理. 然后通过实例和你分享了 Job 对象三种常见的使用方法. 但是根据经验, <strong>大多数情况下用户还是更倾向于自己控制 Job 对象</strong>. 所以相比于这些固定的&quot;模式&quot;, 掌握 Job 的 API 对象, 和它各个字段的准确含义会更加重要. 最后还介绍了一种 Job 的控制器, 叫作: CronJob. 这也印证了前面所说的: <strong>用一个对象控制另一个对象, 是 Kubernetes 编排的精髓所在</strong>.</p> <h4 id="_23-声明式api与kubernetes编程范式"><a href="#_23-声明式api与kubernetes编程范式" class="header-anchor">#</a> 23 | 声明式API与Kubernetes编程范式</h4> <p>前面介绍了很多 Kubernetes 的 API 对象. 这些 API 对象, 有的是用来<strong>描述应用</strong>, 有的则是<strong>为应用提供各种各样的服务</strong>. 但是, 无一例外地, 为了使用这些 API 对象提供的能力, <strong>都需要编写一个对应的 YAML 文件交给 Kubernetes</strong>.</p> <p><strong>这个 YAML 文件, 正是 Kubernetes 声明式 API 所必须具备的一个要素</strong>. 不过, 是不是只要用 YAML 文件代替了命令行操作, 就是声明式 API 了呢?</p> <p>举个例子. Docker Swarm 的编排操作都是基于<strong>命令行</strong>的, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> <span class="token function">service</span> create <span class="token parameter variable">--name</span> nginx <span class="token parameter variable">--replicas</span> <span class="token number">2</span> nginx
$ <span class="token function">docker</span> <span class="token function">service</span> update <span class="token parameter variable">--image</span> nginx:1.7.9 nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>像这样的两条命令, 就是用 Docker Swarm 启动了两个 Nginx 容器实例. 其中第一条 create 命令创建了这两个容器, 而第二条 update 命令则把它们 &quot;滚动更新&quot; 为了一个新的镜像.</p> <p>这种使用方式可以称为<strong>命令式命令行操作</strong>.</p> <p>那么像上面这样的创建和更新两个 Nginx 容器的操作, 在 Kubernetes 里又该怎么做呢? 相信你已经非常熟悉了: 需要在本地编写一个 Deployment 的 YAML 文件:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>然后还需要使用 kubectl create 命令在 Kubernetes 里<strong>创建这个 Deployment 对象</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> nginx.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样两个 Nginx 的 Pod 就会运行起来了.</p> <p>而如果要更新这两个 Pod 使用的 Nginx 镜像, 该怎么办呢? 前面曾经使用过 kubectl set image 和 kubectl edit 命令, 来直接修改 Kubernetes 里的 API 对象. 不过相信很多人都有这样的想法, <strong>能不能通过修改本地 YAML 文件来完成这个操作</strong>呢? 这样改动就会体现在这个本地 YAML 文件里了.</p> <p>当然可以.</p> <p>比如可以修改这个 YAML 文件里的 Pod 模板部分, 把 Nginx 容器的镜像改成 1.7.9, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>而接下来就可以执行一句 <strong>kubectl replace 操作</strong>, 来完成这个 Deployment 的更新:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl replace <span class="token parameter variable">-f</span> nginx.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可是上面<strong>这种基于 YAML 文件的操作方式, 是 &quot;声明式 API&quot; 吗</strong>?</p> <p><strong>并不是</strong>. 对于上面这种先 kubectl create, 再 replace 的操作, 我们称为**命令式配置文件操作. ** 也就是说它的处理方式, 其实跟前面 Docker Swarm 的两句命令, 没什么本质上的区别. 只不过, 它是把 Docker 命令行里的参数, 写在了配置文件里而已.</p> <p>**那么, 到底什么才是&quot;声明式 API&quot;呢? **</p> <p>答案是 <mark><strong>kubectl apply 命令</strong></mark>.</p> <p>前面曾经提到过这个 kubectl apply 命令, 并推荐使用它来代替 kubectl create 命令. 现在就使用 kubectl apply 命令来创建这个 Deployment:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> nginx.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样, Nginx 的 Deployment 就被创建了出来, 这看起来跟 kubectl create 的效果一样.</p> <p>然后, 我再修改一下 nginx.yaml 里定义的镜像:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这时候, 关键来了. 在<strong>修改完这个 YAML 文件之后, 不再使用 kubectl replace 命令进行更新, 而是继续执行一条 kubectl apply 命令</strong>, 即:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> nginx.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时, Kubernetes 就会立即触发这个 Deployment 的&quot;滚动更新&quot;.</p> <p>可是, 它跟 kubectl replace 命令有什么本质区别吗?</p> <p>实际上, 可以简单地理解为, <mark><strong>kubectl replace 的执行过程, 是使用新的 YAML 文件中的 API 对象, 替换原有的 API 对象; 而 kubectl apply, 则是执行了一个对原有 API 对象的 PATCH 操作</strong></mark>. 类似地, kubectl set image 和 kubectl edit 也是对已有 API 对象的修改.</p> <p>更进一步地, 这意味着 kube-apiserver 在响应命令式请求(比如, kubectl replace)的时候, 一次只能处理一个写请求, 否则会有产生冲突的可能. 而<strong>对于声明式请求</strong>(比如, kubectl apply), <strong>一次能处理多个写操作, 并且具备 Merge 能力</strong>.</p> <p>这种区别, 可能乍一听起来没那么重要. 而且正是由于要照顾到这样的 API 设计, 做同样一件事情, Kubernetes 需要的步骤往往要比其他项目多不少. 但是, 如果你仔细思考一下 Kubernetes 项目的工作流程, 就不难体会到这种声明式 API 的独到之处.</p> <p>接下来, 就以 <strong>Istio</strong> 项目为例, 来讲解一下声明式 API 在实际使用时的重要意义. 在 2017 年 5 月, Google, IBM 和 Lyft 公司, 共同宣布了 Istio 开源项目的诞生. 很快这个项目就在技术圈儿里, 掀起了一阵名叫 &quot;微服务&quot; 的热潮, 把 <strong>Service Mesh</strong> 这个新的编排概念推到了风口浪尖. <strong>而 Istio 项目, 实际上就是一个基于 Kubernetes 项目的微服务治理框架</strong>. 它的架构非常清晰, 如下所示:</p> <p><img src="/img/eae2ca5f5818764e614727fbf8ed4ac3-20230731162150-b0pfn3b.png" alt="">
在上面这个架构图中, 不难看到 Istio 项目架构的核心所在. <mark><strong>Istio 最根本的组件, 是运行在每一个应用 Pod 里的 Envoy 容器</strong></mark>. 这个 Envoy 项目是 Lyft 公司推出的一个<strong>高性能 C++ 网络代理</strong>, 也是 Lyft 公司对 Istio 项目的唯一贡献.</p> <p><mark><strong>而 Istio 项目, 则把这个代理服务以 sidecar 容器的方式, 运行在了每一个被治理的应用 Pod 中</strong></mark>. <strong>由于 Pod 里的所有容器都共享同一个 Network Namespace. 所以 Envoy 容器就能够通过配置 Pod 里的 iptables 规则, 把整个 Pod 的进出流量接管下来</strong>.</p> <p>这时候, <strong>Istio 的控制层(Control Plane)里的 Pilot 组件, 就能够通过调用每个 Envoy 容器的 API, 对这个 Envoy 代理进行配置, 从而实现微服务治理.</strong></p> <p>一起来看一个例子.</p> <p>假设这个 Istio 架构图<strong>左边的 Pod 是已经在运行的应用, 而右边的 Pod 则是刚刚上线的应用的新版本</strong>. 这时候 <strong>Pilot 通过调节这两 Pod 里的 Envoy 容器的配置</strong>, 从而<strong>将 90% 的流量分配给旧版本的应用, 将 10% 的流量分配给新版本应用</strong>, 并且还可以在后续的过程中随时调整. 这样一个典型的 &quot;灰度发布&quot; 的场景就完成了. 比如 Istio 可以调节这个流量从 90%-10%, 改到 80%-20%, 再到 50%-50%, 最后到 0%-100%, 就完成了这个灰度发布的过程.</p> <p>更重要的是, 在整个微服务治理的过程中, 无论是对 Envoy 容器的部署, 还是像上面这样对 Envoy 代理的配置, <strong>用户和应用都是完全&quot;无感&quot;的</strong>.</p> <p>这时候你可能会有所疑惑: Istio 项目明明需要在每个 Pod 里安装一个 Envoy 容器, 又怎么能做到&quot;无感&quot;的呢?</p> <p>实际上, <mark><strong>Istio 项目使用的, 是 Kubernetes 中的一个非常重要的功能, 叫作 Dynamic Admission Control</strong></mark>​ **. **</p> <p>在 Kubernetes 项目中, 当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后, 总有一些 &quot;<strong>初始化</strong>&quot; 性质的工作需要在它们被 Kubernetes 项目正式处理之前进行. 比如, <strong>自动为所有 Pod 加上某些标签(Labels)</strong> . 而这个 &quot;初始化&quot; 操作的实现, 借助的是一个叫作 <strong>Admission</strong> 的功能. 它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码, 可以<strong>选择性地被编译进 APIServer 中, 在 API 对象创建之后会被立刻调用到</strong>. 但这就意味着, 如果想要添加一些自己的规则到 Admission Controller, 就会比较困难. 因为这要求重新编译并重启 APIServer. 显然这种使用方法对 Istio 来说, 影响太大了.</p> <p>所以 <strong>Kubernetes 项目额外提供了一种 &quot;热插拔&quot; 式的 Admission 机制, 它就是 Dynamic Admission Control, 也叫作: Initializer</strong>.</p> <p>举个例子. 比如有如下所示的一个应用 Pod:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>pod
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> myapp
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'sh'</span><span class="token punctuation">,</span> <span class="token string">'-c'</span><span class="token punctuation">,</span> <span class="token string">'echo Hello Kubernetes! &amp;&amp; sleep 3600'</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>可以看到, 这个 Pod 里面<strong>只有一个用户容器</strong>, 叫作: myapp-container.</p> <p>接下来, <strong>Istio 项目要做的, 就是在这个 Pod YAML 被提交给 Kubernetes 之后, 在它对应的 API 对象里自动加上 Envoy 容器的配置</strong>, 使这个对象变成如下所示的样子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>pod
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> myapp
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'sh'</span><span class="token punctuation">,</span> <span class="token string">'-c'</span><span class="token punctuation">,</span> <span class="token string">'echo Hello Kubernetes! &amp;&amp; sleep 3600'</span><span class="token punctuation">]</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy
    <span class="token key atrule">image</span><span class="token punctuation">:</span> lyft/envoy<span class="token punctuation">:</span>845747b88f102c0fd262ab234308e9e22f693a1
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/usr/local/bin/envoy&quot;</span><span class="token punctuation">]</span>
    <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>可以看到, <mark><strong>被 Istio 处理后的这个 Pod 里, 除了用户自己定义的 myapp-container 容器之外, 多出了一个叫作 envoy 的容器, 它就是 Istio 要使用的 Envoy 代理</strong></mark>.</p> <p>那么, Istio 又是如何在用户<strong>完全不知情</strong>的前提下完成这个操作的呢? <strong>Istio 要做的, 就是编写一个用来为 Pod &quot;自动注入&quot; Envoy 容器的 Initializer</strong>.</p> <p><strong>首先, Istio 会将这个 Envoy 容器本身的定义, 以 ConfigMap 的方式保存在 Kubernetes 当中</strong>. 这个 ConfigMap(名叫: envoy-initializer)的定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy<span class="token punctuation">-</span>initializer
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">config</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    containers:
      - name: envoy
        image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1
        command: [&quot;/usr/local/bin/envoy&quot;]
        args:
          - &quot;--concurrency 4&quot;
          - &quot;--config-path /etc/envoy/envoy.json&quot;
          - &quot;--mode serve&quot;
        ports:
          - containerPort: 80
            protocol: TCP
        resources:
          limits:
            cpu: &quot;1000m&quot;
            memory: &quot;512Mi&quot;
          requests:
            cpu: &quot;100m&quot;
            memory: &quot;64Mi&quot;
        volumeMounts:
          - name: envoy-conf
            mountPath: /etc/envoy
    volumes:
      - name: envoy-conf
        configMap:
          name: envoy</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br></div></div><p>相信你已经注意到了, 这个 ConfigMap 的 <strong>data 部分, 正是一个 Pod 对象的一部分定义</strong>. 其中可以看到 Envoy 容器对应的 <strong>containers</strong> 字段, 以及一个用来<strong>声明 Envoy 配置文件的 volumes 字段</strong>.</p> <p>不难想到, <strong>Initializer 要做的工作, 就是把这部分 Envoy 相关的字段, 自动添加到用户提交的 Pod 的 API 对象里</strong>. 可是, 用户提交的 Pod 里本来就有 containers 字段和 volumes 字段, <strong>所以 Kubernetes 在处理这样的更新请求时, 就必须使用类似于 git merge 这样的操作, 才能将这两部分内容合并在一起</strong>. 所以说, 在 Initializer 更新用户的 Pod 对象的时候, 必须使用 <strong>PATCH API</strong> 来完成. 而这种 PATCH API, 正是<strong>声明式 API 最主要的能力</strong>.</p> <p><strong>接下来, Istio 将一个编写好的 Initializer, 作为一个 Pod 部署在 Kubernetes 中</strong>. 这个 Pod 的定义非常简单, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> envoy<span class="token punctuation">-</span>initializer
  <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy<span class="token punctuation">-</span>initializer
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy<span class="token punctuation">-</span>initializer
      <span class="token key atrule">image</span><span class="token punctuation">:</span> envoy<span class="token punctuation">-</span>initializer<span class="token punctuation">:</span>0.0.1
      <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>可以看到, 这个 envoy-initializer 使用的 envoy-initializer:0.0.1 镜像, 就是一个事先编写好的 &quot;自定义控制器&quot;(Custom Controller), 下一节中将讲解它的编写方法. 这里先解释一下这个控制器的主要功能.</p> <p>前面提到过, 一个 Kubernetes 的控制器, 实际上就是一个 &quot;死循环&quot;: <strong>它不断地获取 &quot;实际状态&quot;, 然后与 &quot;期望状态&quot; 作对比, 并以此为依据决定下一步的操作</strong>. 而 Initializer 的控制器, 不断获取到的 &quot;实际状态&quot;, 就是用户新创建的 Pod. 而它的  <strong>&quot;期望状态&quot;, 则是: 这个 Pod 里被添加了 Envoy 容器的定义</strong>.</p> <p>还是用一段 Go 语言风格的伪代码, 来描述这个控制逻辑, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">for</span> <span class="token punctuation">{</span>
  <span class="token comment">// 获取新创建的 Pod</span>
  pod <span class="token operator">:=</span> client<span class="token punctuation">.</span><span class="token function">GetLatestPod</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token comment">// Diff 一下, 检查是否已经初始化过</span>
  <span class="token keyword">if</span> <span class="token operator">!</span><span class="token function">isInitialized</span><span class="token punctuation">(</span>pod<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token comment">// 没有? 那就来初始化一下</span>
    <span class="token function">doSomething</span><span class="token punctuation">(</span>pod<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><ul><li>如果这个 Pod 里面已经添加过 Envoy 容器, 那么就 &quot;放过&quot; 这个 Pod, 进入下一个检查周期.</li> <li>而如果还没有添加过 Envoy 容器的话, 它就要进行 Initialize 操作了, 即: <strong>修改该 Pod 的 API 对象(doSomething 函数)</strong> .</li></ul> <p>这时候, 你应该立刻能想到, <strong>Istio 要往这个 Pod 里合并的字段, 正是之前保存在 envoy-initializer 这个 ConfigMap 里的数据(即: 它的 data 字段的值)</strong> .</p> <p>所以在 Initializer 控制器的工作逻辑里, 它首先会从 APIServer 中拿到这个 ConfigMap:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token function">doSomething</span><span class="token punctuation">(</span>pod<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  cm <span class="token operator">:=</span> client<span class="token punctuation">.</span><span class="token function">Get</span><span class="token punctuation">(</span>ConfigMap<span class="token punctuation">,</span> <span class="token string">&quot;envoy-initializer&quot;</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后把这个 ConfigMap 里存储的 <strong>containers 和 volumes 字段, 直接添加进一个空的 Pod 对象</strong>里:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token function">doSomething</span><span class="token punctuation">(</span>pod<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  cm <span class="token operator">:=</span> client<span class="token punctuation">.</span><span class="token function">Get</span><span class="token punctuation">(</span>ConfigMap<span class="token punctuation">,</span> <span class="token string">&quot;envoy-initializer&quot;</span><span class="token punctuation">)</span>
  
  newPod <span class="token operator">:=</span> Pod<span class="token punctuation">{</span><span class="token punctuation">}</span>
  newPod<span class="token punctuation">.</span>Spec<span class="token punctuation">.</span>Containers <span class="token operator">=</span> cm<span class="token punctuation">.</span>Containers
  newPod<span class="token punctuation">.</span>Spec<span class="token punctuation">.</span>Volumes <span class="token operator">=</span> cm<span class="token punctuation">.</span>Volumes
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>现在关键来了.</p> <p>Kubernetes 的 API 库为我们提供了一个方法, <strong>可以直接使用新旧两个 Pod 对象, 生成一个 TwoWayMergePatch</strong>:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token function">doSomething</span><span class="token punctuation">(</span>pod<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  cm <span class="token operator">:=</span> client<span class="token punctuation">.</span><span class="token function">Get</span><span class="token punctuation">(</span>ConfigMap<span class="token punctuation">,</span> <span class="token string">&quot;envoy-initializer&quot;</span><span class="token punctuation">)</span>
  
  newPod <span class="token operator">:=</span> Pod<span class="token punctuation">{</span><span class="token punctuation">}</span>
  newPod<span class="token punctuation">.</span>Spec<span class="token punctuation">.</span>Containers <span class="token operator">=</span> cm<span class="token punctuation">.</span>Containers
  newPod<span class="token punctuation">.</span>Spec<span class="token punctuation">.</span>Volumes <span class="token operator">=</span> cm<span class="token punctuation">.</span>Volumes
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><mark><strong>有了这个 TwoWayMergePatch 之后, Initializer 的代码就可以使用这个 patch 的数据, 调用 Kubernetes 的 Client, 发起一个 PATCH 请求</strong></mark>.</p> <p>这样一个用户提交的 Pod 对象里, 就会被<strong>自动加上 Envoy 容器相关的字段</strong>.</p> <p>当然, Kubernetes 还允许你通过配置来指定要<strong>对什么样的资源进行这个 Initialize 操作</strong>, 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> admissionregistration.k8s.io/v1alpha1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> InitializerConfiguration
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy<span class="token punctuation">-</span>config
<span class="token key atrule">initializers</span><span class="token punctuation">:</span>
  // 这个名字必须至少包括两个 &quot;.&quot;
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy.initializer.kubernetes.io
    <span class="token key atrule">rules</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> &quot;&quot; // 前面说过<span class="token punctuation">,</span>  &quot;&quot; 就是 core API Group 的意思
        <span class="token key atrule">apiVersions</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> v1
        <span class="token key atrule">resources</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> pods
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>这个配置就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize 操作, 并且指定了负责这个操作的 Initializer, 名叫: envoy-initializer. 而一旦这个 InitializerConfiguration 被创建, Kubernetes 就会把这个 Initializer 的名字, 加在所有新创建的 Pod 的 Metadata 上, 格式如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">initializers</span><span class="token punctuation">:</span>
    <span class="token key atrule">pending</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> envoy.initializer.kubernetes.io
  <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>pod
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> myapp
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到, 每一个新创建的 Pod, 都会<strong>自动携带了 metadata.initializers.pending 的 Metadata 信息</strong>. 这个 Metadata, 正是接下来 <strong>Initializer 的控制器判断这个 Pod 有没有执行过自己所负责的初始化操作的重要依据</strong>(也就是前面伪代码中 isInitialized() 方法的含义).</p> <p>**这也就意味着, 当在 Initializer 里完成了要做的操作后, 一定要记得将这个 metadata.initializers.pending 标志清除掉. 这一点, 在编写 Initializer 代码的时候一定要非常注意. **</p> <p>此外, 除了上面的配置方法, 还可以在具体的 Pod 的 <strong>Annotation</strong> 里添加一个如下所示的字段, 从而声明要使用某个 Initializer:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
metadata
  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
    <span class="token key atrule">&quot;initializer.kubernetes.io/envoy&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span>
    <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>在这个 Pod 里, 添加了一个 Annotation, 写明: <code>initializer.kubernetes.io/envoy=true</code>​. 这样就会使用到前面所定义的 envoy-initializer 了.</p> <p>以上就是关于 Initializer 最基本的工作原理和使用方法了. 相信你此时已经明白, <mark><strong>Istio 项目的核心, 就是由无数个运行在应用 Pod 中的 Envoy 容器组成的服务代理网格</strong></mark>. 这也正是 <strong>Service Mesh 的含义</strong>. 而这个机制得以实现的原理, 正是借助了 Kubernetes 能够<strong>对 API 对象进行在线更新的能力,</strong>  这也正是 **Kubernetes &quot;声明式 API&quot; 的独特之处: **</p> <ul><li>首先, 所谓 &quot;声明式&quot;, 指的就是<strong>只需要提交一个定义好的 API 对象来&quot;声明&quot; 所期望的状态是什么样子</strong>.</li> <li>其次, &quot;声明式 API&quot; 允许有多个 API 写端, 以 <strong>PATCH</strong> 的方式对 API 对象进行修改, 而无需关心本地原始 YAML 文件的内容.</li> <li>最后, 也是最重要的, 有了上述两个能力, <strong>Kubernetes 项目才可以基于对 API 对象的增, 删, 改, 查</strong>, 在完全无需外界干预的情况下, 完成对 &quot;实际状态&quot; 和 &quot;期望状态&quot; 的调谐(Reconcile)过程.</li></ul> <p>所以说 <mark><strong>声明式 API, 才是 Kubernetes 项目编排能力 &quot;赖以生存&quot; 的核心所在</strong></mark>, 希望你能够认真理解.</p> <p>此外不难看到, 无论是对 sidecar 容器的巧妙设计, 还是对 Initializer 的合理利用, <strong>Istio 项目的设计与实现, 其实都依托于 Kubernetes 的声明式 API 和它所提供的各种编排能力</strong>. 可以说, Istio 是在 Kubernetes 项目使用上的一位 &quot;集大成者&quot;.要知道, 一个 Istio 项目部署完成后, 会在 Kubernetes 里创建大约 43 个 API 对象.</p> <p>所以 Kubernetes 社区也看得很明白: Istio 项目有多火热, 就说明 Kubernetes 这套 &quot;声明式 API&quot; 有多成功. 这既是 Google Cloud 喜闻乐见的事情, 也是 Istio 项目一推出就被 Google 公司和整个技术圈儿热捧的重要原因.</p> <p>而在使用 Initializer 的流程中, <strong>最核心的步骤莫过于 Initializer &quot;自定义控制器&quot; 的编写过程</strong>. 它遵循的正是标准的 &quot;Kubernetes 编程范式&quot;, 即:</p> <p><mark>**如何使用控制器模式, 同 Kubernetes 里 API 对象的 &quot;增, 删, 改, 查&quot; 进行协作, 进而完成用户业务逻辑的编写过程. **</mark></p> <p>这也正是后面章节将详细讲解的内容.</p> <blockquote><p>总结</p></blockquote> <p>本节重点讲解了 Kubernetes 声明式 API 的含义. 并且通过对 Istio 项目的剖析, 说明了它使用 Kubernetes 的 Initializer 特性, 完成 Envoy 容器 &quot;自动注入&quot; 的原理.</p> <p>事实上, 从 &quot;使用 Kubernetes 部署代码&quot;, 到 &quot;<strong>使用 Kubernetes 编写代码</strong>&quot; 的蜕变过程, 正是你从一个 Kubernetes 用户, 到 Kubernetes 玩家的晋级之路. 而如何<strong>理解 &quot;Kubernetes 编程范式&quot;, 如何为 Kubernetes 添加自定义 API 对象, 编写自定义控制器</strong>, 正是这个晋级过程中的关键点, 也是后面几节的核心内容.</p> <h4 id="_24-深入解析声明式api-一-api对象的奥秘"><a href="#_24-深入解析声明式api-一-api对象的奥秘" class="header-anchor">#</a> 24 | 深入解析声明式API(一):API对象的奥秘</h4> <p>上一节详细讲解了 Kubernetes 声明式 API 的设计, 特点, 以及使用方式. 本节来讲解一下 <strong>Kubernetes 声明式 API 的工作原理, 以及如何利用这套 API 机制, 在 Kubernetes 里添加自定义的 API 对象</strong>.</p> <p>你可能一直就很好奇: <strong>当把一个 YAML 文件提交给 Kubernetes 之后, 它究竟是如何创建出一个 API 对象的呢</strong>?</p> <p>这得从声明式 API 的设计谈起了.</p> <p>在 Kubernetes 项目中, <strong>一个 API 对象在 Etcd 里的完整资源路径, 是由: Group(API 组), Version(API 版本)和 Resource(API 资源类型)三个部分组成的</strong>. 通过这样的结构, 整个 Kubernetes 里的所有 API 对象, 实际上就可以用如下的<strong>树形</strong>结构表示出来:</p> <p><img src="/img/e3d510f4210ac2a0568d48d1a705ae44-20230731162150-208g21z.png" alt="">
在这幅图中可以很清楚地看到 **Kubernetes 里 API 对象的组织方式, 其实是层层递进的. **</p> <p>比如现在要声明要创建一个 CronJob 对象, 那么 YAML 文件的开始部分会这么写:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v2alpha1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在这个 YAML 文件中, &quot;CronJob&quot; 就是这个 API 对象的<strong>资源类型</strong>(Resource), &quot;batch&quot; 就是它的<strong>组</strong>(Group), v2alpha1 就是它的<strong>版本</strong>(Version).</p> <p>当提交了这个 YAML 文件之后, Kubernetes 就会<strong>把这个 YAML 文件里描述的内容, 转换成 Kubernetes 里的一个 CronJob 对象</strong>.</p> <p>那么, Kubernetes 是如何对 Resource, Group 和 Version 进行<strong>解析</strong>, 从而在 Kubernetes 项目里找到 CronJob 对象的定义呢?</p> <p>**首先, Kubernetes 会匹配 API 对象的组. **</p> <p>需要明确的是, 对于 Kubernetes 里的<strong>核心 API 对象, 比如: Pod, Node 等, 是不需要 Group 的</strong>(即: 它们 Group 是 &quot;&quot;). 所以对于这些 API 对象来说, Kubernetes 会<strong>直接在 /api 这个层级进行下一步的匹配过程</strong>.</p> <p>而对于 CronJob 等非核心 API 对象来说, Kubernetes 就必须<strong>在 /apis 这个层级里查找它对应的 Group</strong>, 进而根据 &quot;batch&quot; 这个 Group 的名字, 找到 /apis/batch.</p> <p>不难发现, 这些 API Group 的分类是以<strong>对象功能</strong>为依据的, 比如 Job 和 CronJob 就都属于 &quot;batch&quot; (离线业务)这个 Group.</p> <p>**然后, Kubernetes 会进一步匹配到 API 对象的版本号. **</p> <p>对于 CronJob 这个 API 对象来说, Kubernetes 在 batch 这个 Group 下, 匹配到的版本号就是 v2alpha1.</p> <p>在 Kubernetes 中, <strong>同一种 API 对象可以有多个版本, 这正是 Kubernetes 进行 API 版本化管理的重要手段</strong>. 这样比如在 CronJob 的开发过程中, 对于会影响到用户的变更就可以通过升级新版本来处理, 从而保证了向后兼容.</p> <p>**最后, Kubernetes 会匹配 API 对象的资源类型. **</p> <p>在前面匹配到正确的版本之后, Kubernetes 就知道, 要创建的原来是一个 /apis/batch/v2alpha1 下的 CronJob 对象. 这时候 APIServer 就可以继续创建这个 CronJob 对象了. 为了方便理解, 这里总结了一个如下所示流程图来阐述这个创建过程:</p> <p><img src="/img/26ba3bfa569ef7b9044047e8ba86d08f-20230731162150-gz9gr3h.png" alt=""></p> <p><strong>首先</strong>, 当发起了创建 CronJob 的 POST 请求之后, 编写的 YAML 的信息就被提交给了 <strong>APIServer</strong>. 而 APIServer 的第一个功能, 就是过滤这个请求, 并完成一些<strong>前置性</strong>的工作, 比如<strong>授权, 超时处理, 审计</strong>等.</p> <p><strong>然后</strong>, 请求会进入 MUX 和 Routes 流程. 如果编写过 Web Server 的话就会知道, MUX 和 Routes 是 <strong>APIServer 完成 URL 和 Handler 绑定的场所</strong>. 而 APIServer 的 Handler 要做的事情, 就是按照刚刚介绍的匹配过程, <strong>找到对应的 CronJob 类型定义</strong>.</p> <p><strong>接着</strong>, APIServer 最重要的职责就来了: <strong>根据这个 CronJob 类型定义, 使用用户提交的 YAML 文件里的字段, 创建一个 CronJob 对象</strong>. 而在这个过程中, APIServer 会进行一个 Convert 工作, 即: <strong>把用户提交的 YAML 文件, 转换成一个叫作 Super Version 的对象, 它正是该 API 资源类型所有版本的字段全集</strong>. 这样用户提交的不同版本的 YAML 文件, 就都可以用这个 Super Version 对象来进行处理了.</p> <p><strong>接下来</strong>, APIServer 会先后进行 Admission() 和 Validation() 操作. 比如前面提到的 Admission Controller 和 Initializer, 就都属于 Admission 的内容. 而 Validation, 则负责验证这个对象里的<strong>各个字段是否合法</strong>. 这个被验证过的 API 对象, 都保存在了 APIServer 里一个叫作 <strong>Registry</strong> 的数据结构中. 也就是说只要一个 API 对象的定义能在 Registry 里查到, 它就是一个有效的 Kubernetes API 对象.</p> <p><strong>最后</strong>, APIServer 会<strong>把验证过的 API 对象转换成用户最初提交的版本, 进行序列化操作, 并调用 Etcd 的 API 把它保存起来</strong>.</p> <p>由此可见, 声明式 API 对于 Kubernetes 来说非常重要. 所以 <strong>APIServer 这样一个在其他项目里 &quot;平淡无奇&quot;的组件, 却成了 Kubernetes 项目的重中之重</strong>. 它不仅是 Google Borg 设计思想的集中体现, 也是 Kubernetes 项目里唯一一个被 Google 公司和 RedHat 公司双重控制, 其他势力根本无法参与其中的组件. 此外, 由于同时要兼顾<strong>性能, API 完备性, 版本化, 向后兼容</strong>等很多工程化指标, 所以 Kubernetes 团队在 APIServer 项目里大量使用了 Go 语言的代码生成功能, 来自动化诸如 Convert, DeepCopy 等与 API 资源相关的操作. 这部分自动生成的代码, 曾一度占到 Kubernetes 项目总代码的 20%~30%. 这也是为何在过去很长一段时间里, 在这样一个极其 &quot;复杂&quot; 的 APIServer 中, 添加一个 Kubernetes 风格的 API 资源类型, 是一个非常困难的工作.</p> <p>不过, 在 Kubernetes v1.7 之后, 这个工作就变得轻松得多了. 这当然得益于一个<strong>全新的 API 插件机制: CRD(Custom Resource Definition)</strong> . 顾名思义, 它指的就是<strong>允许用户在 Kubernetes 中添加一个跟 Pod, Node 类似的, 新的 API 资源类型, 即: 自定义 API 资源</strong>.</p> <p>举个例子, 现在要为 Kubernetes 添加一个名叫 <strong>Network</strong> 的 API 资源类型. 它的作用是, <strong>一旦用户创建一个 Network 对象, 那么 Kubernetes 就应该使用这个对象定义的网络参数, 调用真实的网络插件, 比如 Neutron 项目, 为用户创建一个真正的 &quot;网络&quot;. 这样, 将来用户创建的 Pod, 就可以声明使用这个 &quot;网络&quot; 了</strong>.</p> <p>这个 Network 对象的 YAML 文件, 名叫 example-network.yaml, 它的内容如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> samplecrd.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Network
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>network
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">cidr</span><span class="token punctuation">:</span> <span class="token string">&quot;192.168.0.0/16&quot;</span>
  <span class="token key atrule">gateway</span><span class="token punctuation">:</span> <span class="token string">&quot;192.168.0.1&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可以看到, 想要描述 &quot;网络&quot; 的 API <strong>资源类型是 Network</strong>; API 组是 <code>samplecrd.k8s.io</code>​; API 版本是 v1.</p> <p>那么, Kubernetes 又该如何知道这个 API(<code>samplecrd.k8s.io/v1/network</code>​)的存在呢? 其实, 上面的这个 YAML 文件, 就是一个具体的 &quot;<strong>自定义 API 资源</strong>&quot; 实例, 也叫 <strong>CR</strong>(Custom Resource). 而为了能够让 Kubernetes 认识这个 CR, 就需要让 Kubernetes 明白这个 CR 的<strong>宏观定义</strong>是什么, 也就是 <strong>CRD</strong>(Custom Resource Definition).</p> <p>这就好比, 你想让计算机认识各种兔子的照片, 就得先让计算机明白, 兔子的普遍定义是什么. 比如兔子 &quot;是哺乳动物&quot;, &quot;有长耳朵, 三瓣嘴&quot;.</p> <p>所以接下来, 就先需编写一个 CRD 的 YAML 文件, 它的名字叫作 network.yaml, 内容如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apiextensions.k8s.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CustomResourceDefinition
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> networks.samplecrd.k8s.io
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">group</span><span class="token punctuation">:</span> samplecrd.k8s.io
  <span class="token key atrule">version</span><span class="token punctuation">:</span> v1
  <span class="token key atrule">names</span><span class="token punctuation">:</span>
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> Network
    <span class="token key atrule">plural</span><span class="token punctuation">:</span> networks
  <span class="token key atrule">scope</span><span class="token punctuation">:</span> Namespaced
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>在这个 CRD 中, 指定了 &quot;<code>group: samplecrd.k8s.io</code>​&quot;, &quot;<code>version: v1</code>​&quot; 这样的 API 信息, 也指定了这个 CR 的<strong>资源类型叫作 Network</strong>, 复数(plural)是 networks. 然后还声明了它的 scope 是 <strong>Namespaced</strong>, 即: 定义的这个 Network 是一个属于 Namespace 的对象, 类似于 Pod.</p> <p>这就是一个 Network API 资源类型的 API 部分的宏观定义了. 这就等同于告诉了计算机: &quot;兔子是哺乳动物&quot;. 所以这时候, Kubernetes 就能够认识和处理所有声明了 API 类型是 &quot;<code>samplecrd.k8s.io/v1/network</code>​&quot; 的 YAML 文件了.</p> <p>接下来, 还需要让 Kubernetes &quot;认识&quot; 这种 YAML 文件里描述的 &quot;网络&quot; 部分, 比如 &quot;cidr&quot;(网段), &quot;gateway&quot;(网关)这些字段的含义. 这就相当于要告诉计算机: &quot;兔子有长耳朵和三瓣嘴&quot;. 这时候就需要稍微做些代码工作了.</p> <p>**首先, 要在 GOPATH 下, 创建一个结构如下的项目: ** (备注: 这里假设你已经了解了 Golang 的一些基本知识. 比如知道什么是 GOPATH).</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ tree <span class="token variable">$GOPATH</span>/src/github.com/<span class="token operator">&lt;</span>your-name<span class="token operator">&gt;</span>/k8s-controller-custom-resource
<span class="token builtin class-name">.</span>
├── controller.go
├── crd
│   └── network.yaml
├── example
│   └── example-network.yaml
├── main.go
└── pkg
    └── apis
        └── samplecrd
            ├── register.go
            └── v1
                ├── doc.go
                ├── register.go
                └── types.go
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>其中, pkg/apis/samplecrd 就是 <strong>API 组的名字</strong>, v1 是版本, 而 v1 下面的 types.go 文件里, 则定义了 Network 对象的完整描述. 这个项目z已经<a href="https://github.com/resouer/k8s-controller-custom-resource" target="_blank" rel="noopener noreferrer">上传到了 GitHub 上<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 可以随时参考.</p> <p><strong>然后在 pkg/apis/samplecrd 目录下创建了一个 register.go 文件, 用来放置后面要用到的全局变量</strong>. 这个文件的内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">package</span> samplecrd
 
<span class="token keyword">const</span> <span class="token punctuation">(</span>
    GroupName <span class="token operator">=</span> <span class="token string">&quot;samplecrd.k8s.io&quot;</span>
    Version   <span class="token operator">=</span> <span class="token string">&quot;v1&quot;</span>
<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><strong>接着需要在 pkg/apis/samplecrd 目录下添加一个 doc.go 文件(Golang 的文档源文件)</strong> . 这个文件里的内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">package</span> samplecrd
 
<span class="token keyword">const</span> <span class="token punctuation">(</span>
    GroupName <span class="token operator">=</span> <span class="token string">&quot;samplecrd.k8s.io&quot;</span>
    Version   <span class="token operator">=</span> <span class="token string">&quot;v1&quot;</span>
<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>在这个文件中, 你会看到 <code>+&lt;tag_name&gt;[=value]</code>​ 格式的注释, 这就是 Kubernetes 进行代码生成要用的 <strong>Annotation</strong> 风格的注释.</p> <p>其中, +k8s:deepcopy-gen=package 意思是, 请为整个 v1 包里的所有类型定义<strong>自动生成 DeepCopy 方法</strong>; 而<code>+groupName=samplecrd.k8s.io</code>​, 则定义了这个包对应的 API 组的名字.</p> <p>可以看到, 这些定义在 doc.go 文件的注释, 起到的是<strong>全局的代码生成控制</strong>的作用, 所以也被称为 Global Tags.</p> <p><strong>接下来, 需要添加 types.go 文件</strong>. 顾名思义, 它的作用就是<strong>定义一个 Network 类型到底有哪些字段</strong>(比如, spec 字段里的内容). 这个文件的主要内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">package</span> v1
<span class="token operator">...</span>
<span class="token comment">// +genclient</span>
<span class="token comment">// +genclient:noStatus</span>
<span class="token comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span>
 
<span class="token comment">// Network describes a Network resource</span>
<span class="token keyword">type</span> Network <span class="token keyword">struct</span> <span class="token punctuation">{</span>
 <span class="token comment">// TypeMeta is the metadata for the resource, like kind and apiversion</span>
 metav1<span class="token punctuation">.</span>TypeMeta <span class="token string">`json:&quot;,inline&quot;`</span>
 <span class="token comment">// ObjectMeta contains the metadata for the particular object, including</span>
 <span class="token comment">// things like...</span>
 <span class="token comment">//  - name</span>
 <span class="token comment">//  - namespace</span>
 <span class="token comment">//  - self link</span>
 <span class="token comment">//  - labels</span>
 <span class="token comment">//  - ... etc ...</span>
 metav1<span class="token punctuation">.</span>ObjectMeta <span class="token string">`json:&quot;metadata,omitempty&quot;`</span>
 
 Spec networkspec <span class="token string">`json:&quot;spec&quot;`</span>
<span class="token punctuation">}</span>
<span class="token comment">// networkspec is the spec for a Network resource</span>
<span class="token keyword">type</span> networkspec <span class="token keyword">struct</span> <span class="token punctuation">{</span>
 Cidr    <span class="token builtin">string</span> <span class="token string">`json:&quot;cidr&quot;`</span>
 Gateway <span class="token builtin">string</span> <span class="token string">`json:&quot;gateway&quot;`</span>
<span class="token punctuation">}</span>
 
<span class="token comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span>
 
<span class="token comment">// NetworkList is a list of Network resources</span>
<span class="token keyword">type</span> NetworkList <span class="token keyword">struct</span> <span class="token punctuation">{</span>
 metav1<span class="token punctuation">.</span>TypeMeta <span class="token string">`json:&quot;,inline&quot;`</span>
 metav1<span class="token punctuation">.</span>ListMeta <span class="token string">`json:&quot;metadata&quot;`</span>
 
 Items <span class="token punctuation">[</span><span class="token punctuation">]</span>Network <span class="token string">`json:&quot;items&quot;`</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><p>在上面这部分代码里, 可以看到 Network 类型定义方法跟标准的 Kubernetes 对象一样, 都包括了 <strong>TypeMeta</strong>(API 元数据)和 <strong>ObjectMeta</strong>(对象元数据)字段. 而其中的 Spec 字段, 就是需要自己定义的部分. 所以在 networkspec 里, 定义了 <strong>Cidr</strong> 和 <strong>Gateway</strong> 两个字段. 其中每个字段最后面的部分比如 <code>json:&quot;cidr&quot;</code>​, 指的就是这个字段被<strong>转换成 JSON 格式之后的名字</strong>, 也就是 <strong>YAML 文件里的字段名字</strong>. 如果不熟悉这个用法的话, 可以查阅一下 Golang 的文档.</p> <p>此外除了定义 Network 类型, 还需要定义一个 <strong>NetworkList</strong> 类型, 用来描述<strong>一组 Network 对象</strong>应该包括哪些字段. 之所以需要这样一个类型, 是因为在 Kubernetes 中, 获取所有 X 对象的 List() 方法, 返回值都是 List 类型, 而不是 X 类型的数组. 这是不一样的.</p> <p>同样地, 在 Network 和 NetworkList 类型上, 也有代码生成注释. 其中, +genclient 的意思是: 请为下面这个 API 资源类型生成对应的 Client 代码(这个 Client, 我马上会讲到). 而 +genclient:noStatus 的意思是: 这个 API 资源类型定义里, 没有 Status 字段. 否则生成的 Client 就会自动带上 UpdateStatus 方法.</p> <p>如果类型定义包括了 Status 字段的话, 就不需要这句 +genclient:noStatus 注释了. 比如下面这个例子:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token comment">// +genclient</span>
 
<span class="token comment">// Network is a specification for a Network resource</span>
<span class="token keyword">type</span> Network <span class="token keyword">struct</span> <span class="token punctuation">{</span>
 metav1<span class="token punctuation">.</span>TypeMeta   <span class="token string">`json:&quot;,inline&quot;`</span>
 metav1<span class="token punctuation">.</span>ObjectMeta <span class="token string">`json:&quot;metadata,omitempty&quot;`</span>
 
 Spec   NetworkSpec   <span class="token string">`json:&quot;spec&quot;`</span>
 Status NetworkStatus <span class="token string">`json:&quot;status&quot;`</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>需要注意的是, +genclient 只需要写在 Network 类型上, 而不用写在 NetworkList 上. 因为 NetworkList 只是一个返回值类型, Network 才是 &quot;主类型&quot;.</p> <p>而由于在 Global Tags 里已经定义了为所有类型生成 DeepCopy 方法, 所以这里就不需要再显式地加上 +k8s:deepcopy-gen=true 了. 当然这也就意味着可以用 +k8s:deepcopy-gen=false 来阻止为某些类型生成 DeepCopy.</p> <p>你可能已经注意到, 在这两个类型上面还有一句 <code>+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</code>​ 的注释. 它的意思是, 请在生成 DeepCopy 的时候, 实现 Kubernetes 提供的 runtime.Object 接口. 否则在某些版本的 Kubernetes 里, 这个类型定义会出现编译错误. 这是一个固定的操作, 记住即可.</p> <p>不过你或许会有这样的顾虑: 这些代码生成注释这么灵活, 该怎么掌握呢? 其实上面所讲述的内容, 已经足以应对 99% 的场景了. 如果你对代码生成感兴趣的话, 推荐阅读<a href="https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/" target="_blank" rel="noopener noreferrer">这篇博客<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 它详细地介绍了 Kubernetes 的代码生成语法.</p> <p><strong>最后, 需要再编写的一个 pkg/apis/samplecrd/v1/register.go 文件</strong>.</p> <p>在前面对 APIServer 工作原理的讲解中已经提到, &quot;registry&quot; 的作用就是注册一个类型(Type)给 APIServer. 其中, Network 资源类型在服务器端的注册的工作, APIServer 会自动帮我们完成. 但与之对应的, 还需要让客户端也能 &quot;知道&quot; Network 资源类型的定义. 这就需要在项目里添加一个 register.go 文件. 它最主要的功能, 就是定义了如下所示的 addKnownTypes() 方法:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">package</span> v1
<span class="token operator">...</span>
<span class="token comment">// addKnownTypes adds our types to the API scheme by registering</span>
<span class="token comment">// Network and NetworkList</span>
<span class="token keyword">func</span> <span class="token function">addKnownTypes</span><span class="token punctuation">(</span>scheme <span class="token operator">*</span>runtime<span class="token punctuation">.</span>Scheme<span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
 scheme<span class="token punctuation">.</span><span class="token function">AddKnownTypes</span><span class="token punctuation">(</span>
  SchemeGroupVersion<span class="token punctuation">,</span>
  <span class="token operator">&amp;</span>Network<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token operator">&amp;</span>NetworkList<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
 <span class="token punctuation">)</span>
 
 <span class="token comment">// register the type in the scheme</span>
 metav1<span class="token punctuation">.</span><span class="token function">AddToGroupVersion</span><span class="token punctuation">(</span>scheme<span class="token punctuation">,</span> SchemeGroupVersion<span class="token punctuation">)</span>
 <span class="token keyword">return</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>有了这个方法, Kubernetes 就能够在后面生成客户端的时候, &quot;知道&quot; Network 以及 NetworkList 类型的定义了.</p> <p>像上面这种 **register.go 文件里的内容其实是非常固定的, 以后可以直接使用这里提供的这部分代码做模板, 然后把其中的资源类型, GroupName 和 Version 替换成自己的定义即可. **</p> <p>这样, Network 对象的定义工作就全部完成了. 可以看到, 它其实定义了两部分内容:</p> <ul><li>第一部分是, <mark><strong>自定义资源类型的 API 描述</strong></mark>, 包括: 组(Group), 版本(Version), 资源类型(Resource)等. 这相当于告诉了计算机: 兔子是哺乳动物.</li> <li>第二部分是, <mark><strong>自定义资源类型的对象描述</strong></mark>, 包括: Spec, Status 等. 这相当于告诉了计算机: 兔子有长耳朵和三瓣嘴.</li></ul> <p>接下来就要使用 Kubernetes 提供的代码生成工具, 为上面定义的 Network 资源类型<strong>自动生成 clientset, informer 和 lister</strong>. 其中 clientset 就是操作 Network 对象所需要使用的客户端, 而 informer 和 lister 这两个包的主要功能, 会在下一节重点讲解.</p> <p>这个代码生成工具名叫 <code>k8s.io/code-generator</code>​, 使用方法如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 代码生成的工作目录, 也就是我们的项目路径</span>
$ <span class="token assign-left variable">ROOT_PACKAGE</span><span class="token operator">=</span><span class="token string">&quot;github.com/resouer/k8s-controller-custom-resource&quot;</span>
<span class="token comment"># API Group</span>
$ <span class="token assign-left variable">CUSTOM_RESOURCE_NAME</span><span class="token operator">=</span><span class="token string">&quot;samplecrd&quot;</span>
<span class="token comment"># API Version</span>
$ <span class="token assign-left variable">CUSTOM_RESOURCE_VERSION</span><span class="token operator">=</span><span class="token string">&quot;v1&quot;</span>
 
<span class="token comment"># 安装 k8s.io/code-generator</span>
$ go get <span class="token parameter variable">-u</span> k8s.io/code-generator/<span class="token punctuation">..</span>.
$ <span class="token builtin class-name">cd</span> <span class="token variable">$GOPATH</span>/src/k8s.io/code-generator
 
<span class="token comment"># 执行代码自动生成, 其中 pkg/client 是生成目标目录, pkg/apis 是类型定义目录</span>
$ ./generate-groups.sh all <span class="token string">&quot;<span class="token variable">$ROOT_PACKAGE</span>/pkg/client&quot;</span> <span class="token string">&quot;<span class="token variable">$ROOT_PACKAGE</span>/pkg/apis&quot;</span> <span class="token string">&quot;<span class="token variable">$CUSTOM_RESOURCE_NAME</span>:<span class="token variable">$CUSTOM_RESOURCE_VERSION</span>&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>代码生成工作完成之后, 再查看一下这个项目的目录结构:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ tree
<span class="token builtin class-name">.</span>
├── controller.go
├── crd
│   └── network.yaml
├── example
│   └── example-network.yaml
├── main.go
└── pkg
    ├── apis
    │   └── samplecrd
    │       ├── constants.go
    │       └── v1
    │           ├── doc.go
    │           ├── register.go
    │           ├── types.go
    │           └── zz_generated.deepcopy.go
    └── client
        ├── clientset
        ├── informers
        └── listers
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>其中, pkg/apis/samplecrd/v1 下面的 zz_generated.deepcopy.go 文件, 就是自动生成的 DeepCopy 代码文件. 而整个 client 目录, 以及下面的三个包(clientset, informers,  listers), <strong>都是 Kubernetes 为 Network 类型生成的客户端库, 这些库会在后面编写自定义控制器的时候用到</strong>.</p> <p>到目前为止的这些工作, 其实并不要求你写多少代码, 主要考验的是 &quot;复制, 粘贴, 替换&quot; 这样的 &quot;基本功&quot;. 而有了这些内容, 现在就可以在 Kubernetes 集群里<strong>创建一个 Network 类型的 API 对象</strong>了. 不妨一起来实验一下.</p> <p><strong>首先</strong>, 使用 network.yaml 文件, 在 Kubernetes 中创建 Network 对象的 CRD(Custom Resource Definition):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> crd/network.yaml
customresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个操作就告诉了 Kubernetes, 现在要<strong>添加一个自定义的 API 对象</strong>. 而这个对象的 API 信息, 正是 network.yaml 里定义的内容. 可以通过 kubectl get 命令, 查看这个 CRD:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get crd
NAME                        CREATED AT
networks.samplecrd.k8s.io   <span class="token number">2018</span>-09-15T10:57:12Z
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>然后</strong>, 就可以创建一个 Network 对象了, 这里用到的是 example-network.yaml:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> example/example-network.yaml 
network.samplecrd.k8s.io/example-network created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>通过这个操作, 就在 Kubernetes 集群里创建了一个 Network 对象. 它的 API 资源路径是 <code>samplecrd.k8s.io/v1/networks</code>​.</p> <p>这时候就可以通过 kubectl get 命令, 查看到新创建的 Network 对象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get network
NAME              AGE
example-network   8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>还可以通过 kubectl describe 命令, 看到这个 Network 对象的细节:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe network example-network
Name:         example-network
Namespace:    default
Labels:       <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token punctuation">..</span>.API Version:  samplecrd.k8s.io/v1
Kind:         Network
Metadata:
  <span class="token punctuation">..</span>.
  Generation:          <span class="token number">1</span>
  Resource Version:    <span class="token number">468239</span>
  <span class="token punctuation">..</span>.
Spec:
  Cidr:     <span class="token number">192.168</span>.0.0/16
  Gateway:  <span class="token number">192.168</span>.0.1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>当然, 也可以编写更多的 YAML 文件来创建更多的 Network 对象, 这和创建 Pod, Deployment 的操作, 没有任何区别.</p> <blockquote><p>总结</p></blockquote> <p>本节详细解析了 Kubernetes 声明式 API 的工作原理, 讲解了如何遵循声明式 API 的设计, 为 Kubernetes 添加一个名叫 Network 的 API 资源类型. 从而达到了通过标准的 kubectl create 和 get 操作, 来管理自定义 API 对象的目的. 不过, 创建出这样一个自定义 API 对象, 还只是完成了 Kubernetes 声明式 API 的一半工作.</p> <p>接下来的另一半工作是: <strong>为这个 API 对象编写一个自定义控制器</strong>(Custom Controller). 这样 Kubernetes 才能根据 Network API 对象的 &quot;增, 删, 改&quot; 操作, 在真实环境中做出相应的响应. 比如, &quot;创建, 删除, 修改&quot; 真正的 Neutron 网络. 而这正是 Network 这个 API 对象所关注的 &quot;业务逻辑&quot;. 这个业务逻辑的实现过程, 以及它所使用的 Kubernetes API 编程库的工作原理, 就是下节的主要内容.</p> <h4 id="_25-深入解析声明式api-二-编写自定义控制器"><a href="#_25-深入解析声明式api-二-编写自定义控制器" class="header-anchor">#</a> 25 | 深入解析声明式API(二):编写自定义控制器</h4> <p>上一节介绍了 Kubernetes 中声明式 API 的实现原理, 并且通过一个添加 Network 对象的实例, 并在 Kubernetes 里添加 API 资源的过程. 本节就继续完成剩下一半的工作, 即: <strong>为 Network 这个自定义 API 对象编写一个自定义控制器(Custom Controller)</strong> .</p> <p>正如上一节结尾处提到的, &quot;声明式 API&quot; 并不像 &quot;命令式 API&quot; 那样有着明显的执行逻辑. 这就使得**基于声明式 API 的业务功能实现, 往往需要通过控制器模式来 &quot;监视&quot; API 对象的变化(比如创建或者删除 Network), 然后以此来决定实际要执行的具体工作. **</p> <p>接下来就一起通过<strong>编写代码</strong>来实现这个过程. 这个项目和上一篇文章里的代码是同一个项目, 可以从<a href="https://github.com/resouer/k8s-controller-custom-resource" target="_blank" rel="noopener noreferrer">这个 GitHub 库<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>里找到它们, 代码里有丰富的注释可以参考. 总的来说, <strong>编写自定义控制器代码的过程包括: 编写 main 函数, 编写自定义控制器的定义, 以及编写控制器里的业务逻辑三个部分</strong>.</p> <p>首先, <strong>来编写这个自定义控制器的 main 函数</strong>. main 函数的主要工作就是, 定义并初始化一个自定义控制器(Custom Controller), 然后启动它. 这部分代码的主要内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token operator">...</span>
  
  cfg<span class="token punctuation">,</span> err <span class="token operator">:=</span> clientcmd<span class="token punctuation">.</span><span class="token function">BuildConfigFromFlags</span><span class="token punctuation">(</span>masterURL<span class="token punctuation">,</span> kubeconfig<span class="token punctuation">)</span>
  <span class="token operator">...</span>
  kubeClient<span class="token punctuation">,</span> err <span class="token operator">:=</span> kubernetes<span class="token punctuation">.</span><span class="token function">NewForConfig</span><span class="token punctuation">(</span>cfg<span class="token punctuation">)</span>
  <span class="token operator">...</span>
  networkClient<span class="token punctuation">,</span> err <span class="token operator">:=</span> clientset<span class="token punctuation">.</span><span class="token function">NewForConfig</span><span class="token punctuation">(</span>cfg<span class="token punctuation">)</span>
  <span class="token operator">...</span>
  
  networkInformerFactory <span class="token operator">:=</span> informers<span class="token punctuation">.</span><span class="token function">NewSharedInformerFactory</span><span class="token punctuation">(</span>networkClient<span class="token punctuation">,</span> <span class="token operator">...</span><span class="token punctuation">)</span>
  
  controller <span class="token operator">:=</span> <span class="token function">NewController</span><span class="token punctuation">(</span>kubeClient<span class="token punctuation">,</span> networkClient<span class="token punctuation">,</span>
  networkInformerFactory<span class="token punctuation">.</span><span class="token function">Samplecrd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">V1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">Networks</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  
  <span class="token keyword">go</span> networkInformerFactory<span class="token punctuation">.</span><span class="token function">Start</span><span class="token punctuation">(</span>stopCh<span class="token punctuation">)</span>
 
  <span class="token keyword">if</span> err <span class="token operator">=</span> controller<span class="token punctuation">.</span><span class="token function">Run</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> stopCh<span class="token punctuation">)</span><span class="token punctuation">;</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
    glog<span class="token punctuation">.</span><span class="token function">Fatalf</span><span class="token punctuation">(</span><span class="token string">&quot;Error running controller: %s&quot;</span><span class="token punctuation">,</span> err<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>可以看到, 这个 main 函数主要<strong>通过三步完成了初始化并启动一个自定义控制器</strong>的工作.</p> <p><strong>第一步</strong>: main 函数根据提供的 Master 配置(APIServer 的地址端口和 kubeconfig 的路径), 创建一个 Kubernetes 的 <strong>client</strong>(kubeClient)和 Network 对象的 <strong>client</strong>(networkClient).</p> <p>但是如果没有提供 Master 配置呢?</p> <p>这时 main 函数会直接使用一种名叫 <strong>InClusterConfig ** 的方式来创建这个 client. 这个方式会假设你的自定义控制器是以 <strong>Pod</strong> 的方式运行在 Kubernetes 集群里的. 前面曾经提到过, Kubernetes 里所</strong>有的 Pod 都会以 Volume 的方式自动挂载 Kubernetes 的默认 ServiceAccount**. 所以这个控制器就会<strong>直接使用默认 ServiceAccount 数据卷里的授权信息</strong>来访问 APIServer.</p> <p><strong>第二步</strong>: main 函数为 Network 对象创建一个叫作 <strong>InformerFactory</strong>(即: networkInformerFactory)的工厂, 并使用它生成一个 Network 对象的 Informer, 传递给控制器.</p> <p><strong>第三步</strong>: main 函数启动上述的 Informer, 然后执行 controller.Run, <strong>启动自定义控制器</strong>.</p> <p>至此, main 函数就结束了.</p> <p>到这你可能会感到非常困惑: 编写自定义控制器的过程难道就这么简单吗? 这个 Informer 又是个什么东西?</p> <p>接下来就**详细解释一下这个自定义控制器的工作原理. ** 在 Kubernetes 项目中, <strong>一个自定义控制器的工作原理</strong>, 可以用下面这样一幅流程图来表示(在后面的叙述中, 会用 &quot;示意图&quot; 来指代它):</p> <p><img src="/img/cc104c51702dbe2fbd9442c85cfcdf9a-20230731162150-6qiekx3.png" alt="图 1 自定义控制器的工作流程示意图" title="图1 自定义控制器的工作流程示意图">​</p> <p>先从这幅示意图的最左边看起. <strong>这个控制器要做的第一件事, 是从 Kubernetes 的 APIServer 里获取它所关心的对象, 也就是定义的 Network 对象</strong>. 这个操作, 依靠的是一个叫作 <strong>Informer</strong>(可以翻译为: 通知器)的代码库完成的. Informer 与 API 对象是<strong>一一对应</strong>的, 所以<strong>传递给自定义控制器的, 正是一个 Network 对象的 Informer</strong>(Network Informer).</p> <p>不知你是否已经注意到, 在创建这个 Informer 工厂的时候, 需要给它传递一个 networkClient. 事实上, Network Informer 正是使用这个 networkClient, 跟 APIServer 建立了<strong>连接</strong>. 不过真正负责维护这个连接的, 则是 Informer 所使用的 Reflector 包.</p> <p>更具体地说, Reflector 使用的是一种叫作 <strong>ListAndWatch ** 的方法, 来 &quot;获取&quot; 并 &quot;监听&quot; 这些 Network 对象实例的变化. 在 <strong>ListAndWatch 机制</strong>下, 一旦 APIServer 端有新的 Network 实例被创建, 删除或者更新, <strong>Reflector 都会收到 &quot;事件通知&quot;</strong> . 这时该事件及它对应的 API 对象这个组合, 就被称为</strong>增量**(Delta), 它会被放进一个 Delta FIFO Queue(即: 增量先进先出队列)中.</p> <p>而另一方面, <strong>Informer 会不断地从这个 Delta FIFO Queue 里读取(Pop)增量. 每拿到一个增量, Informer 就会判断这个增量里的事件类型, 然后创建或者更新本地对象的缓存. 这个缓存, 在 Kubernetes 里一般被叫作 Store</strong>. 比如, 如果事件类型是 Added(添加对象), 那么 Informer 就会通过一个叫作 Indexer 的库把这个增量里的 API 对象保存在本地缓存中, 并为它创建索引. 相反地, 如果增量的事件类型是 Deleted(删除对象), 那么 Informer 就会从本地缓存中删除这个对象.</p> <p>这个**同步本地缓存的工作, 是 Informer 的第一个职责, 也是它最重要的职责. **</p> <p>而 <strong>Informer 的第二个职责, 则是根据这些事件的类型, 触发事先注册好的 ResourceEventHandler</strong>. 这些 Handler 需要在创建控制器的时候<strong>注册给它对应的 Informer</strong>.</p> <p>接下来就来编写这个控制器的定义, 它的主要内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token function">NewController</span><span class="token punctuation">(</span>
  kubeclientset kubernetes<span class="token punctuation">.</span>Interface<span class="token punctuation">,</span>
  networkclientset clientset<span class="token punctuation">.</span>Interface<span class="token punctuation">,</span>
  networkInformer informers<span class="token punctuation">.</span>NetworkInformer<span class="token punctuation">)</span> <span class="token operator">*</span>Controller <span class="token punctuation">{</span>
  <span class="token operator">...</span>
  controller <span class="token operator">:=</span> <span class="token operator">&amp;</span>Controller<span class="token punctuation">{</span>
    kubeclientset<span class="token punctuation">:</span>    kubeclientset<span class="token punctuation">,</span>
    networkclientset<span class="token punctuation">:</span> networkclientset<span class="token punctuation">,</span>
    networksLister<span class="token punctuation">:</span>   networkInformer<span class="token punctuation">.</span><span class="token function">Lister</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    networksSynced<span class="token punctuation">:</span>   networkInformer<span class="token punctuation">.</span><span class="token function">Informer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>HasSynced<span class="token punctuation">,</span>
    workqueue<span class="token punctuation">:</span>        workqueue<span class="token punctuation">.</span><span class="token function">NewNamedRateLimitingQueue</span><span class="token punctuation">(</span><span class="token operator">...</span><span class="token punctuation">,</span>  <span class="token string">&quot;Networks&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token operator">...</span>
  <span class="token punctuation">}</span>
    networkInformer<span class="token punctuation">.</span><span class="token function">Informer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">AddEventHandler</span><span class="token punctuation">(</span>cache<span class="token punctuation">.</span>ResourceEventHandlerFuncs<span class="token punctuation">{</span>
    AddFunc<span class="token punctuation">:</span> controller<span class="token punctuation">.</span>enqueueNetwork<span class="token punctuation">,</span>
    UpdateFunc<span class="token punctuation">:</span> <span class="token keyword">func</span><span class="token punctuation">(</span>old<span class="token punctuation">,</span> <span class="token builtin">new</span> <span class="token keyword">interface</span><span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
      oldNetwork <span class="token operator">:=</span> old<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">*</span>samplecrdv1<span class="token punctuation">.</span>Network<span class="token punctuation">)</span>
      newNetwork <span class="token operator">:=</span> <span class="token builtin">new</span><span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">*</span>samplecrdv1<span class="token punctuation">.</span>Network<span class="token punctuation">)</span>
      <span class="token keyword">if</span> oldNetwork<span class="token punctuation">.</span>ResourceVersion <span class="token operator">==</span> newNetwork<span class="token punctuation">.</span>ResourceVersion <span class="token punctuation">{</span>
        <span class="token keyword">return</span>
      <span class="token punctuation">}</span>
      controller<span class="token punctuation">.</span><span class="token function">enqueueNetwork</span><span class="token punctuation">(</span><span class="token builtin">new</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    DeleteFunc<span class="token punctuation">:</span> controller<span class="token punctuation">.</span>enqueueNetworkForDelete<span class="token punctuation">,</span>
 <span class="token keyword">return</span> controller
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>**前面在 main 函数里创建了两个 client(kubeclientset 和 networkclientset), 然后在这段代码里, 使用这两个 client 和前面创建的 Informer, 初始化了自定义控制器. **</p> <p>值得注意的是, 在这个自定义控制器里, 还设置了一个<strong>工作队列</strong>(work queue), 它正是处于示意图中间位置的 WorkQueue. 这个工作队列的作用是, 负责<strong>同步 Informer 和控制循环之间的数据</strong>. 实际上, Kubernetes 项目提供了很多个工作队列的实现, 可以根据需要选择合适的库直接使用.</p> <p><strong>然后, 为 networkInformer 注册了三个 Handler(AddFunc, UpdateFunc 和 DeleteFunc), 分别对应 API 对象的 &quot;添加&quot;, &quot;更新&quot; 和 &quot;删除&quot; 事件. 而具体的处理操作, 都是将该事件对应的 API 对象加入到工作队列中. ** 需要注意的是, 实际</strong>入队的并不是 API 对象本身, 而是它们的 Key**, 即: 该 API 对象的 <code>/</code>​. 而后面即将编写的<strong>控制循环</strong>, 则会不断地从这个工作队列里拿到这些 Key, 然后开始执行真正的控制逻辑.</p> <p>综合上面的讲述, 你现在应该就能明白, <strong>所谓 Informer, 其实就是一个带有本地缓存和索引机制的, 可以注册 EventHandler 的 client</strong>. 它是自定义控制器跟 APIServer 进行数据同步的重要组件. 更具体地说, <strong>Informer 通过一种叫作 ListAndWatch 的方法, 把 APIServer 中的 API 对象缓存在了本地, 并负责更新和维护这个缓存</strong>.</p> <p>其中, ListAndWatch 方法的含义是: 首先, 通过 APIServer 的 LIST API &quot;获取&quot; 所有最新版本的 API 对象; 然后, 再通过 WATCH API 来 &quot;监听&quot; 所有这些 API 对象的变化. 而通过监听到的事件变化, Informer 就可以实时地更新本地缓存, 并且调用这些事件对应的 <strong>EventHandler</strong> 了. 此外在这个过程中, 每经过 resyncPeriod 指定的时间, Informer 维护的本地缓存, 都会使用最近一次 LIST 返回的结果<strong>强制更新</strong>一次, 从而保证缓存的有效性. 在 Kubernetes 中, 这个缓存强制更新的操作就叫作: <strong>resync</strong>.</p> <p>需要注意的是, 这个定时 resync 操作, 也会触发 Informer 注册的 &quot;更新&quot; 事件. 但此时, 这个 &quot;更新&quot; 事件对应的 Network 对象实际上<strong>并没有发生变化</strong>, 即: <strong>新, 旧两个 Network 对象的 ResourceVersion 是一样的</strong>. 在这种情况下, Informer 就不需要对这个更新事件再做进一步的处理了. 这也是为什么在上面的 UpdateFunc 方法里, 先判断了一下新, 旧两个 Network 对象的版本(ResourceVersion)是否发生了变化, 然后才开始进行的入队操作.</p> <p>以上, 就是 Kubernetes 中的 Informer 库的工作原理了.</p> <p>接下来, 就来到了示意图中最后面的<strong>控制循环</strong>(Control Loop)部分, 也正是在 main 函数最后调用 controller.Run() 启动的 &quot;控制循环&quot;. 它的主要内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>c <span class="token operator">*</span>Controller<span class="token punctuation">)</span> <span class="token function">Run</span><span class="token punctuation">(</span>threadiness <span class="token builtin">int</span><span class="token punctuation">,</span> stopCh <span class="token operator">&lt;-</span><span class="token keyword">chan</span> <span class="token keyword">struct</span><span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
  <span class="token keyword">if</span> ok <span class="token operator">:=</span> cache<span class="token punctuation">.</span><span class="token function">WaitForCacheSync</span><span class="token punctuation">(</span>stopCh<span class="token punctuation">,</span> c<span class="token punctuation">.</span>networksSynced<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token operator">!</span>ok <span class="token punctuation">{</span>
    <span class="token keyword">return</span> fmt<span class="token punctuation">.</span><span class="token function">Errorf</span><span class="token punctuation">(</span><span class="token string">&quot;failed to wait for caches to sync&quot;</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
  
  <span class="token operator">...</span>
  <span class="token keyword">for</span> i <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> threadiness<span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">{</span>
    <span class="token keyword">go</span> wait<span class="token punctuation">.</span><span class="token function">Until</span><span class="token punctuation">(</span>c<span class="token punctuation">.</span>runWorker<span class="token punctuation">,</span> time<span class="token punctuation">.</span>Second<span class="token punctuation">,</span> stopCh<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
  
  <span class="token operator">...</span>
  <span class="token keyword">return</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>可以看到, 启动控制循环的逻辑非常简单:</p> <ul><li>首先, 等待 Informer 完成一次<strong>本地缓存的数据同步</strong>操作;</li> <li>然后, 直接通过 goroutine 启动一个(或者并发启动多个) &quot;<strong>无限循环</strong>&quot; 的任务.</li></ul> <p><strong>而这个 &quot;无限循环&quot; 任务的每一个循环周期, 执行的正是我们真正关心的业务逻辑.</strong></p> <p>所以接下来就来编写这个<strong>自定义控制器的业务逻辑</strong>, 它的主要内容如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>c <span class="token operator">*</span>Controller<span class="token punctuation">)</span> <span class="token function">runWorker</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">for</span> c<span class="token punctuation">.</span><span class="token function">processNextWorkItem</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
 
<span class="token keyword">func</span> <span class="token punctuation">(</span>c <span class="token operator">*</span>Controller<span class="token punctuation">)</span> <span class="token function">processNextWorkItem</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token builtin">bool</span> <span class="token punctuation">{</span>
  obj<span class="token punctuation">,</span> shutdown <span class="token operator">:=</span> c<span class="token punctuation">.</span>workqueue<span class="token punctuation">.</span><span class="token function">Get</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  
  <span class="token operator">...</span>
  
  err <span class="token operator">:=</span> <span class="token keyword">func</span><span class="token punctuation">(</span>obj <span class="token keyword">interface</span><span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
    <span class="token operator">...</span>
    <span class="token keyword">if</span> err <span class="token operator">:=</span> c<span class="token punctuation">.</span><span class="token function">syncHandler</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span><span class="token punctuation">;</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
     <span class="token keyword">return</span> fmt<span class="token punctuation">.</span><span class="token function">Errorf</span><span class="token punctuation">(</span><span class="token string">&quot;error syncing '%s': %s&quot;</span><span class="token punctuation">,</span> key<span class="token punctuation">,</span> err<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
  
    c<span class="token punctuation">.</span>workqueue<span class="token punctuation">.</span><span class="token function">Forget</span><span class="token punctuation">(</span>obj<span class="token punctuation">)</span>
    <span class="token operator">...</span>
    <span class="token keyword">return</span> <span class="token boolean">nil</span>
  <span class="token punctuation">}</span><span class="token punctuation">(</span>obj<span class="token punctuation">)</span>
  
  <span class="token operator">...</span>
  
  <span class="token keyword">return</span> <span class="token boolean">true</span>
<span class="token punctuation">}</span>
 
<span class="token keyword">func</span> <span class="token punctuation">(</span>c <span class="token operator">*</span>Controller<span class="token punctuation">)</span> <span class="token function">syncHandler</span><span class="token punctuation">(</span>key <span class="token builtin">string</span><span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
 
  namespace<span class="token punctuation">,</span> name<span class="token punctuation">,</span> err <span class="token operator">:=</span> cache<span class="token punctuation">.</span><span class="token function">SplitMetaNamespaceKey</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span>
  <span class="token operator">...</span>
  
  network<span class="token punctuation">,</span> err <span class="token operator">:=</span> c<span class="token punctuation">.</span>networksLister<span class="token punctuation">.</span><span class="token function">Networks</span><span class="token punctuation">(</span>namespace<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">Get</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>
  <span class="token keyword">if</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
    <span class="token keyword">if</span> errors<span class="token punctuation">.</span><span class="token function">IsNotFound</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>
      glog<span class="token punctuation">.</span><span class="token function">Warningf</span><span class="token punctuation">(</span><span class="token string">&quot;Network does not exist in local cache: %s/%s, will delete it from Neutron ...&quot;</span><span class="token punctuation">,</span>
      namespace<span class="token punctuation">,</span> name<span class="token punctuation">)</span>
    
      glog<span class="token punctuation">.</span><span class="token function">Warningf</span><span class="token punctuation">(</span><span class="token string">&quot;Network: %s/%s does not exist in local cache, will delete it from Neutron ...&quot;</span><span class="token punctuation">,</span>
    namespace<span class="token punctuation">,</span> name<span class="token punctuation">)</span>
  
     <span class="token comment">// FIX ME: call Neutron API to delete this network by name.</span>
     <span class="token comment">//</span>
     <span class="token comment">// neutron.Delete(namespace, name)</span>
   
     <span class="token keyword">return</span> <span class="token boolean">nil</span>
  <span class="token punctuation">}</span>
    <span class="token operator">...</span>
  
    <span class="token keyword">return</span> err
  <span class="token punctuation">}</span>
  
  glog<span class="token punctuation">.</span><span class="token function">Infof</span><span class="token punctuation">(</span><span class="token string">&quot;[Neutron] Try to process network: %#v ...&quot;</span><span class="token punctuation">,</span> network<span class="token punctuation">)</span>
  
  <span class="token comment">// FIX ME: Do diff().</span>
  <span class="token comment">//</span>
  <span class="token comment">// actualNetwork, exists := neutron.Get(namespace, name)</span>
  <span class="token comment">//</span>
  <span class="token comment">// if !exists {</span>
  <span class="token comment">//   neutron.Create(namespace, name)</span>
  <span class="token comment">// } else if !reflect.DeepEqual(actualNetwork, network) {</span>
  <span class="token comment">//   neutron.Update(namespace, name)</span>
  <span class="token comment">// }</span>
  
  <span class="token keyword">return</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br></div></div><p>可以看到, 在这个执行周期里(processNextWorkItem), <strong>首先</strong>从工作队列里出队(workqueue.Get)了一个成员, 也就是一个 <strong>Key</strong>(Network 对象的: namespace/name).</p> <p><strong>然后</strong>, 在 syncHandler 方法中, 使用这个 Key, <strong>尝试从 Informer 维护的缓存中拿到了它所对应的 Network 对象</strong>.</p> <p>可以看到, 在这里使用了 <strong>networksLister</strong> 来尝试获取这个 Key 对应的 Network 对象. 这个操作其实就是在访问本地缓存的索引. 实际上在 Kubernetes 的源码中, 会经常看到控制器从各种 Lister 里获取对象, 比如: podLister, nodeLister 等等, 它们使用的<strong>都是 Informer 和缓存机制</strong>.</p> <p>而如果控制循环从缓存中拿不到这个对象(即: networkLister 返回了 IsNotFound 错误), 那就意味着这个 Network 对象的 Key 是通过前面的 &quot;删除&quot; 事件添加进工作队列的. 所以尽管队列里有这个 Key, 但是对应的 Network 对象已经被删除了. 这时候就需要调用 Neutron 的 API, 把这个 Key 对应的 Neutron 网络从真实的集群里删除掉.</p> <p>**而如果能够获取到对应的 Network 对象, 就可以执行控制器模式里的对比 &quot;期望状态&quot; 和 &quot;实际状态&quot; 的逻辑了. ** 其中, 自定义控制器 &quot;千辛万苦&quot; 拿到的这个 Network 对象, <strong>正是 APIServer 里保存的 &quot;期望状态&quot;</strong> , 即: <strong>用户通过 YAML 文件提交到 APIServer 里的信息</strong>. 当然这个例子里, 它已经被 Informer 缓存在了本地.</p> <p><strong>那么 &quot;实际状态&quot;又从哪里来呢? ** 当然是</strong>来自于实际的集群<strong>了. 所以</strong>控制循环需要通过 Neutron API 来查询实际的网络情况**.比如可以先通过 Neutron 来查询这个 Network 对象对应的真实网络是否存在.</p> <ul><li>如果不存在, 这就是一个典型的 &quot;期望状态&quot; 与 &quot;实际状态&quot; 不一致的情形. 这时就需要使用这个 Network 对象里的信息(比如: CIDR 和 Gateway), 调用 Neutron API 来<strong>创建</strong>真实的网络.</li> <li>如果存在, 那就要读取这个真实网络的信息, 判断它是否跟 Network 对象里的信息一致, 从而决定是否要通过 Neutron 来<strong>更新</strong>这个已经存在的真实网络.</li></ul> <p><strong>这样就通过对比 &quot;期望状态&quot; 和 &quot;实际状态&quot; 的差异, 完成了一次调协(Reconcile)的过程.</strong></p> <p>至此, 一个完整的自定义 API 对象和它所对应的自定义控制器, 就编写完毕了. 与 Neutron 相关的业务代码并不是节的重点, 所以仅仅通过注释里的伪代码来表述了这部分内容. 如果你对这些代码感兴趣的话, 可以自行完成. 最简单的情况可以自己编写一个 Neutron Mock, 然后输出对应的操作日志.</p> <p>接下来就把这个项目运行起来, 查看一下它的工作情况. 可以自己编译这个项目, 也可以直接使用我编译好的二进制文件(samplecrd-controller). 编译并启动这个项目的具体流程如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># Clone repo</span>
$ <span class="token function">git</span> clone https://github.com/resouer/k8s-controller-custom-resource$ <span class="token builtin class-name">cd</span> k8s-controller-custom-resource
 
<span class="token comment">### Skip this part if you don't want to build</span>
<span class="token comment"># Install dependency</span>
$ go get github.com/tools/godep
$ godep restore
<span class="token comment"># Build</span>
$ go build <span class="token parameter variable">-o</span> samplecrd-controller <span class="token builtin class-name">.</span>
 
$ ./samplecrd-controller <span class="token parameter variable">-kubeconfig</span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/.kube/config <span class="token parameter variable">-alsologtostderr</span><span class="token operator">=</span>true
I0915 <span class="token number">12</span>:50:29.051349   <span class="token number">27159</span> controller.go:84<span class="token punctuation">]</span> Setting up event handlers
I0915 <span class="token number">12</span>:50:29.051615   <span class="token number">27159</span> controller.go:113<span class="token punctuation">]</span> Starting Network control loop
I0915 <span class="token number">12</span>:50:29.051630   <span class="token number">27159</span> controller.go:116<span class="token punctuation">]</span> Waiting <span class="token keyword">for</span> informer caches to <span class="token function">sync</span>
E0915 <span class="token number">12</span>:50:29.066745   <span class="token number">27159</span> reflector.go:134<span class="token punctuation">]</span> github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1.Network: the server could not <span class="token function">find</span> the requested resource <span class="token punctuation">(</span>get networks.samplecrd.k8s.io<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>可以看到, 自定义控制器被启动后, 一开始会报错. 这是因为, 此时 Network 对象的 <strong>CRD 还没有被创建</strong>出来, 所以 Informer 去 APIServer 里 &quot;获取&quot;(List)Network 对象时, 并不能找到 Network 这个 API 资源类型的定义, 即:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>Failed to list *v1.Network: the server could not <span class="token function">find</span> the requested resource <span class="token punctuation">(</span>get networks.samplecrd.k8s.io<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>所以接下来就需要创建 Network 对象的 CRD, 这个操作在上一篇文章里已经介绍过了.</p> <p>在另一个 shell 窗口里执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> crd/network.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时候就会看到控制器的日志恢复了正常, 控制循环启动成功:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:50:29.051630   <span class="token number">27159</span> controller.go:116<span class="token punctuation">]</span> Waiting <span class="token keyword">for</span> informer caches to <span class="token function">sync</span>
<span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:52:54.346854   <span class="token number">25245</span> controller.go:121<span class="token punctuation">]</span> Starting workers
I0915 <span class="token number">12</span>:52:54.346914   <span class="token number">25245</span> controller.go:127<span class="token punctuation">]</span> Started workers
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>接下来就可以<strong>进行 Network 对象的增删改查</strong>操作了.</p> <p>首先创建一个 Network 对象:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">cat</span> example/example-network.yaml 
apiVersion: samplecrd.k8s.io/v1
kind: Network
metadata:
  name: example-network
spec:
  cidr: <span class="token string">&quot;192.168.0.0/16&quot;</span>
  gateway: <span class="token string">&quot;192.168.0.1&quot;</span>
  
$ kubectl apply <span class="token parameter variable">-f</span> example/example-network.yaml 
network.samplecrd.k8s.io/example-network created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>这时候, 查看一下控制器的输出:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:50:29.051349   <span class="token number">27159</span> controller.go:84<span class="token punctuation">]</span> Setting up event handlers
I0915 <span class="token number">12</span>:50:29.051615   <span class="token number">27159</span> controller.go:113<span class="token punctuation">]</span> Starting Network control loop
I0915 <span class="token number">12</span>:50:29.051630   <span class="token number">27159</span> controller.go:116<span class="token punctuation">]</span> Waiting <span class="token keyword">for</span> informer caches to <span class="token function">sync</span>
<span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:52:54.346854   <span class="token number">25245</span> controller.go:121<span class="token punctuation">]</span> Starting workers
I0915 <span class="token number">12</span>:52:54.346914   <span class="token number">25245</span> controller.go:127<span class="token punctuation">]</span> Started workers
I0915 <span class="token number">12</span>:53:18.064409   <span class="token number">25245</span> controller.go:229<span class="token punctuation">]</span> <span class="token punctuation">[</span>Neutron<span class="token punctuation">]</span> Try to process network: <span class="token operator">&amp;</span>v1.Network<span class="token punctuation">{</span>TypeMeta:v1.TypeMeta<span class="token punctuation">{</span>Kind:<span class="token string">&quot;&quot;</span>, APIVersion:<span class="token string">&quot;&quot;</span><span class="token punctuation">}</span>, ObjectMeta:v1.ObjectMeta<span class="token punctuation">{</span>Name:<span class="token string">&quot;example-network&quot;</span>, GenerateName:<span class="token string">&quot;&quot;</span>, Namespace:<span class="token string">&quot;default&quot;</span>, <span class="token punctuation">..</span>. ResourceVersion:<span class="token string">&quot;479015&quot;</span>, <span class="token punctuation">..</span>. Spec:v1.NetworkSpec<span class="token punctuation">{</span>Cidr:<span class="token string">&quot;192.168.0.0/16&quot;</span>, Gateway:<span class="token string">&quot;192.168.0.1&quot;</span><span class="token punctuation">}</span><span class="token punctuation">}</span> <span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:53:18.064650   <span class="token number">25245</span> controller.go:183<span class="token punctuation">]</span> Successfully synced <span class="token string">'default/example-network'</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到上面创建 example-network 的操作, 触发了 EventHandler 的 &quot;添加&quot; 事件, 从而被放进了工作队列. 紧接着, <strong>控制循环就从队列里拿到了这个对象, 并且打印出了正在 &quot;处理&quot; 这个 Network 对象的日志</strong>. 可以看到, 这个 Network 的 ResourceVersion, 也就是 API 对象的版本号, 是 479015, 而它的 Spec 字段的内容, 跟提交的 YAML 文件一摸一样, 比如, 它的 CIDR 网段是: 192.168.0.0/16.</p> <p>这时候来修改一下这个 YAML 文件的内容, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">cat</span> example/example-network.yaml 
apiVersion: samplecrd.k8s.io/v1
kind: Network
metadata:
  name: example-network
spec:
  cidr: <span class="token string">&quot;192.168.1.0/16&quot;</span>
  gateway: <span class="token string">&quot;192.168.1.1&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 这里把这个 YAML 文件里的 CIDR 和 Gateway 字段的修改成了 192.168.1.0/16 网段.</p> <p>然后执行 kubectl apply 命令来提交这次更新, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> example/example-network.yaml 
network.samplecrd.k8s.io/example-network configured
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这时候就可以观察一下控制器的输出:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:53:51.126029   <span class="token number">25245</span> controller.go:229<span class="token punctuation">]</span> <span class="token punctuation">[</span>Neutron<span class="token punctuation">]</span> Try to process network: <span class="token operator">&amp;</span>v1.Network<span class="token punctuation">{</span>TypeMeta:v1.TypeMeta<span class="token punctuation">{</span>Kind:<span class="token string">&quot;&quot;</span>, APIVersion:<span class="token string">&quot;&quot;</span><span class="token punctuation">}</span>, ObjectMeta:v1.ObjectMeta<span class="token punctuation">{</span>Name:<span class="token string">&quot;example-network&quot;</span>, GenerateName:<span class="token string">&quot;&quot;</span>, Namespace:<span class="token string">&quot;default&quot;</span>, <span class="token punctuation">..</span>.  ResourceVersion:<span class="token string">&quot;479062&quot;</span>, <span class="token punctuation">..</span>. Spec:v1.NetworkSpec<span class="token punctuation">{</span>Cidr:<span class="token string">&quot;192.168.1.0/16&quot;</span>, Gateway:<span class="token string">&quot;192.168.1.1&quot;</span><span class="token punctuation">}</span><span class="token punctuation">}</span> <span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:53:51.126348   <span class="token number">25245</span> controller.go:183<span class="token punctuation">]</span> Successfully synced <span class="token string">'default/example-network'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到, 这一次, Informer 注册的  <strong>&quot;更新&quot; 事件被触发</strong>, 更新后的 Network 对象的 Key 被添加到了工作队列之中.</p> <p>所以, 接下来控制循环从工作队列里拿到的 Network 对象, 与前一个对象是不同的: 它的 <strong>ResourceVersion</strong> 的值变成了 479062; 而 Spec 里的字段, 则变成了 192.168.1.0/16 网段.</p> <p>最后再把这个对象删除掉:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete <span class="token parameter variable">-f</span> example/example-network.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这一次在控制器的输出里就可以看到, Informer 注册的  <strong>&quot;删除&quot; 事件被触发</strong>, 并且控制循环 &quot;调用&quot; Neutron API &quot;删除&quot; 了真实环境里的网络. 这个输出如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>W0915 <span class="token number">12</span>:54:09.738464   <span class="token number">25245</span> controller.go:212<span class="token punctuation">]</span> Network: default/example-network does not exist <span class="token keyword">in</span> <span class="token builtin class-name">local</span> cache, will delete it from Neutron <span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:54:09.738832   <span class="token number">25245</span> controller.go:215<span class="token punctuation">]</span> <span class="token punctuation">[</span>Neutron<span class="token punctuation">]</span> Deleting network: default/example-network <span class="token punctuation">..</span>.
I0915 <span class="token number">12</span>:54:09.738854   <span class="token number">25245</span> controller.go:183<span class="token punctuation">]</span> Successfully synced <span class="token string">'default/example-network'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>以上就是编写和使用自定义控制器的全部流程了.</p> <p>实际上, 这套流程不仅可以用在自定义 API 资源上, 也完全可以用在 Kubernetes <strong>原生的默认 API 对象</strong>上.</p> <p>比如在 main 函数里, 除了创建一个 Network Informer 外, 还可以初始化一个 Kubernetes 默认 API 对象的 Informer 工厂, 比如 Deployment 对象的 Informer. 这个具体做法如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token operator">...</span>
  
  kubeInformerFactory <span class="token operator">:=</span> kubeinformers<span class="token punctuation">.</span><span class="token function">NewSharedInformerFactory</span><span class="token punctuation">(</span>kubeClient<span class="token punctuation">,</span> time<span class="token punctuation">.</span>Second<span class="token operator">*</span><span class="token number">30</span><span class="token punctuation">)</span>
  
  controller <span class="token operator">:=</span> <span class="token function">NewController</span><span class="token punctuation">(</span>kubeClient<span class="token punctuation">,</span> exampleClient<span class="token punctuation">,</span>
  kubeInformerFactory<span class="token punctuation">.</span><span class="token function">Apps</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">V1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">Deployments</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
  networkInformerFactory<span class="token punctuation">.</span><span class="token function">Samplecrd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">V1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">Networks</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  
  <span class="token keyword">go</span> kubeInformerFactory<span class="token punctuation">.</span><span class="token function">Start</span><span class="token punctuation">(</span>stopCh<span class="token punctuation">)</span>
  <span class="token operator">...</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>在这段代码中, <strong>首先</strong>使用 Kubernetes 的 client(kubeClient)创建了一个工厂; <strong>然后</strong>用跟 Network 类似的处理方法, 生成了一个 Deployment Informer; <strong>接着</strong>把 Deployment Informer 传递给了自定义控制器; 当然也要调用 Start 方法来启动这个 Deployment Informer.</p> <p>而有了这个 Deployment Informer 后, 这个控制器也就<strong>持有了所有 Deployment 对象的信息</strong>. 接下来, 它既可以通过 deploymentInformer.Lister() 来获取 Etcd 里的所有 Deployment 对象, 也可以为这个 Deployment Informer 注册具体的 Handler 来.</p> <p>更重要的是, <strong>这就使得在这个自定义控制器里面, 可以通过对自定义 API 对象和默认 API 对象进行协同, 从而实现更加复杂的编排功能</strong>.</p> <p>比如: 用户每创建一个新的 Deployment, 这个自定义控制器, 就可以为它创建一个对应的 Network 供它使用.</p> <blockquote><p>总结</p></blockquote> <p>本节剖析了 Kubernetes API 编程范式的具体原理, 并编写了一个自定义控制器.</p> <p>这其中, 有如下几个概念和机制, 是一定要理解清楚的:</p> <p>所谓的 <strong>Informer</strong>, 就是一个<strong>自带缓存和索引机制</strong>, 可以触发 Handler 的客户端库. 这个本地缓存在 Kubernetes 中一般被称为 Store, 索引一般被称为 Index.</p> <p>Informer 使用了 Reflector 包, 它是一个可以<strong>通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装</strong>. Reflector 和 Informer 之间, 用到了一个 &quot;增量先进先出队列&quot; 进行协同. 而 Informer 与要编写的控制循环之间, 则使用了一个工作队列来进行协同.</p> <p>在实际应用中, <strong>除了控制循环之外的所有代码, 实际上都是 Kubernetes 为你自动生成的</strong>, 即: pkg/client/{informers, listers, clientset} 里的内容. 而这些自动生成的代码, 就为我们提供了一个可靠而高效地获取 API 对象 &quot;期望状态&quot; 的编程库.</p> <p><strong>所以作为开发者就只需要关注如何拿到 &quot;实际状态&quot;, 然后如何拿它去跟 &quot;期望状态&quot; 做对比, 从而决定接下来要做的业务逻辑即可</strong>. 以上内容, 就是 Kubernetes API 编程范式的核心思想.</p> <h4 id="_26-基于角色的权限控制-rbac"><a href="#_26-基于角色的权限控制-rbac" class="header-anchor">#</a> 26 | 基于角色的权限控制:RBAC</h4> <p>前面已经讲解了很多种 Kubernetes 内置的<strong>编排对象</strong>, 以及对应的控制器模式的实现原理. 此外还剖析了自定义 API 资源类型和控制器的编写方式. 这时候可能有这样一个想法: 控制器模式看起来好像也不难, 我<strong>能不能自己写一个编排对象</strong>呢?</p> <p>答案当然是可以的. 而且这才是 Kubernetes 项目最具吸引力的地方. 毕竟在互联网级别的大规模集群里, Kubernetes 内置的编排对象, 很难做到完全满足所有需求. 所以<strong>很多实际的容器化工作, 都会要求你设计一个自己的编排对象, 实现自己的控制器模式</strong>.</p> <p>而在 Kubernetes 项目里, 可以<strong>基于插件机制</strong>来完成这些工作, 而完全不需要修改任何一行代码. 不过要通过一个外部插件, 在 Kubernetes 里新增和操作 API 对象, 那么就必须先了解一个非常重要的知识: <strong>RBAC</strong>.</p> <p>前面提到 Kubernetes 中所有的 API 对象, 都保存在 <strong>Etcd</strong> 里. 可对这些 API 对象的操作, 却一定都是通过访问 kube-apiserver 实现的. 其中一个非常重要的原因, 就是需要 APIServer 来帮助你做<strong>授权工作</strong>. <strong>在 Kubernetes 项目中, 负责完成授权(Authorization)工作的机制, 就是 RBAC</strong>: <strong>基于角色的访问控制</strong>(Role-Based Access Control).</p> <p>如果直接查看 Kubernetes 项目中关于 RBAC 的文档的话, 可能会感觉非常复杂. 但实际上, 等需要用到这些 RBAC 的细节时, 再去查阅也不迟.</p> <p>而在这里, 需要明确三个最基本的概念.</p> <ol><li><strong>Role</strong>: 角色, 它其实是一组规则, 定义了一组对 Kubernetes API 对象的操作权限.</li> <li><strong>Subject</strong>: 被作用者, 既可以是 &quot;人&quot;, 也可以是 &quot;机器&quot;, 也可以是在 Kubernetes 里定义的 &quot;用户&quot;.</li> <li><strong>RoleBinding</strong>: 定义了 &quot;被作用者&quot; 和 &quot;角色&quot; 的绑定关系.</li></ol> <p>而这三个概念, 其实就是整个 RBAC 体系的核心所在.</p> <p>先来讲解一下 Role. 实际上, <strong>Role 本身就是一个 Kubernetes 的 API 对象</strong>, 定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Role
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> mynamespace
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>role
<span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;pods&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;get&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;watch&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;list&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>首先这个 Role 对象指定了它能产生作用的 Namepace 是: <strong>mynamespace</strong>.</p> <p><strong>Namespace 是 Kubernetes 项目里的一个逻辑管理单位. 不同 Namespace 的 API 对象, 在通过 kubectl 命令进行操作的时候, 是互相隔离开的</strong>. 比如, kubectl get pods -n mynamespace. 当然, 这仅限于<strong>逻辑</strong>上的 &quot;隔离&quot;, Namespace 并不会提供任何实际的隔离或者多租户能力. 而在前面用到的大多数例子里, 都没有指定 Namespace, 那就是使用的是<strong>默认 Namespace: default</strong>.</p> <p>然后, 这个 Role 对象的 <strong>rules 字段</strong>, 就是它所定义的<strong>权限规则</strong>. 在上面的例子里, 这条规则的含义就是: 允许 &quot;被作用者&quot;, 对 mynamespace 下面的 Pod 对象, 进行 GET, WATCH 和 LIST 操作.</p> <p>那么, 这个具体的 &quot;被作用者&quot; 又是如何指定的呢? 这就需要通过 <strong>RoleBinding</strong> 来实现了. 当然, <strong>RoleBinding 本身也是一个 Kubernetes 的 API 对象</strong>. 它的定义如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> RoleBinding
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>rolebinding
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> mynamespace
<span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> User
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>user
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>
  <span class="token key atrule">kind</span><span class="token punctuation">:</span> Role
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>role
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>可以看到, 这个 RoleBinding 对象里定义了一个 <strong>subjects 字段, 即 &quot;被作用者&quot;.</strong>  它的类型是 User, 即 Kubernetes 里的用户. 这个用户的名字是 example-user. 可是在 Kubernetes 中, 其实并没有一个叫作 &quot;User&quot; 的 API 对象. 而且在前面和部署使用 Kubernetes 的流程里, 既不需要 User, 也没有创建过 User.</p> <p>**这个 User 到底是从哪里来的呢? **</p> <p>实际上, Kubernetes 里的 &quot;User&quot;, 也就是 &quot;用户&quot;, 只是一个授权系统里的<strong>逻辑概念</strong>. 它需要通过外部认证服务, 比如 Keystone 来提供. 或者也可以直接给 APIServer 指定一个用户名, 密码文件. 那么 Kubernetes 的授权系统, 就能够从这个文件里找到对应的 &quot;用户&quot;了. 当然, 在大多数私有的使用环境中, 只要使用 Kubernetes 提供的内置 &quot;用户&quot;, 就足够了. 这部分后面马上会讲到.</p> <p>接下来会看到一个 <strong>roleRef 字段</strong>. 正是通过这个字段, RoleBinding 对象就可以直接通过名字, 来引用前面定义的 Role 对象(example-role), 从而定义了  <strong>&quot;被作用者(Subject)&quot; 和 &quot;角色(Role)&quot; 之间的绑定关系</strong>.</p> <p>需要再次提醒的是, Role 和 RoleBinding 对象都是 <strong>Namespaced</strong> 对象(Namespaced Object), 它们对权限的限制规则仅在它们<strong>自己的 Namespace 内有效</strong>, roleRef 也只能引用当前 Namespace 里的 Role 对象.</p> <p>那么, **对于非 Namespaced(Non-namespaced)对象(比如: Node), 或者某一个 Role 想要作用于所有的 Namespace 的时候, 又该如何去做授权呢? **</p> <p>这时候就必须要使用 <strong>ClusterRole 和 ClusterRoleBinding</strong> 这两个组合了. 这两个 API 对象的用法跟 Role 和 RoleBinding 完全一样. 只不过它们的定义里, <strong>没有了 Namespace 字段</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>clusterrole
<span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;pods&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;get&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;watch&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;list&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>clusterrolebinding
<span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> User
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>user
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>
  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>clusterrole
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>上面的例子里的 ClusterRole 和 ClusterRoleBinding 的组合, 意味着名叫 example-user 的用户, 拥有对所有 Namespace 里的 Pod 进行 <strong>GET, WATCH 和 LIST</strong> 操作的权限. 更进一步地, 在 Role 或者 ClusterRole 里面, 如果要赋予用户 example-user 所有权限, 那就可以给它指定一个 verbs 字段的全集, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;get&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;list&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;watch&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;create&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;update&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;patch&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;delete&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这些就是当前 Kubernetes(v1.11)里能够对 API 对象进行的所有操作了.</p> <p>类似的, Role 对象的 rules 字段也可以进一步细化. 比如可以只<strong>针对某一个具体的对象</strong>进行权限设置, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;configmaps&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;my-config&quot;</span><span class="token punctuation">]</span>
  <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;get&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这个例子就表示, 这条规则的 &quot;被作用者&quot;, 只对名叫 &quot;my-config&quot; 的 ConfigMap 对象, 有进行 GET 操作的权限.</p> <p>而正我前面介绍过的, 在大多数时候, 其实都不太使用 &quot;用户&quot; 这个功能, 而是直接使用 Kubernetes 里的 &quot;内置用户&quot;. 这个由 Kubernetes 负责管理的 &quot;<strong>内置用户</strong>&quot;, 正是前面曾经提到过的: <strong>ServiceAccount</strong>.</p> <p>接下来通过一个具体的实例来讲解一下为 ServiceAccount 分配权限的过程.</p> <p><strong>首先, 要定义一个 ServiceAccount</strong>. 它的 API 对象非常简单, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> mynamespace
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>sa
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, 一个最简单的 ServiceAccount 对象只需要 <strong>Name 和 Namespace</strong> 这两个最基本的字段.</p> <p>**然后, 通过编写 RoleBinding 的 YAML 文件, 来为这个 ServiceAccount 分配权限: **</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> RoleBinding
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>rolebinding
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> mynamespace
<span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>sa
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> mynamespace
<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>
  <span class="token key atrule">kind</span><span class="token punctuation">:</span> Role
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>role
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>可以看到, 在这个 RoleBinding 对象里, subjects 字段的类型(kind), 不再是一个 User, 而是一个名叫 example-sa 的 ServiceAccount. 而 roleRef 引用的 Role 对象, 依然名叫 example-role, 也就是在本节一开始定义的 Role 对象.</p> <p>**接着, 用 kubectl 命令创建这三个对象: **</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> svc-account.yaml
$ kubectl create <span class="token parameter variable">-f</span> role-binding.yaml
$ kubectl create <span class="token parameter variable">-f</span> role.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后来查看一下这个 ServiceAccount 的详细信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get sa <span class="token parameter variable">-n</span> mynamespace <span class="token parameter variable">-o</span> yaml
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: <span class="token number">2018</span>-09-08T12:59:17Z
    name: example-sa
    namespace: mynamespace
    resourceVersion: <span class="token string">&quot;409327&quot;</span>
    <span class="token punctuation">..</span>.
  secrets:
  - name: example-sa-token-vmfg6
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>可以看到, Kubernetes 会<strong>为一个 ServiceAccount 自动创建并分配一个 Secret 对象</strong>, 即: 上述 ServiceAcount 定义里最下面的 secrets 字段. 这个 Secret, 就是这个 ServiceAccount 对应的, 用来<strong>跟 APIServer 进行交互的授权文件</strong>, 一般称它为: Token. <strong>Token 文件的内容一般是证书或者密码, 它以一个 Secret 对象的方式保存在 Etcd 当中</strong>.</p> <p>这时候<strong>用户的 Pod, 就可以声明使用这个 ServiceAccount</strong> 了, 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> mynamespace
  <span class="token key atrule">name</span><span class="token punctuation">:</span> sa<span class="token punctuation">-</span>token<span class="token punctuation">-</span>test
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.7.9
  <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>sa
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>在这个例子里, 定义了 Pod 要使用的要使用的 ServiceAccount 的名字是: example-sa.</p> <p>等这个 Pod 运行起来之后, 就可以看到, 该 ServiceAccount 的 token, 也就是一个 <strong>Secret 对象</strong>, 被 Kubernetes <strong>自动挂载到了容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录</strong>下, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pod sa-token-test <span class="token parameter variable">-n</span> mynamespace
Name:               sa-token-test
Namespace:          mynamespace
<span class="token punctuation">..</span>.
Containers:
  nginx:
    <span class="token punctuation">..</span>.
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from example-sa-token-vmfg6 <span class="token punctuation">(</span>ro<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>这时候可以通过 kubectl exec 查看到这个目录里的文件:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> sa-token-test <span class="token parameter variable">-n</span> mynamespace -- /bin/bash
root@sa-token-test:/<span class="token comment"># ls /var/run/secrets/kubernetes.io/serviceaccount</span>
ca.crt namespace  token
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>如上所示, <mark><strong>容器里的应用, 就可以使用这个 ca.crt 来访问 APIServer 了</strong></mark>. 更重要的是, 此时它只能够做 GET, WATCH 和 LIST 操作. 因为 example-sa 这个 ServiceAccount 的权限, 已经被我们绑定了 Role 做了限制.</p> <p>此外, 前面曾经提到过, <strong>如果一个 Pod 没有声明 serviceAccountName, Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount, 然后分配给这个 Pod</strong>. 但在这种情况下, 这个默认 ServiceAccount 并没有关联任何 Role. 也就是说, 此时<strong>它有访问 APIServer 的绝大多数权限</strong>. 当然这个访问所需要的 Token, 还是默认 ServiceAccount 对应的 Secret 对象为它提供的, 如下所示.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token variable">$kubectl</span> describe sa default
Name:                default
Namespace:           default
Labels:              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Annotations:         <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Image pull secrets:  <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Mountable secrets:   default-token-s8rbq
Tokens:              default-token-s8rbq
Events:              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
 
$ kubectl get secret
NAME                  TYPE                                  DATA      AGE
kubernetes.io/service-account-token   <span class="token number">3</span>         82d
 
$ kubectl describe secret default-token-s8rbq
Name:         default-token-s8rbq
Namespace:    default
Labels:       <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Annotations:  kubernetes.io/service-account.name<span class="token operator">=</span>default
              kubernetes.io/service-account.uid<span class="token operator">=</span>ffcb12b2-917f-11e8-abde-42010aa80002
 
Type:  kubernetes.io/service-account-token
 
Data
<span class="token operator">&lt;</span>mark<span class="token operator">&gt;</span><span class="token operator">&lt;</span>/mark<span class="token operator">&gt;</span>
ca.crt:     <span class="token number">1025</span> bytes
namespace:  <span class="token number">7</span> bytes
token:      <span class="token operator">&lt;</span>TOKEN 数据 <span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><p>可以看到, Kubernetes 会自动为默认 ServiceAccount 创建并绑定一个特殊的 Secret: 它的类型是<code>kubernetes.io/service-account-token</code>​; 它的 Annotation 字段, 声明了 <code>kubernetes.io/service-account.name=default</code>​, 即这个 Secret 会跟同一 Namespace 下名叫 default 的 ServiceAccount 进行绑定.</p> <p>所以在<mark><strong>生产环境中强烈建议为所有 Namespace 下的默认 ServiceAccount, 绑定一个只读权限的 Role</strong></mark>.</p> <p>除了前面使用的 &quot;用户&quot;(User), Kubernetes 还拥有 &quot;<strong>用户组</strong>&quot;(Group)的概念, 也就是一组 &quot;用户&quot; 的意思. 如果为 Kubernetes 配置了外部认证服务的话, 这个 &quot;用户组&quot; 的概念就会由外部认证服务提供. 而对于 Kubernetes 的内置 &quot;用户&quot; ServiceAccount 来说, 上述 &quot;用户组&quot; 的概念也同样适用.</p> <p>实际上, 一个 ServiceAccount, 在 Kubernetes 里对应的&quot;用户&quot;的名字是:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>system:serviceaccount:<span class="token operator">&lt;</span>ServiceAccount 名字<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>而它对应的内置 &quot;用户组&quot; 的名字, 就是:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>system:serviceaccounts:<span class="token operator">&lt;</span>Namespace 名字<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>**这两个对应关系, 请一定要牢记. **</p> <p>比如现在可以在 RoleBinding 里定义如下的 subjects:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> Group
  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>serviceaccounts<span class="token punctuation">:</span>mynamespace
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这就意味着这个 Role 的权限规则, 作用于 mynamespace 里的所有 ServiceAccount. 这就用到了 &quot;用户组&quot; 的概念.</p> <p>而下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> Group
  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>serviceaccounts
  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>就意味着这个 Role 的权限规则, 作用于<strong>整个系统里的所有 ServiceAccount</strong>.</p> <p>最后值得一提的是, <strong>在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole, 它们的名字都以 system: 开头</strong>. 可以通过 kubectl get clusterroles 查看到它们. 一般来说, 这些系统 ClusterRole 是绑定给 Kubernetes <strong>系统组件</strong>对应的 ServiceAccount 使用的.</p> <p>比如, 其中一个名叫 system:kube-scheduler 的 ClusterRole, 定义的权限规则是 kube-scheduler(Kubernetes 的调度器组件)运行所需要的必要权限. 可以通过如下指令查看这些权限的列表:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl describe clusterrole system<span class="token punctuation">:</span>kube<span class="token punctuation">-</span>scheduler
<span class="token key atrule">Name</span><span class="token punctuation">:</span>         system<span class="token punctuation">:</span>kube<span class="token punctuation">-</span>scheduler
<span class="token punctuation">...</span>
<span class="token key atrule">PolicyRule</span><span class="token punctuation">:</span>
  Resources                    Non<span class="token punctuation">-</span>Resource URLs Resource Names    Verbs
  <span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span>                    <span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">-</span><span class="token punctuation">-</span>  <span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">-</span><span class="token punctuation">-</span>    <span class="token punctuation">---</span><span class="token punctuation">-</span><span class="token punctuation">-</span>
<span class="token punctuation">...</span>
  services                     <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token punctuation">[</span>get list watch<span class="token punctuation">]</span>
  replicasets.apps             <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token punctuation">[</span>get list watch<span class="token punctuation">]</span>
  statefulsets.apps            <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token punctuation">[</span>get list watch<span class="token punctuation">]</span>
  replicasets.extensions       <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token punctuation">[</span>get list watch<span class="token punctuation">]</span>
  poddisruptionbudgets.policy  <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token punctuation">[</span>get list watch<span class="token punctuation">]</span>
  pods/status                  <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token punctuation">[</span>patch update<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>这个 system:kube-scheduler 的 ClusterRole, 就会被绑定给 kube-system Namesapce 下名叫 kube-scheduler 的 ServiceAccount, 它正是 Kubernetes 调度器的 Pod 声明使用的 ServiceAccount.</p> <p>除此之外, Kubernetes 还提供了<strong>四个预先定义好的 ClusterRole</strong> 来供用户直接使用:</p> <ol><li>cluster-admin;</li> <li>admin;</li> <li>edit;</li> <li>view.</li></ol> <p>通过它们的名字应该能大致猜出它们都定义了哪些权限. 比如这个名叫 view 的 ClusterRole, 就规定了被作用者只有 Kubernetes API 的<strong>只读</strong>权限. 注意上面这个 cluster-admin 角色, 对应的是整个 Kubernetes 项目中的<strong>最高权限</strong>(verbs=*), 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl describe clusterrole cluster<span class="token punctuation">-</span>admin <span class="token punctuation">-</span>n kube<span class="token punctuation">-</span>system
<span class="token key atrule">Name</span><span class="token punctuation">:</span>         cluster<span class="token punctuation">-</span>admin
<span class="token key atrule">Labels</span><span class="token punctuation">:</span>       kubernetes.io/bootstrapping=rbac<span class="token punctuation">-</span>defaults
<span class="token key atrule">Annotations</span><span class="token punctuation">:</span>  rbac.authorization.kubernetes.io/autoupdate=true
<span class="token key atrule">PolicyRule</span><span class="token punctuation">:</span>
  Resources  Non<span class="token punctuation">-</span>Resource URLs Resource Names  Verbs
  <span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span>  <span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">-</span><span class="token punctuation">-</span>  <span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token punctuation">-</span><span class="token punctuation">-</span>  <span class="token punctuation">---</span><span class="token punctuation">-</span><span class="token punctuation">-</span>
  <span class="token important">*.*</span>        <span class="token punctuation">[</span><span class="token punctuation">]</span>                 <span class="token punctuation">[</span><span class="token punctuation">]</span>              <span class="token punctuation">[</span>*<span class="token punctuation">]</span>
             <span class="token punctuation">[</span>*<span class="token punctuation">]</span>                <span class="token punctuation">[</span><span class="token punctuation">]</span>              <span class="token punctuation">[</span>*<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>所以务必要谨慎而小心地使用 cluster-admin.</p> <blockquote><p>总结</p></blockquote> <p>本节注意讲解了基于角色的访问控制(RBAC). 所谓角色(Role), 其实就是<strong>一组权限规则列表</strong>. 而<strong>分配这些权限的方式, 就是通过创建 RoleBinding 对象, 将被作用者(subject)和权限列表进行绑定</strong>. 另外, 与之对应的 ClusterRole 和 ClusterRoleBinding, 则是 Kubernetes 集群级别的 Role 和 RoleBinding, 它们的作用范围不受 Namespace 限制.</p> <p>而尽管权限的被作用者可以有很多种(比如, User, Group 等), 但在平常的使用中, 最普遍的用法还是 <strong>ServiceAccount</strong>. 所以, <mark><strong>Role + RoleBinding + ServiceAccount 的权限分配方式</strong></mark>是要重点掌握的内容. 在后面编写和安装各种插件的时候, 会经常用到这个组合.</p> <h4 id="_27-聪明的微创新-operator工作原理解读"><a href="#_27-聪明的微创新-operator工作原理解读" class="header-anchor">#</a> 27 | 聪明的微创新:Operator工作原理解读</h4> <p>前面部分分享了 Kubernetes 项目中的大部分<strong>编排对象</strong>(比如 Deployment, StatefulSet, DaemonSet, 以及 Job), 也介绍了 &quot;有状态应用&quot; 的管理方法, 还阐述了为 Kubernetes 添加自定义 API 对象和编写自定义控制器的原理和流程.</p> <p>可能你已经感觉到, 在 Kubernetes 中, <strong>管理 &quot;有状态应用&quot; 是一个比较复杂的过程</strong>, 尤其是编写 Pod 模板的时候, 总有一种 &quot;在 YAML 文件里编程序&quot; 的感觉, 让人很不舒服. 而在 Kubernetes 生态中, 还有一个<strong>相对更加灵活和编程友好的管理 &quot;有状态应用&quot; 的解决方案, 它就是: Operator</strong>.</p> <p>接下来就以 Etcd Operator 为例, 来讲解一下 Operator 的工作原理和编写方法.</p> <p>Etcd Operator 的使用方法非常简单, 只需要两步即可完成:</p> <p>**第一步, 将这个 Operator 的代码 Clone 到本地: **</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">git</span> clone https://github.com/coreos/etcd-operator
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>**第二步, 将这个 Etcd Operator 部署在 Kubernetes 集群里. ** 不过, 在部署 Etcd Operator 的 Pod 之前, 需要先执行这样一个脚本:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ example/rbac/create_role.sh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个脚本的作用, 就是为 Etcd Operator <strong>创建 RBAC 规则</strong>. 这是因为 Etcd Operator 需要<strong>访问 Kubernetes 的 APIServer 来创建对象</strong>. 更具体地说, 上述脚本为 Etcd Operator 定义了如下所示的权限:</p> <ol><li>对 Pod, Service, PVC, Deployment, Secret 等 API 对象, 有所有权限;</li> <li>对 CRD 对象, 有所有权限;</li> <li>对属于 etcd.database.coreos.com 这个 API Group 的 CR(Custom Resource)对象, 有所有权限.</li></ol> <p>而 <strong>Etcd Operator 本身, 其实就是一个 Deployment</strong>, 它的 YAML 文件如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>operator
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>operator
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>operator
        <span class="token key atrule">image</span><span class="token punctuation">:</span> quay.io/coreos/etcd<span class="token punctuation">-</span>operator<span class="token punctuation">:</span>v0.9.2
        <span class="token key atrule">command</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> etcd<span class="token punctuation">-</span>operator
        <span class="token key atrule">env</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> MY_POD_NAMESPACE
          <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
            <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
              <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.namespace
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> MY_POD_NAME
          <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
            <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
              <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.name
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>所以就可以使用<strong>上述的 YAML 文件来创建 Etcd Operator</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> example/deployment.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>而一旦 Etcd Operator 的 Pod 进入了 Running 状态, 就会发现, <strong>有一个 CRD 被自动创建</strong>了出来, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME                              READY     STATUS      RESTARTS   AGE
etcd-operator-649dbdb5cb-bzfzp    <span class="token number">1</span>/1       Running     <span class="token number">0</span>          20s

$ kubectl get crd
NAME                                    CREATED AT
etcdclusters.etcd.database.coreos.com   <span class="token number">2018</span>-09-18T11:42:55Z
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这个 CRD 名叫 <code>etcdclusters.etcd.database.coreos.com</code>​. 可以通过 kubectl describe 命令看到它的细节, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe crd  etcdclusters.etcd.database.coreos.com
<span class="token punctuation">..</span>.
Group:   etcd.database.coreos.com
  Names:
    Kind:       EtcdCluster
    List Kind:  EtcdClusterList
    Plural:     etcdclusters
    Short Names:
      etcd
    Singular:  etcdcluster
  Scope:       Namespaced
  Version:     v1beta2
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>可以看到, 这个 CRD 相当于<strong>告诉了 Kubernetes</strong>: 接下来如果有 API 组(Group)是 <code>etcd.database.coreos.com</code>​, API 资源类型(Kind)是 &quot;EtcdCluster&quot; 的 YAML 文件被提交上来, 你可一定要认识啊.</p> <p>所以通过上述两步操作, <strong>实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型. 而 Etcd Operator 本身, 就是这个自定义资源类型对应的自定义控制器</strong>.</p> <p>而当 Etcd Operator 部署好之后, 接下来在这个 Kubernetes 里<strong>创建一个 Etcd 集群</strong>的工作就非常简单了. 只需要编写一个 EtcdCluster 的 YAML 文件, 然后把它提交给 Kubernetes 即可, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> example/example-etcd-cluster.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个 example-etcd-cluster.yaml 文件里描述的, 是一个 3 个节点的 Etcd 集群. 可以看到它被提交给 Kubernetes 之后, 就会有三个 Etcd 的 Pod 运行起来, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
example-etcd-cluster-dp8nqtjznc   <span class="token number">1</span>/1       Running     <span class="token number">0</span>          1m
example-etcd-cluster-mbzlg6sd56   <span class="token number">1</span>/1       Running     <span class="token number">0</span>          2m
example-etcd-cluster-v6v6s6stxd   <span class="token number">1</span>/1       Running     <span class="token number">0</span>          2m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>那究竟发生了什么, 让创建一个 Etcd 集群的工作如此简单呢? 当然还是得从这个 example-etcd-cluster.yaml 文件开始说起.</p> <p>不难想到, 这个文件里定义的, <strong>正是 EtcdCluster 这个 CRD 的一个具体实例</strong>, 也就是一个 Custom Resource(CR). 而它的内容非常简单, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> <span class="token string">&quot;etcd.database.coreos.com/v1beta2&quot;</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> <span class="token string">&quot;EtcdCluster&quot;</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">&quot;example-etcd-cluster&quot;</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">version</span><span class="token punctuation">:</span> <span class="token string">&quot;3.2.13&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可以看到, EtcdCluster 的 <strong>spec 字段非常简单</strong>. 其中 size=3 指定了它所描述的 Etcd 集群的节点个数. 而 version=&quot;3.2.13&quot;, 则指定了 Etcd 的版本, 仅此而已.</p> <p>**而真正把这样一个 Etcd 集群创建出来的逻辑, 就是 Etcd Operator 要实现的主要工作了. **</p> <p>看到这里, 相信你应该已经对 Operator 有了一个初步的认知: **Operator 的工作原理, 实际上是利用了 Kubernetes 的自定义 API 资源(CRD), 来描述想要部署的 &quot;有状态应用&quot;; 然后在自定义控制器里, 根据自定义 API 对象的变化, 来完成具体的部署和运维工作. **</p> <p>所以编写一个 Etcd Operator, 与前面编写一个自定义控制器的过程, 没什么不同.</p> <p>不过, 考虑到你可能还不太清楚 Etcd 集群的组建方式, 这里先简单介绍一下这部分知识.</p> <p><strong>Etcd Operator 部署 Etcd 集群, 采用的是静态集群(Static)的方式</strong>. 静态集群的好处是, 它不必依赖于一个额外的服务发现机制来组建集群, 非常适合本地容器化部署. 而它的难点则在于必须在部署的时候, 就规划好这个集群的拓扑结构, 并且能够知道这些节点固定的 IP 地址. 比如下面这个例子:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ etcd <span class="token parameter variable">--name</span> infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 <span class="token punctuation">\</span>
  --listen-peer-urls http://10.0.1.10:2380 <span class="token punctuation">\</span>
<span class="token punctuation">..</span>.
  --initial-cluster-token etcd-cluster-1 <span class="token punctuation">\</span>
  --initial-cluster <span class="token assign-left variable">infra0</span><span class="token operator">=</span>http://10.0.1.10:2380,infra1<span class="token operator">=</span>http://10.0.1.11:2380,infra2<span class="token operator">=</span>http://10.0.1.12:2380 <span class="token punctuation">\</span>
  --initial-cluster-state new
  
$ etcd <span class="token parameter variable">--name</span> infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 <span class="token punctuation">\</span>
  --listen-peer-urls http://10.0.1.11:2380 <span class="token punctuation">\</span>
<span class="token punctuation">..</span>.
  --initial-cluster-token etcd-cluster-1 <span class="token punctuation">\</span>
  --initial-cluster <span class="token assign-left variable">infra0</span><span class="token operator">=</span>http://10.0.1.10:2380,infra1<span class="token operator">=</span>http://10.0.1.11:2380,infra2<span class="token operator">=</span>http://10.0.1.12:2380 <span class="token punctuation">\</span>
  --initial-cluster-state new
  
$ etcd <span class="token parameter variable">--name</span> infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 <span class="token punctuation">\</span>
  --listen-peer-urls http://10.0.1.12:2380 <span class="token punctuation">\</span>
<span class="token punctuation">..</span>.
  --initial-cluster-token etcd-cluster-1 <span class="token punctuation">\</span>
  --initial-cluster <span class="token assign-left variable">infra0</span><span class="token operator">=</span>http://10.0.1.10:2380,infra1<span class="token operator">=</span>http://10.0.1.11:2380,infra2<span class="token operator">=</span>http://10.0.1.12:2380 <span class="token punctuation">\</span>
  --initial-cluster-state new
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>在这个例子中, 启动了三个 Etcd 进程, 组成了一个三节点的 Etcd 集群.</p> <p>其中这些节点启动参数里的 –initial-cluster 参数, 非常值得关注. 它的含义正是<strong>当前节点启动时集群的拓扑结构. <strong>​<strong><strong>说得更详细一点, 就是</strong></strong>​</strong>当前这个节点启动时, 需要跟哪些节点通信来组成集群</strong>.</p> <p>举个例子, 可以看一下上述 infra2 节点的 –initial-cluster 的值, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
--initial-cluster <span class="token assign-left variable">infra0</span><span class="token operator">=</span>http://10.0.1.10:2380,infra1<span class="token operator">=</span>http://10.0.1.11:2380,infra2<span class="token operator">=</span>http://10.0.1.12:2380 <span class="token punctuation">\</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以看到, –initial-cluster 参数是由 &quot;<code>&lt; 节点名字 &gt;=&lt; 节点地址 &gt;</code>​&quot; 格式组成的一个数组. 而上面这个配置的意思就是, 当 infra2 节点启动之后, 这个 Etcd 集群里就会有 <strong>infra0, infra1 和 infra2</strong> 三个节点. 同时这些 Etcd 节点, 需要通过 2380 端口进行通信以便组成集群, 这也正是上述配置中–listen-peer-urls 字段的含义.</p> <p>此外, 一个 Etcd 集群还需要用 –initial-cluster-token 字段, 来声明一个该集群独一无二的 <strong>Token</strong> 名字.</p> <p>像上述这样<strong>为每一个 Ectd 节点配置好它对应的启动参数之后把它们启动起来</strong>, 一个 Etcd 集群就可以自动组建起来了. 而要<strong>编写的 Etcd Operator, 就是要把上述过程自动化</strong>. 这其实等同于: <strong>用代码来生成每个 Etcd 节点 Pod 的启动命令, 然后把它们启动起来</strong>.</p> <p>接下来实践一下这个流程.</p> <p>当然在编写自定义控制器之前, 首先需要<strong>完成 EtcdCluster 这个 CRD 的定义</strong>, 它对应的 types.go 文件的主要内容, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token comment">// +genclient</span>
<span class="token comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span>

<span class="token keyword">type</span> EtcdCluster <span class="token keyword">struct</span> <span class="token punctuation">{</span>
  metav1<span class="token punctuation">.</span>TypeMeta   <span class="token string">`json:&quot;,inline&quot;`</span>
  metav1<span class="token punctuation">.</span>ObjectMeta <span class="token string">`json:&quot;metadata,omitempty&quot;`</span>
  Spec              ClusterSpec   <span class="token string">`json:&quot;spec&quot;`</span>
  Status            ClusterStatus <span class="token string">`json:&quot;status&quot;`</span>
<span class="token punctuation">}</span>

<span class="token keyword">type</span> ClusterSpec <span class="token keyword">struct</span> <span class="token punctuation">{</span>
 <span class="token comment">// Size is the expected size of the etcd cluster.</span>
 <span class="token comment">// The etcd-operator will eventually make the size of the running</span>
 <span class="token comment">// cluster equal to the expected size.</span>
 <span class="token comment">// The vaild range of the size is from 1 to 7.</span>
 Size <span class="token builtin">int</span> <span class="token string">`json:&quot;size&quot;`</span>
 <span class="token operator">...</span> 
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>可以看到, EtcdCluster 是一个有 Status 字段的 CRD. 在这里可以不必关心 ClusterSpec 里的其他字段, 只关注 Size(即: Etcd 集群的大小)字段即可. Size 字段的存在, 就意味着将来<strong>如果想要调整集群大小的话, 应该直接修改 YAML 文件里 size 的值, 并执行 kubectl apply -f</strong>. 这样 Operator 就会帮我们完成 Etcd 节点的<strong>增删</strong>操作. 这种 &quot;scale&quot; 能力, 也是 Etcd Operator 自动化运维 Etcd 集群需要实现的主要功能.</p> <p>而为了能够支持这个功能, 就不再像前面那样在 –initial-cluster 参数里把拓扑结构固定死. 所以 Etcd Operator 的实现, 虽然选择的也是静态集群, 但这个<strong>集群具体的组建过程, 是逐个节点动态添加的方式</strong>, 即:</p> <ul><li>**首先, Etcd Operator 会创建一个 &quot;种子节点&quot;; **</li> <li>**然后, Etcd Operator 会不断创建新的 Etcd 节点, 然后将它们逐一加入到这个集群当中, 直到集群的节点数等于 size. **</li></ul> <p>这就意味着, 在生成不同角色的 Etcd Pod 时, Operator 需要能够区分种子节点与普通节点. 而这两种节点的不同之处, 就在于一个名叫  <strong>–initial-cluster-state</strong> 的启动参数:</p> <ul><li>当这个参数值设为 <strong>new</strong> 时, 就代表了该节点是<strong>种子节点</strong>. 前面提到过, 种子节点还必须通过 –initial-cluster-token 声明一个独一无二的 Token.</li> <li>而如果这个参数值设为 <strong>existing</strong>, 那就是说明这个节点是一个<strong>普通节点</strong>, Etcd Operator 需要把它加入到已有集群里.</li></ul> <p>那么接下来的问题就是, 每个 Etcd 节点的 –initial-cluster 字段的值又是怎么生成的呢?</p> <p>由于这个方案<strong>要求种子节点先启动</strong>, 所以对于种子节点 infra0 来说, 它启动后的集群只有它自己, 即: –initial-cluster=infra0=http://10.0.1.10:2380.</p> <p>而对于接下来要加入的节点, 比如 infra1 来说, 它启动后的集群就有两个节点了, 所以它的 –initial-cluster 参数的值应该是: infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380.</p> <p>其他节点, 都以此类推.</p> <p>现在你就应该能在脑海中构思出上述三节点 Etcd 集群的部署过程了.</p> <p>首先, <strong>只要用户提交 YAML 文件时声明创建一个 EtcdCluster 对象(一个 Etcd 集群), 那么 Etcd Operator 都应该先创建一个单节点的种子集群(Seed Member), 并启动这个种子节点</strong>.</p> <p>以 infra0 节点为例, 它的 IP 地址是 10.0.1.10, 那么 Etcd Operator 生成的种子节点的启动命令, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ etcd
  --data-dir<span class="token operator">=</span>/var/etcd/data
  <span class="token parameter variable">--name</span><span class="token operator">=</span>infra0
  --initial-advertise-peer-urls<span class="token operator">=</span>http://10.0.1.10:2380
  --listen-peer-urls<span class="token operator">=</span>http://0.0.0.0:2380
  --listen-client-urls<span class="token operator">=</span>http://0.0.0.0:2379
  --advertise-client-urls<span class="token operator">=</span>http://10.0.1.10:2379
  --initial-cluster<span class="token operator">=</span>infra0<span class="token operator">=</span>http://10.0.1.10:2380
  --initial-cluster-state<span class="token operator">=</span>new
  --initial-cluster-token<span class="token operator">=</span>4b5215fa-5401-4a95-a8c6-892317c9bef8
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到, 这个种子节点的 initial-cluster-state 是 <strong>new</strong>, 并且指定了唯一的 initial-cluster-token 参数.</p> <p>可以把这个创建种子节点(集群)的阶段称为: <strong>Bootstrap</strong>.</p> <p>接下来, <strong>对于其他每一个节点, Operator 只需要执行如下两个操作即可</strong>, 以 infra1 为例.</p> <p>第一步: 通过 Etcd 命令行添加一个新成员:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ etcdctl member <span class="token function">add</span> infra1 http://10.0.1.11:2380
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>第二步: 为这个成员节点生成对应的启动参数, 并启动它:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ etcd
    --data-dir<span class="token operator">=</span>/var/etcd/data
    <span class="token parameter variable">--name</span><span class="token operator">=</span>infra1
    --initial-advertise-peer-urls<span class="token operator">=</span>http://10.0.1.11:2380
    --listen-peer-urls<span class="token operator">=</span>http://0.0.0.0:2380
    --listen-client-urls<span class="token operator">=</span>http://0.0.0.0:2379
    --advertise-client-urls<span class="token operator">=</span>http://10.0.1.11:2379
    --initial-cluster<span class="token operator">=</span>infra0<span class="token operator">=</span>http://10.0.1.10:2380,infra1<span class="token operator">=</span>http://10.0.1.11:2380
    --initial-cluster-state<span class="token operator">=</span>existing
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>可以看到, 对于这个 infra1 成员节点来说, 它的 initial-cluster-state 是 <strong>existing</strong>, 也就是要加入已有集群. 而它的 initial-cluster 的值, 则<strong>变成了 infra0 和 infra1 两个节点的 IP 地址</strong>. 以此类推, 不断地将 infra2 等后续成员添加到集群中, 直到整个集群的节点数目等于用户指定的 size 之后, 部署就完成了.</p> <p>在熟悉了这个部署思路之后, 再讲解 Etcd Operator 的工作原理, 就非常简单了. 跟所有的自定义控制器一样, Etcd Operator 的<strong>启动流程也是围绕着 Informer 展开</strong>的, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>c <span class="token operator">*</span>Controller<span class="token punctuation">)</span> <span class="token function">Start</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">{</span>
  err <span class="token operator">:=</span> c<span class="token punctuation">.</span><span class="token function">initResource</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token operator">...</span>
  time<span class="token punctuation">.</span><span class="token function">Sleep</span><span class="token punctuation">(</span>initRetryWaitTime<span class="token punctuation">)</span>
 <span class="token punctuation">}</span>
 c<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">func</span> <span class="token punctuation">(</span>c <span class="token operator">*</span>Controller<span class="token punctuation">)</span> <span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
 
 <span class="token boolean">_</span><span class="token punctuation">,</span> informer <span class="token operator">:=</span> cache<span class="token punctuation">.</span><span class="token function">NewIndexerInformer</span><span class="token punctuation">(</span>source<span class="token punctuation">,</span> <span class="token operator">&amp;</span>api<span class="token punctuation">.</span>EtcdCluster<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> cache<span class="token punctuation">.</span>ResourceEventHandlerFuncs<span class="token punctuation">{</span>
  AddFunc<span class="token punctuation">:</span>    c<span class="token punctuation">.</span>onAddEtcdClus<span class="token punctuation">,</span>
  UpdateFunc<span class="token punctuation">:</span> c<span class="token punctuation">.</span>onUpdateEtcdClus<span class="token punctuation">,</span>
  DeleteFunc<span class="token punctuation">:</span> c<span class="token punctuation">.</span>onDeleteEtcdClus<span class="token punctuation">,</span>
 <span class="token punctuation">}</span><span class="token punctuation">,</span> cache<span class="token punctuation">.</span>Indexers<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
 
 ctx <span class="token operator">:=</span> context<span class="token punctuation">.</span><span class="token function">TODO</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token comment">// TODO: use workqueue to avoid blocking</span>
 informer<span class="token punctuation">.</span><span class="token function">Run</span><span class="token punctuation">(</span>ctx<span class="token punctuation">.</span><span class="token function">Done</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>可以看到, <strong>Etcd Operator 启动要做的第一件事</strong>(c.initResource), 是<strong>创建 EtcdCluster 对象所需要的 CRD</strong>, 即: 前面提到的 <code>etcdclusters.etcd.database.coreos.com</code>​. 这样 Kubernetes 就能够 &quot;认识&quot; EtcdCluster 这个自定义 API 资源了.</p> <p>而<strong>接下来, Etcd Operator 会定义一个 EtcdCluster 对象的 Informer</strong>.</p> <p>不过需要注意的是, 由于 Etcd Operator 的完成时间相对较早, 所以它里面有些代码的编写方式会跟之前讲解的最新的编写方式不太一样. 在具体实践的时候, 还是应该以我讲解的模板为主.</p> <p>比如, 在上面的代码最后有这样一句注释:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token comment">// TODO: use workqueue to avoid blocking...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>也就是说, Etcd Operator 并没有用工作队列来协调 Informer 和控制循环.</p> <p>具体来讲, 在控制循环里执行的业务逻辑, 往往是<strong>比较耗时间</strong>的. 比如创建一个真实的 Etcd 集群. 而 Informer 的 WATCH 机制对 API 对象变化的响应, 则<strong>非常迅速</strong>. 所以控制器里的业务逻辑就很可能会拖慢 Informer 的执行周期, 甚至可能 Block 它. 而要协调这样两个快, 慢任务的一个典型解决方法, 就是引入一个<strong>工作队列</strong>. 由于 Etcd Operator 里没有工作队列, 那么在它的 EventHandler 部分, 就不会有什么入队操作, 而直接就是每种事件对应的具体的业务逻辑了.</p> <p>不过 Etcd Operator 在业务逻辑的实现方式上, 与常规的自定义控制器略有不同. 这里把在这一部分的工作原理, 提炼成了一个详细的流程图, 如下所示:</p> <p><img src="/img/42ca60e721d34cd7c14fe7a38ce4680c-20230731162150-ss6xwfa.png" alt="">可以看到, Etcd Operator 的特殊之处在于, <strong>它为每一个 EtcdCluster 对象, 都启动了一个控制循环, &quot;并发&quot; 地响应这些对象的变化</strong>. 显然, 这种做法不仅可以简化 Etcd Operator 的代码实现, 还有助于提高它的响应速度.</p> <p>以一开始的 example-etcd-cluster 的 YAML 文件为例. 当这个 YAML 文件第一次被提交到 Kubernetes 之后, <strong>Etcd Operator 的 Informer</strong>, 就会立刻 &quot;感知&quot; 到一个新的 EtcdCluster 对象被创建了出来. 所以 EventHandler 里的 &quot;添加&quot; 事件会被触发.</p> <p>而这个 Handler 要做的操作也很简单, 即: <strong>在 Etcd Operator 内部创建一个对应的 Cluster 对象</strong>(cluster.New), 比如流程图里的 Cluster1. 这个 Cluster 对象, 就是一个 Etcd 集群在 Operator 内部的描述, 所以它与真实的 Etcd 集群的生命周期是一致的.</p> <p>而一个 Cluster 对象需要具体负责的, 其实有两个工作.</p> <p><strong>其中, 第一个工作只在该 Cluster 对象第一次被创建的时候才会执行. 这个工作就是前面提到过的 Bootstrap, 即: 创建一个单节点的种子集群. ** 由于种子集群只有一个节点, 所以</strong>这一步直接就会生成一个 Etcd 的 Pod 对象**. 这个 Pod 里有一个 InitContainer, 负责检查 Pod 的 DNS 记录是否正常. 如果检查通过, 用户容器也就是 Etcd 容器就会启动起来.</p> <p>而这个 Etcd 容器最重要的部分, 当然就是它的<strong>启动命令</strong>了.</p> <p>以一开始部署的集群为例, 它的种子节点的容器启动命令如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/usr/local/bin/etcd
  --data-dir<span class="token operator">=</span>/var/etcd/data
  <span class="token parameter variable">--name</span><span class="token operator">=</span>example-etcd-cluster-mbzlg6sd56
  --initial-advertise-peer-urls<span class="token operator">=</span>http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380
  --listen-peer-urls<span class="token operator">=</span>http://0.0.0.0:2380
  --listen-client-urls<span class="token operator">=</span>http://0.0.0.0:2379
  --advertise-client-urls<span class="token operator">=</span>http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2379
  --initial-cluster<span class="token operator">=</span>example-etcd-cluster-mbzlg6sd56<span class="token operator">=</span>http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380
  --initial-cluster-state<span class="token operator">=</span>new
  --initial-cluster-token<span class="token operator">=</span>4b5215fa-5401-4a95-a8c6-892317c9bef8
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>上述启动命令里的各个参数的含义, 已经在前面介绍过. 可以看到, 在这些启动参数(比如: initial-cluster)里, Etcd Operator <strong>只会使用 Pod 的 DNS 记录</strong>, 而不是它的 IP 地址. 这当然是因为在 Operator 生成上述启动命令的时候, Etcd 的 <strong>Pod 还没有被创建出来</strong>, 它的 IP 地址自然也无从谈起. 这也就意味着, <strong>每个 Cluster 对象, 都会事先创建一个与该 EtcdCluster 同名的 Headless Service. 这样 Etcd Operator 在接下来的所有创建 Pod 的步骤里, 就都可以使用 Pod 的 DNS 记录来代替它的 IP 地址了</strong>. ​备注: Headless Service 的 DNS 记录格式是: <code>...svc.cluster.local</code>​.</p> <p>**Cluster 对象的第二个工作, 则是启动该集群所对应的控制循环. **</p> <p>这个控制循环每隔一定时间, 就会执行一次下面的 <strong>Diff 流程</strong>. 首先, 控制循环要获取到所有正在运行的, 属于这个 Cluster 的 Pod 数量, 也就是该 Etcd 集群的 &quot;<strong>实际状态</strong>&quot;. 而这个 Etcd 集群的 &quot;<strong>期望状态</strong>&quot;, 正是用户在 EtcdCluster 对象里定义的 size.</p> <p>所以接下来, <strong>控制循环会对比这两个状态的差异. ** 如果实际的 Pod 数量不够, 那么控制循环就会执行一个</strong>添加<strong>成员节点的操作(即: 上述流程图中的 addOneMember 方法); 反之, 就执行</strong>删除**成员节点的操作(即: 上述流程图中的 removeOneMember 方法).</p> <p>以 addOneMember 方法为例, 它执行的流程如下所示:</p> <ol><li>生成一个新节点的 Pod 的名字, 比如: example-etcd-cluster-v6v6s6stxd;</li> <li>调用 Etcd Client, 执行前面提到过的 etcdctl member add example-etcd-cluster-v6v6s6stxd 命令;</li> <li>使用这个 Pod 名字, 和已经存在的所有节点列表, 组合成一个新的 initial-cluster 字段的值;</li> <li>使用这个 initial-cluster 的值, 生成这个 Pod 里 Etcd 容器的启动命令. 如下所示:</li></ol> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/usr/local/bin/etcd
  --data-dir<span class="token operator">=</span>/var/etcd/data
  <span class="token parameter variable">--name</span><span class="token operator">=</span>example-etcd-cluster-v6v6s6stxd
  --initial-advertise-peer-urls<span class="token operator">=</span>http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380
  --listen-peer-urls<span class="token operator">=</span>http://0.0.0.0:2380
  --listen-client-urls<span class="token operator">=</span>http://0.0.0.0:2379
  --advertise-client-urls<span class="token operator">=</span>http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2379
  --initial-cluster<span class="token operator">=</span>example-etcd-cluster-mbzlg6sd56<span class="token operator">=</span>http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380,example-etcd-cluster-v6v6s6stxd<span class="token operator">=</span>http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380
  --initial-cluster-state<span class="token operator">=</span>existing
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>这样当这个容器启动之后, 一个新的 Etcd 成员节点就被加入到了集群当中. 控制循环会<strong>重复</strong>这个过程, 直到<strong>正在运行的 Pod 数量与 EtcdCluster 指定的 size 一致</strong>.</p> <p>在有了这样一个与 EtcdCluster 对象一一对应的控制循环之后, 后续对这个 EtcdCluster 的任何修改, 比如: 修改 size 或者 Etcd 的 version, 它们<strong>对应的更新事件都会由这个 Cluster 对象的控制循环进行处理</strong>. 以上就是一个 Etcd Operator 的工作原理了.</p> <p>如果对比一下 Etcd Operator 与前面《深入理解 StatefulSet(三): 有状态应用实践》中讲解过的 MySQL StatefulSet 的话, 你可能会有两个问题.</p> <p><strong>第一个问题是</strong>, 在 StatefulSet 里, 它为 Pod 创建的名字是带<strong>编号</strong>的, 这样就把整个集群的拓扑状态固定了下来(比如: 一个三节点的集群一定是由名叫 web-0, web-1 和 web-2 的三个 Pod 组成). 可是**在 Etcd Operator 里, 为什么使用随机名字就可以了呢? **</p> <p>这是因为 Etcd Operator 在<strong>每次添加 Etcd 节点的时候</strong>, 都会先执行 <code>etcdctl member add &lt;Pod 名字&gt;;</code>​ 每次删除节点的时候, 则会执行 <code>etcdctl member remove &lt;Pod 名字&gt;</code>​. 这些操作其实就会<strong>更新 Etcd 内部维护的拓扑信息</strong>, 所以 Etcd Operator 无需在集群外部通过编号来固定这个拓扑关系.</p> <p>**第二个问题是, 为什么没有在 EtcdCluster 对象里声明 Persistent Volume?  ** 难道不担心节点宕机之后 Etcd 的数据会丢失吗?</p> <p>Etcd 是一个基于 Raft 协议实现的高可用 Key-Value 存储. 根据 Raft 协议的设计原则, 当 Etcd 集群里只有半数以下(本例中小于等于一个)的节点失效时, 当前集群依然可以正常工作. 此时 Etcd Operator 只需要通过控制循环创建出新的 Pod, 然后将它们加入到现有集群里, 就完成了 &quot;期望状态&quot; 与 &quot;实际状态&quot; 的调谐工作. 这个集群是一直可用的 .</p> <p>但是当这个 Etcd 集群里有半数以上(在我们的例子里, 大于等于两个)的节点失效的时候, 这个集群就会丧失数据<strong>写入</strong>的能力, 从而进入 &quot;不可用&quot; 状态. 此时即使 Etcd Operator 创建出新的 Pod 出来, Etcd 集群本身也无法自动恢复起来. 这个时候,就必须使用 Etcd 本身的<strong>备份数据</strong>来对集群进行<strong>恢复操作</strong>.</p> <p>在有了 Operator 机制之后, 上述 Etcd 的备份操作, 是由一个单独的 <strong>Etcd Backup Operator</strong> 负责完成的. 创建和使用这个 Operator 的流程, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 首先, 创建 etcd-backup-operator</span>
$ kubectl create <span class="token parameter variable">-f</span> example/etcd-backup-operator/deployment.yaml

<span class="token comment"># 确认 etcd-backup-operator 已经在正常运行</span>
$ kubectl get pod
NAME                                    READY     STATUS    RESTARTS   AGE
etcd-backup-operator-1102130733-hhgt7   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          3s

<span class="token comment"># 可以看到, Backup Operator 会创建一个叫 etcdbackups 的 CRD</span>
$ kubectl get crd
NAME                                    KIND
etcdbackups.etcd.database.coreos.com    CustomResourceDefinition.v1beta1.apiextensions.k8s.io

<span class="token comment"># 我们这里要使用 AWS S3 来存储备份, 需要将 S3 的授权信息配置在文件里</span>
$ <span class="token function">cat</span> <span class="token variable">$AWS_DIR</span>/credentials
<span class="token punctuation">[</span>default<span class="token punctuation">]</span>
aws_access_key_id <span class="token operator">=</span> XXX
aws_secret_access_key <span class="token operator">=</span> XXX

$ <span class="token function">cat</span> <span class="token variable">$AWS_DIR</span>/config
<span class="token punctuation">[</span>default<span class="token punctuation">]</span>
region <span class="token operator">=</span> <span class="token operator">&lt;</span>region<span class="token operator">&gt;</span>

<span class="token comment"># 然后, 将上述授权信息制作成一个 Secret</span>
$ kubectl create secret generic aws --from-file<span class="token operator">=</span><span class="token variable">$AWS_DIR</span>/credentials --from-file<span class="token operator">=</span><span class="token variable">$AWS_DIR</span>/config

<span class="token comment"># 使用上述 S3 的访问信息, 创建一个 EtcdBackup 对象</span>
$ <span class="token function">sed</span> <span class="token parameter variable">-e</span> <span class="token string">'s|&lt;full-s3-path&gt;|mybucket/etcd.backup|g'</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">-e</span> <span class="token string">'s|&lt;aws-secret&gt;|aws|g'</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">-e</span> <span class="token string">'s|&lt;etcd-cluster-endpoints&gt;|&quot;http://example-etcd-cluster-client:2379&quot;|g'</span> <span class="token punctuation">\</span>
    example/etcd-backup-operator/backup_cr.yaml <span class="token punctuation">\</span>
    <span class="token operator">|</span> kubectl create <span class="token parameter variable">-f</span> -
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><p>需要注意的是, 每当创建一个 EtcdBackup 对象(<a href="https://github.com/coreos/etcd-operator/blob/master/example/etcd-backup-operator/backup_cr.yaml" target="_blank" rel="noopener noreferrer">backup_cr.yaml<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>), 就相当于为它所指定的 Etcd 集群做了一次<strong>备份</strong>. EtcdBackup 对象的 etcdEndpoints 字段, 会指定它要备份的 Etcd 集群的访问地址.</p> <p>**所以在实际的环境里, 建议把最后这个备份操作, 编写成一个 Kubernetes 的 CronJob 以便定时运行. ** 而当 Etcd 集群发生了故障之后, 就可以通过创建一个 EtcdRestore 对象来完成恢复操作. 当然这就意味着需要事先启动 Etcd Restore Operator.</p> <p>这个流程的完整过程, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 创建 etcd-restore-operator</span>
$ kubectl create <span class="token parameter variable">-f</span> example/etcd-restore-operator/deployment.yaml

<span class="token comment"># 确认它已经正常运行</span>
$ kubectl get pods
NAME                                     READY     STATUS    RESTARTS   AGE
etcd-restore-operator-4203122180-npn3g   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          7s

<span class="token comment"># 创建一个 EtcdRestore 对象, 来帮助 Etcd Operator 恢复数据, 记得替换模板里的 S3 的访问信息</span>
$ <span class="token function">sed</span> <span class="token parameter variable">-e</span> <span class="token string">'s|&lt;full-s3-path&gt;|mybucket/etcd.backup|g'</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">-e</span> <span class="token string">'s|&lt;aws-secret&gt;|aws|g'</span> <span class="token punctuation">\</span>
    example/etcd-restore-operator/restore_cr.yaml <span class="token punctuation">\</span>
    <span class="token operator">|</span> kubectl create <span class="token parameter variable">-f</span> -
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>上面例子里的 EtcdRestore 对象(<a href="https://github.com/coreos/etcd-operator/blob/master/example/etcd-restore-operator/restore_cr.yaml" target="_blank" rel="noopener noreferrer">restore_cr.yaml<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>), 会指定它要恢复的 Etcd 集群的名字和备份数据所在的 S3 存储的访问信息.</p> <p>而当一个 EtcdRestore 对象成功创建后, Etcd Restore Operator 就会通过上述信息, 恢复出一个全新的 Etcd 集群. 然后 Etcd Operator 会把这个新集群直接接管过来, 从而重新进入可用的状态.</p> <p>EtcdBackup 和 EtcdRestore 这两个 Operator 的工作原理, 与 Etcd Operator 的实现方式非常类似.</p> <blockquote><p>总结</p></blockquote> <p>本节以 Etcd Operator 为例, 详细介绍了一个 Operator 的工作原理和编写过程. 可以看到, Etcd 集群本身就拥有良好的分布式设计和一定的高可用能力. 在这种情况下, StatefulSet &quot;为 Pod 编号&quot; 和 &quot;将 Pod 同 PV 绑定&quot; 这两个主要的特性, 就不太有用武之地了.</p> <p>而相比之下, <strong>Etcd Operator 把一个 Etcd 集群, 抽象成了一个具有一定 &quot;自治能力&quot; 的整体</strong>. 而当这个 &quot;自治能力&quot; 本身不足以解决问题的时候, 可以通过两个专门负责备份和恢复的 Operator 进行修正. 这种实现方式, 不仅更加贴近 Etcd 的设计思想, 也更加编程友好.</p> <p>不过, 如果现在要部署的应用, 既需要用 StatefulSet 的方式维持拓扑状态和存储状态, 又有大量的编程工作要做, 那到底该如何选择呢?</p> <p>其实, <strong>Operator 和 StatefulSet 并不是竞争关系</strong>. 完全可以编写一个 Operator, 然后在 Operator 的控制循环里创建和控制 StatefulSet 而不是 Pod. 比如业界知名的 <a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener noreferrer">Prometheus 项目的 Operator<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 正是这么实现的.</p> <p>此外, CoreOS 公司在被 RedHat 公司收购之后, 已经把 Operator 的编写过程封装成了一个叫作 Operator SDK 的工具(整个项目叫作 Operator Framework), 它可以帮助你生成 Operator 的框架代码.</p> <h3 id="kubernetes容器持久化存储"><a href="#kubernetes容器持久化存储" class="header-anchor">#</a> Kubernetes容器持久化存储</h3> <h4 id="_28-pv-pvc-storageclass-这些到底在说啥"><a href="#_28-pv-pvc-storageclass-这些到底在说啥" class="header-anchor">#</a> 28 | PV,PVC,StorageClass,这些到底在说啥?</h4> <p>前面分析了 Kubernetes 的各种编排能力. 你应该已经发现, 容器化一个应用比较麻烦的地方, 莫过于对其  <strong>&quot;状态&quot; 的管理</strong>. 而最常见的&quot;状态&quot;, 又莫过于存储状态了. 所以后面<strong>通过几节来剖析 Kubernetes 项目处理容器持久化存储的核心原理</strong>, 从而帮助更好地理解和使用这部分内容.</p> <p>首先回忆一下前面分享 StatefulSet 如何管理存储状态的时候, 介绍过的 <strong>Persistent Volume(PV) 和 Persistent Volume Claim(PVC)</strong>  这套持久化存储体系. 其中 <strong>PV 描述的, 是持久化存储数据卷</strong>. 这个 API 对象主要定义的是一个<strong>持久化存储在宿主机上的目录</strong>, 比如一个 NFS 的挂载目录. 通常情况下, PV 对象是由运维人员事先创建在 Kubernetes 集群里待用的. 比如运维人员可以定义这样一个 NFS 类型的 PV, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nfs
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> manual
  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>
    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> ReadWriteMany
  <span class="token key atrule">nfs</span><span class="token punctuation">:</span>
    <span class="token key atrule">server</span><span class="token punctuation">:</span> 10.244.1.4
    <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;/&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>而 <mark><strong>PVC 描述的, 则是 Pod 所希望使用的持久化存储的属性</strong></mark>. 比如 Volume 存储的大小, 可读写权限等等. PVC 对象通常由<strong>开发人员</strong>创建; 或者以 PVC 模板的方式成为 StatefulSet 的一部分, 然后由 StatefulSet 控制器负责创建带编号的 PVC.</p> <p>比如, 开发人员可以声明一个 1 GiB 大小的 PVC, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nfs
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> ReadWriteMany
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> manual
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>而用户创建的 PVC 要真正被容器使用起来, 就<strong>必须先和某个符合条件的 PV 进行绑定</strong>. 这里要检查的条件, 包括两部分:</p> <ul><li>第一个条件, 当然是 PV 和 PVC 的 spec 字段. 比如 PV 的存储(storage)大小, 就必须满足 PVC 的要求.</li> <li>而第二个条件, 则是 PV 和 PVC 的 <strong>storageClassName</strong> 字段必须一样. 这个机制会在本节的最后一部分专门介绍.</li></ul> <p>在成功地将 PVC 和 PV 进行绑定之后, Pod 就能够像使用 hostPath 等常规类型的 Volume 一样, 在自己的 YAML 文件里声明使用这个 PVC 了, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">role</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> web
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> web
        <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nfs
          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&quot;/usr/share/nginx/html&quot;</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nfs
    <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span>
      <span class="token key atrule">claimName</span><span class="token punctuation">:</span> nfs
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>可以看到, <strong>Pod 需要做的, 就是在 volumes 字段里声明自己要使用的 PVC 名字</strong>. 接下来, 等这个 Pod 创建之后, kubelet 就会把这个 PVC 所对应的 PV, 也就是一个 NFS 类型的 Volume, 挂载在这个 Pod 容器内的目录上.</p> <p>不难看出, **PVC 和 PV 的设计, 其实跟 &quot;面向对象&quot; 的思想完全一致. **</p> <p>PVC 可以理解为持久化存储的 &quot;<strong>接口</strong>&quot;, 它提供了对某种持久化存储的<strong>描述</strong>, 但不提供具体的实现; 而这个<strong>持久化存储的实现部分则由 PV 负责完成</strong>. 这样做的好处是, 作为应用开发者, <strong>只需要跟 PVC 这个 &quot;接口&quot; 打交道</strong>, 而不必关心具体的实现是 NFS 还是 Ceph. 毕竟这些存储相关的知识太专业了, 应该交给专业的人去做.</p> <p>而在上面的讲述中, 其实还有一个比较棘手的情况. 比如在创建 Pod 的时候, 系统里并没有合适的 PV 跟它定义的 PVC 绑定, 也就是说此时容器想要使用的 Volume 不存在. 这时候 Pod 的启动就会报错. 但过了一会儿, 运维人员也发现了这个情况, 所以他赶紧创建了一个对应的 PV. 这时候当然希望 Kubernetes 能够<strong>再次完成 PVC 和 PV 的绑定操作, 从而启动 Pod</strong>.</p> <p>所以在 Kubernetes 中, 实际上存在着一个<strong>专门处理持久化存储</strong>的控制器, 叫作 <strong>Volume Controller</strong>. 这个 Volume Controller 维护着<strong>多个控制循环</strong>, 其中有一个循环, 扮演的就是撮合 PV 和 PVC 的 &quot;红娘&quot; 的角色. 它的名字叫作 <strong>PersistentVolumeController</strong>. PersistentVolumeController 会<strong>不断地查看当前每一个 PVC, 是不是已经处于 Bound(已绑定)状态. 如果不是, 那它就会遍历所有的可用的 PV, 并尝试将其与这个 &quot;单身&quot; 的 PVC 进行绑定</strong>. 这样 Kubernetes 就可以保证用户提交的每一个 PVC, 只要有合适的 PV 出现, 它就能够很快进入<strong>绑定</strong>状态, 从而结束 &quot;单身&quot; 之旅.</p> <p><strong>而所谓将一个 PV 与 PVC 进行 &quot;绑定&quot;, 其实就是将这个 PV 对象的名字, 填在了 PVC 对象的 spec.volumeName 字段上. 所以接下来 Kubernetes 只要获取到这个 PVC 对象, 就一定能够找到它所绑定的 PV.</strong></p> <p>那么这个 PV 对象, 又是如何变成容器里的一个<strong>持久化存储</strong>的呢?</p> <p>在前面讲解容器基础的时候, 已经详细剖析了容器 Volume 的挂载机制. 用一句话总结, <mark><strong>所谓容器的 Volume, 其实就是将一个宿主机上的目录, 跟一个容器里的目录绑定挂载在了一起. 而所谓的 &quot;持久化 Volume&quot;, 指的就是这个宿主机上的目录, 具备&quot;持久性&quot;</strong></mark> . 即: <strong>这个目录里面的内容, 既不会因为容器的删除而被清理掉, 也不会跟当前的宿主机绑定</strong>. 这样当容器被重启或者在其他节点上重建出来之后, 它仍然能够通过挂载这个 Volume, 访问到这些内容.</p> <p>显然, 前面使用的 <strong>hostPath 和 emptyDir 类型</strong>的 Volume 并不具备这个特征: <strong>它们既可能被 kubelet 清理掉, 也不能被 &quot;迁移&quot; 到其他节点上</strong>.</p> <p>所以大多数情况下, <mark><strong>持久化 Volume 的实现, 往往依赖于一个远程存储服务</strong></mark>, 比如: 远程文件存储(比如 NFS, GlusterFS), 远程块存储(比如, 公有云提供的远程磁盘)等等. 而 Kubernetes 需要做的工作, 就是使用这些存储服务, 来<strong>为容器准备一个持久化的宿主机目录, 以供将来进行绑定挂载时使用</strong>. 而所谓 &quot;持久化&quot;, 指的是<strong>容器在这个目录里写入的文件, 都会保存在远程存储中, 从而使得这个目录具备了 &quot;持久性&quot;</strong> .</p> <p>**这个准备 &quot;持久化&quot; 宿主机目录的过程, 可以形象地称为 &quot;两阶段处理&quot;. **</p> <p>接下来通过一个具体的例子进行说明.</p> <p>当一个 Pod 调度到一个<strong>节点</strong>上之后, kubelet 就要负责<strong>为这个 Pod 创建它的 Volume 目录</strong>. 默认情况下, kubelet 为 Volume 创建的目录是如下所示的<strong>一个宿主机上的路径</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/var/lib/kubelet/pods/<span class="token operator">&lt;</span>Pod 的 ID<span class="token operator">&gt;</span>/volumes/kubernetes.io~<span class="token operator">&lt;</span>Volume 类型 <span class="token operator">&gt;</span>/<span class="token operator">&lt;</span>Volume 名字<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>接下来, kubelet 要做的操作就取决于 Volume 的类型了. 如果 Volume 类型是<strong>远程块存储</strong>, 比如 Google Cloud 的 Persistent Disk(GCE 提供的远程磁盘服务), 那么 kubelet 就需要先调用 Goolge Cloud 的 API, 将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上. 如果不太了解块存储的话, 可以直接把它理解为: 一块<strong>磁盘</strong>.</p> <p>这相当于执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud compute instances attach-disk <span class="token operator">&lt;</span> 虚拟机名字 <span class="token operator">&gt;</span> <span class="token parameter variable">--disk</span> <span class="token operator">&lt;</span> 远程磁盘名字 <span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这一步**为虚拟机挂载远程磁盘的操作, 对应的正是 &quot;两阶段处理&quot; 的第一阶段. 在 Kubernetes 中, 可以把这个阶段称为 **​<mark><strong>Attach</strong></mark>​ **. **</p> <p>Attach 阶段完成后, 为了能够使用这个远程磁盘, kubelet 还要进行第二个操作, 即: <strong>格式化这个磁盘设备, 然后将它挂载到宿主机指定的挂载点上</strong>. 不难理解, <strong>这个挂载点正是前面反复提到的 Volume 的宿主机目录</strong>. 所以这一步相当于执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 通过 lsblk 命令获取磁盘设备 ID</span>
$ <span class="token function">sudo</span> lsblk
<span class="token comment"># 格式化成 ext4 格式</span>
$ <span class="token function">sudo</span> mkfs.ext4 <span class="token parameter variable">-m</span> <span class="token number">0</span> <span class="token parameter variable">-F</span> <span class="token parameter variable">-E</span> <span class="token assign-left variable">lazy_itable_init</span><span class="token operator">=</span><span class="token number">0</span>,lazy_journal_init<span class="token operator">=</span><span class="token number">0</span>,discard /dev/<span class="token operator">&lt;</span> 磁盘设备 ID<span class="token operator">&gt;</span>
<span class="token comment"># 挂载到挂载点</span>
$ <span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /var/lib/kubelet/pods/<span class="token operator">&lt;</span>Pod 的 ID<span class="token operator">&gt;</span>/volumes/kubernetes.io~<span class="token operator">&lt;</span>Volume 类型 <span class="token operator">&gt;</span>/<span class="token operator">&lt;</span>Volume 名字 <span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这个**将磁盘设备格式化并挂载到 Volume 宿主机目录的操作, 对应的正是 &quot;两阶段处理&quot; 的第二个阶段, 一般称为: **​<mark><strong>Mount</strong></mark>​ **. **</p> <p><strong>Mount 阶段完成后, 这个 Volume 的宿主机目录就是一个 &quot;持久化&quot; 的目录了, 容器在它里面写入的内容, 会保存在 Google Cloud 的远程磁盘中</strong>. 而如果 Volume 类型是<strong>远程文件存储</strong>(比如 NFS)的话, kubelet 的处理过程就会更简单一些.</p> <p>因为在这种情况下, kubelet 可以跳过 &quot;第一阶段&quot; (Attach)的操作, 这是因为一般来说, 远程文件存储并没有一个 &quot;<strong>存储设备</strong>&quot; 需要挂载在宿主机上. 所以 kubelet 会<strong>直接从 &quot;第二阶段&quot;(Mount)开始准备宿主机上的 Volume 目录</strong>.</p> <p>在这一步, kubelet 需要作为 client, <strong>将远端 NFS 服务器的目录(比如: &quot;/&quot;目录), 挂载到 Volume 的宿主机目录上</strong>, 即相当于执行如下所示的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">mount</span> <span class="token parameter variable">-t</span> nfs <span class="token operator">&lt;</span>NFS 服务器地址 <span class="token operator">&gt;</span>:/ /var/lib/kubelet/pods/<span class="token operator">&lt;</span>Pod 的 ID<span class="token operator">&gt;</span>/volumes/kubernetes.io~<span class="token operator">&lt;</span>Volume 类型 <span class="token operator">&gt;</span>/<span class="token operator">&lt;</span>Volume 名字 <span class="token operator">&gt;</span> 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>通过这个挂载操作, Volume 的<strong>宿主机目录就成为了一个远程 NFS 目录的挂载点</strong>, 后面在这个目录里写入的所有文件, 都会被保存在远程 NFS 服务器上. 所以也就完成了对这个 Volume 宿主机目录的 &quot;持久化&quot;.</p> <p>**到这里你可能会有疑问, Kubernetes 又是如何定义和区分这两个阶段的呢? **</p> <p>其实很简单, 在具体的 Volume 插件的实现接口上, Kubernetes 分别<strong>给这两个阶段提供了两种不同的参数列表</strong>:</p> <ul><li>对于&quot;第一阶段&quot;(<strong>Attach</strong>), Kubernetes 提供的可用参数是 <strong>nodeName</strong>, 即<strong>宿主机的名字</strong>.</li> <li>而对于&quot;第二阶段&quot;(<strong>Mount</strong>), Kubernetes 提供的可用参数是 <strong>dir</strong>, 即 <strong>Volume 的宿主机目录</strong>.</li></ul> <p>所以作为一个存储插件, 只需要根据自己的需求进行选择和实现即可. 在后面关于编写存储插件的文章中, 会对这个过程做深入讲解.</p> <p>而经过了 &quot;两阶段处理&quot;, 就<strong>得到了一个 &quot;持久化&quot; 的 Volume 宿主机目录</strong>. 所以接下来, kubelet 只要<strong>把这个 Volume 目录通过 CRI 里的 Mounts 参数, 传递给 Docker, 然后就可以为 Pod 里的容器挂载这个 &quot;持久化&quot; 的 Volume 了</strong>. 其实这一步相当于执行了如下所示的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-v</span> /var/lib/kubelet/pods/<span class="token operator">&lt;</span>Pod 的 ID<span class="token operator">&gt;</span>/volumes/kubernetes.io~<span class="token operator">&lt;</span>Volume 类型 <span class="token operator">&gt;</span>/<span class="token operator">&lt;</span>Volume 名字 <span class="token operator">&gt;</span>:/<span class="token operator">&lt;</span> 容器内的目标目录 <span class="token operator">&gt;</span> 我的镜像 <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>以上就是 Kubernetes <strong>处理 PV 的具体原理</strong>了.</p> <p>对应地, 在删除一个 PV 的时候, Kubernetes 也需要 <strong>Unmount 和 Dettach 两个阶段</strong>来处理. 这个过程就不再详细介绍了, 执行 &quot;反向操作&quot; 即可.</p> <p>你可能已经发现, 这个 PV 的处理流程似乎跟 Pod 以及容器的启动流程没有太多的耦合, 只要 kubelet 在向 Docker 发起 CRI 请求之前, 确保 &quot;持久化&quot; 的宿主机目录已经处理完毕即可. 所以在 Kubernetes 中, 上述<mark><strong>关于 PV 的 &quot;两阶段处理&quot; 流程, 是靠独立于 kubelet 主控制循环(Kubelet Sync Loop)之外的两个控制循环来实现的</strong></mark>​ **. **</p> <p>其中, &quot;第一阶段&quot; 的 <strong>Attach</strong>(以及 Dettach)操作, 是由 <strong>Volume Controller</strong> 负责维护的, 这个控制循环的名字叫作: <strong>AttachDetachController</strong>. 而它的作用就是不断地检查每一个 Pod 对应的 PV, 和这个 Pod 所在宿主机之间挂载情况. 从而决定是否需要对这个 PV 进行 Attach(或者 Dettach)操作.</p> <p>需要注意, 作为一个 Kubernetes 内置的控制器, Volume Controller 自然是 kube-controller-manager 的一部分. 所以 AttachDetachController 也一定是运行在 Master 节点上的. 当然 Attach 操作只需要调用公有云或者具体存储项目的 API, 并不需要在具体的宿主机上执行操作, 所以这个设计没有任何问题.</p> <p>而 &quot;第二阶段&quot; 的 <strong>Mount</strong>(以及 Unmount)操作, <strong>必须发生在 Pod 对应的宿主机上</strong>, 所以它必须是 <strong>kubelet 组件的一部分</strong>. 这个控制循环的名字, 叫作: <strong>VolumeManagerReconciler</strong>, 它运行起来之后, 是一个独立于 kubelet 主循环的 Goroutine.</p> <p><strong>通过这样将 Volume 的处理同 kubelet 的主循环解耦, Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环, 进而导致 Pod 的创建效率大幅下降的问题</strong>. 实际上, <mark><strong>kubelet 的一个主要设计原则, 就是它的主控制循环绝对不可以被 block</strong></mark>. 这个思想在后续的讲述容器运行时的时候还会提到.</p> <p>在了解了 Kubernetes 的 Volume 处理机制之后, 再来介绍这个体系里最后一个重要概念: <strong>StorageClass</strong>.</p> <p>前面介绍 PV 和 PVC 的时候, 曾经提到过, PV 这个对象的创建是由运维人员完成的. 但是在大规模的生产环境里, 这其实是一个非常麻烦的工作. 这是因为一个大规模的 Kubernetes 集群里很可能有成千上万个 PVC, 这就意味着运维人员必须得事先创建出成千上万个 PV. 更麻烦的是, 随着新的 PVC 不断被提交, 运维人员就不得不继续添加新的, 能满足条件的 PV, 否则新的 Pod 就会因为 PVC 绑定不到 PV 而失败. 在实际操作中这几乎没办法靠人工做到. 这种人工管理 PV 的方式就叫作 Static Provisioning.</p> <p><strong>所以 Kubernetes 提供了一套可以自动创建 PV 的机制, 即: Dynamic Provisioning</strong>. Dynamic Provisioning 机制工作的核心, 在于一个名叫 <strong>StorageClass 的 API 对象</strong>. **而 StorageClass 对象的作用, 其实就是创建 PV 的模板. **</p> <p>具体地说, StorageClass 对象会定义如下两个部分内容:</p> <ul><li>第一, <strong>PV 的属性</strong>. 比如存储类型, Volume 的大小等等.</li> <li>第二, <strong>创建这种 PV 需要用到的存储插件</strong>. 比如 Ceph 等等.</li></ul> <p>有了这样两个信息之后, Kubernetes 就能够根据用户提交的 PVC, 找到一个对应的 <strong>StorageClass</strong> 了. 然后 Kubernetes 就会调用该 StorageClass 声明的存储插件, 创建出需要的 PV.</p> <p>举个例子, 假如 Volume 的类型是 GCE 的 Persistent Disk 的话, 运维人员就需要定义一个如下所示的 StorageClass:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> storage.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> StorageClass
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> block<span class="token punctuation">-</span>service
<span class="token key atrule">provisioner</span><span class="token punctuation">:</span> kubernetes.io/gce<span class="token punctuation">-</span>pd
<span class="token key atrule">parameters</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> pd<span class="token punctuation">-</span>ssd
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>在这个 YAML 文件里定义了一个名叫 block-service 的 StorageClass. 这个 StorageClass 的 provisioner 字段的值是: <code>kubernetes.io/gce-pd</code>​, 这正是 Kubernetes <strong>内置</strong>的 GCE PD 存储插件的名字. 而这个 StorageClass 的 <strong>parameters</strong> 字段, 就是 PV 的参数. 比如上面例子里的 type=pd-ssd, 指的是这个 PV 的类型是 &quot;SSD 格式的 GCE 远程磁盘&quot;.</p> <p>需要注意的是, 由于需要使用 GCE Persistent Disk, 上面这个例子只有在 <strong>GCE</strong> 提供的 Kubernetes 服务里才能实践. 如果想使用之前部署在本地的 Kubernetes 集群以及 Rook 存储服务的话, StorageClass 需要使用如下所示的 YAML 文件来定义:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> ceph.rook.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pool
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> replicapool
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> rook<span class="token punctuation">-</span>ceph
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicated</span><span class="token punctuation">:</span>
    <span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">3</span>
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> storage.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> StorageClass
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> block<span class="token punctuation">-</span>service
<span class="token key atrule">provisioner</span><span class="token punctuation">:</span> ceph.rook.io/block
<span class="token key atrule">parameters</span><span class="token punctuation">:</span>
  <span class="token key atrule">pool</span><span class="token punctuation">:</span> replicapool
  <span class="token comment">#The value of &quot;clusterNamespace&quot; MUST be the same as the one in which your rook cluster exist</span>
  <span class="token key atrule">clusterNamespace</span><span class="token punctuation">:</span> rook<span class="token punctuation">-</span>ceph
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>在这个 YAML 文件中, 定义的还是一个名叫 block-service 的 StorageClass, 只不过<strong>它声明使的存储插件是由 Rook 项目</strong>.</p> <p>有了 StorageClass 的 YAML 文件之后, 运维人员就可以在 Kubernetes 里<strong>创建这个 StorageClass</strong> 了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> sc.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时候, 作为应用开发者就<strong>只需要在 PVC 里指定要使用的 StorageClass 名字</strong>即可, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> claim1
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> block<span class="token punctuation">-</span>service
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 30Gi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>可以看到在这个 PVC 里添加了一个叫作 <strong>storageClassName</strong> 的字段, 用于指定该 PVC 所要使用的 StorageClass 的名字是: block-service.</p> <p>以 Google Cloud 为例.</p> <p><strong>当通过 kubectl create 创建上述 PVC 对象之后, Kubernetes 就会调用 Google Cloud 的 API, 创建出一块 SSD 格式的 Persistent Disk. 然后再使用这个 Persistent Disk 的信息, 自动创建出一个对应的 PV 对象.</strong></p> <p>一起来实践一下这个过程(如果使用 Rook 的话下面的流程也是一样的, 只不过 Rook 创建出的是 Ceph 类型的 PV):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> pvc.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到, 这里创建的 PVC 会绑定一个 Kubernetes 自动创建的 PV, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pvc claim1
Name:           claim1
Namespace:      default
StorageClass:   block-service
Status:         Bound
Volume:         pvc-e5578707-c626-11e6-baf6-08002729a32b
Labels:         <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Capacity:       30Gi
Access Modes:   RWO
No Events.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>而且通过查看这个自动创建的 PV 的属性, 就可以看到它<strong>跟 PVC 里声明的存储的属性是一致</strong>的, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pvc claim1
Name:           claim1
Namespace:      default
StorageClass:   block-service
Status:         Bound
Volume:         pvc-e5578707-c626-11e6-baf6-08002729a32b
Labels:         <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Capacity:       30Gi
Access Modes:   RWO
No Events.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>此外还可以看到, 这个自动创建出来的 PV 的 StorageClass 字段的值, 也是 block-service. <mark><strong>这是因为 Kubernetes 只会将 StorageClass 相同的 PVC 和 PV 绑定起来</strong></mark>​ **. **</p> <p>有了 Dynamic Provisioning 机制, 运维人员只需要在 Kubernetes 集群里<strong>创建出数量有限的 StorageClass 对象</strong>就可以了. 这就好比运维人员在 Kubernetes 集群里创建出了各种各样的 PV 模板. 这时候当开发人员提交了包含 StorageClass 字段的 PVC 之后, Kubernetes 就会根据这个 StorageClass 创建出对应的 PV.</p> <p>Kubernetes 的官方文档里已经列出了默认支持 Dynamic Provisioning 的内置存储插件. 而对于不在文档里的插件, 比如 NFS, 或者其他非内置存储插件, 其实可以通过 <a href="https://github.com/kubernetes-incubator/external-storage" target="_blank" rel="noopener noreferrer">kubernetes-incubator/external-storage<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 这个库来自己编写一个外部插件完成这个工作. 像之前部署的 Rook, 已经内置了 external-storage 的实现, 所以 Rook 是完全支持 Dynamic Provisioning 特性的.</p> <p>需要注意的是, **StorageClass 并不是专门为了 Dynamic Provisioning 而设计的. **</p> <p>比如本节一开始的例子里, 在 PV 和 PVC 里都声明了 <strong>storageClassName=manual</strong>. 而我的集群里, 实际上并没有一个名叫 manual 的 StorageClass 对象. 这完全没有问题, 这个时候 Kubernetes 进行的是 Static Provisioning, 但在<strong>做绑定决策的时候, 它依然会考虑 PV 和 PVC 的 StorageClass 定义</strong>.</p> <p>而这么做的好处也很明显: <strong>这个 PVC 和 PV 的绑定关系, 就完全在自己的掌控之中</strong>.</p> <p>这里你可能会有疑问, 之前讲 <strong>StatefulSet 存储状态</strong>的例子时, 好像并没有声明 StorageClass 啊?</p> <p>实际上, 如果你的集群已经开启了名叫 DefaultStorageClass 的 Admission Plugin, 它就会为 PVC 和 PV 自动添加一个默认的 StorageClass; **否则 PVC 的 storageClassName 的值就是 &quot;&quot;, 这也意味着它只能够跟 storageClassName 也是 &quot;&quot; 的 PV 进行绑定. **</p> <blockquote><p>总结</p></blockquote> <p>本节详细解释了 PVC 和 PV 的设计与实现原理, 并阐述了 StorageClass 到底是干什么用的. 这些概念之间的关系, 可以用如下所示的一幅示意图描述:</p> <p><img src="/img/e111cccdc2ac582f5a77e3414a37e10b-20230731162150-ir3g5p3.png" alt="">
从图中可以看到, 在这个体系中:</p> <ul><li>**PVC 描述的, 是 Pod 想要使用的持久化存储的属性, 比如存储的大小, 读写权限等. **</li> <li>**PV 描述的, 则是一个具体的 Volume 的属性, 比如 Volume 的类型, 挂载目录, 远程存储服务器地址等. **</li> <li>**而 StorageClass 的作用, 则是充当 PV 的模板. 并且只有同属于一个 StorageClass 的 PV 和 PVC, 才可以绑定在一起. **</li></ul> <p>当然 StorageClass 的另一个重要作用, 是指定 PV 的 Provisioner(存储插件). 这时候如果存储插件支持 Dynamic Provisioning 的话, Kubernetes 就可以自动创建 PV 了.</p> <p>基于上述讲述, 为了统一概念和方便叙述, 之后以后凡是提到 &quot;Volume&quot;, <strong>指的就是一个远程存储服务挂载在宿主机上的持久化目录</strong>; 而 &quot;PV&quot;, 指的是这个 Volume 在 Kubernetes 里的 API 对象.</p> <p>需要注意的是, 这套容器持久化存储体系, 完全是 Kubernetes 项目自己负责管理的, 并不依赖于 docker volume 命令和 Docker 的存储插件. 当然这套体系本身就比 docker volume 命令的诞生时间还要早得多.</p> <h4 id="_29-pv-pvc体系是不是多此一举-从本地持久化卷谈起"><a href="#_29-pv-pvc体系是不是多此一举-从本地持久化卷谈起" class="header-anchor">#</a> 29 | PV,PVC体系是不是多此一举?从本地持久化卷谈起</h4> <p>上一节讲解了 <strong>PV, PVC 持久化存储体系</strong>在 Kubernetes 项目中的设计和实现原理. 有人会问, 像 PV, PVC 这样的用法, 是不是有&quot;过度设计&quot;的嫌疑?</p> <p>比如运维人员可以像往常一样维护一套 NFS 或 Ceph 服务器, 根本不必学习 Kubernetes. 而开发人员, 则完全可以靠 &quot;复制粘贴&quot; 的方式, 在 Pod 的 YAML 文件里填上 Volumes 字段, 而不需要去使用 PV 和 PVC. 实际上, 如果只是为了职责划分, PV, PVC 体系确实不见得比直接在 Pod 里声明 Volumes 字段有什么优势.</p> <p>不过, 你有没有想过这样一个问题, 如果 Kubernetes 内置的 20 种持久化数据卷实现, <strong>都没办法满足你的容器存储需求时</strong>, 该怎么办?</p> <p>这个情况乍一听起来有点不可思议. 但实际上, 凡是鼓捣过开源项目的读者应该都有所体会, &quot;不能用&quot;, &quot;不好用&quot;, &quot;需要定制开发&quot;, 这才是落地开源基础设施项目的三大常态.</p> <p>而<strong>在持久化存储领域, 用户呼声最高的定制化需求, 莫过于支持 &quot;本地&quot; 持久化存储</strong>了. 也就是说, <strong>用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录</strong>, 而不依赖于远程存储服务, 来提供 &quot;持久化&quot; 的容器 Volume. 这样做的好处很明显, 由于这个 Volume 直接使用的是本地磁盘, 尤其是 SSD 盘, 它的<strong>读写性能相比于大多数远程存储</strong>来说, 要好得多. 这个需求对本地物理服务器部署的私有 Kubernetes 集群来说, 非常常见. 所以 Kubernetes 在 v1.10 之后, 就逐渐<strong>依靠 PV, PVC 体系实现了这个特性</strong>. 这个特性的名字叫作: <strong>Local Persistent Volume</strong>.</p> <p>不过首先需要明确的是, <strong>Local Persistent Volume 并不适用于所有应用</strong>. 事实上, 它的适用范围<strong>非常固定</strong>, 比如: <strong>高优先级的系统应用, 需要在多个不同节点上存储数据, 并且对 I/O 较为敏感</strong>. 典型的应用包括: 分布式数据存储比如 <strong>MongoDB</strong>, Cassandra 等, 分布式文件系统比如 GlusterFS, Ceph 等, 以及需要在本地磁盘上进行大量数据缓存的分布式应用. 其次相比于正常的 PV, 一旦这些节点宕机且不能恢复时, Local Persistent Volume 的数据就<strong>可能丢失</strong>. 这就要求<strong>使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力</strong>, 允许你把这些数据定时备份在其他位置.</p> <p>接下来就深入讲解一下这个特性.</p> <p>不难想象, Local Persistent Volume 的设计, 主要面临两个难点.</p> <p><strong>第一个难点在于</strong>: 如何<strong>把本地磁盘抽象成 PV</strong>.</p> <p>可能你会说, Local Persistent Volume 不就等同于 hostPath 加 NodeAffinity 吗? 比如一个 Pod 可以声明使用类型为 Local 的 PV, 而这个 PV 其实就是一个 hostPath 类型的 Volume. 如果这个 hostPath 对应的目录, 已经在节点 A 上被事先创建好了. 那么只需要再给这个 Pod 加上一个 nodeAffinity=nodeA, 不就可以使用这个 Volume 了吗?</p> <p>事实上, <mark><strong>绝不应该把一个宿主机上的目录当作 PV 使用</strong></mark>. 这是因为这种本地目录的存储行为<strong>完全不可控</strong>, 它所在的磁盘随时都可能被应用写满, 甚至造成整个宿主机宕机. 而且不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制. 所以<strong>一个 Local Persistent Volume 对应的存储介质, 一定是一块额外挂载在宿主机的磁盘或者块设备</strong>(&quot;额外&quot; 的意思是, 它<strong>不应该是宿主机根目录所使用的主硬盘</strong>). 这个原则可以称为 &quot;<strong>一个 PV 一块盘</strong>&quot;.</p> <p><strong>第二个难点在于</strong>: 调度器如何<strong>保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上</strong>呢?</p> <p>造成这个问题的原因在于, 对于常规的 PV 来说, Kubernetes 都是先调度 Pod 到某个节点上, 然后再通过 &quot;两阶段处理&quot; 来 &quot;持久化&quot; 这台机器上的 Volume 目录, 进而完成 Volume 目录与容器的绑定挂载. 可是对于 Local PV 来说, 节点上可供使用的磁盘(或者块设备), 必须是运维人员提前准备好的. 它们在不同节点上的挂载情况可以完全不同, 甚至有的节点可以没这种磁盘. 所以这时候, <strong>调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系, 然后根据这个信息来调度 Pod</strong>.</p> <p>这个原则, 可以称为 &quot;<strong>在调度的时候考虑 Volume 分布</strong>&quot;. 在 Kubernetes 的调度器里, 有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情. 在 Kubernetes v1.11 中, 这个过滤条件已经默认开启了.</p> <p>基于上述讲述, 在开始使用 Local Persistent Volume 之前, <strong>首先需要在集群里配置好磁盘或者块设备</strong>. 在公有云上, 这个操作等同于给虚拟机<strong>额外挂载一个磁盘</strong>, 比如 GCE 的 Local SSD 类型的磁盘就是一个典型例子.</p> <p>而在部署的私有环境中, 有两种办法来完成这个步骤.</p> <ul><li>第一种, 当然就是<strong>给宿主机挂载并格式化一个可用的本地磁盘</strong>, 这也是最常规的操作;</li> <li>第二种, 对于实验环境, 其实可以在宿主机上挂载几个 RAM Disk(内存盘)来模拟本地磁盘.</li></ul> <p>接下来会使用第二种方法, 在之前部署的 Kubernetes 集群上进行实践.</p> <p><strong>首先</strong>, 在名叫 node-1 的宿主机上<strong>创建一个挂载点</strong>, 比如 /mnt/disks; <strong>然后</strong>用几个 RAM Disk 来模拟本地磁盘, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 node-1 上执行</span>
$ <span class="token function">mkdir</span> /mnt/disks
$ <span class="token keyword">for</span> <span class="token for-or-select variable">vol</span> <span class="token keyword">in</span> vol1 vol2 vol3<span class="token punctuation">;</span> <span class="token keyword">do</span>
    <span class="token function">mkdir</span> /mnt/disks/<span class="token variable">$vol</span>
    <span class="token function">mount</span> <span class="token parameter variable">-t</span> tmpfs <span class="token variable">$vol</span> /mnt/disks/<span class="token variable">$vol</span>
<span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>需要注意的是, 如果你希望其他节点也能支持 Local Persistent Volume 的话, 那就需要为它们也执行上述操作, 并且确保这些磁盘的名字(vol1, vol2 等)都不重复.</p> <p>接下来就可以为这些本地磁盘定义对应的 PV 了, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>pv
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>
    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 5Gi
  <span class="token key atrule">volumeMode</span><span class="token punctuation">:</span> Filesystem
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token key atrule">persistentVolumeReclaimPolicy</span><span class="token punctuation">:</span> Delete
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> local<span class="token punctuation">-</span>storage
  <span class="token key atrule">local</span><span class="token punctuation">:</span>
    <span class="token key atrule">path</span><span class="token punctuation">:</span> /mnt/disks/vol1
  <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>
    <span class="token key atrule">required</span><span class="token punctuation">:</span>
      <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> kubernetes.io/hostname
          <span class="token key atrule">operator</span><span class="token punctuation">:</span> In
          <span class="token key atrule">values</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> node<span class="token punctuation">-</span><span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>可以看到, 这个 PV 的定义里: <strong>local 字段</strong>, 指定了它是一个 Local Persistent Volume; 而 <strong>path 字段</strong>, 指定的正是这个 PV 对应的本地磁盘的路径, 即: /mnt/disks/vol1.</p> <p>当然这也就意味着如果 Pod 要想使用这个 PV, 那它就必须运行在 node-1 上. 所以在这个 PV 的定义里, 需要有一个 <strong>nodeAffinity</strong> 字段指定 node-1 这个节点的名字. 这样调度器在调度 Pod 的时候, 就能够知道<strong>一个 PV 与节点的对应关系</strong>, 从而做出正确的选择. **这正是 Kubernetes 实现 &quot;在调度的时候就考虑 Volume 分布&quot; 的主要方法. **</p> <p><strong>接下来</strong>, 就可以使用 kubect create 来创建这个 PV, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> local-pv.yaml 
persistentvolume/example-pv created

$ kubectl get <span class="token function">pv</span>
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY  STATUS      CLAIM             STORAGECLASS    REASON    AGE
example-pv   5Gi        RWO            Delete           Available                     local-storage             16s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, 这个 PV 创建后, 进入了 <strong>Available</strong>(可用)状态.</p> <p>而正如前一节建议的那样, <strong>使用 PV 和 PVC 的最佳实践, 是创建一个 StorageClass 来描述这个 PV</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> StorageClass
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> storage.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> local<span class="token punctuation">-</span>storage
<span class="token key atrule">provisioner</span><span class="token punctuation">:</span> kubernetes.io/no<span class="token punctuation">-</span>provisioner
<span class="token key atrule">volumeBindingMode</span><span class="token punctuation">:</span> WaitForFirstConsumer
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这个 StorageClass 的名字, 叫作 <strong>local-storage</strong>. 需要注意的是, 在它的 provisioner 字段, 指定的是 no-provisioner. 这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning, 所以它没办法在用户创建 PVC 的时候, 就自动创建出对应的 PV. 也就是说前面创建 PV 的操作, 是不可以省略的.</p> <p>与此同时, 这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性. 它是 Local Persistent Volume 里一个非常重要的特性, 即: <strong>延迟绑定</strong>.</p> <p>前面已经知道, 当提交了 PV 和 PVC 的 YAML 文件之后, Kubernetes 就会根据它们俩的属性, 以及它们指定的 StorageClass 来进行绑定. 只有绑定成功后, Pod 才能通过声明这个 PVC 来使用对应的 PV. 可是如果使用的是 Local Persistent Volume 的话, 就会发现这个流程根本<strong>行不通</strong>.</p> <p>比如现在有一个 Pod, 它声明使用的 PVC 叫作 pvc-1. 并且规定这个 Pod 只能运行在 node-2 上. 而在 Kubernetes 集群中, 有两个属性(比如: 大小, 读写权限)相同的 Local 类型的 PV. 其中第一个 PV 的名字叫作 pv-1, 它对应的磁盘所在的节点是 node-1. 而第二个 PV 的名字叫作 pv-2, 它对应的磁盘所在的节点是 node-2.</p> <p>假设现在, Kubernetes 的 Volume 控制循环里, 首先检查到了 pvc-1 和 pv-1 的属性是匹配的, 于是就将它们俩绑定在一起. 然后你用 kubectl create 创建了这个 Pod.</p> <p>这时候, 问题就出现了.</p> <p>调度器看到, 这个 Pod 所声明的 pvc-1 已经绑定了 pv-1, 而 pv-1 所在的节点是 node-1, 根据 &quot;调度器必须在调度的时候考虑 Volume 分布&quot; 的原则, 这个 Pod 自然会被调度到 node-1 上. 可是前面已经规定过, 这个 Pod 根本不允许运行在 node-1 上. 所以最后的结果就是, 这个 Pod 的调度必然会失败.</p> <p>**这就是为什么, 在使用 Local Persistent Volume 的时候, 必须想办法推迟这个 &quot;绑定&quot; 操作. **</p> <p>那么, 具体推迟到什么时候呢?</p> <p>**答案是: **​<mark><strong>推迟到调度的时候</strong></mark>​ **. **</p> <p>所以说 StorageClass 里的 volumeBindingMode=WaitForFirstConsumer 的含义, 就是告诉 Kubernetes 里的 Volume 控制循环(&quot;红娘&quot;): 虽然你已经发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起, 但<strong>请不要现在就执行</strong>绑定操作(即: 设置 PVC 的 VolumeName 字段). 而<strong>要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后, 调度器再综合考虑所有的调度规则, 当然也包括每个 PV 所在的节点位置, 来统一决定这个 Pod 声明的 PVC 到底应该跟哪个 PV 进行绑定</strong>.</p> <p>这样在上面的例子里, 由于这个 Pod 不允许运行在 pv-1 所在的节点 node-1, 所以它的 PVC 最后会跟 pv-2 绑定, 并且 Pod 也会被调度到 node-2 上.</p> <p>所以通过这个延迟绑定机制, 原本<strong>实时</strong>发生的 PVC 和 PV 的绑定过程, 就被<strong>延迟到了 Pod 第一次调度的时候在调度器中进行</strong>, 从而保证了这个<strong>绑定结果不会影响 Pod 的正常调度</strong>.</p> <p>当然在具体实现中, 调度器实际上维护了一个与 Volume Controller 类似的控制循环, 专门负责为那些声明了 &quot;延迟绑定&quot; 的 PV 和 PVC 进行绑定工作. 通过这样的设计, 这个额外的绑定操作, <strong>并不会拖慢调度器的性能</strong>. 而当一个 Pod 的 PVC 尚未完成绑定时, 调度器也不会等待, 而是会直接把这个 Pod 重新放回到待调度队列, 等到下一个调度周期再做处理.</p> <p>在明白了这个机制之后, 就可以创建 StorageClass 了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> local-sc.yaml 
storageclass.storage.k8s.io/local-storage created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>接下来只需要定义一个非常普通的 PVC, 就可以让 Pod 使用到上面定义好的 Local Persistent Volume 了, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>local<span class="token punctuation">-</span>claim
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 5Gi
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> local<span class="token punctuation">-</span>storage
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>可以看到, 这个 PVC 没有任何特别的地方. 唯一需要注意的是, 它声明的 storageClassName 是 <strong>local-storage</strong>. 所以将来 Kubernetes 的 Volume Controller 看到这个 PVC 的时候, 不会为它进行绑定操作.</p> <p>现在来创建这个 PVC:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> local-pvc.yaml 
persistentvolumeclaim/example-local-claim created

$ kubectl get pvc
NAME                  STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS    AGE
example-local-claim   Pending                                       local-storage   7s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, 尽管这个时候 Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV, 但这个 PVC 依然处于 <strong>Pending</strong> 状态, 也就是<strong>等待绑定</strong>的状态.</p> <p>然后编写一个 Pod 来声明<strong>使用这个 PVC</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>pv<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>pv<span class="token punctuation">-</span>storage
      <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span>
       <span class="token key atrule">claimName</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>local<span class="token punctuation">-</span>claim
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>pv<span class="token punctuation">-</span>container
      <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
      <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">&quot;http-server&quot;</span>
      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&quot;/usr/share/nginx/html&quot;</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>pv<span class="token punctuation">-</span>storage
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>这个 Pod 没有任何特别的地方, 你只需要注意它的 volumes 字段声明要使用前面定义的, 名叫 example-local-claim 的 <strong>PVC</strong> 即可. 而一旦使用 kubectl create 创建这个 Pod, 就会发现前面定义的 PVC, 会立刻变成 <strong>Bound</strong> 状态, 与前面定义的 PV 绑定在了一起, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> local-pod.yaml 
pod/example-pv-pod created

$ kubectl get pvc
NAME                  STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS    AGE
example-local-claim   Bound     example-pv   5Gi        RWO            local-storage   6h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>也就是说, 在创建的 Pod 进入调度器之后, &quot;绑定&quot; 操作才开始进行. 这时候可以尝试在这个 Pod 的 Volume 目录里, 创建一个测试文件, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> example-pv-pod -- /bin/sh
<span class="token comment"># cd /usr/share/nginx/html</span>
<span class="token comment"># touch test.txt</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后登录到 node-1 这台机器上, 查看一下它的 /mnt/disks/vol1 目录下的内容, 就可以看到刚刚创建的这个文件:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 node-1 上</span>
$ <span class="token function">ls</span> /mnt/disks/vol1
test.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>而如果重新创建这个 Pod 的话, 就会发现之前创建的测试文件, <strong>依然被保存</strong>在这个持久化 Volume 当中:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete <span class="token parameter variable">-f</span> local-pod.yaml 

$ kubectl create <span class="token parameter variable">-f</span> local-pod.yaml 

$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> example-pv-pod -- /bin/sh
<span class="token comment"># ls /usr/share/nginx/html</span>
<span class="token comment"># touch test.txt</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这就说明, 像 Kubernetes 这样构建出来的, <strong>基于本地存储的 Volume, 完全可以提供容器持久化存储的功能</strong>. 所以像 StatefulSet 这样的有状态编排工具, 也完全可以通过声明 Local 类型的 PV 和 PVC, 来管理应用的存储状态.</p> <p>**需要注意的是, 上面手动创建 PV 的方式, 即 Static 的 PV 管理方式, 在删除 PV 时需要按如下流程执行操作: **</p> <ol><li>删除使用这个 PV 的 Pod;</li> <li>从宿主机移除本地磁盘(比如 umount 它);</li> <li>删除 PVC;</li> <li>删除 PV.</li></ol> <p>**如果不按照这个流程的话, 这个 PV 的删除就会失败. ** 当然, 由于上面这些创建 PV 和删除 PV 的操作比较繁琐, Kubernetes 其实提供了一个 Static Provisioner 来帮助你管理这些 PV.</p> <p>比如现在的所有磁盘, 都挂载在宿主机的 /mnt/disks 目录下.</p> <p>那么当 Static Provisioner 启动后, 它就会通过 DaemonSet, 自动检查每个宿主机的 /mnt/disks 目录. 然后调用 Kubernetes API 为这些目录下面的每一个挂载, <strong>创建一个对应的 PV 对象出来</strong>. 这些自动创建的 PV, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">pv</span>
NAME                CAPACITY    ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS    REASON    AGE
local-pv-ce05be60   1024220Ki   RWO           Delete          Available             local-storage             26s

$ kubectl describe <span class="token function">pv</span> local-pv-ce05be60 
Name:  local-pv-ce05be60
<span class="token punctuation">..</span>.
StorageClass: local-storage
Status:  Available
Claim:  
Reclaim Policy: Delete
Access Modes: RWO
Capacity: 1024220Ki
NodeAffinity:
  Required Terms:
      Term <span class="token number">0</span>:  kubernetes.io/hostname <span class="token keyword">in</span> <span class="token punctuation">[</span>node-1<span class="token punctuation">]</span>
Message: 
Source:
    Type: LocalVolume <span class="token punctuation">(</span>a persistent volume backed by <span class="token builtin class-name">local</span> storage on a <span class="token function">node</span><span class="token punctuation">)</span>
    Path: /mnt/disks/vol1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>这个 PV 里的各种定义, 比如 StorageClass 的名字, 本地磁盘挂载点的位置, 都可以通过 provisioner 的<a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume/helm" target="_blank" rel="noopener noreferrer">配置文件指定<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. 当然 provisioner 也会负责前面提到的 PV 的删除工作. 而这个 provisioner 本身, 其实也是一个前面提到过的 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume" target="_blank" rel="noopener noreferrer">External Provisioner<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 它的部署方法, 在对应的文档里有详细描述. 这部分内容就不说明了.</p> <blockquote><p>总结</p></blockquote> <p>本节介绍了 Kubernetes 里 <strong>Local Persistent Volume 的实现方式</strong>.</p> <p>可以看到, 正是通过 PV 和 PVC, 以及 StorageClass 这套存储体系, 这个后来新添加的持久化存储方案, 对 Kubernetes 已有用户的影响, 几乎可以忽略不计. 作为用户, 你的 Pod 的 YAML 和 PVC 的 YAML, 并没有任何特殊的改变, 这个特性所有的实现只会影响到 PV 的处理, 也就是由运维人员负责的那部分工作.</p> <p>而这, 正是这套存储体系带来的 &quot;解耦&quot; 的好处.</p> <p>其实, Kubernetes 很多看起来比较 &quot;繁琐&quot; 的设计(比如 &quot;声明式 API&quot;, 以及本节的 &quot;PV, PVC 体系&quot;)的主要目的, 都是<strong>希望为开发者提供更多的 &quot;可扩展性&quot;, 给使用者带来更多的 &quot;稳定性&quot; 和 &quot;安全感&quot;</strong> . 这两个能力的高低, 是衡量开源基础设施项目水平的重要标准.</p> <h4 id="_30-编写自己的存储插件-flexvolume与csi"><a href="#_30-编写自己的存储插件-flexvolume与csi" class="header-anchor">#</a> 30 | 编写自己的存储插件:FlexVolume与CSI</h4> <p>前一节介绍了 Kubernetes 里的持久化存储体系, 讲解了 PV 和 PVC 的具体实现原理, 并提到了这样的设计实际上是出于对整个<strong>存储体系的可扩展性</strong>的考虑. 本节分享一下如何借助这些机制, 来<strong>开发自己的存储插件</strong>.</p> <p>在 Kubernetes 中, 存储插件的开发有两种方式: <strong>FlexVolume 和 CSI</strong>.</p> <p>接下来就先剖析一下 Flexvolume 的原理和使用方法.</p> <p>举个例子, 现在要编写的是一个使用 <strong>NFS</strong> 实现的 FlexVolume 插件. 对于一个 FlexVolume 类型的 PV 来说, 它的 YAML 文件如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>flex<span class="token punctuation">-</span>nfs
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>
    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 10Gi
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> ReadWriteMany
  <span class="token key atrule">flexVolume</span><span class="token punctuation">:</span>
    <span class="token key atrule">driver</span><span class="token punctuation">:</span> <span class="token string">&quot;k8s/nfs&quot;</span>
    <span class="token key atrule">fsType</span><span class="token punctuation">:</span> <span class="token string">&quot;nfs&quot;</span>
    <span class="token key atrule">options</span><span class="token punctuation">:</span>
      <span class="token key atrule">server</span><span class="token punctuation">:</span> <span class="token string">&quot;10.10.0.25&quot;</span> <span class="token comment"># 改成你自己的 NFS 服务器地址</span>
      <span class="token key atrule">share</span><span class="token punctuation">:</span> <span class="token string">&quot;export&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>可以看到, 这个 PV 定义的 Volume 类型是 <strong>flexVolume</strong>. 并且<strong>指定了这个 Volume 的 driver 叫作 k8s/nfs</strong>. 这个名字很重要, 后面马上会解释它的含义. 而 Volume 的 <strong>options</strong> 字段, 则是一个自定义字段. 也就是说它的类型, 其实是 <strong>map[string]string</strong>. 所以可以在这一部分自由地加上想要定义的参数. 在这个例子里, options 字段指定了 <strong>NFS 服务器的地址</strong>(server: &quot;10.10.0.25&quot;), 以及 NFS 共享目录的<strong>名字</strong>(share: &quot;export&quot;). 当然, 这里定义的所有参数, 后面都会被 FlexVolume 拿到. 这里可以使用<a href="https://github.com/ehough/docker-nfs-server" target="_blank" rel="noopener noreferrer">这个 Docker 镜像<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>轻松地部署一个试验用的 NFS 服务器.</p> <p><strong>像这样的一个 PV 被创建后, 一旦和某个 PVC 绑定起来, 这个 FlexVolume 类型的 Volume 就会进入到前面讲解过的 Volume 处理流程</strong>. 你应该还记得, 这个流程的名字叫作 &quot;两阶段处理&quot;, 即 &quot;Attach 阶段&quot; 和 &quot;Mount 阶段&quot;. 它们的<strong>主要作用是在 Pod 所绑定的宿主机上, 完成这个 Volume 目录的持久化过程, 比如为虚拟机挂载磁盘(Attach), 或者挂载一个 NFS 的共享目录(Mount)</strong> .</p> <p>而在具体的控制循环中, 这两个操作实际上调用的, 正是 Kubernetes 的 pkg/volume 目录下的<strong>存储插件</strong>(Volume Plugin). 在这个例子里, 就是 pkg/volume/flexvolume 这个目录里的代码.</p> <p>当然了, 这个目录其实只是 FlexVolume 插件的入口. 以 &quot;Mount 阶段&quot; 为例, 在 FlexVolume 目录里, 它的处理过程非常简单, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token comment">// SetUpAt creates new directory.</span>
<span class="token keyword">func</span> <span class="token punctuation">(</span>f <span class="token operator">*</span>flexVolumeMounter<span class="token punctuation">)</span> <span class="token function">SetUpAt</span><span class="token punctuation">(</span>dir <span class="token builtin">string</span><span class="token punctuation">,</span> fsGroup <span class="token operator">*</span><span class="token builtin">int64</span><span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
  <span class="token operator">...</span>
  call <span class="token operator">:=</span> f<span class="token punctuation">.</span>plugin<span class="token punctuation">.</span><span class="token function">NewDriverCall</span><span class="token punctuation">(</span>mountCmd<span class="token punctuation">)</span>
  
  <span class="token comment">// Interface parameters</span>
  call<span class="token punctuation">.</span><span class="token function">Append</span><span class="token punctuation">(</span>dir<span class="token punctuation">)</span>
  
  extraOptions <span class="token operator">:=</span> <span class="token function">make</span><span class="token punctuation">(</span><span class="token keyword">map</span><span class="token punctuation">[</span><span class="token builtin">string</span><span class="token punctuation">]</span><span class="token builtin">string</span><span class="token punctuation">)</span>
  
  <span class="token comment">// pod metadata</span>
  extraOptions<span class="token punctuation">[</span>optionKeyPodName<span class="token punctuation">]</span> <span class="token operator">=</span> f<span class="token punctuation">.</span>podName
  extraOptions<span class="token punctuation">[</span>optionKeyPodNamespace<span class="token punctuation">]</span> <span class="token operator">=</span> f<span class="token punctuation">.</span>podNamespace
  
  <span class="token operator">...</span>
  
  call<span class="token punctuation">.</span><span class="token function">AppendSpec</span><span class="token punctuation">(</span>f<span class="token punctuation">.</span>spec<span class="token punctuation">,</span> f<span class="token punctuation">.</span>plugin<span class="token punctuation">.</span>host<span class="token punctuation">,</span> extraOptions<span class="token punctuation">)</span>
  
  <span class="token boolean">_</span><span class="token punctuation">,</span> err <span class="token operator">=</span> call<span class="token punctuation">.</span><span class="token function">Run</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  
  <span class="token operator">...</span>
  
  <span class="token keyword">return</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p>上面这个名叫 SetUpAt() 的方法, 正是 FlexVolume 插件对 &quot;Mount 阶段&quot; 的<strong>实现位置</strong>. 而 SetUpAt() 实际上只做了一件事, 那就是封装出了一行命令(即: NewDriverCall), 由 kubelet 在 &quot;Mount 阶段&quot; 去执行.</p> <p>在这个例子中, <strong>kubelet 要通过插件在宿主机上执行的命令, 如下所示</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs <span class="token function">mount</span> <span class="token operator">&lt;</span>mount dir<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>json param<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中, /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs 就是<strong>插件的可执行文件的路径</strong>. 这个名叫 nfs 的文件, 正是需要编写的插件的实现. 它可以是一个<strong>二进制文件, 也可以是一个脚本</strong>. 总之只要能<strong>在宿主机上被执行</strong>起来即可.</p> <p>而且这个路径里的 k8s~nfs 部分, 正是这个插件在 Kubernetes 里的名字. 它是从 driver=&quot;k8s/nfs&quot; 字段解析出来的.</p> <p>这个 driver 字段的格式是: <strong>vendor/driver</strong>. 比如一家存储插件的提供商(vendor)的名字叫作 k8s, 提供的存储驱动(driver)是 nfs, 那么 Kubernetes 就会使用 k8s~nfs 来作为插件名.</p> <p>所以说<mark><strong>当编写完了 FlexVolume 的实现之后, 一定要把它的可执行文件放在每个节点的插件目录下</strong></mark>​ **. **</p> <p>而紧跟在可执行文件后面的 &quot;mount&quot; 参数, 定义的就是当前的操作. 在 FlexVolume 里, 这些操作参数的名字是<strong>固定</strong>的, 比如 <strong>init, mount, unmount, attach, 以及 dettach</strong> 等等, 分别对应不同的 Volume 处理操作. 而跟在 mount 参数后面的两个字段: <code>&lt;mount dir&gt;</code>​ 和 <code>&lt;json params&gt;</code>​, 则是 FlexVolume <strong>必须提供给这条命令的两个执行参数</strong>.</p> <p>其中第一个执行参数 <code>&lt;mount dir&gt;</code>​, 正是 kubelet 调用 SetUpAt() 方法传递来的 dir 的值. 它代表的是<strong>当前正在处理的 Volume 在宿主机上的目录</strong>. 在这个例子里, 这个路径如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/var/lib/kubelet/pods/<span class="token operator">&lt;</span>Pod ID<span class="token operator">&gt;</span>/volumes/k8s~nfs/test
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中, test 正是前面定义的 <strong>PV 的名字</strong>; 而 k8s~nfs, 则是<strong>插件的名字</strong>. 可以看到, 插件的名字正是从声明的  driver=&quot;k8s/nfs&quot; 字段里解析出来的.</p> <p>而第二个执行参数 <code>&lt;json params&gt;</code>​, 则是一个 JSON Map 格式的参数列表. 在前面 PV 里定义的 <strong>options 字段</strong>的值, 都会被追加在这个参数里. 此外在 SetUpAt() 方法里可以看到, 这个参数列表里还包括了 <strong>Pod 的名字</strong>, Namespace 等元数据(Metadata).</p> <p>在明白了存储插件的<strong>调用方式和参数列表</strong>之后, 这个插件的可执行文件的实现部分就非常容易理解了. 在这个例子中, 直接编写了一个简单的 shell 脚本来作为插件的实现, 它对 &quot;Mount 阶段&quot; 的处理过程, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token function">domount</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 MNTPATH<span class="token operator">=</span>$<span class="token number">1</span>
 
 NFS_SERVER<span class="token operator">=</span>$<span class="token punctuation">(</span>echo $<span class="token number">2</span> <span class="token operator">|</span> jq <span class="token operator">-</span>r <span class="token char">'.server'</span><span class="token punctuation">)</span>
 SHARE<span class="token operator">=</span>$<span class="token punctuation">(</span>echo $<span class="token number">2</span> <span class="token operator">|</span> jq <span class="token operator">-</span>r <span class="token char">'.share'</span><span class="token punctuation">)</span>
 
 <span class="token operator">...</span>
 
 mkdir <span class="token operator">-</span>p $<span class="token punctuation">{</span>MNTPATH<span class="token punctuation">}</span> <span class="token operator">&amp;</span><span class="token operator">&gt;</span> <span class="token operator">/</span>dev<span class="token operator">/</span>null
 
 mount <span class="token operator">-</span>t nfs $<span class="token punctuation">{</span>NFS_SERVER<span class="token punctuation">}</span><span class="token punctuation">:</span><span class="token operator">/</span>$<span class="token punctuation">{</span>SHARE<span class="token punctuation">}</span> $<span class="token punctuation">{</span>MNTPATH<span class="token punctuation">}</span> <span class="token operator">&amp;</span><span class="token operator">&gt;</span> <span class="token operator">/</span>dev<span class="token operator">/</span>null
 <span class="token keyword">if</span> <span class="token punctuation">[</span> $? <span class="token operator">-</span>ne <span class="token number">0</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> then
  err <span class="token string">&quot;{ \&quot;status\&quot;: \&quot;Failure\&quot;, \&quot;message\&quot;: \&quot;Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\&quot;}&quot;</span>
  exit <span class="token number">1</span>
 fi
 log '<span class="token punctuation">{</span><span class="token string">&quot;status&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;Success&quot;</span><span class="token punctuation">}</span>'
 exit <span class="token number">0</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>可以看到, 当 kubelet 在宿主机上执行 &quot;<code>nfs mount &lt;mount dir&gt; &lt;json params&gt;</code>​&quot; 的时候, 这个名叫 nfs 的脚本, 就可以直接从 <code>&lt;mount dir&gt;</code>​ 参数里拿到 Volume 在宿主机上的目录, 即:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token assign-left variable">MNTPATH</span><span class="token operator">=</span>$<span class="token operator">&lt;</span>json params<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>有了这三个参数之后, 这个脚本最关键的一步, 当然就是执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token function">mount</span> <span class="token parameter variable">-t</span> nfs <span class="token punctuation">{</span>NFS_SERVER<span class="token punctuation">}</span>:/<span class="token variable">${SHARE}</span> <span class="token variable">${MNTPATH}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>需要注意的是, 当这个 mount -t nfs 操作完成后, 必须把一个 <strong>JOSN</strong> 格式的字符串, 比如: {&quot;status&quot;: &quot;Success&quot;}, 返回给调用者, 也就是 <strong>kubelet</strong>. 这是 kubelet 判断这次调用是否成功的唯一依据.</p> <p>综上所述, 在 &quot;Mount 阶段&quot;, kubelet 的 VolumeManagerReconcile 控制循环里的一次 &quot;调谐&quot; 操作的执行流程, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>kubelet --<span class="token operator">&gt;</span> pkg/volume/flexvolume.SetUpAt<span class="token punctuation">(</span><span class="token punctuation">)</span> --<span class="token operator">&gt;</span> /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs <span class="token function">mount</span> <span class="token operator">&lt;</span>mount dir<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>json param<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>备注: 这个 NFS 的 FlexVolume 的完整实现, 在<a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nfs" target="_blank" rel="noopener noreferrer">这个 GitHub 库<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>里. 而如果想用 Go 语言编写 FlexVolume 的话, 也有一个<a href="https://github.com/kubernetes/frakti/tree/master/pkg/flexvolume" target="_blank" rel="noopener noreferrer">很好的例子<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>供你参考.</p> <p>当然在前面文章中也提到过, 像 NFS 这样的文件系统存储, 并不需要在宿主机上挂载磁盘或者块设备. 所以也就不需要实现 attach 和 dettach 操作了.</p> <p>不过**像这样的 FlexVolume 实现方式, 虽然简单, 但局限性却很大. **</p> <p>比如跟 Kubernetes 内置的 NFS 插件类似, 这个 NFS FlexVolume 插件, 也<strong>不能支持 Dynamic Provisioning</strong>(即: 为每个 PVC 自动创建 PV 和对应的 Volume). 除非再为它编写一个专门的 External Provisioner.</p> <p>再比如, 我的插件在执行 mount 操作的时候, 可能会生成一些挂载信息. 这些信息在后面执行 unmount 操作的时候会被用到. 可是在上述 FlexVolume 的实现里, 没办法把这些信息保存在一个变量里, 等到 unmount 的时候直接使用.</p> <p>这个原因也很容易理解: <strong>FlexVolume 每一次对插件可执行文件的调用, 都是一次完全独立的操作</strong>. 所以只能<strong>把这些信息写在一个宿主机上的临时文件里, 等到 unmount 的时候再去读取</strong>.</p> <p>这也是为什么需要有 <strong>Container Storage Interface(CSI)</strong> 这样更完善, 更编程友好的插件方式.</p> <p>接下来就来讲解一下开发存储插件的第二种方式 <strong>CSI</strong>. 先来看一下 <strong>CSI 插件体系的设计原理</strong>.</p> <p>其实通过前面对 FlexVolume 的讲述应该可以明白, 默认情况下, <strong>Kubernetes 里通过存储插件管理容器持久化存储的原理</strong>, 可以用如下所示的示意图来描述:</p> <p><img src="/img/e431f0ec4ebddcfa614b8bb7ea897554-20230731162150-9zs9835.png" alt=""></p> <p>可以看到, 在上述体系下, <strong>无论是 FlexVolume, 还是 Kubernetes 内置的其他存储插件, 它们实际上担任的角色, 仅仅是 Volume 管理中的 &quot;Attach 阶段&quot; 和 &quot;Mount 阶段&quot; 的具体执行者</strong>. 而像 Dynamic Provisioning 这样的功能, 就不是存储插件的责任, 而是 Kubernetes 本身存储管理功能的一部分.</p> <p>相比之下, <mark><strong>CSI 插件体系的设计思想, 就是把这个 Provision 阶段, 以及 Kubernetes 里的一部分存储管理功能, 从主干代码里剥离出来, 做成了几个单独的组件</strong></mark>. 这些组件会通过 Watch API 监听 Kubernetes 里与存储相关的事件变化, 比如 PVC 的创建, 来执行具体的存储管理动作. 而这些管理动作, 比如 &quot;Attach 阶段&quot; 和 &quot;Mount 阶段&quot; 的具体操作, 实际上就是<strong>通过调用 CSI 插件来完成</strong>的.</p> <p>这种设计思路, 可以用如下所示的一幅示意图来表示:</p> <p><img src="/img/a4093c67e8bf2c0fb1d5b4f0a56eecc1-20230731162150-ur5bte9.png" alt=""></p> <p>可以看到, 这套存储插件体系多了三个独立的外部组件(External Components), 即: <strong>Driver Registrar, External Provisioner 和 External Attacher, 对应的正是从 Kubernetes 项目里面剥离出来的那部分存储管理功能</strong>. 需要注意的是, External Components 虽然是外部组件, 但依然由 Kubernetes 社区来开发和维护.</p> <p>而图中最右侧的部分, 就是需要<strong>编写代码来实现的 CSI 插件</strong>. 一个 CSI 插件只有一个二进制文件, 但它会以 <strong>gRPC</strong> 的方式对外提供三个服务(gRPC Service), 分别叫作: <strong>CSI Identity, CSI Controller 和 CSI Node</strong>.</p> <p>先来为讲解一下这三个 External Components.</p> <p>其中 <strong>Driver Registrar 组件, 负责将插件注册到 kubelet 里面</strong>(这可以类比为, 将可执行文件放在插件目录下). 而在具体实现上, Driver Registrar 需要请求 CSI 插件的 Identity 服务来获取插件信息.</p> <p>而 <strong>External Provisioner 组件, 负责的正是 Provision 阶段</strong>. 在具体实现上, External Provisioner 监听(Watch)了 APIServer 里的 PVC 对象. 当一个 PVC 被创建时, 它就会调用 CSI Controller 的 CreateVolume 方法, 并<strong>创建</strong>对应 PV. 此外, 如果使用的存储是公有云提供的磁盘(或者块设备)的话, 这一步就需要调用公有云(或者块设备服务)的 API 来创建这个 PV 所描述的磁盘(或者块设备)了.</p> <p>不过由于 CSI 插件是独立于 Kubernetes 之外的, 所以在 CSI 的 API 里不会直接使用 Kubernetes 定义的 PV 类型, 而是会自己定义一个单独的 Volume 类型. <strong>为了方便叙述, 本专栏会</strong>​<mark><strong>把 Kubernetes 里的持久化卷类型叫作 PV, 把 CSI 里的持久化卷类型叫作 CSI Volume</strong></mark>​ **, 请务必区分清楚. **</p> <p>最后一个 <strong>External Attacher 组件, 负责的正是 &quot;Attach 阶段&quot;</strong> . 在具体实现上, 它监听了 APIServer 里 <strong>VolumeAttachment 对象</strong>的变化. VolumeAttachment 对象是 Kubernetes 确认一个 Volume 可以进入 &quot;Attach 阶段&quot; 的重要标志, 会在下一结详细讲解. 一旦出现了 VolumeAttachment 对象, External Attacher 就会<strong>调用 CSI Controller 服务的 ControllerPublish 方法, 完成它所对应的 Volume 的 Attach 阶段</strong>.</p> <p>而 Volume 的 &quot;Mount 阶段&quot;, 并不属于 External Components 的职责. 当 kubelet 的 VolumeManagerReconciler 控制循环检查到它需要执行 Mount 操作的时候, 会通过 pkg/volume/csi 包, 直接调用 CSI Node 服务完成 Volume 的 &quot;Mount 阶段&quot;.</p> <p>在实际使用 CSI 插件的时候, <strong>会将这三个 External Components 作为 sidecar 容器和 CSI 插件放置在同一个 Pod 中</strong>. 由于 External Components 对 CSI 插件的调用非常频繁, 所以这种 sidecar 的部署方式非常高效.</p> <p>接下来再讲解一下 CSI 插件的里三个服务: <strong>CSI Identity, CSI Controller 和 CSI Node</strong>.</p> <p>其中, <strong>CSI 插件的 CSI Identity 服务, 负责对外暴露这个插件本身的信息</strong>, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code>service Identity <span class="token punctuation">{</span>
  <span class="token comment">// return the version and name of the plugin</span>
  rpc <span class="token function">GetPluginInfo</span><span class="token punctuation">(</span>GetPluginInfoRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>GetPluginInfoResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token comment">// reports whether the plugin has the ability of serving the Controller interface</span>
  rpc <span class="token function">GetPluginCapabilities</span><span class="token punctuation">(</span>GetPluginCapabilitiesRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>GetPluginCapabilitiesResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token comment">// called by the CO just to check whether the plugin is running or not</span>
  rpc Probe <span class="token punctuation">(</span>ProbeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>ProbeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>而 <strong>CSI Controller 服务, 定义的则是对 CSI Volume(对应 Kubernetes 里的 PV)的管理接口</strong>, 比如: 创建和删除 CSI Volume, 对 CSI Volume 进行 Attach/Dettach(在 CSI 里, 这个操作被叫作 Publish/Unpublish), 以及对 CSI Volume 进行 Snapshot 等, 它们的接口定义如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code>service Controller <span class="token punctuation">{</span>
  <span class="token comment">// provisions a volume</span>
  rpc CreateVolume <span class="token punctuation">(</span>CreateVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>CreateVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// deletes a previously provisioned volume</span>
  rpc DeleteVolume <span class="token punctuation">(</span>DeleteVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>DeleteVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// make a volume available on some required node</span>
  rpc ControllerPublishVolume <span class="token punctuation">(</span>ControllerPublishVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>ControllerPublishVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// make a volume un-available on some required node</span>
  rpc ControllerUnpublishVolume <span class="token punctuation">(</span>ControllerUnpublishVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>ControllerUnpublishVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token operator">...</span>
  
  <span class="token comment">// make a snapshot</span>
  rpc CreateSnapshot <span class="token punctuation">(</span>CreateSnapshotRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>CreateSnapshotResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// Delete a given snapshot</span>
  rpc DeleteSnapshot <span class="token punctuation">(</span>DeleteSnapshotRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>DeleteSnapshotResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token operator">...</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br></div></div><p>不难发现, CSI Controller 服务里定义的这些操作有个共同特点, 那就是<strong>它们都无需在宿主机上进行, 而是属于 Kubernetes 里 Volume Controller 的逻辑, 也就是属于 Master 节点的一部分</strong>.</p> <p>需要注意的是, 正如前面提到的那样, CSI Controller 服务的<strong>实际调用者</strong>, 并不是 Kubernetes(即: 通过 pkg/volume/csi 发起 CSI 请求), 而是 <strong>External Provisioner 和 External Attacher</strong>. 这两个 External Components, <strong>分别通过监听 PVC 和 VolumeAttachement 对象, 来跟 Kubernetes 进行协作</strong>.</p> <p>而 CSI Volume 需要在宿主机上执行的操作, 都定义在了 CSI Node 服务里面, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code>service Node <span class="token punctuation">{</span>
  <span class="token comment">// temporarily mount the volume to a staging path</span>
  rpc NodeStageVolume <span class="token punctuation">(</span>NodeStageVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>NodeStageVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// unmount the volume from staging path</span>
  rpc NodeUnstageVolume <span class="token punctuation">(</span>NodeUnstageVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>NodeUnstageVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// mount the volume from staging to target path</span>
  rpc NodePublishVolume <span class="token punctuation">(</span>NodePublishVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>NodePublishVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// unmount the volume from staging path</span>
  rpc NodeUnpublishVolume <span class="token punctuation">(</span>NodeUnpublishVolumeRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>NodeUnpublishVolumeResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token comment">// stats for the volume</span>
  rpc NodeGetVolumeStats <span class="token punctuation">(</span>NodeGetVolumeStatsRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>NodeGetVolumeStatsResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  
  <span class="token operator">...</span>
  
  <span class="token comment">// Similar to NodeGetId</span>
  rpc NodeGetInfo <span class="token punctuation">(</span>NodeGetInfoRequest<span class="token punctuation">)</span>
    returns <span class="token punctuation">(</span>NodeGetInfoResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p>需要注意的是, &quot;Mount 阶段&quot; 在 CSI Node 里的接口, 是由 <strong>NodeStageVolume 和 NodePublishVolume</strong> 两个接口共同实现的. 下一节会详细介绍这个设计的目的和具体的实现方式.</p> <blockquote><p>总结</p></blockquote> <p>本节详细讲解了 FlexVolume 和 CSI 这两种自定义存储插件的工作原理.</p> <p><strong>可以看到, 相比于 FlexVolume, CSI 的设计思想把插件的职责从 &quot;两阶段处理&quot;, 扩展成了 Provision, Attach 和 Mount 三个阶段</strong>. 其中 <strong>Provision 等价于 &quot;创建磁盘&quot;, Attach 等价于 &quot;挂载磁盘到虚拟机&quot;, Mount 等价于 &quot;将该磁盘格式化后, 挂载在 Volume 的宿主机目录上&quot;</strong> .</p> <p>在有了 CSI 插件之后, Kubernetes 本身依然按照在<a href="https://time.geekbang.org/column/article/42698" target="_blank" rel="noopener noreferrer">《PV, PVC, StorageClass, 这些到底在说啥? 》<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>一节中所讲述的方式工作, 唯一区别在于:</p> <ul><li>当 AttachDetachController 需要进行 &quot;<strong>Attach</strong>&quot; 操作时(&quot;Attach 阶段&quot;), 它<strong>实际上会执行到 pkg/volume/csi 目录中, 创建一个 VolumeAttachment 对象, 从而触发 External Attacher 调用 CSI Controller 服务的 ControllerPublishVolume 方法</strong>.</li> <li>当 VolumeManagerReconciler 需要进行 &quot;<strong>Mount</strong>&quot; 操作时(&quot;Mount 阶段&quot;), 它<strong>实际上也会执行到 pkg/volume/csi 目录中, 直接向 CSI Node 服务发起调用 NodePublishVolume 方法的请求</strong>.</li></ul> <p>以上就是 CSI 插件最基本的工作原理了. 下一节会实践一个 CSI 存储插件的完整实现过程.</p> <h4 id="_31-容器存储实践-csi插件编写指南"><a href="#_31-容器存储实践-csi插件编写指南" class="header-anchor">#</a> 31 | 容器存储实践:CSI插件编写指南</h4> <p>上一节讲解了 CSI 插件机制的设计原理. 本节将一起实践一个 CSI 插件的编写过程.</p> <p>为了能够覆盖到 CSI 插件的所有功能, 这一次选择了 <strong>DigitalOcean</strong> 的块存储(Block Storage)服务, 来作为实践对象. DigitalOcean 是业界知名的 &quot;最简&quot; 公有云服务, 即: 它只提供虚拟机, 存储, 网络等为数不多的几个基础功能, 其他功能一概不管. 而这恰恰就使得 DigitalOcean 成了我们在公有云上实践 Kubernetes 的最佳选择.</p> <p>这次编写的 CSI 插件的功能就是: <strong>让我们运行在 DigitalOcean 上的 Kubernetes 集群能够使用它的块存储服务, 作为容器的持久化存储</strong>. 注意在 DigitalOcean 上部署一个 Kubernetes 集群的过程也很简单. 只需要先在 DigitalOcean 上创建几个虚拟机, 然后按照在前面<a href="https://time.geekbang.org/column/article/39724" target="_blank" rel="noopener noreferrer">《从 0 到 1: 搭建一个完整的 Kubernetes 集群》<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>中从 0 到 1 的步骤直接部署即可.</p> <p>而有了 CSI 插件之后, 持久化存储的用法就非常简单了, 只需要创建一个如下所示的 StorageClass 对象即可:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> StorageClass
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> storage.k8s.io/v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> do<span class="token punctuation">-</span>block<span class="token punctuation">-</span>storage
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
    <span class="token key atrule">storageclass.kubernetes.io/is-default-class</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span>
<span class="token key atrule">provisioner</span><span class="token punctuation">:</span> com.digitalocean.csi.dobs
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>有了这个 StorageClass, External Provisoner 就会<strong>为集群中新出现的 PVC 自动创建出 PV, 然后调用 CSI 插件创建出这个 PV 对应的 Volume, 这正是 CSI 体系中 Dynamic Provisioning 的实现方式</strong>. 这里 <code>storageclass.kubernetes.io/is-default-class: &quot;true&quot;</code>​ 的意思是使用这个 StorageClass 作为默认的持久化存储提供者.</p> <p>不难看到, 这个 StorageClass 里唯一引人注意的, 是 <strong>provisioner=com.digitalocean.csi.dobs</strong> 这个字段. 显然这个字段告诉了 Kubernetes, 请<strong>使用名叫 com.digitalocean.csi.dobs 的 CSI 插件来为我处理这个 StorageClass 相关的所有操作</strong>.</p> <p>那么 Kubernetes 又是如何知道一个 CSI 插件的名字的呢?</p> <p>**这就需要从 CSI 插件的第一个服务 CSI Identity 说起了. **</p> <p>其实, 一个 CSI 插件的代码结构非常简单, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>tree <span class="token variable">$GOPATH</span>/src/github.com/digitalocean/csi-digitalocean/driver  
<span class="token variable">$GOPATH</span>/src/github.com/digitalocean/csi-digitalocean/driver 
├── controller.go
├── driver.go
├── identity.go
├── mounter.go
└── node.go
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>其中 <strong>CSI Identity 服务</strong>的实现, 就定义在了 driver 目录下的 identity.go 文件里. 当然为了能够让 Kubernetes 访问到 CSI Identity 服务, 需要先在 driver.go 文件里, 定义一个<strong>标准的 gRPC Server</strong>, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token comment">// Run starts the CSI plugin by communication over the given endpoint</span>
<span class="token keyword">func</span> <span class="token punctuation">(</span>d <span class="token operator">*</span>Driver<span class="token punctuation">)</span> <span class="token function">Run</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
 
 listener<span class="token punctuation">,</span> err <span class="token operator">:=</span> net<span class="token punctuation">.</span><span class="token function">Listen</span><span class="token punctuation">(</span>u<span class="token punctuation">.</span>Scheme<span class="token punctuation">,</span> addr<span class="token punctuation">)</span>
 <span class="token operator">...</span>
 
 d<span class="token punctuation">.</span>srv <span class="token operator">=</span> grpc<span class="token punctuation">.</span><span class="token function">NewServer</span><span class="token punctuation">(</span>grpc<span class="token punctuation">.</span><span class="token function">UnaryInterceptor</span><span class="token punctuation">(</span>errHandler<span class="token punctuation">)</span><span class="token punctuation">)</span>
 csi<span class="token punctuation">.</span><span class="token function">RegisterIdentityServer</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>srv<span class="token punctuation">,</span> d<span class="token punctuation">)</span>
 csi<span class="token punctuation">.</span><span class="token function">RegisterControllerServer</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>srv<span class="token punctuation">,</span> d<span class="token punctuation">)</span>
 csi<span class="token punctuation">.</span><span class="token function">RegisterNodeServer</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>srv<span class="token punctuation">,</span> d<span class="token punctuation">)</span>
 
 d<span class="token punctuation">.</span>ready <span class="token operator">=</span> <span class="token boolean">true</span> <span class="token comment">// we're now ready to go!</span>
 <span class="token operator">...</span>
 <span class="token keyword">return</span> d<span class="token punctuation">.</span>srv<span class="token punctuation">.</span><span class="token function">Serve</span><span class="token punctuation">(</span>listener<span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>可以看到, 只要把编写好的 gRPC Server <strong>注册给 CSI, 它就可以响应来自 External Components 的 CSI 请求</strong>了.</p> <p><strong>CSI Identity 服务中, 最重要的接口是 GetPluginInfo</strong>, 它返回的就是这个<strong>插件的名字和版本号</strong>(备注: CSI 各个服务的接口上一节已经介绍过, 也可以在这里找到<a href="https://github.com/container-storage-interface/spec/blob/master/csi.proto" target="_blank" rel="noopener noreferrer">它的 protoc 文件<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.), 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>d <span class="token operator">*</span>Driver<span class="token punctuation">)</span> <span class="token function">GetPluginInfo</span><span class="token punctuation">(</span>ctx context<span class="token punctuation">.</span>Context<span class="token punctuation">,</span> req <span class="token operator">*</span>csi<span class="token punctuation">.</span>GetPluginInfoRequest<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token operator">*</span>csi<span class="token punctuation">.</span>GetPluginInfoResponse<span class="token punctuation">,</span> <span class="token builtin">error</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 resp <span class="token operator">:=</span> <span class="token operator">&amp;</span>csi<span class="token punctuation">.</span>GetPluginInfoResponse<span class="token punctuation">{</span>
  Name<span class="token punctuation">:</span>          driverName<span class="token punctuation">,</span>
  VendorVersion<span class="token punctuation">:</span> version<span class="token punctuation">,</span>
 <span class="token punctuation">}</span>
 <span class="token operator">...</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>其中 driverName 的值, 正是 &quot;com.digitalocean.csi.dobs&quot;. 所以说, <strong>Kubernetes 正是通过 GetPluginInfo 的返回值, 来找到你在 StorageClass 里声明要使用的 CSI 插件的</strong>. 备注: CSI 要求插件的名字遵守 <a href="https://en.wikipedia.org/wiki/Reverse_domain_name_notation" target="_blank" rel="noopener noreferrer">&quot;反向 DNS&quot;格式<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>另外一个 <strong>GetPluginCapabilities 接口也很重要</strong>. 这个接口返回的是这个 <strong>CSI 插件的 &quot;能力&quot;</strong> .</p> <p>比如当编写的 CSI 插件不准备实现 &quot;Provision 阶段&quot; 和 &quot;Attach 阶段&quot;(比如一个最简单的 NFS 存储插件就不需要这两个阶段)时, 就可以通过这个接口返回: 本插件不提供 CSI Controller 服务, 即: 没有 csi.PluginCapability_Service_CONTROLLER_SERVICE 这个 &quot;能力&quot;. 这样 Kubernetes 就知道这个信息了.</p> <p>最后, <strong>CSI Identity 服务还提供了一个 Probe 接口</strong>. Kubernetes 会调用它来检查这个 CSI 插件是否正常工作. 一般情况下, 建议在编写插件时给它设置一个 <strong>Ready</strong> 标志, 当插件的 gRPC Server 停止的时候, 把这个 Ready 标志设置为 false. 或者可以在这里访问一下插件的端口, 类似于健康检查的做法.</p> <p>然后要开始编写 CSI 插件的第二个服务, 即 <strong>CSI Controller 服务</strong>了. 它的代码实现在 controller.go 文件里. 上一节中已经讲解过, 这个服务主要实现的就是 <strong>Volume 管理流程中的 &quot;Provision 阶段&quot; 和 &quot;Attach 阶段&quot;</strong> .</p> <p><strong>&quot;Provision 阶段&quot; 对应的接口是 CreateVolume 和 DeleteVolume</strong>, 它们的调用者是 External Provisoner. 以 CreateVolume 为例, 它的主要逻辑如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>d <span class="token operator">*</span>Driver<span class="token punctuation">)</span> <span class="token function">CreateVolume</span><span class="token punctuation">(</span>ctx context<span class="token punctuation">.</span>Context<span class="token punctuation">,</span> req <span class="token operator">*</span>csi<span class="token punctuation">.</span>CreateVolumeRequest<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token operator">*</span>csi<span class="token punctuation">.</span>CreateVolumeResponse<span class="token punctuation">,</span> <span class="token builtin">error</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
 
 volumeReq <span class="token operator">:=</span> <span class="token operator">&amp;</span>godo<span class="token punctuation">.</span>VolumeCreateRequest<span class="token punctuation">{</span>
  Region<span class="token punctuation">:</span>        d<span class="token punctuation">.</span>region<span class="token punctuation">,</span>
  Name<span class="token punctuation">:</span>          volumeName<span class="token punctuation">,</span>
  Description<span class="token punctuation">:</span>   createdByDO<span class="token punctuation">,</span>
  SizeGigaBytes<span class="token punctuation">:</span> size <span class="token operator">/</span> GB<span class="token punctuation">,</span>
 <span class="token punctuation">}</span>
 
 <span class="token operator">...</span>
 
 vol<span class="token punctuation">,</span> <span class="token boolean">_</span><span class="token punctuation">,</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>doClient<span class="token punctuation">.</span>Storage<span class="token punctuation">.</span><span class="token function">CreateVolume</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> volumeReq<span class="token punctuation">)</span>
 
 <span class="token operator">...</span>
 
 resp <span class="token operator">:=</span> <span class="token operator">&amp;</span>csi<span class="token punctuation">.</span>CreateVolumeResponse<span class="token punctuation">{</span>
  Volume<span class="token punctuation">:</span> <span class="token operator">&amp;</span>csi<span class="token punctuation">.</span>Volume<span class="token punctuation">{</span>
   Id<span class="token punctuation">:</span>            vol<span class="token punctuation">.</span>ID<span class="token punctuation">,</span>
   CapacityBytes<span class="token punctuation">:</span> size<span class="token punctuation">,</span>
   AccessibleTopology<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">*</span>csi<span class="token punctuation">.</span>Topology<span class="token punctuation">{</span>
    <span class="token punctuation">{</span>
     Segments<span class="token punctuation">:</span> <span class="token keyword">map</span><span class="token punctuation">[</span><span class="token builtin">string</span><span class="token punctuation">]</span><span class="token builtin">string</span><span class="token punctuation">{</span>
      <span class="token string">&quot;region&quot;</span><span class="token punctuation">:</span> d<span class="token punctuation">.</span>region<span class="token punctuation">,</span>
     <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
   <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
 <span class="token punctuation">}</span>
 
 <span class="token keyword">return</span> resp<span class="token punctuation">,</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><p>可以看到, 对于 DigitalOcean 这样的公有云来说, CreateVolume 需要做的操作, 就是<strong>调用 DigitalOcean 块存储服务的 API, 创建出一个存储卷</strong>(d.doClient.Storage.CreateVolume). 如果使用的是其他类型的块存储(比如 Cinder, Ceph RBD 等), 对应的操作也是类似地调用创建存储卷的 API.</p> <p>而 &quot;<strong>Attach 阶段&quot; 对应的接口是 ControllerPublishVolume 和 ControllerUnpublishVolume</strong>, 它们的调用者是 External Attacher. 以 ControllerPublishVolume 为例, 它的逻辑如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>d <span class="token operator">*</span>Driver<span class="token punctuation">)</span> <span class="token function">ControllerPublishVolume</span><span class="token punctuation">(</span>ctx context<span class="token punctuation">.</span>Context<span class="token punctuation">,</span> req <span class="token operator">*</span>csi<span class="token punctuation">.</span>ControllerPublishVolumeRequest<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token operator">*</span>csi<span class="token punctuation">.</span>ControllerPublishVolumeResponse<span class="token punctuation">,</span> <span class="token builtin">error</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
 
  dropletID<span class="token punctuation">,</span> err <span class="token operator">:=</span> strconv<span class="token punctuation">.</span><span class="token function">Atoi</span><span class="token punctuation">(</span>req<span class="token punctuation">.</span>NodeId<span class="token punctuation">)</span>
  
  <span class="token comment">// check if volume exist before trying to attach it</span>
  <span class="token boolean">_</span><span class="token punctuation">,</span> resp<span class="token punctuation">,</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>doClient<span class="token punctuation">.</span>Storage<span class="token punctuation">.</span><span class="token function">GetVolume</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> req<span class="token punctuation">.</span>VolumeId<span class="token punctuation">)</span>
 
 <span class="token operator">...</span>
 
  <span class="token comment">// check if droplet exist before trying to attach the volume to the droplet</span>
  <span class="token boolean">_</span><span class="token punctuation">,</span> resp<span class="token punctuation">,</span> err <span class="token operator">=</span> d<span class="token punctuation">.</span>doClient<span class="token punctuation">.</span>Droplets<span class="token punctuation">.</span><span class="token function">Get</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> dropletID<span class="token punctuation">)</span>
 
 <span class="token operator">...</span>
 
  action<span class="token punctuation">,</span> resp<span class="token punctuation">,</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>doClient<span class="token punctuation">.</span>StorageActions<span class="token punctuation">.</span><span class="token function">Attach</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> req<span class="token punctuation">.</span>VolumeId<span class="token punctuation">,</span> dropletID<span class="token punctuation">)</span>

 <span class="token operator">...</span>
 
 <span class="token keyword">if</span> action <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;waiting until volume is attached&quot;</span><span class="token punctuation">)</span>
 <span class="token keyword">if</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span><span class="token function">waitAction</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> req<span class="token punctuation">.</span>VolumeId<span class="token punctuation">,</span> action<span class="token punctuation">.</span>ID<span class="token punctuation">)</span><span class="token punctuation">;</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
  <span class="token keyword">return</span> <span class="token boolean">nil</span><span class="token punctuation">,</span> err
  <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
  
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;volume is attached&quot;</span><span class="token punctuation">)</span>
 <span class="token keyword">return</span> <span class="token operator">&amp;</span>csi<span class="token punctuation">.</span>ControllerPublishVolumeResponse<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br></div></div><p>可以看到, 对于 DigitalOcean 来说, ControllerPublishVolume 在 &quot;Attach 阶段&quot; 需要做的工作, 是调用 DigitalOcean 的 API, <strong>将前面创建的存储卷, 挂载到指定的虚拟机上</strong>(d.doClient.StorageActions.Attach). 其中存储卷由请求中的 VolumeId 来指定. 而虚拟机也就是将要运行 Pod 的宿主机, 则由请求中的 NodeId 来指定. 这些参数都是 External Attacher 在发起请求时需要设置的.</p> <p>前面介绍过 External Attacher 的工作原理, <strong>是监听(Watch)了一种名叫 VolumeAttachment 的 API 对象</strong>. 这种 API 对象的主要字段如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token comment">// VolumeAttachmentSpec is the specification of a VolumeAttachment request.</span>
<span class="token keyword">type</span> VolumeAttachmentSpec <span class="token keyword">struct</span> <span class="token punctuation">{</span>
 <span class="token comment">// Attacher indicates the name of the volume driver that MUST handle this</span>
 <span class="token comment">// request. This is the name returned by GetPluginName().</span>
 Attacher <span class="token builtin">string</span>
 
 <span class="token comment">// Source represents the volume that should be attached.</span>
 Source VolumeAttachmentSource
 
 <span class="token comment">// The node that the volume should be attached to.</span>
 NodeName <span class="token builtin">string</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>而这个对象的生命周期, 正是由 <strong>AttachDetachController</strong> 负责管理的(这里可以再回顾一下前面<a href="https://time.geekbang.org/column/article/42698" target="_blank" rel="noopener noreferrer">《PV, PVC, StorageClass, 这些到底在说啥? 》<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>中的相关内容). 这个控制循环的职责, <strong>是不断检查 Pod 所对应的 PV, 在它所绑定的宿主机上的挂载情况, 从而决定是否需要对这个 PV 进行 Attach(或者 Dettach)操作</strong>. 而这个 Attach 操作, 在 CSI 体系里就是创建出上面这样一个 VolumeAttachment 对象. 可以看到, Attach 操作所需的 PV 的名字(Source), 宿主机的名字(NodeName), 存储插件的名字(Attacher), 都是这个 <strong>VolumeAttachment 对象的一部分</strong>.</p> <p>**而当 External Attacher 监听到这样的一个对象出现之后, 就可以立即使用 VolumeAttachment 里的这些字段, 封装成一个 gRPC 请求调用 CSI Controller 的 ControllerPublishVolume 方法. **</p> <p>最后就可以编写 <strong>CSI Node 服务</strong>了.</p> <p>CSI Node 服务对应的, 是 Volume 管理流程里的 &quot;Mount 阶段&quot;. 它的代码实现在 node.go 文件里.</p> <p>前面提到过, kubelet 的 VolumeManagerReconciler 控制循环会<strong>直接调用 CSI Node 服务来完成 Volume 的 &quot;Mount 阶段&quot;</strong> . 不过在具体的实现中, 这个 &quot;Mount 阶段&quot; 的处理其实被细分成了 <strong>NodeStageVolume</strong> 和 <strong>NodePublishVolume</strong> 这两个接口. 这里的原因其实也很容易理解: 前面曾经介绍过, 对于磁盘以及块设备来说, 它们被 Attach 到宿主机上之后, 就成为了宿主机上的一个<strong>待用存储设备</strong>. 而到了 &quot;Mount 阶段&quot;, 首先需要格式化这个设备, 然后才能把它挂载到 Volume 对应的宿主机目录上.</p> <p>在 kubelet 的 VolumeManagerReconciler 控制循环中, 这两步操作分别叫作 <strong>MountDevice 和 SetUp. ** 其中 MountDevice 操作, 就是</strong>直接调用了 CSI Node 服务里的 NodeStageVolume 接口**. 顾名思义, 这个接口的作用, 就是格式化 Volume 在宿主机上对应的存储设备, 然后挂载到一个临时目录(Staging 目录)上.</p> <p>对于 DigitalOcean 来说, 它对 NodeStageVolume 接口的实现如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>d <span class="token operator">*</span>Driver<span class="token punctuation">)</span> <span class="token function">NodeStageVolume</span><span class="token punctuation">(</span>ctx context<span class="token punctuation">.</span>Context<span class="token punctuation">,</span> req <span class="token operator">*</span>csi<span class="token punctuation">.</span>NodeStageVolumeRequest<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token operator">*</span>csi<span class="token punctuation">.</span>NodeStageVolumeResponse<span class="token punctuation">,</span> <span class="token builtin">error</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
 
 vol<span class="token punctuation">,</span> resp<span class="token punctuation">,</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>doClient<span class="token punctuation">.</span>Storage<span class="token punctuation">.</span><span class="token function">GetVolume</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> req<span class="token punctuation">.</span>VolumeId<span class="token punctuation">)</span>
 
 <span class="token operator">...</span>
 
 source <span class="token operator">:=</span> <span class="token function">getDiskSource</span><span class="token punctuation">(</span>vol<span class="token punctuation">.</span>Name<span class="token punctuation">)</span>
 target <span class="token operator">:=</span> req<span class="token punctuation">.</span>StagingTargetPath
 
 <span class="token operator">...</span>
 
 <span class="token keyword">if</span> <span class="token operator">!</span>formatted <span class="token punctuation">{</span>
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;formatting the volume for staging&quot;</span><span class="token punctuation">)</span>
  <span class="token keyword">if</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>mounter<span class="token punctuation">.</span><span class="token function">Format</span><span class="token punctuation">(</span>source<span class="token punctuation">,</span> fsType<span class="token punctuation">)</span><span class="token punctuation">;</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
   <span class="token keyword">return</span> <span class="token boolean">nil</span><span class="token punctuation">,</span> status<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span>codes<span class="token punctuation">.</span>Internal<span class="token punctuation">,</span> err<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
 <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;source device is already formatted&quot;</span><span class="token punctuation">)</span>
 <span class="token punctuation">}</span>
 
<span class="token operator">...</span>

 <span class="token keyword">if</span> <span class="token operator">!</span>mounted <span class="token punctuation">{</span>
  <span class="token keyword">if</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>mounter<span class="token punctuation">.</span><span class="token function">Mount</span><span class="token punctuation">(</span>source<span class="token punctuation">,</span> target<span class="token punctuation">,</span> fsType<span class="token punctuation">,</span> options<span class="token operator">...</span><span class="token punctuation">)</span><span class="token punctuation">;</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
   <span class="token keyword">return</span> <span class="token boolean">nil</span><span class="token punctuation">,</span> status<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span>codes<span class="token punctuation">.</span>Internal<span class="token punctuation">,</span> err<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
 <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;source device is already mounted to the target path&quot;</span><span class="token punctuation">)</span>
 <span class="token punctuation">}</span>
 
 <span class="token operator">...</span>
 <span class="token keyword">return</span> <span class="token operator">&amp;</span>csi<span class="token punctuation">.</span>NodeStageVolumeResponse<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br></div></div><p>可以看到, 在 NodeStageVolume 的实现里, 首先通过 DigitalOcean 的 API <strong>获取到了这个 Volume 对应的设备路径</strong>(getDiskSource); 然后把这个设备格式化成指定的格式(d.mounter.Format); 最后把格式化后的设备挂载到了一个<strong>临时</strong>的 Staging 目录(StagingTargetPath)下.</p> <p>而 SetUp 操作则会调用 CSI Node 服务的 <strong>NodePublishVolume</strong> 接口. 有了上述对设备的预处理工作后, 它的实现就非常简单了, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">func</span> <span class="token punctuation">(</span>d <span class="token operator">*</span>Driver<span class="token punctuation">)</span> <span class="token function">NodePublishVolume</span><span class="token punctuation">(</span>ctx context<span class="token punctuation">.</span>Context<span class="token punctuation">,</span> req <span class="token operator">*</span>csi<span class="token punctuation">.</span>NodePublishVolumeRequest<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token operator">*</span>csi<span class="token punctuation">.</span>NodePublishVolumeResponse<span class="token punctuation">,</span> <span class="token builtin">error</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token operator">...</span>
 source <span class="token operator">:=</span> req<span class="token punctuation">.</span>StagingTargetPath
 target <span class="token operator">:=</span> req<span class="token punctuation">.</span>TargetPath
 
 mnt <span class="token operator">:=</span> req<span class="token punctuation">.</span>VolumeCapability<span class="token punctuation">.</span><span class="token function">GetMount</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 options <span class="token operator">:=</span> mnt<span class="token punctuation">.</span>MountFlag
    <span class="token operator">...</span>
  
 <span class="token keyword">if</span> <span class="token operator">!</span>mounted <span class="token punctuation">{</span>
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;mounting the volume&quot;</span><span class="token punctuation">)</span>
  <span class="token keyword">if</span> err <span class="token operator">:=</span> d<span class="token punctuation">.</span>mounter<span class="token punctuation">.</span><span class="token function">Mount</span><span class="token punctuation">(</span>source<span class="token punctuation">,</span> target<span class="token punctuation">,</span> fsType<span class="token punctuation">,</span> options<span class="token operator">...</span><span class="token punctuation">)</span><span class="token punctuation">;</span> err <span class="token operator">!=</span> <span class="token boolean">nil</span> <span class="token punctuation">{</span>
   <span class="token keyword">return</span> <span class="token boolean">nil</span><span class="token punctuation">,</span> status<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span>codes<span class="token punctuation">.</span>Internal<span class="token punctuation">,</span> err<span class="token punctuation">.</span><span class="token function">Error</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
 <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
  ll<span class="token punctuation">.</span><span class="token function">Info</span><span class="token punctuation">(</span><span class="token string">&quot;volume is already mounted&quot;</span><span class="token punctuation">)</span>
 <span class="token punctuation">}</span>
 
 <span class="token keyword">return</span> <span class="token operator">&amp;</span>csi<span class="token punctuation">.</span>NodePublishVolumeResponse<span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>可以看到, 在这一步实现中, 只需要做一步操作, 即: <strong>将 Staging 目录, 绑定挂载到 Volume 对应的宿主机目录上</strong>. <strong>由于 Staging 目录正是 Volume 对应的设备被格式化后挂载在宿主机上的位置, 所以当它和 Volume 的宿主机目录绑定挂载之后, 这个 Volume 宿主机目录的 &quot;持久化&quot; 处理也就完成了.</strong></p> <p>当然前面也曾经提到过, 对于文件系统类型的存储服务来说, 比如 NFS 和 GlusterFS 等, 它们并没有一个对应的磁盘 &quot;设备&quot; 存在于宿主机上, 所以 kubelet 在 VolumeManagerReconciler 控制循环中, 会<strong>跳过 MountDevice 操作</strong>而直接执行 SetUp 操作. 所以对于它们来说, 也就不需要实现 NodeStageVolume 接口了.</p> <p>在编写完了 CSI 插件之后, 就可以<strong>把这个插件和 External Components 一起部署</strong>起来.</p> <p>首先需要创建一个 DigitalOcean client 授权需要使用的 Secret 对象, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> digitalocean
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
<span class="token key atrule">stringData</span><span class="token punctuation">:</span>
  <span class="token key atrule">access-token</span><span class="token punctuation">:</span> <span class="token string">&quot;a05dd2f26b9b9ac2asdas__REPLACE_ME____123cb5d1ec17513e06da&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>接下来通过一句指令就可以将 CSI 插件部署起来:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-v0.2.0.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个 CSI 插件的 YAML 文件的主要内容如下所示(其中非重要的内容已经被略去):</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1beta2
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>node
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>node
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>node
        <span class="token key atrule">role</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">serviceAccount</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>node<span class="token punctuation">-</span>sa
      <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> driver<span class="token punctuation">-</span>registrar
          <span class="token key atrule">image</span><span class="token punctuation">:</span> quay.io/k8scsi/driver<span class="token punctuation">-</span>registrar<span class="token punctuation">:</span>v0.3.0
          <span class="token punctuation">...</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>plugin
          <span class="token key atrule">image</span><span class="token punctuation">:</span> digitalocean/do<span class="token punctuation">-</span>csi<span class="token punctuation">-</span>plugin<span class="token punctuation">:</span>v0.2.0
          <span class="token key atrule">args</span> <span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;--token=$(DIGITALOCEAN_ACCESS_TOKEN)&quot;</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;--url=$(DIGITALOCEAN_API_URL)&quot;</span>
          <span class="token key atrule">env</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CSI_ENDPOINT
              <span class="token key atrule">value</span><span class="token punctuation">:</span> unix<span class="token punctuation">:</span>///csi/csi.sock
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> DIGITALOCEAN_API_URL
              <span class="token key atrule">value</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//api.digitalocean.com/
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> DIGITALOCEAN_ACCESS_TOKEN
              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
                <span class="token key atrule">secretKeyRef</span><span class="token punctuation">:</span>
                  <span class="token key atrule">name</span><span class="token punctuation">:</span> digitalocean
                  <span class="token key atrule">key</span><span class="token punctuation">:</span> access<span class="token punctuation">-</span>token
          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> <span class="token string">&quot;Always&quot;</span>
          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>
            <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
            <span class="token key atrule">capabilities</span><span class="token punctuation">:</span>
              <span class="token key atrule">add</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;SYS_ADMIN&quot;</span><span class="token punctuation">]</span>
            <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> plugin<span class="token punctuation">-</span>dir
              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /csi
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pods<span class="token punctuation">-</span>mount<span class="token punctuation">-</span>dir
              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/kubelet
              <span class="token key atrule">mountPropagation</span><span class="token punctuation">:</span> <span class="token string">&quot;Bidirectional&quot;</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> device<span class="token punctuation">-</span>dir
              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /dev
      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> plugin<span class="token punctuation">-</span>dir
          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
            <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/lib/kubelet/plugins/com.digitalocean.csi.dobs
            <span class="token key atrule">type</span><span class="token punctuation">:</span> DirectoryOrCreate
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pods<span class="token punctuation">-</span>mount<span class="token punctuation">-</span>dir
          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
            <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/lib/kubelet
            <span class="token key atrule">type</span><span class="token punctuation">:</span> Directory
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> device<span class="token punctuation">-</span>dir
          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
            <span class="token key atrule">path</span><span class="token punctuation">:</span> /dev
<span class="token punctuation">---</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1beta1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>controller
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> <span class="token string">&quot;csi-do&quot;</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>controller
        <span class="token key atrule">role</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">serviceAccount</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>controller<span class="token punctuation">-</span>sa
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>provisioner
          <span class="token key atrule">image</span><span class="token punctuation">:</span> quay.io/k8scsi/csi<span class="token punctuation">-</span>provisioner<span class="token punctuation">:</span>v0.3.0
          <span class="token punctuation">...</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>attacher
          <span class="token key atrule">image</span><span class="token punctuation">:</span> quay.io/k8scsi/csi<span class="token punctuation">-</span>attacher<span class="token punctuation">:</span>v0.3.0
          <span class="token punctuation">...</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>do<span class="token punctuation">-</span>plugin
          <span class="token key atrule">image</span><span class="token punctuation">:</span> digitalocean/do<span class="token punctuation">-</span>csi<span class="token punctuation">-</span>plugin<span class="token punctuation">:</span>v0.2.0
          <span class="token key atrule">args</span> <span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;--endpoint=$(CSI_ENDPOINT)&quot;</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;--token=$(DIGITALOCEAN_ACCESS_TOKEN)&quot;</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;--url=$(DIGITALOCEAN_API_URL)&quot;</span>
          <span class="token key atrule">env</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CSI_ENDPOINT
              <span class="token key atrule">value</span><span class="token punctuation">:</span> unix<span class="token punctuation">:</span>///var/lib/csi/sockets/pluginproxy/csi.sock
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> DIGITALOCEAN_API_URL
              <span class="token key atrule">value</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//api.digitalocean.com/
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> DIGITALOCEAN_ACCESS_TOKEN
              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
                <span class="token key atrule">secretKeyRef</span><span class="token punctuation">:</span>
                  <span class="token key atrule">name</span><span class="token punctuation">:</span> digitalocean
                  <span class="token key atrule">key</span><span class="token punctuation">:</span> access<span class="token punctuation">-</span>token
          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> <span class="token string">&quot;Always&quot;</span>
          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> socket<span class="token punctuation">-</span>dir
              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/csi/sockets/pluginproxy/
      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> socket<span class="token punctuation">-</span>dir
          <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br></div></div><p>可以看到, 这里编写的 CSI 插件<strong>只有一个二进制文件</strong>, 它的镜像是 digitalocean/do-csi-plugin:v0.2.0.</p> <p>而**部署 CSI 插件的常用原则是: **</p> <p><strong>第一, 通过 DaemonSet 在每个节点上都启动一个 CSI 插件, 来为 kubelet 提供 CSI Node 服务</strong>. 这是因为 CSI Node 服务需要被 kubelet 直接调用, 所以它要和 kubelet &quot;一对一&quot; 地部署起来.</p> <p>此外在上述 DaemonSet 的定义里面, 除了 CSI 插件, 还以 sidecar 的方式运行着 <strong>driver-registrar</strong> 这个外部组件. 它的作用是向 kubelet 注册这个 CSI 插件. 这个注册过程使用的插件信息, 则通过访问同一个 Pod 里的 CSI 插件容器的 Identity 服务获取到.</p> <p>需要注意的是, 由于 CSI 插件运行在一个容器里, 那么 CSI Node 服务在 &quot;Mount 阶段&quot; 执行的挂载操作, 实际上是发生在这个容器的 Mount Namespace 里的. 可是真正希望执行挂载操作的对象, 都是宿主机 /var/lib/kubelet 目录下的文件和目录. 所以在定义 DaemonSet Pod 的时候, 需要把宿主机的 /var/lib/kubelet 以 Volume 的方式挂载进 CSI 插件容器的同名目录下, 然后设置这个 Volume 的 mountPropagation=Bidirectional, 即开启双向挂载传播, 从而将容器在这个目录下进行的挂载操作 &quot;传播&quot; 给宿主机, 反之亦然.</p> <p><strong>第二, 通过 StatefulSet 在任意一个节点上再启动一个 CSI 插件, 为 External Components 提供 CSI Controller 服务</strong>. 所以作为 CSI Controller 服务的调用者, External Provisioner 和 External Attacher 这两个外部组件, 就需要以 <strong>sidecar</strong> 的方式和这次部署的 CSI 插件定义在同一个 Pod 里.</p> <p>你可能会好奇, 为什么会用 <strong>StatefulSet</strong> 而不是 Deployment 来运行这个 CSI 插件呢.</p> <p>这是因为由于 StatefulSet 需要确保应用拓扑状态的稳定性, 所以它对 Pod 的更新, 是严格保证顺序的, 即: 只有在前一个 Pod 停止并删除之后, 它才会创建并启动下一个 Pod.</p> <p>而像上面这样将 StatefulSet 的 replicas 设置为 1 的话, StatefulSet 就会确保 Pod 被删除重建的时候, 永远有且只有一个 CSI 插件的 Pod 运行在集群中. 这对 CSI 插件的正确性来说, 至关重要.</p> <p>而在本节开始, 就已经定义了这个 CSI 插件对应的 StorageClass(即: do-block-storage), 所以接下来只需要定义一个<strong>声明使用这个 StorageClass 的 PVC 即可</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> csi<span class="token punctuation">-</span>pvc
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 5Gi
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> do<span class="token punctuation">-</span>block<span class="token punctuation">-</span>storage
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>当把上述 PVC 提交给 Kubernetes 之后, 就<strong>可以在 Pod 里声明使用这个 csi-pvc 来作为持久化存储了</strong>. 这一部分使用 PV 和 PVC 的内容, 就不再赘述了.</p> <blockquote><p>总结</p></blockquote> <p>本节以一个 DigitalOcean 的 CSI 插件为例, 分享了编写 CSI 插件的具体流程.</p> <p>举个例子, 对于一个部署了 CSI 存储插件的 Kubernetes 集群来说: <strong>当用户创建了一个 PVC 之后, 前面部署的 StatefulSet 里的 External Provisioner 容器, 就会监听到这个 PVC 的诞生, 然后调用同一个 Pod 里的 CSI 插件的 CSI Controller 服务的 CreateVolume 方法, 为你创建出对应的 PV</strong>. 这时候, 运行在 Kubernetes Master 节点上的 Volume Controller, 就会通过 PersistentVolumeController 控制循环, 发现这对新创建出来的 PV 和 PVC, 并且看到它们声明的是同一个 StorageClass. 所以它会把这一对 PV 和 PVC 绑定起来, 使 PVC 进入 Bound 状态.</p> <p>然后用户创建了一个声明使用上述 PVC 的 Pod, 并且这个 Pod 被调度器调度到了宿主机 A 上. 这时候 Volume Controller 的 AttachDetachController 控制循环就会发现, 上述 PVC 对应的 Volume, 需要被 Attach 到宿主机 A 上. 所以, AttachDetachController 会创建一个 VolumeAttachment 对象, 这个对象携带了宿主机 A 和待处理的 Volume 的名字.</p> <p>这样, StatefulSet 里的 External Attacher 容器, 就会监听到这个 VolumeAttachment 对象的诞生. 于是它就会使用这个对象里的宿主机和 Volume 名字, 调用同一个 Pod 里的 CSI 插件的 CSI Controller 服务的 ControllerPublishVolume 方法, 完成 &quot;Attach 阶段&quot;.</p> <p>上述过程完成后, 运行在宿主机 A 上的 kubelet, 就会通过 VolumeManagerReconciler 控制循环, 发现当前宿主机上有一个 Volume 对应的存储设备(比如磁盘)已经被 Attach 到了某个设备目录下. 于是 kubelet 就会调用同一台宿主机上的 CSI 插件的 CSI Node 服务的 NodeStageVolume 和 NodePublishVolume 方法, 完成这个 Volume 的 &quot;Mount 阶段&quot;.</p> <p>至此一个完整的持久化 Volume 的创建和挂载流程就结束了.</p> <h4 id="_32-浅谈容器网络"><a href="#_32-浅谈容器网络" class="header-anchor">#</a> 32 | 浅谈容器网络</h4> <p>在前面讲解容器基础时曾经提到过<strong>一个 Linux 容器能看见的 &quot;网络栈&quot;, 实际上是被隔离在它自己的 Network Namespace 当中的</strong>. 而所谓&quot;网络栈&quot;, 就包括了: <strong>网卡</strong>(Network Interface), <strong>回环设备</strong>(Loopback Device), <strong>路由表</strong>(Routing Table)和 <strong>iptables 规则</strong>. 对于一个进程来说, 这些要素其实就构成了它发起和响应网络请求的基本环境.</p> <p>需要指出的是, 作为一个容器, 它可以声明<strong>直接使用宿主机的网络栈</strong>(–net=host), 即: 不开启 Network Namespace, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run –d –net<span class="token operator">=</span>host <span class="token parameter variable">--name</span> nginx-host nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在这种情况下, 这个容器启动后, 直接监听的就是宿主机的 80 端口.</p> <p>像这样直接使用宿主机网络栈的方式, 虽然可以为容器提供良好的网络性能, 但也会不可避免地引入<strong>共享网络资源</strong>的问题, 比如端口冲突. 所以<mark><strong>在大多数情况下, 都希望容器进程能使用自己 Network Namespace 里的网络栈, 即: 拥有属于自己的 IP 地址和端口</strong></mark>​ **. **</p> <p>这时候一个显而易见的问题就是: <strong>这个被隔离的容器进程, 该如何跟其他 Network Namespace 里的容器进程进行交互</strong>呢? 为了理解这个问题, <mark><strong>其实可以把每一个容器看做一台主机, 它们都有一套独立的 &quot;网络栈&quot;</strong></mark> .</p> <p>如果想要实现两台主机之间的通信, 最直接的办法, 就是把它们用一根网线连接起来; 而如果你想要实现多台主机之间的通信, 那就需要用网线, 把它们连接在一台<strong>交换机</strong>上. 在 Linux 中, 能够<strong>起到虚拟交换机作用的网络设备, 是</strong>​<mark><strong>网桥</strong></mark>​ <strong>(Bridge)</strong> . 它是一个工作在<strong>数据链路层</strong>(Data Link)的设备, 主要功能是<strong>根据 MAC 地址学习来将数据包转发到网桥的不同端口(Port)上</strong>.</p> <p>而为了实现上述目的, Docker 项目会默认在<strong>宿主机</strong>上创建一个名叫 <strong>docker0 的网桥</strong>, 凡是连接在 docker0 网桥上的容器, 就可以通过它来进行通信.</p> <p>可是又该如何把这些容器 &quot;连接&quot; 到 docker0 网桥上呢? 这时候就需要使用一种名叫 <mark><strong>Veth Pair</strong></mark>​ ** ** 的虚拟设备了. Veth Pair 设备的特点是: <strong>它被创建出来后, 总是以两张虚拟网卡(Veth Peer)的形式成对出现的. 并且从其中一个 &quot;网卡&quot; 发出的数据包, 可以直接出现在与它对应的另一张 &quot;网卡&quot;上, 哪怕这两个 &quot;网卡&quot; 在不同的 Network Namespace 里</strong>. 这就**使得 Veth Pair 常常被用作连接不同 Network Namespace 的 &quot;**​<mark><strong>网线</strong></mark>​ <strong>&quot;</strong> .</p> <p>比如现在启动了一个叫作 nginx-1 的容器:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run –d <span class="token parameter variable">--name</span> nginx-1 nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后进入到这个容器中查看一下它的网络设备:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在宿主机上</span>
$ <span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> nginx-1 /bin/bash
<span class="token comment"># 在容器里</span>
root@2b3c181aecf1:/<span class="token comment"># ifconfig</span>
eth0: <span class="token assign-left variable">flags</span><span class="token operator">=</span><span class="token number">416</span><span class="token operator"><span class="token file-descriptor important">3</span>&lt;</span>UP,BROADCAST,RUNNING,MULTICAST<span class="token operator">&gt;</span>  mtu <span class="token number">1500</span>
        inet <span class="token number">172.17</span>.0.2  netmask <span class="token number">255.255</span>.0.0  broadcast <span class="token number">0.0</span>.0.0
        inet6 fe80::42:acff:fe11:2  prefixlen <span class="token number">64</span>  scopeid 0x2<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>link<span class="token operator">&gt;</span>
        ether 02:42:ac:11:00:02  txqueuelen <span class="token number">0</span>  <span class="token punctuation">(</span>Ethernet<span class="token punctuation">)</span>
        RX packets <span class="token number">364</span>  bytes <span class="token number">8137175</span> <span class="token punctuation">(</span><span class="token number">7.7</span> MiB<span class="token punctuation">)</span>
        RX errors <span class="token number">0</span>  dropped <span class="token number">0</span>  overruns <span class="token number">0</span>  frame <span class="token number">0</span>
        TX packets <span class="token number">281</span>  bytes <span class="token number">21161</span> <span class="token punctuation">(</span><span class="token number">20.6</span> KiB<span class="token punctuation">)</span>
        TX errors <span class="token number">0</span>  dropped <span class="token number">0</span> overruns <span class="token number">0</span>  carrier <span class="token number">0</span>  collisions <span class="token number">0</span>
      
lo: <span class="token assign-left variable">flags</span><span class="token operator">=</span><span class="token number">7</span><span class="token operator"><span class="token file-descriptor important">3</span>&lt;</span>UP,LOOPBACK,RUNNING<span class="token operator">&gt;</span>  mtu <span class="token number">65536</span>
        inet <span class="token number">127.0</span>.0.1  netmask <span class="token number">255.0</span>.0.0
        inet6 ::1  prefixlen <span class="token number">128</span>  scopeid 0x1<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>host<span class="token operator">&gt;</span>
        loop  txqueuelen <span class="token number">1000</span>  <span class="token punctuation">(</span>Local Loopback<span class="token punctuation">)</span>
        RX packets <span class="token number">0</span>  bytes <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0.0</span> B<span class="token punctuation">)</span>
        RX errors <span class="token number">0</span>  dropped <span class="token number">0</span>  overruns <span class="token number">0</span>  frame <span class="token number">0</span>
        TX packets <span class="token number">0</span>  bytes <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0.0</span> B<span class="token punctuation">)</span>
        TX errors <span class="token number">0</span>  dropped <span class="token number">0</span> overruns <span class="token number">0</span>  carrier <span class="token number">0</span>  collisions <span class="token number">0</span>
      
$ route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         <span class="token number">172.17</span>.0.1      <span class="token number">0.0</span>.0.0         UG    <span class="token number">0</span>      <span class="token number">0</span>        <span class="token number">0</span> eth0
<span class="token number">172.17</span>.0.0      <span class="token number">0.0</span>.0.0         <span class="token number">255.255</span>.0.0     U     <span class="token number">0</span>      <span class="token number">0</span>        <span class="token number">0</span> eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p>可以看到, 这个<strong>容器里有一张叫作 eth0 的网卡</strong>, 它正是<strong>一个 Veth Pair 设备在容器里的这一端</strong>.</p> <p>通过 route 命令查看 nginx-1 容器的路由表, 可以看到, <strong>这个 eth0 网卡是这个容器里的默认路由设备</strong>; 所有对 172.17.0.0/16 网段的请求, 也会被交给 eth0 来处理(第二条 172.17.0.0 路由规则).</p> <p>而<strong>这个 Veth Pair 设备的另一端, 则在宿主机</strong>上. 可以通过查看宿主机的网络设备看到它, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在宿主机上</span>
$ <span class="token function">ifconfig</span>
<span class="token punctuation">..</span>.
docker0   Link encap:Ethernet  HWaddr 02:42:d8:e4:df:c1  
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:d8ff:fee4:dfc1/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:309 errors:0 dropped:0 overruns:0 frame:0
          TX packets:372 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0 
          RX bytes:18944 <span class="token punctuation">(</span><span class="token number">18.9</span> KB<span class="token punctuation">)</span>  TX bytes:8137789 <span class="token punctuation">(</span><span class="token number">8.1</span> MB<span class="token punctuation">)</span>
veth9c02e56 Link encap:Ethernet  HWaddr <span class="token number">52</span>:81:0b:24:3d:da  
          inet6 addr: fe80::5081:bff:fe24:3dda/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:288 errors:0 dropped:0 overruns:0 frame:0
          TX packets:371 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0 
          RX bytes:21608 <span class="token punctuation">(</span><span class="token number">21.6</span> KB<span class="token punctuation">)</span>  TX bytes:8137719 <span class="token punctuation">(</span><span class="token number">8.1</span> MB<span class="token punctuation">)</span>
        
$ brctl show
bridge name bridge <span class="token function">id</span>  STP enabled interfaces
docker0  <span class="token number">8000</span>.0242d8e4dfc1 no  veth9c02e56
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>通过 ifconfig 命令的输出可以看到, nginx-1 容器对应的 Veth Pair 设备, 在宿主机上是一张<strong>虚拟网卡</strong>. 它的名字叫作 <strong>veth9c02e56</strong>. 并且通过 brctl show 的输出, 可以看到<strong>这张网卡被 &quot;插&quot; 在了 docker0 上</strong>.</p> <p>这时候, 如果再在这台宿主机上启动另一个 Docker 容器, 比如 nginx-2:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run –d <span class="token parameter variable">--name</span> nginx-2 nginx
$ brctl show
bridge name bridge <span class="token function">id</span>  STP enabled interfaces
docker0  <span class="token number">8000</span>.0242d8e4dfc1 no  veth9c02e56
       vethb4963f3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>你就会发现一个<strong>新的名叫 vethb4963f3 的虚拟网卡, 也被 &quot;插&quot; 在了 docker0 网桥上</strong>. 这时候, 如果在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址(172.17.0.3), 就会发现<strong>同一宿主机上的两个容器默认就是相互连通</strong>的.</p> <p>解释一下这其中的原理. 当在 nginx-1 容器里访问 nginx-2 容器的 IP 地址(比如 ping 172.17.0.3)的时候, 这个目的 IP 地址会<strong>匹配到 nginx-1 容器里的第二条路由规则</strong>. 可以看到, 这条路由规则的<strong>网关(Gateway)是 0.0.0.0</strong>, 这就意味着这是一条直连规则, 即: <strong>凡是匹配到这条规则的 IP 包, 应该经过本机的 eth0 网卡, 通过二层网络直接发往目的主机</strong>.</p> <p>而要通过二层网络到达 nginx-2 容器, 就需要<strong>有 172.17.0.3 这个 IP 地址对应的 MAC 地址</strong>. 所以 nginx-1 容器的网络协议栈, 就需要通过 eth0 网卡发送一个 <strong>ARP 广播</strong>, 来通过 IP 地址查找对应的 MAC 地址.</p> <p>前面提到过, 这个 <strong>eth0 网卡, 是一个 Veth Pair</strong>, 它的<strong>一端在这个 nginx-1 容器的 Network Namespace 里, 而另一端则位于宿主机上(Host Namespace), 并且被 &quot;插&quot; 在了宿主机的 docker0 网桥上</strong>.</p> <p><mark><strong>一旦一张虚拟网卡被 &quot;插&quot; 在网桥上, 它就会变成该网桥的 &quot;从设备&quot;. 从设备会被 &quot;剥夺&quot; 调用网络协议栈处理数据包的资格, 从而 &quot;降级&quot; 成为网桥上的一个端口</strong></mark>. 而这个端口唯一的作用, 就是接收流入的数据包, 然后把这些数据包的 &quot;生杀大权&quot;(比如转发或者丢弃), <strong>全部交给对应的网桥</strong>.</p> <p>所以在收到这些 ARP 请求之后, docker0 网桥就会扮演二层<strong>交换机</strong>的角色, 把 ARP 广播转发到其他被 &quot;插&quot; 在 docker0 上的虚拟网卡上. 这样同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求, 从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器. <strong>有了这个目的 MAC 地址, nginx-1 容器的 eth0 网卡就可以将数据包发出去</strong>.</p> <p>而根据 Veth Pair 设备的原理, 这个<strong>数据包会立刻出现在宿主机上的 veth9c02e56 虚拟网卡上</strong>. 不过此时这个 veth9c02e56 网卡的网络协议栈的资格已经被 &quot;剥夺&quot;, 所以<strong>这个数据包就直接流入到了 docker0 网桥里</strong>.</p> <p>docker0 处理转发的过程, 则继续扮演二层<strong>交换机</strong>的角色. 此时 docker0 网桥根据数据包的目的 MAC 地址(也就是 nginx-2 容器的 MAC 地址), <strong>在它的 CAM 表(即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表)里查到对应的端口(Port)为: vethb4963f3, 然后把数据包发往这个端口</strong>. 而这个端口, <strong>正是 nginx-2 容器 &quot;插&quot; 在 docker0 网桥上的另一块虚拟网卡</strong>, 当然它也是一个 Veth Pair 设备. 这样数据包就进入到了 nginx-2 容器的 Network Namespace 里.</p> <p>所以 nginx-2 容器看到的情况是, 它<strong>自己的 eth0 网卡上出现了流入的数据包</strong>. 这样 nginx-2 的网络协议栈就会对请求进行处理, 最后将响应(Pong)返回到 nginx-1.</p> <p>以上就是<strong>同一个宿主机上的不同容器通过 docker0 网桥进行通信的流程</strong>了. 这里把这个流程总结成了一幅示意图, 如下所示:</p> <p><img src="/img/a278d003652c5d50bb88243e4ca88448-20230731162150-r6su41i.png" alt=""></p> <p>需要注意的是, 在实际的数据传递时, 上述数据的传递过程在<strong>网络协议栈的不同层次</strong>, 都有 Linux 内核 Netfilter 参与其中. 所以如果感兴趣的话, 可以通过<strong>打开 iptables 的 TRACE 功能查看到数据包的传输过程</strong>, 具体方法如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在宿主机上执行</span>
$ iptables <span class="token parameter variable">-t</span> raw <span class="token parameter variable">-A</span> OUTPUT <span class="token parameter variable">-p</span> icmp <span class="token parameter variable">-j</span> TRACE
$ iptables <span class="token parameter variable">-t</span> raw <span class="token parameter variable">-A</span> PREROUTING <span class="token parameter variable">-p</span> icmp <span class="token parameter variable">-j</span> TRACE
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>通过上述设置, 就可以在 /var/log/syslog 里看到数据包传输的日志了. 这一部分内容, 可以结合 <a href="https://en.wikipedia.org/wiki/Iptables" target="_blank" rel="noopener noreferrer">iptables 的相关知识<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>进行实践, 从而验证上述数据包传递流程.</p> <p>熟悉了 docker0 网桥的工作方式, 就可以理解, 在默认情况下, <mark><strong>被限制在 Network Namespace 里的容器进程, 实际上是通过 Veth Pair 设备 + 宿主机网桥的方式, 实现了跟同其他容器的数据交换</strong></mark>​ **. **</p> <p>与之类似地, 当<strong>在一台宿主机上, 访问该宿主机上的容器的 IP 地址时, 这个请求的数据包, 也是先根据路由规则到达 docker0 网桥, 然后被转发到对应的 Veth Pair 设备, 最后出现在容器里</strong>. 这个过程的示意图, 如下所示:</p> <p><img src="/img/9ec4d646fcd4474b4c0c9c09cab1cebb-20230731162150-e08qnmb.png" alt="">同样地, 当<strong>一个容器试图连接到另外一个宿主机</strong>时, 比如: ping 10.168.0.3, 它发出的请求数据包, 首先<strong>经过 docker0 网桥</strong>出现在宿主机上. 然后根据<strong>宿主机的路由表里的直连路由规则</strong>(10.168.0.0/24 via eth0)), 对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理.</p> <p>所以接下来, 这个数据包就会<strong>经宿主机的 eth0 网卡转发到宿主机网络</strong>上, 最终到达 10.168.0.3 对应的宿主机上. 当然这个过程的实现要求这<strong>两台宿主机本身是连通</strong>的. 这个过程的示意图如下所示:</p> <p><img src="/img/c160b98a4944d89761bdc10016ffd37b-20230731162150-x3ioumo.png" alt=""></p> <p>所以说, <mark><strong>当遇到容器连不通 &quot;外网&quot; 的时候, 应该先试试 docker0 网桥能不能 ping 通, 然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常, 往往就能够找到问题的答案了</strong></mark>​ **. **</p> <p>不过在最后一个 &quot;Docker 容器连接其他宿主机&quot; 的例子里, 你可能已经联想到了这样一个问题: <strong>如果在另外一台宿主机(比如: 10.168.0.3)上, 也有一个 Docker 容器. 那么 nginx-1 容器又该如何访问它呢</strong>? 这个问题其实就是<strong>容器的 &quot;跨主通信&quot; 问题</strong>.</p> <p><strong>在 Docker 的默认配置下, 一台宿主机上的 docker0 网桥, 和其他宿主机上的 docker0 网桥, 没有任何关联, 它们互相之间也没办法连通. 所以连接在这些网桥上的容器自然也没办法进行通信了.</strong></p> <p>不过, 万变不离其宗.</p> <p>如果通过软件的方式, 创建一个<strong>整个集群 &quot;公用&quot; 的网桥, 然后把集群里的所有容器都连接到这个网桥上</strong>, 不就可以相互通信了吗? 这样一来, 整个集群里的容器网络就会类似于下图所示的样子:</p> <p><img src="/img/c4ff6da4293ca8c28854dc22547d8cce-20230731162150-biculwy.png" alt=""></p> <p>可以看到, 构建这种容器网络的核心在于: <strong>需要在已有的宿主机网络上, 再通过软件构建一个覆盖在已有宿主机网络之上的, 可以把所有容器连通在一起的虚拟网络. 所以这种技术就被称为: Overlay Network(覆盖网络)</strong> .</p> <p>而这个 Overlay Network 本身, 可以由每台宿主机上的一个 &quot;特殊网桥&quot; 共同组成. 比如, 当 Node 1 上的 Container 1 要访问 Node 2 上的 Container 3 的时候, Node 1 上的 &quot;特殊网桥&quot; 在收到数据包之后, 能够通过某种方式, 把数据包发送到正确的宿主机, 比如 Node 2 上. 而 Node 2 上的 &quot;特殊网桥&quot; 在收到数据包后, 也能够通过某种方式, 把数据包转发给正确的容器, 比如 Container 3.</p> <p>甚至每台宿主机上, 都不需要有一个这种特殊的网桥, 而<strong>仅仅通过某种方式配置宿主机的路由表, 就能够把数据包转发到正确的宿主机上</strong>. 这些内容在后面会一一讲述.</p> <blockquote><p>总结</p></blockquote> <p>本节介绍了在本地环境下, 单机容器网络的实现原理和 docker0 网桥的作用. 这里的关键在于, <strong>容器要想跟外界进行通信, 它发出的 IP 包就必须从它的 Network Namespace 里出来, 来到宿主机上</strong>. 而解决这个问题的方法就是: <strong>为容器创建一个一端在容器里充当默认网卡, 另一端在宿主机上的 Veth Pair 设备</strong>.</p> <p>上述单机容器网络的知识, 是后面讲解多机容器网络的重要基础, 请务必认真消化理解.</p> <h3 id="kubernetes容器网络"><a href="#kubernetes容器网络" class="header-anchor">#</a> Kubernetes容器网络</h3> <h4 id="_33-深入解析容器跨主机网络"><a href="#_33-深入解析容器跨主机网络" class="header-anchor">#</a> 33 | 深入解析容器跨主机网络</h4> <p>上一节讲解了在单机环境下, <strong>Linux 容器网络的实现原理(网桥模式)</strong> . 并且提到了<strong>在 Docker 的默认配置下, 不同宿主机上的容器通过 IP 地址进行互相访问是根本做不到的</strong>.</p> <p>而正是为了解决这个容器 &quot;跨主通信&quot; 的问题, 社区里才出现了那么多的<strong>容器网络方案</strong>. 相信你一直以来都有这样的疑问: 这些网络方案的工作原理到底是什么?</p> <p>要理解<strong>容器 &quot;跨主通信&quot; 的原理</strong>, 就一定要先从 <strong>Flannel</strong> 这个项目说起. Flannel 项目是 CoreOS 公司主推的容器网络方案. 事实上, Flannel 项目本身只是一个框架, 真正提供容器网络功能的, 是 Flannel 的<strong>后端实现</strong>. 目前 Flannel 支持三种后端实现, 分别是:</p> <ol><li><strong>VXLAN</strong>;</li> <li><strong>host-gw</strong>;</li> <li><strong>UDP</strong>.</li></ol> <p>这三种不同的后端实现, 正代表了<strong>三种容器跨主网络的主流实现方法</strong>. 其中 host-gw 模式, 会在下一节再做详细介绍. 而 UDP 模式是 Flannel 项目最早支持的一种方式, 却也是<strong>性能最差</strong>的一种方式. 所以这个模式目前已经被<strong>弃用</strong>. 不过 Flannel 之所以最先选择 UDP 模式, 就是因为这种模式是最直接, 也是最容易理解的容器跨主网络实现.</p> <p>所以本节先从 UDP 模式开始, 来讲解容器 &quot;跨主网络&quot; 的实现原理.</p> <p>在这个例子中, 有两台宿主机.</p> <ul><li>宿主机 Node 1 上有一个容器 container-1, 它的 IP 地址是 100.96.1.2, 对应的 docker0 网桥的地址是: 100.96.1.1/24.</li> <li>宿主机 Node 2 上有一个容器 container-2, 它的 IP 地址是 100.96.2.3, 对应的 docker0 网桥的地址是: 100.96.2.1/24.</li></ul> <p>现在的任务, 就是<strong>让 container-1 访问 container-2</strong>.</p> <p>这种情况下, container-1 容器里的进程发起的 IP 包, 其<strong>源地址就是 100.96.1.2, 目的地址就是 100.96.2.3</strong>. 由于目的地址 100.96.2.3 并<strong>不在 Node 1 的 docker0 网桥的网段</strong>里, 所以这个 IP 包会被<strong>交给默认路由规则</strong>, 通过<strong>容器的网关进入 docker0 网桥(如果是同一台宿主机上的容器间通信, 走的是直连规则), 从而出现在宿主机上</strong>.</p> <p>这时候, 这个 IP 包的下一个目的地, 就取决于<strong>宿主机上的路由规则</strong>了. 此时 <strong>Flannel 已经在宿主机上创建出了一系列的路由规则</strong>, 以 Node 1 为例, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 Node 1 上</span>
$ <span class="token function">ip</span> route
default via <span class="token number">10.168</span>.0.1 dev eth0
<span class="token number">100.96</span>.0.0/16 dev flannel0  proto kernel  scope <span class="token function">link</span>  src <span class="token number">100.96</span>.1.0
<span class="token number">100.96</span>.1.0/24 dev docker0  proto kernel  scope <span class="token function">link</span>  src <span class="token number">100.96</span>.1.1
<span class="token number">10.168</span>.0.0/24 dev eth0  proto kernel  scope <span class="token function">link</span>  src <span class="token number">10.168</span>.0.2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, 由于 IP 包的目的地址是 100.96.2.3, 它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段, 只能匹配到第二条, 也就是 100.96.0.0/16 对应的这条路由规则, 从而进入到一个叫作 <strong>flannel0 的设备</strong>中. 而这个 flannel0 设备的类型就比较有意思了: 它是一个 <strong>TUN 设备(Tunnel 设备)</strong> .</p> <p>在 Linux 中, TUN 设备是一种工作在<strong>三层</strong>(Network Layer)的虚拟网络设备. TUN 设备的功能非常简单, 即: **在操作系统内核和用户应用程序之间传递 IP 包. **</p> <p>以 flannel0 设备为例:</p> <p>像上面提到的情况, 当操作系统将一个 IP 包发送给 flannel0 设备之后, flannel0 就会把这个 IP 包, <strong>交给创建这个设备的应用程序, 也就是 Flannel 进程</strong>. 这是一个从<strong>内核态</strong>(Linux 操作系统)向<strong>用户态</strong>(Flannel 进程)的流动方向.  反之, 如果 Flannel 进程向 flannel0 设备发送了一个 IP 包, 那么这个 IP 包就会出现在<strong>宿主机网络栈</strong>中, 然后根据宿主机的路由表进行下一步处理. 这是一个从<strong>用户态向内核态</strong>的流动方向.</p> <p>所以<strong>当 IP 包从容器经过 docker0 出现在宿主机, 然后又根据路由表进入 flannel0 设备后, 宿主机上的 flanneld 进程(Flannel 项目在每个宿主机上的主进程), 就会收到这个 IP 包</strong>. 然后, flanneld 看到了这个 IP 包的目的地址, 是 100.96.2.3, 就把它发送给了 Node 2 宿主机.</p> <p><mark>等一下, </mark>​<mark>**flanneld 又是如何知道这个 IP 地址对应的容器, 是运行在 Node 2 上的呢? **</mark></p> <p>这里就用到了 Flannel 项目里一个非常重要的概念: <strong>子网</strong>(Subnet).</p> <p>事实上, 在由 Flannel 管理的容器网络里, <strong>一台宿主机上的所有容器, 都属于该宿主机被分配的一个 &quot;子网&quot;</strong> . 在本例子中, Node 1 的子网是 100.96.1.0/24, container-1 的 IP 地址是 100.96.1.2. Node 2 的子网是 100.96.2.0/24, container-2 的 IP 地址是 100.96.2.3.</p> <p><mark><strong>而这些子网与宿主机的对应关系, 正是保存在 Etcd 当中</strong></mark>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ etcdctl <span class="token function">ls</span> /coreos.com/network/subnets
/coreos.com/network/subnets/100.96.1.0-24
/coreos.com/network/subnets/100.96.2.0-24
/coreos.com/network/subnets/100.96.3.0-24
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>所以 flanneld 进程在处理由 flannel0 传入的 IP 包时, 就可以<strong>根据目的 IP 的地址(比如 100.96.2.3), 匹配到对应的子网(比如 100.96.2.0/24), 从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24
<span class="token punctuation">{</span><span class="token string">&quot;PublicIP&quot;</span><span class="token builtin class-name">:</span><span class="token string">&quot;10.168.0.3&quot;</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>而对于 flanneld 来说, 只要 Node 1 和 Node 2 是互通的, 那么 flanneld 作为 Node 1 上的一个普通进程, 就一定可以通过上述 IP 地址(10.168.0.3)访问到 Node 2, 这没有任何问题.</p> <p>所以说, flanneld 在收到 container-1 发给 container-2 的 IP 包之后, <strong>就会把这个 IP 包直接封装在一个 UDP 包里, 然后发送给 Node 2</strong>. 不难理解, 这个 UDP 包的源地址, 就是 flanneld 所在的 Node 1 的地址, 而目的地址, 则是 container-2 所在的宿主机 Node 2 的地址.</p> <p>当然这个请求得以完成的原因是, <strong>每台宿主机上的 flanneld, 都监听着一个 8285 端口</strong>, 所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可.</p> <p>通过这样一个普通的, <strong>宿主机之间的 UDP 通信, 一个 UDP 包就从 Node 1 到达了 Node 2</strong>. 而 Node 2 上监听 8285 端口的进程也是 flanneld, 所以这时候, flanneld 就可以从这个 UDP 包里<strong>解析</strong>出封装在里面的, container-1 发来的原 IP 包.</p> <p>而接下来 flanneld 的工作就非常简单了: <strong>flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备, 即 flannel0 设备</strong>. 根据前面讲解的 TUN 设备的原理, 这正是一个从用户态向内核态的流动方向(Flannel 进程向 TUN 设备发送数据包), 所以 Linux <strong>内核网络栈就会负责处理这个 IP 包</strong>, 具体的处理方法就是<strong>通过本机的路由表来寻找这个 IP 包的下一步流向</strong>.</p> <p>而 Node 2 上的路由表, 跟 Node 1 非常类似, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 Node 2 上</span>
$ <span class="token function">ip</span> route
default via <span class="token number">10.168</span>.0.1 dev eth0
<span class="token number">100.96</span>.0.0/16 dev flannel0  proto kernel  scope <span class="token function">link</span>  src <span class="token number">100.96</span>.2.0
<span class="token number">100.96</span>.2.0/24 dev docker0  proto kernel  scope <span class="token function">link</span>  src <span class="token number">100.96</span>.2.1
<span class="token number">10.168</span>.0.0/24 dev eth0  proto kernel  scope <span class="token function">link</span>  src <span class="token number">10.168</span>.0.3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>由于这个 IP 包的目的地址是 100.96.2.3, 它跟第三条, 也就是 100.96.2.0/24 网段对应的<strong>路由规则匹配更加精确</strong>. 所以 Linux 内核就会按照这条路由规则, 把这个 IP 包转发给 <strong>docker0 网桥</strong>.</p> <p>接下来的流程就如同在上一节的流程那样, docker0 网桥会扮演<strong>二层交换机</strong>的角色, 将数据包发送给正确的端口, 进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里. 而 container-2 返回给 container-1 的数据包, 则会经过与上述过程<strong>完全相反的路径</strong>回到 container-1 中.</p> <p>需要注意的是, 上述流程要正确工作还有一个重要的前提, 那就是 <strong>docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网</strong>. 这个很容易实现, 以 Node 1 为例, 只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token assign-left variable">FLANNEL_SUBNET</span><span class="token operator">=</span><span class="token number">100.96</span>.1.1/24
$ dockerd <span class="token parameter variable">--bip</span><span class="token operator">=</span><span class="token variable">$FLANNEL_SUBNET</span> <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>以上就是基于 Flannel UDP 模式的跨主通信的基本原理了. 这里总结成了一幅原理图, 如下所示.</p> <div><center></center></div> <p><img src="/img/92a937217002f8ffc27eb7d5f042dbb4-20230731162150-6uj2xdw.png" alt="" title="图1 基于 Flannel UDP 模式的跨主通信的基本原理"></p> <p>可以看到, Flannel UDP 模式提供的其实是一个<strong>三层的 Overlay 网络</strong>, 即: 它首先对发出端的 IP 包进行 <strong>UDP 封装</strong>, 然后在接收端进行<strong>解封装拿到原始的 IP 包</strong>, 进而把这个 IP 包转发给目标容器. 这就好比 Flannel 在不同宿主机上的两个容器之间打通了一条 &quot;隧道&quot;, 使得这两个容器可以<strong>直接使用 IP 地址进行通信</strong>, 而无需关心容器和宿主机的分布情况.</p> <p>前面曾经提到, 上述 UDP 模式有严重的性能问题, 所以已经被废弃了. 通过上面的讲述, 你有没有发现性能问题出现在了哪里呢?</p> <p>实际上, 相比于两台宿主机之间的直接通信, <strong>基于 Flannel UDP 模式的容器通信多了一个额外的步骤, 即 flanneld 的处理过程</strong>. 而这个过程, 由于使用到了 flannel0 这个 TUN 设备, 仅在发出 IP 包的过程中, 就需要<strong>经过三次用户态与内核态之间的数据拷贝</strong>, 如下所示:</p> <p>可以看到:</p> <p>第一次: <strong>用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态</strong>;</p> <p>第二次: <strong>IP 包根据路由表进入 TUN(flannel0)设备, 从而回到用户态的 flanneld 进程</strong>;</p> <p>第三次: <strong>flanneld 进行 UDP 封包之后重新进入内核态, 将 UDP 包通过宿主机的 eth0 发出去</strong>.</p> <p>此外还可以看到, Flannel 进行 UDP 封装(Encapsulation)和解封装(Decapsulation)的过程, 也都是在<strong>用户态</strong>完成的. 在 Linux 操作系统中, 上述这些上下文切换和用户态操作的代价其实是比较高的, 这也正是造成 Flannel UDP 模式性能不好的主要原因.</p> <p><mark><strong>在进行系统级编程的时候, 有一个非常重要的优化原则, 就是要减少用户态到内核态的切换次数, 并且把核心的处理逻辑都放在内核态进行</strong></mark>.</p> <p>这也是为什么, Flannel 后来支持的 <strong>VXLAN 模式</strong>, 逐渐成为了主流的容器网络方案的原因. <strong>VXLAN</strong>, 即 Virtual Extensible LAN(虚拟可扩展局域网), 是 Linux <strong>内核本身就支持的一种网络虚似化技术</strong>. 所以说 VXLAN 可以完全在内核态实现上述封装和解封装的工作, 从而通过与前面相似的 &quot;隧道&quot; 机制, 构建出<strong>覆盖网络</strong>(Overlay Network).</p> <p>VXLAN 的覆盖网络的设计思想是: 在现有的三层网络之上,  <strong>&quot;覆盖&quot; 一层虚拟的, 由内核 VXLAN 模块负责维护的二层网络, 使得连接在这个 VXLAN 二层网络上的 &quot;主机&quot;(虚拟机或者容器都可以)之间, 可以像在同一个局域网(LAN)里那样自由通信</strong>. 当然实际上, 这些 &quot;主机&quot; 可能分布在不同的宿主机上, 甚至是分布在不同的物理机房里.</p> <p>而为了能够在二层网络上打通 &quot;隧道&quot;, **VXLAN 会在宿主机上设置一个特殊的网络设备作为 &quot;隧道&quot; 的两端. 这个设备就叫作 **​<mark><strong>VTEP</strong></mark>​ <strong>, 即: VXLAN Tunnel End Point(虚拟隧道端点)</strong> .</p> <p>而 VTEP 设备的作用, 其实跟前面的 flanneld 进程非常相似. 只不过<strong>它进行封装和解封装的对象, 是二层数据帧</strong>(Ethernet frame); 而且这个工作的执行流程, 全部是在<strong>内核</strong>里完成的(因为 VXLAN 本身就是 Linux 内核中的一个模块).</p> <p>上述基于 VTEP 设备进行 &quot;隧道&quot; 通信的流程, 也总结成了一幅图, 如下所示:</p> <p><img src="/img/eeee91cfc8e72e2901003582260311a6-20230731162150-8lpmsbj.png" alt="" title="图3 基于 Flannel VXLAN 模式的跨主通信的基本原理"></p> <p>可以看到, 图中每台宿主机上名叫 <strong>flannel.1</strong> 的设备, 就是 <strong>VXLAN 所需的 VTEP 设备, 它既有 IP 地址, 也有 MAC 地址</strong>.</p> <p>现在 container-1 的 IP 地址是 10.1.15.2, 要访问的 container-2 的 IP 地址是 10.1.16.3.</p> <p>那么与前面 UDP 模式的流程类似, 当 container-1 发出请求之后, 这个目的地址是 10.1.16.3 的 IP 包, 会先出现在 <strong>docker0 网桥</strong>, 然后被<strong>路由到本机 flannel.1 设备进行处理</strong>. 也就是说, 来到了 &quot;隧道&quot; 的入口. 为了方便叙述, 接下来会把这个 IP 包称为&quot;原始 IP 包&quot;. 为了能够将 &quot;原始 IP 包&quot; 封装并且发送到正确的宿主机, <strong>VXLAN 就需要找到这条 &quot;隧道&quot; 的出口, 即: 目的宿主机的 VTEP 设备</strong>. 而这个设备的<strong>信息</strong>, 正是每台宿主机上的 <strong>flanneld 进程</strong>负责维护的.</p> <p>比如, 当 Node 2 启动并加入 Flannel 网络之后, 在 Node 1(以及所有其他节点)上, flanneld 就会添加一条如下所示的路由规则:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ route <span class="token parameter variable">-n</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
<span class="token punctuation">..</span>.
<span class="token number">10.1</span>.16.0       <span class="token number">10.1</span>.16.0       <span class="token number">255.255</span>.255.0   UG    <span class="token number">0</span>      <span class="token number">0</span>        <span class="token number">0</span> flannel.1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这条规则的意思是: <strong>凡是发往 10.1.16.0/24 网段的 IP 包, 都需要经过 flannel.1 设备发出, 并且它最后被发往的网关地址是: 10.1.16.0</strong>.</p> <p>从图 3 的 Flannel VXLAN 模式的流程图中可以看到, <strong>10.1.16.0 正是 Node 2 上的 VTEP 设备</strong>(也就是 flannel.1 设备)的 IP 地址. 为了方便叙述, 接下来会把 Node 1 和 Node 2 上的 flannel.1 设备分别称为 &quot;源 VTEP 设备&quot; 和 &quot;目的 VTEP 设备&quot;.</p> <p><strong>而这些 VTEP 设备之间, 就需要想办法组成一个虚拟的二层网络, 即: 通过二层数据帧进行通信</strong>.</p> <p>所以在这个例子中, &quot;源 VTEP 设备&quot; 收到 &quot;原始 IP 包&quot;后, 就要想办法把 &quot;原始 IP 包&quot; 加上一个<strong>目的 MAC 地址</strong>, 封装成一个二层数据帧, 然后发送给 &quot;目的 VTEP 设备&quot;(当然这么做还是因为这个 IP 包的目的地址不是本机).</p> <p>这里需要解决的问题就是:  **&quot;目的 VTEP 设备&quot; 的 MAC 地址是什么? **</p> <p>此时根据前面的路由记录, 已经知道了 &quot;目的 VTEP 设备&quot; 的 IP 地址. 而要根据三层 IP 地址查询对应的二层 MAC 地址, 这正是 <strong>ARP</strong>(Address Resolution Protocol )表的功能. 而这里要用到的 ARP 记录, 也是 <strong>flanneld 进程在 Node 2 节点启动时, 自动添加在 Node 1 上的</strong>. 可以通过 ip 命令看到它, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 Node 1 上</span>
$ <span class="token function">ip</span> neigh show dev flannel.1
<span class="token number">10.1</span>.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这条记录的意思非常明确, 即: IP 地址 10.1.16.0, 对应的 MAC 地址是 5e:f8:4f:00:e3:37. ​可以看到, 最新版本的 Flannel 并不依赖 L3 MISS 事件和 ARP 学习, 而<strong>会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录, 直接下放到其他每台宿主机上</strong>.</p> <p>有了这个 &quot;目的 VTEP 设备&quot; 的 MAC 地址, <strong>Linux 内核就可以开始二层封包工作了</strong>. 这个二层帧的格式, 如下所示:</p> <p><img src="/img/3297ce189a3f06b13a9e267f1ca513dd-20230731162150-yffj03e.png" alt="" title="图4 Flannel VXLAN 模式的内部帧"></p> <p>可以看到, Linux 内核会把  <strong>&quot;目的 VTEP 设备&quot; 的 MAC 地址, 填写在图中的 Inner Ethernet Header 字段</strong>, 得到一个二层数据帧. 需要注意的是, 上述封包过程只是<strong>加一个二层头</strong>, 不会改变 &quot;原始 IP 包&quot; 的内容. 所以图中的 Inner IP Header 字段, 依然是 container-2 的 IP 地址, 即 10.1.16.3.</p> <p>但是, 上面提到的这些 VTEP 设备的 MAC 地址, 对于<strong>宿主机网络</strong>来说并没有什么实际意义. 所以上面封装出来的这个数据帧, 并不能在宿主机二层网络里传输. 为了方便叙述, 可以把它称为 &quot;<strong>内部数据帧</strong>&quot;(Inner Ethernet Frame).</p> <p>所以接下来, Linux 内核还需要再把 &quot;内部数据帧&quot; 进一步<strong>封装成为宿主机网络里的一个普通的数据帧</strong>, 好让它 &quot;载着内部数据帧&quot;, 通过<strong>宿主机的 eth0 网卡</strong>进行传输. 这里把这次要封装出来的, 宿主机对应的数据帧称为 &quot;<strong>外部数据帧</strong>&quot;(Outer Ethernet Frame).</p> <p>为了实现这个 &quot;搭便车&quot; 的机制, Linux 内核会在 &quot;内部数据帧&quot; 前面, 加上一个<strong>特殊的 VXLAN 头, 用来表示这个 &quot;乘客&quot; 实际上是一个 VXLAN 要使用的数据帧</strong>. 而这个 VXLAN 头里有一个重要的标志叫作 <strong>VNI</strong>, 它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识. 而在 Flannel 中, VNI 的默认值是 1, 这也是为何, <strong>宿主机上的 VTEP 设备都叫作 flannel.1 的原因</strong>, 这里的 &quot;1&quot;, 其实就是 VNI 的值.</p> <p>**然后 Linux 内核会把这个数据帧封装进一个 UDP 包里发出去. **</p> <p>所以, 跟 UDP 模式类似, <strong>在宿主机看来, 它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备, 发起了一次普通的 UDP 链接</strong>. 它哪里会知道, 这个 UDP 包里面其实是一个<strong>完整的二层数据帧</strong>. 这是不是跟特洛伊木马的故事非常像呢?</p> <p>不过不要忘了, 一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址, 却<strong>不知道对应的宿主机地址</strong>是什么. 也就是说, <strong>这个 UDP 包该发给哪台宿主机</strong>呢?</p> <p>在这种场景下, <strong>flannel.1 设备实际上要扮演一个 &quot;网桥&quot; 的角色, 在二层网络进行 UDP 包的转发</strong>. 而在 Linux 内核里面, &quot;网桥&quot; 设备进行转发的依据, 来自于一个叫作 <strong>FDB</strong>(Forwarding Database)的转发数据库. 不难想到, 这个 flannel.1 &quot;网桥&quot; 对应的 FDB 信息, 也是 <strong>flanneld 进程负责维护</strong>的. 它的内容可以通过 bridge fdb 命令查看到, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 Node 1 上, 使用“目的 VTEP 设备”的 MAC 地址进行查询</span>
$ bridge fdb show flannel.1 <span class="token operator">|</span> <span class="token function">grep</span> 5e:f8:4f:00:e3:37
5e:f8:4f:00:e3:37 dev flannel.1 dst <span class="token number">10.168</span>.0.3 self permanent
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到, 在上面这条 FDB 记录里, 指定了这样一条规则, 即:</p> <p>发往前面提到的 &quot;目的 VTEP 设备&quot;(MAC 地址是 5e:f8:4f:00:e3:37)的二层数据帧, 应该通过 flannel.1 设备, 发往 IP 地址为 10.168.0.3 的主机.</p> <p>显然这台主机正是 Node 2, UDP 包要发往的<strong>目的地</strong>就找到了. 所以**接下来的流程, 就是一个正常的, 宿主机网络上的封包工作. **</p> <p>由于 UDP 包是一个四层数据包, 所以 Linux 内核会在它前面加上一个 <strong>IP 头</strong>, 即原理图中的 Outer IP Header, 组成一个 IP 包. 并且在这个 IP 头里, 会填上前面<strong>通过 FDB 查询出来的目的主机的 IP 地址</strong>, 即 Node 2 的 IP 地址 10.168.0.3.</p> <p>然后 Linux 内核再在这个 IP 包前面加上<strong>二层数据帧头</strong>, 即原理图中的 Outer Ethernet Header, 并把 Node 2 的 MAC 地址填进去. 这个 MAC 地址本身, 是 Node 1 的 ARP 表要学习的内容, 无需 Flannel 维护. 这时候封装出来的 &quot;外部数据帧&quot; 的格式, 如下所示:</p> <p><img src="/img/3ddff1e720ce2ab0b6da2537e5d75e54-20230731162150-tijh7le.png" alt="" title="图5 Flannel VXLAN 模式的外部帧"></p> <p>这样, 封包工作就宣告完成了.</p> <p>接下来, Node 1 上的 <strong>flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去</strong>. 显然, 这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡.</p> <p>这时候, Node 2 的内核网络栈会发现这个<strong>数据帧里有 VXLAN Header, 并且 VNI=1. 所以 Linux 内核会对它进行拆包, 拿到里面的内部数据帧, 然后根据 VNI 的值, 把它交给 Node 2 上的 flannel.1 设备</strong>. 而 flannel.1 设备则会<strong>进一步拆包</strong>, 取出 &quot;原始 IP 包&quot;. 接下来就回到了上一节单机容器网络的处理流程. 最终 IP 包就进入到了 container-2 容器的 Network Namespace 里.</p> <p>以上就是 Flannel VXLAN 模式的具体工作原理了.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Flannel UDP 和 VXLAN 模式的工作原理. 这两种模式其实都可以称作  <strong>&quot;隧道&quot; 机制</strong>, 也是很多其他容器网络插件的基础. 比如 Weave 的两种模式, 以及 Docker 的 Overlay 模式.</p> <p>此外可以看到, <strong>VXLAN 模式组建的覆盖网络, 其实就是一个由不同宿主机上的 VTEP 设备, 也就是 flannel.1 设备组成的虚拟二层网络.</strong>  对于 VTEP 设备来说, <strong>它发出的 &quot;内部数据帧&quot; 就仿佛是一直在这个虚拟的二层网络上流动. 这也正是覆盖网络的含义</strong>.</p> <p>备注: 如果想要在前面部署的集群中实践 Flannel 的话, 可以在 Master 节点上执行如下命令来替换网络插件.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 第一步, 执行</span>
<span class="token function">rm</span> <span class="token parameter variable">-rf</span> /etc/cni/net.d/*<span class="token punctuation">;</span> 
<span class="token comment"># 第二步, 执行</span>
$ kubectl delete <span class="token parameter variable">-f</span> <span class="token string">&quot;https://cloud.weave.works/k8s/net?k8s-version=1.11&quot;</span><span class="token punctuation">;</span> 
<span class="token comment"># 第三步, 在/etc/kubernetes/manifests/kube-controller-manager.yaml里, 为容器启动命令添加如下两个参数: </span>
--allocate-node-cidrs<span class="token operator">=</span>true
--cluster-cidr<span class="token operator">=</span><span class="token number">10.244</span>.0.0/16
<span class="token comment"># 第四步,  重启所有 kubelet; </span>
<span class="token comment"># 第五步,  执行$ kubectl create -f</span>
https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h4 id="_34-kubernetes网络模型与cni网络插件"><a href="#_34-kubernetes网络模型与cni网络插件" class="header-anchor">#</a> 34 | Kubernetes网络模型与CNI网络插件</h4> <p>上一节以 Flannel 项目为例, 详细讲解了容器跨主机网络的两种实现方法: <strong>UDP 和 VXLAN</strong>. 不难看到, 这些例子有一个共性, 那就是<strong>用户的容器都连接在 docker0 网桥上</strong>. 而<strong>网络插件则在宿主机上创建了一个特殊的设备(UDP 模式创建的是 TUN 设备, VXLAN 模式创建的则是 VTEP 设备), docker0 与这个设备之间, 通过 IP 转发(路由表)进行协作</strong>. 然后, <strong>网络插件真正要做的事情, 则是通过某种方法, 把不同宿主机上的特殊设备连通, 从而达到容器跨主机通信的目的</strong>.</p> <p>实际上, 上面这个流程, 也正是 <strong>Kubernetes 对容器网络的主要处理方法</strong>. 只不过, <mark><strong>Kubernetes 是通过一个叫作 CNI 的接口, 维护了一个单独的网桥来代替 docker0. 这个网桥的名字就叫作: CNI 网桥, 它在宿主机上的设备名称默认是: cni0</strong></mark>.</p> <p>以 Flannel 的 VXLAN 模式为例, 在 Kubernetes 环境里, 它的工作方式跟在上一节中讲解的没有任何不同. 只不过 <strong>docker0 网桥被替换成了 CNI 网桥</strong>而已, 如下所示:</p> <p><img src="/img/850b4fcf90f1991a831500e6824b42fd-20230731162150-wqmz7fi.png" alt=""></p> <p>在这里, Kubernetes 为 Flannel 分配的<strong>子网范围是 10.244.0.0/16</strong>. 这个参数可以在部署的时候指定, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubeadm init --pod-network-cidr<span class="token operator">=</span><span class="token number">10.244</span>.0.0/16
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>也可以在部署完成后, 通过修改 kube-controller-manager 的配置文件来指定.</p> <p>这时候, 假设 Infra-container-1 要访问 Infra-container-2(也就是 Pod-1 要访问 Pod-2), 这个 IP 包的源地址就是 10.244.0.2, 目的 IP 地址是 10.244.1.3. 而此时, Infra-container-1 里的 eth0 设备, 同样是以 <strong>Veth Pair</strong> 的方式连接在 Node 1 的 <strong>cni0 网桥</strong>上. 所以<strong>这个 IP 包就会经过 cni0 网桥出现在宿主机</strong>上.</p> <p>此时 Node 1 上的路由表, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在 Node 1 上</span>
$ route <span class="token parameter variable">-n</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
<span class="token punctuation">..</span>.
<span class="token number">10.244</span>.0.0      <span class="token number">0.0</span>.0.0         <span class="token number">255.255</span>.255.0   U     <span class="token number">0</span>      <span class="token number">0</span>        <span class="token number">0</span> cni0
<span class="token number">10.244</span>.1.0      <span class="token number">10.244</span>.1.0      <span class="token number">255.255</span>.255.0   UG    <span class="token number">0</span>      <span class="token number">0</span>        <span class="token number">0</span> flannel.1
<span class="token number">172.17</span>.0.0      <span class="token number">0.0</span>.0.0         <span class="token number">255.255</span>.0.0     U     <span class="token number">0</span>      <span class="token number">0</span>        <span class="token number">0</span> docker0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>因为 IP 包的目的 IP 地址是 10.244.1.3, 所以它只能<strong>匹配到第二条规则</strong>, 也就是 10.244.1.0 对应的这条路由规则.</p> <p>可以看到, 这条规则<strong>指定了本机的 flannel.1 设备进行处理</strong>. 并且 flannel.1 在处理完后, 要将 IP 包转发到的<strong>网关</strong>(Gateway), <strong>正是 &quot;隧道&quot; 另一端的 VTEP 设备, 也就是 Node 2 的 flannel.1 设备</strong>. 所以接下来的流程, 就跟上一节介绍的 Flannel VXLAN 模式完全一样了.</p> <p>需要注意的是, <strong>CNI 网桥只是接管所有 CNI 插件负责的, 即 Kubernetes 创建的容器(Pod)</strong> . 而此时, 如果用 docker run 单独启动一个容器, 那么 <strong>Docker 项目还是会把这个容器连接到 docker0 网桥</strong>上. 所以这个容器的 IP 地址, 一定是属于 docker0 网桥的 172.17.0.0/16 网段.</p> <p>Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥, 主要原因包括两个方面:</p> <ul><li>一方面, Kubernetes 项目并没有使用 Docker 的网络模型(CNM), 所以它并不希望也不具备配置 docker0 网桥的能力;</li> <li>另一方面, 这还与 Kubernetes 如何配置 Pod, 也就是 Infra 容器的 Network Namespace 密切相关.</li></ul> <p>我们知道, Kubernetes 创建一个 Pod 的第一步, 就是<strong>创建并启动一个 Infra 容器</strong>, 用来 &quot;hold&quot; 住这个 Pod 的 <strong>Network Namespace</strong>(这里可以再回顾一下<a href="https://time.geekbang.org/column/article/40092" target="_blank" rel="noopener noreferrer">《为什么我们需要 Pod? 》<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>中的相关内容).</p> <p>所以 CNI 的设计思想, 就是: <mark><strong>Kubernetes 在启动 Infra 容器之后, 就可以直接调用 CNI 网络插件, 为这个 Infra 容器的 Network Namespace, 配置符合预期的网络栈</strong></mark>​ **. **</p> <p>那么这个网络栈的配置工作又是如何完成的呢?</p> <p>为了回答这个问题, 就需要从 <strong>CNI 插件的部署和实现</strong>方式谈起了. 在部署 Kubernetes 的时候, 有一个步骤是安装 kubernetes-cni 包, 它的目的就是在<strong>宿主机</strong>上安装 <strong>CNI 插件所需的基础可执行文件</strong>.</p> <p>在安装完成后, 可以在宿主机的 /opt/cni/bin 目录下看到它们, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">ls</span> <span class="token parameter variable">-al</span> /opt/cni/bin/
total <span class="token number">73088</span>
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3890407</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> bridge
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">9921982</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> dhcp
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">2814104</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> flannel
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">2991965</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> host-local
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3475802</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> ipvlan
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3026388</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> loopback
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3520724</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> macvlan
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3470464</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> portmap
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3877986</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> ptp
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">2605279</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> sample
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">2808402</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> tuning
-rwxr-xr-x <span class="token number">1</span> root root  <span class="token number">3475750</span> Aug <span class="token number">17</span>  <span class="token number">2017</span> vlan
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>这些 CNI 的基础<strong>可执行文件</strong>, 按照功能可以分为三类:</p> <p><strong>第一类, 叫作 Main 插件, 它是用来创建具体网络设备的二进制文件</strong>. 比如 bridge(网桥设备), ipvlan, loopback(lo 设备), macvlan, ptp(Veth Pair 设备), 以及 vlan. 在前面提到过的 Flannel, Weave 等项目, 都属于 &quot;<strong>网桥</strong>&quot; 类型的 CNI 插件. 所以在具体的实现中, 它们往往会调用 bridge 这个二进制文件. 这个流程马上就会详细介绍到.</p> <p><strong>第二类, 叫作 IPAM(IP Address Management)插件, 它是负责分配 IP 地址的二进制文件</strong>. 比如 dhcp, 这个文件会向 DHCP 服务器发起请求; host-local 则会使用预先配置的 IP 地址段来进行分配.</p> <p><strong>第三类, 是由 CNI 社区维护的内置 CNI 插件</strong>. 比如: flannel, 就是专门为 Flannel 项目提供的 CNI 插件; tuning 是一个通过 sysctl 调整网络设备参数的二进制文件; portmap 是一个通过 iptables 配置端口映射的二进制文件; bandwidth 是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件.</p> <p>从这些二进制文件中可以看到, 如果要<strong>实现一个给 Kubernetes 用的容器网络方案</strong>, 其实需要做<strong>两部分</strong>工作, 以 Flannel 项目为例:</p> <p><strong>首先, 实现这个网络方案本身</strong>. 这一部分需要编写的, 其实就是 <strong>flanneld 进程里的主要逻辑</strong>. 比如创建和配置 flannel.1 设备, 配置宿主机路由, 配置 ARP 和 FDB 表里的信息等.</p> <p><strong>然后, 实现该网络方案对应的 CNI 插件</strong>. 这一部分主要需要做的, 就是<strong>配置 Infra 容器里面的网络栈, 并把它连接在 CNI 网桥上</strong>.</p> <p>由于 Flannel 项目对应的 CNI 插件已经被<strong>内置</strong>了, 所以它无需再单独安装. 而对于 Weave, Calico 等其他项目来说, 就必须在安装插件的时候, 把对应的 <strong>CNI 插件的可执行文件放在 /opt/cni/bin/ 目录</strong>下. 实际上, 对于 Weave, Calico 这样的网络方案来说, 它们的 <strong>DaemonSet 只需要挂载宿主机的 /opt/cni/bin/</strong> , 就可以实现插件可执行文件的安装了.</p> <p>接下来就需要在宿主机上安装 <strong>flanneld</strong>(网络方案本身). 而在这个过程中, flanneld 启动后会在<strong>每台宿主机</strong>上生成它对应的 <strong>CNI 配置文件</strong>(它其实是一个 ConfigMap), 从而告诉 Kubernetes, 这个集群要<strong>使用 Flannel 作为容器网络方案</strong>.</p> <p>这个 CNI 配置文件的内容如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">cat</span> /etc/cni/net.d/10-flannel.conflist 
<span class="token punctuation">{</span>
  <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;cbr0&quot;</span>,
  <span class="token string">&quot;plugins&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token string">&quot;type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;flannel&quot;</span>,
      <span class="token string">&quot;delegate&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;hairpinMode&quot;</span><span class="token builtin class-name">:</span> true,
        <span class="token string">&quot;isDefaultGateway&quot;</span><span class="token builtin class-name">:</span> <span class="token boolean">true</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{</span>
      <span class="token string">&quot;type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;portmap&quot;</span>,
      <span class="token string">&quot;capabilities&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;portMappings&quot;</span><span class="token builtin class-name">:</span> <span class="token boolean">true</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>需要注意的是, 在 Kubernetes 中, 处理容器网络相关的逻辑并不会在 kubelet 主干代码里执行, 而是会在具体的 <strong>CRI</strong>(Container Runtime Interface, 容器运行时接口)实现里完成. 对于 Docker 项目来说, 它的 CRI 实现叫作 dockershim, 可以在 kubelet 的代码里找到它.</p> <p>所以接下来 dockershim 会加载上述的 CNI 配置文件.</p> <p>需要注意, Kubernetes 目前不支持多个 CNI 插件混用. 如果在 CNI 配置目录(/etc/cni/net.d)里放置了多个 CNI 配置文件的话, dockershim 只会加载按字母顺序排序的第一个插件. 但另一方面, CNI 允许在一个 CNI 配置文件里, 通过 plugins 字段, 定义多个插件进行协作.</p> <p>比如在上面这个例子里, Flannel 项目就指定了 flannel 和 portmap 这两个插件. <strong>这时候, dockershim 会把这个 CNI 配置文件加载起来, 并且把列表里的第一个插件, 也就是 flannel 插件, 设置为默认插件</strong>. 而在后面的执行过程中, flannel 和 portmap 插件会按照定义顺序被调用, 从而依次完成 &quot;配置容器网络&quot; 和 &quot;配置端口映射&quot; 这两步操作.</p> <p>接下来就讲解一下这样一个 <strong>CNI 插件的工作原理</strong>.</p> <p>当 kubelet 组件<strong>需要创建 Pod 的时候, 它第一个创建的一定是 Infra 容器</strong>. 所以在这一步, dockershim 就会先调用 Docker API 创建并启动 Infra 容器, 紧接着执行一个叫作 SetUpPod 的方法. 这个方法的作用就是: <strong>为 CNI 插件准备参数, 然后调用 CNI 插件为 Infra 容器配置网络</strong>.</p> <p>这里要调用的 CNI 插件, 就是 /opt/cni/bin/flannel; 而调用它所需要的参数, 分为两部分.</p> <p>**第一部分, 是由 dockershim 设置的一组 CNI 环境变量. **</p> <p>其中最重要的环境变量参数叫作: <strong>CNI_COMMAND</strong>. 它的取值只有两种: ADD 和 DEL. **这个 ADD 和 DEL 操作, 就是 CNI 插件唯一需要实现的两个方法. ** 其中 ADD 操作的含义是: <strong>把容器添加到 CNI 网络里</strong>; DEL 操作的含义则是: <strong>把容器从 CNI 网络里移除掉</strong>.<mark><strong>而对于网桥类型的 CNI 插件来说, 这两个操作意味着把容器以 Veth Pair 的方式 &quot;插&quot; 到 CNI 网桥上, 或者从网桥上 &quot;拔&quot; 掉</strong></mark>.</p> <p>接下来以 ADD 操作为重点进行讲解.</p> <p>CNI 的 ADD 操作需要的参数包括: <strong>容器里网卡的名字 eth0(CNI_IFNAME), Pod 的 Network Namespace 文件的路径(CNI_NETNS), 容器的 ID(CNI_CONTAINERID)</strong> 等. 这些参数都属于上述环境变量里的内容. 其中, Pod(Infra 容器)的 Network Namespace 文件的路径在前面讲解容器基础的时候提到过, 即: <code>/proc/&lt;容器进程的PID&gt;/ns/net</code>​. 除此之外, 在 CNI 环境变量里, 还有一个叫作 CNI_ARGS 的参数. 通过这个参数, CRI 实现(比如 dockershim)就可以以 Key-Value 的格式, 传递自定义信息给网络插件. 这是用户将来<strong>自定义 CNI 协议</strong>的一个重要方法.</p> <p>**第二部分, 则是 dockershim 从 CNI 配置文件里加载到的, 默认插件的配置信息. **</p> <p>这个配置信息在 CNI 中被叫作 <strong>Network Configuration</strong>. dockershim 会把 Network Configuration 以 JSON 数据的格式, 通过标准输入(stdin)的方式传递给 Flannel CNI 插件.</p> <p>而有了这两部分参数, Flannel CNI 插件实现 ADD 操作的过程就非常简单了.</p> <p>不过需要注意的是, Flannel 的 <strong>CNI 配置文件</strong>(/etc/cni/net.d/10-flannel.conflist)里有这么一个字段, 叫作 delegate:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
     <span class="token key atrule">&quot;delegate&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token key atrule">&quot;hairpinMode&quot;</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token punctuation">,</span>
        <span class="token key atrule">&quot;isDefaultGateway&quot;</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
      <span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>delegate 字段的意思是, 这个 CNI 插件并不会自己做事儿, 而是会调用 delegate 指定的某种 <strong>CNI 内置插件</strong>来完成. 对于 Flannel 来说, 它调用的 delegate 插件就是前面介绍到的 CNI bridge 插件.</p> <p>所以 dockershim 对 Flannel CNI 插件的调用, 其实就是走了个过场. Flannel CNI 插件唯一需要做的, 就是对 dockershim 传来的 Network Configuration 进行补充. 比如将 delegate 的 Type 字段设置为 bridge, 将 delegate 的 IPAM 字段设置为 host-local 等.</p> <p>经过 Flannel CNI 插件补充后的, 完整的 delegate 字段如下所示:</p> <div class="language-json line-numbers-mode"><pre class="language-json"><code><span class="token punctuation">{</span>
    <span class="token property">&quot;hairpinMode&quot;</span><span class="token operator">:</span><span class="token boolean">true</span><span class="token punctuation">,</span>
    <span class="token property">&quot;ipMasq&quot;</span><span class="token operator">:</span><span class="token boolean">false</span><span class="token punctuation">,</span>
    <span class="token property">&quot;ipam&quot;</span><span class="token operator">:</span><span class="token punctuation">{</span>
        <span class="token property">&quot;routes&quot;</span><span class="token operator">:</span><span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token property">&quot;dst&quot;</span><span class="token operator">:</span><span class="token string">&quot;10.244.0.0/16&quot;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token property">&quot;subnet&quot;</span><span class="token operator">:</span><span class="token string">&quot;10.244.1.0/24&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;type&quot;</span><span class="token operator">:</span><span class="token string">&quot;host-local&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token property">&quot;isDefaultGateway&quot;</span><span class="token operator">:</span><span class="token boolean">true</span><span class="token punctuation">,</span>
    <span class="token property">&quot;isGateway&quot;</span><span class="token operator">:</span><span class="token boolean">true</span><span class="token punctuation">,</span>
    <span class="token property">&quot;mtu&quot;</span><span class="token operator">:</span><span class="token number">1410</span><span class="token punctuation">,</span>
    <span class="token property">&quot;name&quot;</span><span class="token operator">:</span><span class="token string">&quot;cbr0&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;type&quot;</span><span class="token operator">:</span><span class="token string">&quot;bridge&quot;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>其中, ipam 字段里的信息, 比如 10.244.1.0/24, 读取自 Flannel 在宿主机上生成的 Flannel 配置文件, 即: 宿主机上的 /run/flannel/subnet.env 文件.</p> <p>接下来 Flannel CNI 插件就会调用 CNI bridge 插件, 也就是执行: /opt/cni/bin/bridge 二进制文件. 这一次, 调用 CNI bridge 插件需要的两部分参数的第一部分, 也就是 CNI 环境变量, 并没有变化. 所以它里面的 CNI_COMMAND 参数的值还是 &quot;ADD&quot;.</p> <p>而第二部分 Network Configration, 正是上面补充好的 delegate 字段. Flannel CNI 插件会把 delegate 字段的内容以标准输入(stdin)的方式传递给 CNI bridge 插件. 此外, Flannel CNI 插件还会把 Delegate 字段以 JSON 文件的方式, 保存在 /var/lib/cni/flannel 目录下. 这是为了给后面删除容器调用 DEL 操作时使用的.</p> <p>有了这两部分参数, <strong>接下来 CNI bridge 插件就可以 &quot;代表&quot; Flannel, 进行 &quot;将容器加入到 CNI 网络里&quot; 这一步操作</strong>了. 而这一部分内容, 与容器 Network Namespace 密切相关, 所以详细讲解一下.</p> <p>首先 CNI bridge 插件会在宿主机上检查 CNI 网桥是否存在. 如果没有的话, 那就创建它. 这相当于在宿主机上执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在宿主机上</span>
$ <span class="token function">ip</span> <span class="token function">link</span> <span class="token function">add</span> cni0 <span class="token builtin class-name">type</span> bridge
$ <span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> cni0 up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>接下来 CNI bridge 插件会通过 Infra 容器的 Network Namespace 文件, 进入到这个 Network Namespace 里面, 然后创建一对 Veth Pair 设备</strong>.</p> <p>紧接着, 它会把这个 <strong>Veth Pair 的其中一端, &quot;移动&quot; 到宿主机上</strong>. 这相当于在容器里执行如下所示的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在容器里</span>

<span class="token comment"># 创建一对 Veth Pair 设备. 其中一个叫作 eth0, 另一个叫作 vethb4963f3</span>
$ <span class="token function">ip</span> <span class="token function">link</span> <span class="token function">add</span> eth0 <span class="token builtin class-name">type</span> veth peer name vethb4963f3

<span class="token comment"># 启动 eth0 设备</span>
$ <span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> eth0 up 

<span class="token comment"># 将 Veth Pair 设备的另一端(也就是 vethb4963f3 设备)放到宿主机(也就是 Host Namespace)里</span>
$ <span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> vethb4963f3 netns <span class="token variable">$HOST_NS</span>

<span class="token comment"># 通过 Host Namespace, 启动宿主机上的 vethb4963f3 设备</span>
$ <span class="token function">ip</span> netns <span class="token builtin class-name">exec</span> <span class="token variable">$HOST_NS</span> <span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> vethb4963f3 up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>这样, vethb4963f3 就出现在了<strong>宿主机</strong>上, 而且这个 Veth Pair 设备的另一端, 就是容器里面的 eth0.</p> <p>你可能已经想到, 上述创建 Veth Pair 设备的操作, 其实也可以先在宿主机上执行, 然后再把该设备的一端放到容器的 Network Namespace 里, 这个原理是一样的.</p> <p>不过, CNI 插件之所以要 &quot;反着&quot; 来, 是因为 CNI 里对 Namespace 操作函数的设计就是如此, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code>err <span class="token operator">:=</span> containerNS<span class="token punctuation">.</span><span class="token function">Do</span><span class="token punctuation">(</span><span class="token keyword">func</span><span class="token punctuation">(</span>hostNS ns<span class="token punctuation">.</span>NetNS<span class="token punctuation">)</span> <span class="token builtin">error</span> <span class="token punctuation">{</span>
  <span class="token operator">...</span>
  <span class="token keyword">return</span> <span class="token boolean">nil</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这个设计其实很容易理解. 在编程时, 容器的 Namespace 是可以直接通过 Namespace 文件拿到的; 而 Host Namespace, 则是一个隐含在上下文的参数. 所以像上面这样, 先通过容器 Namespace 进入容器里面, 然后再反向操作 Host Namespace, 对于编程来说要更加方便.</p> <p>接下来, CNI bridge 插件就可以<strong>把 vethb4963f3 设备连接在 CNI 网桥</strong>上. 这相当于在宿主机上执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在宿主机上</span>
$ <span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> vethb4963f3 master cni0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在将 vethb4963f3 设备连接在 CNI 网桥之后, CNI bridge 插件还会为它设置 <strong>Hairpin Mode(发夹模式)</strong> . 这是因为在默认情况下, 网桥设备是不允许一个数据包从一个端口进来后, 再从这个端口发出去的. 但是它允许你为这个端口开启 Hairpin Mode, 从而取消这个限制. 这个特性主要用在容器需要通过 <strong>NAT</strong>(即: 端口映射)的方式, &quot;自己访问自己&quot; 的场景下.</p> <p>举个例子, 比如执行 docker run -p 8080:80, 就是在宿主机上通过 iptables 设置了一条 <strong>DNAT</strong>(目的地址转换)转发规则. 这条规则的作用是, 当宿主机上的进程访问 &quot;<code>&lt;宿主机的 IP 地址&gt;:8080</code>​&quot; 时, iptables 会把该请求直接转发到 &quot;<code>&lt;容器的 IP 地址&gt;:80</code>​&quot; 上. 也就是说, 这个请求最终会经过 docker0 网桥进入容器里面.</p> <p>但如果是在<strong>容器里面访问宿主机的 8080 端口</strong>, 那么这个容器里发出的 IP 包会经过 vethb4963f3 设备(端口)和 docker0 网桥, 来到宿主机上. 此时根据上述 DNAT 规则, 这个 IP 包又需要回到 docker0 网桥, 并且还是通过 vethb4963f3 端口进入到容器里. 所以这种情况下, 就需要开启 vethb4963f3 端口的 Hairpin Mode 了. 所以 Flannel 插件要在 CNI 配置文件里声明 hairpinMode=true. 这样将来这个集群里的 Pod 才可以通过它自己的 Service 访问到自己.</p> <p>接下来, CNI bridge 插件会调用 CNI ipam 插件, 从 ipam.subnet 字段规定的网段里为容器分配一个可用的 IP 地址. 然后, CNI bridge 插件就会把这个 IP 地址添加在容器的 eth0 网卡上, 同时为容器设置默认路由. 这相当于在容器里执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在容器里</span>
$ <span class="token function">ip</span> addr <span class="token function">add</span> <span class="token number">10.244</span>.0.2/24 dev eth0
$ <span class="token function">ip</span> route <span class="token function">add</span> default via <span class="token number">10.244</span>.0.1 dev eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>最后, CNI bridge 插件会为 CNI 网桥添加 IP 地址. 这相当于在宿主机上执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在宿主机上</span>
$ <span class="token function">ip</span> addr <span class="token function">add</span> <span class="token number">10.244</span>.0.1/24 dev cni0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在执行完上述操作之后, CNI 插件会把容器的 IP 地址等信息返回给 dockershim, 然后被 kubelet 添加到 Pod 的 Status 字段.</p> <p>至此, CNI 插件的 ADD 方法就宣告结束了. 接下来的流程, 就跟上一节中容器跨主机通信的过程完全一致了.</p> <p>需要注意的是, 对于非网桥类型的 CNI 插件, 上述 &quot;将容器添加到 CNI 网络&quot; 的操作流程, 以及网络方案本身的工作原理, 就都不太一样了. 后面将会分析这部分内容.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Kubernetes 中 CNI 网络的实现原理. 根据这个原理, 其实就很容易理解所谓的 &quot;Kubernetes 网络模型&quot; 了:</p> <ol><li>**所有容器都可以直接使用 IP 地址与其他容器通信, 而无需使用 NAT. **</li> <li>**所有宿主机都可以直接使用 IP 地址与所有容器通信, 而无需使用 NAT. 反之亦然. **</li> <li>**容器自己 &quot;看到&quot; 的自己的 IP 地址, 和别人(宿主机或者容器)看到的地址是完全一样的. **</li></ol> <p>这个网络模型, 其实可以用一个字总结, 那就是 &quot;<mark><strong>通</strong></mark>&quot;. 容器与容器之间要 &quot;通&quot;, 容器与宿主机之间也要 &quot;通&quot;. 并且 Kubernetes 要求这个 &quot;通&quot;, 还必须是<strong>直接基于容器和宿主机的 IP 地址来进行的</strong>.</p> <p>当然考虑到不同用户之间的隔离性, 在很多场合下, 还要求容器之间的网络 &quot;不通&quot;. 这个问题会在后面介绍.</p> <h4 id="_35-解读kubernetes三层网络方案"><a href="#_35-解读kubernetes三层网络方案" class="header-anchor">#</a> 35 | 解读Kubernetes三层网络方案</h4> <p>上一节以网桥类型的 Flannel 插件为例, 讲解了 Kubernetes 里容器网络和 CNI 插件的主要工作原理. 不过除了这种模式之外, 还有一种<strong>纯三层(Pure Layer 3)网络方案</strong>非常值得你注意. 其中的典型例子, 莫过于 Flannel 的 <strong>host-gw</strong> 模式和 Calico 项目了.</p> <p>先来看一下 Flannel 的 host-gw 模式.</p> <p>它的工作原理非常简单, 用一张图就可以说清楚. 为了方便叙述, 接下来会称这张图为 &quot;host-gw 示意图&quot;.</p> <p><img src="/img/50df98a8d7630c43ba2b6bd798dd0c12-20230731162150-un2rl9b.png" alt="" title="图1 Flannel host-gw 示意图"></p> <p>假设现在 Node 1 上的 Infra-container-1, 要访问 Node 2 上的 Infra-container-2.</p> <p>当设置 Flannel 使用 host-gw 模式之后, <strong>flanneld 会在宿主机上创建这样一条规则</strong>, 以 Node 1 为例:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">ip</span> route
<span class="token punctuation">..</span>.
<span class="token number">10.244</span>.1.0/24 via <span class="token number">10.168</span>.0.3 dev eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这条路由规则的含义是:</p> <p>目的 IP 地址属于 10.244.1.0/24 网段的 IP 包, 应该经过<strong>本机的 eth0 设备</strong>发出去(即: dev eth0); 并且它下一跳地址(next-hop)是 10.168.0.3(即: via 10.168.0.3).</p> <p>所谓下一跳地址就是: <strong>如果 IP 包从主机 A 发到主机 B, 需要经过路由设备 X 的中转. 那么 X 的 IP 地址就应该配置为主机 A 的下一跳地址</strong>.</p> <p>而从 host-gw 示意图中可以看到, 这个下一跳地址对应的, 正是目的宿主机 Node 2. 一旦配置了下一跳地址, 那么接下来, 当 IP 包从网络层进入链路层封装成帧的时候, eth0 设备就会<strong>使用下一跳地址对应的 MAC 地址</strong>, 作为该数据帧的目的 MAC 地址. 显然这个 MAC 地址, 正是 Node 2 的 MAC 地址.</p> <p>这样这个数据帧就会从 Node 1 通过宿主机的二层网络顺利到达 Node 2 上.</p> <p>而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后, 会 &quot;看到&quot; 这个 IP 包的目的 IP 地址是 10.244.1.3, 即 Infra-container-2 的 IP 地址. 这时候, 根据 Node 2 上的路由表, 该目的地址会匹配到第二条路由规则(也就是 10.244.1.0 对应的路由规则), 从而进入 cni0 网桥, 进而进入到 Infra-container-2 当中.</p> <p>可以看到, <mark><strong>host-gw 模式的工作原理, 其实就是将每个 Flannel 子网(Flannel Subnet, 比如: 10.244.1.0/24)的 &quot;下一跳&quot;, 设置成了该子网对应的宿主机的 IP 地址</strong></mark>​ <strong>. ** 也就是说, 这台  <mark></mark></strong>&quot;主机&quot;(Host)会充当这条容器通信路径里的 &quot;网关&quot;(Gateway)** . 这也正是 &quot;host-gw&quot; 的含义.</p> <p>当然 Flannel 子网和主机的信息, 都是保存在 <strong>Etcd</strong> 当中的. flanneld 只需要 <strong>WACTH</strong> 这些数据的变化, 然后实时更新路由表即可. 注意在 Kubernetes v1.7 之后, 类似 Flannel, Calico 的 CNI 网络插件都是可以<strong>直接连接</strong> Kubernetes 的 APIServer 来访问 Etcd 的, 无需额外部署 Etcd 给它们使用.</p> <p>而在这种模式下, <strong>容器通信的过程就免除了额外的封包和解包带来的性能损耗</strong>. 根据实际的测试, host-gw 的性能损失大约在 10% 左右, 而其他所有基于 VXLAN &quot;隧道&quot; 机制的网络方案, 性能损失都在 20%~30% 左右.</p> <p>当然通过上面的叙述, 也应该看到, host-gw 模式<strong>能够正常工作的核心, 就在于 IP 包在封装成帧发送出去的时候, 会使用路由表里的 &quot;下一跳&quot; 来设置目的 MAC 地址</strong>. 这样它就会经过二层网络到达目的宿主机. <mark><strong>所以 Flannel host-gw 模式必须要求集群宿主机之间是二层连通的</strong></mark>​ **. **</p> <p>需要注意的是, <strong>宿主机之间二层不连通的情况也是广泛存在</strong>的. 比如宿主机分布在了不同的子网(VLAN)里. 但是在一个 Kubernetes 集群里, 宿主机之间必须可以通过 IP 地址进行通信, 也就是说<strong>至少是三层可达</strong>的. 否则的话, 集群将不满足上一节提到的宿主机之间 IP 互通的假设(Kubernetes 网络模型). 当然, &quot;三层可达&quot; 也可以通过为几个子网设置三层转发来实现.</p> <p>而在容器生态中, 要说到像 Flannel host-gw 这样的三层网络方案, 就不得不提到这个领域里的 &quot;龙头老大&quot; <strong>Calico</strong> 项目了. 实际上, Calico 项目提供的网络解决方案, 与 Flannel 的 host-gw 模式, <strong>几乎是完全一样</strong>的. 也就是说, <strong>Calico 也会在每台宿主机上, 添加一个格式如下所示的路由规则</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token operator">&lt;</span>目的容器 IP 地址段<span class="token operator">&gt;</span> via <span class="token operator">&lt;</span>网关的 IP 地址<span class="token operator">&gt;</span> dev eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中<strong>网关的 IP 地址, 正是目的容器所在宿主机的 IP 地址</strong>. 而正如前所述, 这个三层网络方案得以正常工作的核心, 是为每个容器的 IP 地址找到它所对应的 &quot;下一跳&quot; 的<strong>网关</strong>.</p> <p>不过, **不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法, Calico 项目使用了一个 &quot;重型武器&quot; 来自动地在整个集群中分发路由信息. ** 这个 &quot;重型武器&quot;, 就是 BGP. <strong>BGP 的全称是 Border Gateway Protocol, 即: 边界网关协议</strong>. 它是一个 Linux 内核原生就支持的, 专门用在大规模数据中心里维护不同的 &quot;自治系统&quot; 之间路由信息的, 无中心的路由协议.</p> <p>这个概念可能听起来有点儿 &quot;吓人&quot;, 但实际上可以用一个非常简单的例子来讲清楚.</p> <p><img src="/img/f6125d858447d7a772d229a3b9b1d936-20230731162150-e8gom8u.png" alt="" title="图2 自治系统"></p> <p>在这个图中, 有两个自治系统(Autonomous System, 简称为 AS): AS 1 和 AS 2. 而所谓的一个自治系统, 指的是一个组织管辖下的所有 IP 网络和路由器的全体. 可以把它想象成一个小公司里的所有主机和路由器. 在正常情况下, 自治系统之间不会有任何 &quot;来往&quot;.</p> <p>但如果这样两个自治系统里的主机, 要通过 IP 地址直接进行通信, 就必须使用<strong>路由器</strong>把这两个自治系统连接起来. 比如, AS 1 里面的主机 10.10.0.2, 要访问 AS 2 里面的主机 172.17.0.3 的话. 它发出的 IP 包, 就会先到达自治系统 AS 1 上的路由器 Router 1.</p> <p>而在此时, Router 1 的路由表里有这样一条规则, 即: 目的地址是 172.17.0.2 包, 应该经过 Router 1 的 C 接口, 发往网关 Router 2(即: 自治系统 AS 2 上的路由器). 所以 IP 包就会到达 Router 2 上, 然后经过 Router 2 的路由表, 从 B 接口出来到达目的主机 172.17.0.3.</p> <p>但是反过来, 如果主机 172.17.0.3 要访问 10.10.0.2, 那么这个 IP 包, 在到达 Router 2 之后, 就不知道该去哪儿了. 因为在 Router 2 的路由表里, 并没有关于 AS 1 自治系统的任何路由规则.</p> <p>所以这时候, 网络管理员就应该给 Router 2 也添加一条路由规则, 比如: 目标地址是 10.10.0.2 的 IP 包, 应该经过 Router 2 的 C 接口, 发往网关 Router 1.</p> <p>像上面这样<strong>负责把自治系统连接在一起的路由器</strong>, 就把它形象地称为: <strong>边界网关</strong>. 它跟普通路由器的不同之处在于, <strong>它的路由表里拥有其他自治系统里的主机路由信息</strong>. 这部分原理理解起来应该很容易. 毕竟路由器这个设备本身的主要作用, 就是连通不同的网络.</p> <p>但是可以想象一下, 假设现在的网络拓扑结构<strong>非常复杂</strong>, 每个自治系统都有成千上万个主机, 无数个路由器, 甚至是由多个公司, 多个网络提供商, 多个自治系统组成的复合自治系统呢? 这时候如果还要依靠人工来对边界网关的路由表进行配置和维护, 那是绝对不现实的. 而这种情况下, BGP 大显身手的时刻就到了.</p> <p>在使用了 BGP 之后, 可以认为在每个边界网关上都会运行着一个小程序, <strong>它们会将各自的路由表信息, 通过 TCP 传输给其他的边界网关</strong>. 而其他边界网关上的这个小程序, 则会对收到的这些数据进行分析, 然后<strong>将需要的信息添加到自己的路由表</strong>里.</p> <p>这样, 图 2 中 Router 2 的路由表里, 就会<strong>自动出现 10.10.0.2 和 10.10.0.3 对应的路由规则</strong>了. 所以说, **所谓 BGP, 就是在大规模网络中实现节点路由信息共享的一种协议. **</p> <p>而 BGP 的这个能力, 正好可以<strong>取代 Flannel 维护主机上路由表的功能</strong>. 而且 BGP 这种原生就是为大规模网络环境而实现的协议, 其可靠性和可扩展性, 远非 Flannel 自己的方案可比. 需要注意的是, BGP 协议实际上是最复杂的一种路由协议. 这里的讲述和所举的例子, 仅是为了建立对 BGP 的感性认识, 并不代表 BGP 真正的实现方式.</p> <p>接下来还是回到 Calico 项目上来. 在了解了 BGP 之后, Calico 项目的架构就非常容易理解了. 它由三个部分组成:</p> <ol><li><strong>Calico 的 CNI 插件</strong>. 这是 Calico 与 Kubernetes 对接的部分. 我已经在上一篇文章中, 和你详细分享了 CNI 插件的工作原理, 这里就不再赘述了.</li> <li><strong>Felix</strong>. 它是一个 DaemonSet, 负责在宿主机上插入路由规则(即: 写入 Linux 内核的 FIB 转发信息库), 以及维护 Calico 所需的网络设备等工作.</li> <li><strong>BIRD</strong>. 它就是 BGP 的客户端, 专门负责在集群里分发路由规则信息.</li></ol> <p><strong>除了对路由信息的维护方式之外, Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处, 就是</strong>​<mark><strong>它不会在宿主机上创建任何网桥设备</strong></mark>. 这时候, Calico 的工作方式, 可以用一幅示意图来描述, 如下所示(在接下来的讲述中, 会统一用 &quot;BGP 示意图&quot; 来指代它):</p> <p><img src="/img/9b9acc89642e85d0fee6fa6a480038f8-20230731162150-cji3ksy.png" alt="" title="图3 Calico 工作原理"></p> <p>其中绿色实线标出的路径, 就是一个 IP 包从 Node 1 上的 Container 1, 到达 Node 2 上的 Container 4 的完整路径.</p> <p>可以看到, Calico 的 CNI 插件会<strong>为每个容器设置一个 Veth Pair 设备, 然后把其中的一端放置在宿主机上</strong>(它的名字以 cali 前缀开头). 此外, 由于 Calico 没有使用 CNI 的网桥模式, Calico 的 CNI 插件还需要<strong>在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则, 用于接收传入的 IP 包</strong>. 比如宿主机 Node 2 上的 Container 4 对应的路由规则, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">10.233</span>.2.3 dev cali5863f3 scope <span class="token function">link</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>即: 发往 10.233.2.3 的 IP 包, 应该进入 cali5863f3 设备.</p> <p>基于上述原因, Calico 项目在宿主机上设置的路由规则, 肯定要比 Flannel 项目多得多. 不过 Flannel host-gw 模式使用 CNI 网桥的主要原因, 其实是为了跟 VXLAN 模式保持一致. 否则的话, Flannel 就需要维护两套 CNI 插件了.</p> <p>有了这样的 Veth Pair 设备之后, <strong>容器发出的 IP 包就会经过 Veth Pair 设备出现在宿主机上</strong>. 然后宿主机网络栈就会根据路由规则的下一跳 IP 地址, 把它们转发给正确的网关. 接下来的流程就跟 Flannel host-gw 模式<strong>完全一致</strong>了.</p> <p>其中, 这里最核心的 &quot;下一跳&quot; 路由规则, 就是由 Calico 的 Felix 进程负责维护的. <strong>这些路由规则信息, 则是通过 BGP Client 也就是 BIRD 组件, 使用 BGP 协议传输而来的</strong>.</p> <p>而这些通过 BGP 协议传输的消息, 可以简单地理解为如下格式:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">[</span>BGP 消息<span class="token punctuation">]</span>
我是宿主机 <span class="token number">192.168</span>.1.2
<span class="token number">10.233</span>.2.0/24 网段的容器都在我这里
这些容器的下一跳地址是我
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>不难发现, Calico 项目实际上<strong>将集群里的所有节点, 都当作是边界路由器</strong>来处理, 它们<strong>一起组成了一个全连通的网络, 互相之间通过 BGP 协议交换路由规则</strong>. 这些节点称为 BGP Peer.</p> <p>需要注意的是, <strong>Calico 维护的网络在默认配置下, 是一个被称为 &quot;Node-to-Node Mesh&quot; 的模式</strong>. 这时候<strong>每台</strong>宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息. 但是随着节点数量 N 的增加, 这些连接的数量就会以 N² 的规模快速增长, 从而给集群本身的网络带来巨大的压力. 所以 Node-to-Node Mesh 模式<strong>一般推荐用在少于 100 个节点</strong>的集群里.</p> <p>而在更大规模的集群中, 需要用到的是一个叫作 <strong>Route Reflector 的模式</strong>. 在这种模式下, Calico 会指定一个或者几个专门的节点, 来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则. 而其他节点只需要跟这几个专门的节点交换路由信息, 就可以获得整个集群的路由规则信息了. 这些专门的节点, 就是所谓的 Route Reflector 节点, 它们实际上扮演了 &quot;中间代理&quot; 的角色, 从而把 BGP 连接的规模控制在 N 的数量级上.</p> <p>此外, 前面提到过, <strong>Flannel host-gw 模式最主要的限制, 就是要求集群宿主机之间是二层连通的. 而这个限制对于 Calico 来说, 也同样存在</strong>.</p> <p>举个例子, 假如有两台处于不同子网的宿主机 Node 1 和 Node 2, 对应的 IP 地址分别是 192.168.1.2 和 192.168.2.2. 需要注意的是, 这两台机器通过路由器实现了三层转发, 所以这两个 IP 地址之间是可以相互通信的.</p> <p>而现在的需求, 还是 Container 1 要访问 Container 4. 按照前面的讲述, Calico 会尝试在 Node 1 上添加如下所示的一条路由规则:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">10.233</span>.2.0/16 via <span class="token number">192.168</span>.2.2 eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>但是这时候问题就来了.</p> <p>上面这条规则里的下一跳地址是 192.168.2.2, 可是它对应的 Node 2 跟 Node 1 却根本<strong>不在一个子网</strong>里, 没办法通过二层网络把 IP 包发送到下一跳地址.</p> <p>**在这种情况下, 就需要为 Calico 打开 IPIP 模式. **</p> <p>把这个模式下容器通信的原理总结成一副示意图, 如下所示(接下来会称之为: IPIP 示意图):</p> <p><img src="/img/e04e9ce89c441b877e846837f03be99a-20230731162150-xhjc5pe.png" alt="" title="图4 Calico IPIP 模式工作原理"></p> <p>在 Calico 的 IPIP 模式下, Felix 进程在 Node 1 上添加的<strong>路由规则</strong>, 会稍微不同, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">10.233</span>.2.0/24 via <span class="token number">192.168</span>.2.2 tunl0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到, 尽管这条规则的下一跳地址仍然是 Node 2 的 IP 地址, 但这一次, 要<strong>负责将 IP 包发出去的设备, 变成了 tunl0</strong>. 注意, 是 T-U-N-L-0, 而不是 Flannel UDP 模式使用的 T-U-N-0(tun0), 这两种设备的功能是完全不一样的.</p> <p>Calico 使用的这个 tunl0 设备, 是一个 <strong>IP 隧道(IP tunnel)设备</strong>.</p> <p>在上面的例子中, IP 包进入 IP 隧道设备之后, 就会被 Linux 内核的 IPIP 驱动接管. IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中, 如下所示:</p> <p><img src="/img/02bb50c68cf72aa677cc1296b73abdae-20230731162150-lw2s648.png" alt="" title="图5 IPIP 封包方式"></p> <p>其中, 经过封装后的新的 IP 包的目的地址(图 5 中的 Outer IP Header 部分), 正是原 IP 包的下一跳地址, 即 Node 2 的 IP 地址: 192.168.2.2. 而原 IP 包本身, 则会被直接封装成新 IP 包的 Payload.</p> <p>这样, 原先从容器到 Node 2 的 IP 包, 就被伪装成了一个从 Node 1 到 Node 2 的 IP 包.</p> <p>由于宿主机之间已经使用路由器配置了三层转发, 也就是设置了宿主机之间的 &quot;下一跳&quot;. 所以这个 IP 包在离开 Node 1 之后, 就可以经过路由器, 最终 &quot;跳&quot; 到 Node 2 上. 这时 Node 2 的网络内核栈会使用 IPIP 驱动进行解包, 从而拿到原始的 IP 包. 然后原始 IP 包就会经过路由规则和 Veth Pair 设备到达目的容器内部.</p> <p>以上就是 Calico 项目主要的工作原理了.</p> <p>不难看到, 当 Calico 使用 IPIP 模式的时候, <strong>集群的网络性能会因为额外的封包和解包工作而下降</strong>. 在实际测试中, Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当. 所以在实际使用时, 如非硬性需求, <strong>建议将所有宿主机节点放在一个子网里, 避免使用 IPIP</strong>.</p> <p>不过通过上面对 Calico 工作原理的讲述, 应该能发现这样一个事实: 如果 Calico 项目能够让宿主机之间的路由设备(也就是网关), 也通过 BGP 协议 &quot;学习&quot; 到 Calico 网络里的路由规则, 那么从容器发出的 IP 包, 不就可以通过这些设备路由到目的宿主机了么?</p> <p>比如只要在上面 &quot;IPIP 示意图&quot; 中的 Node 1 上, 添加如下所示的一条路由规则:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">10.233</span>.2.0/24 via <span class="token number">192.168</span>.1.1 eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后在 Router 1 上(192.168.1.1), 添加如下所示的一条路由规则:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">10.233</span>.2.0/24 via <span class="token number">192.168</span>.2.1 eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>那么 Container 1 发出的 IP 包, 就可以通过两次 &quot;下一跳&quot;, 到达 Router 2(192.168.2.1)了. 以此类推, 可以继续在 Router 2 上添加 &quot;下一条&quot; 路由, 最终把 IP 包转发到 Node 2 上.</p> <p>遗憾的是, 上述流程虽然简单明了, 但是在 Kubernetes 被广泛使用的公有云场景里, 却完全不可行. 这里的原因在于: <strong>公有云环境下, 宿主机之间的网关, 肯定不会允许用户进行干预和设置</strong>. 当然, 在大多数公有云环境下, 宿主机(公有云提供的虚拟机)本身往往就是<strong>二层连通</strong>的, 所以这个需求也不强烈.</p> <p>不过在私有部署的环境下, <strong>宿主机属于不同子网(VLAN)反而是更加常见的部署状态</strong>. 这时候想办法将宿主机网关也加入到 BGP Mesh 里从而避免使用 IPIP, 就成了一个非常迫切的需求.</p> <p>而在 Calico 项目中, 它已经为你提供了两种将宿主机网关设置成 BGP Peer 的解决方案.</p> <p><strong>第一种方案</strong>, 就是<strong>所有宿主机都跟宿主机网关建立 BGP Peer 关系</strong>.</p> <p>这种方案下, Node 1 和 Node 2 就需要主动跟宿主机网关 Router 1 和 Router 2 建立 BGP 连接. 从而将类似于 10.233.2.0/24 这样的路由信息同步到网关上去.</p> <p>需要注意的是, 这种方式下, Calico 要求宿主机网关必须支持一种叫作 Dynamic Neighbors 的 BGP 配置方式. 这是因为在常规的路由器 BGP 配置里, 运维人员必须明确给出所有 BGP Peer 的 IP 地址. 考虑到 Kubernetes 集群可能会有成百上千个宿主机, 而且还会动态地添加和删除节点, 这时候再手动管理路由器的 BGP 配置就非常麻烦了. 而 Dynamic Neighbors 则允许你给路由器配置一个网段, 然后路由器就会自动跟该网段里的主机建立起 BGP Peer 关系.</p> <p>不过相比之下, 更愿意推荐<strong>第二种方案</strong>.</p> <p>这种方案, 是使用一个或多个独立组件负责搜集整个集群里的所有路由信息, 然后通过 BGP 协议同步给网关. 而前面提到, 在大规模集群中, Calico 本身就推荐使用 Route Reflector 节点的方式进行组网. 所以这里负责跟宿主机网关进行沟通的独立组件, 直接由 Route Reflector 兼任即可.</p> <p>更重要的是, 这种情况下网关的 BGP Peer 个数是有限并且固定的. 所以就可以直接把这些独立组件配置成路由器的 BGP Peer, 而无需 Dynamic Neighbors 的支持.</p> <p>当然这些独立组件的工作原理也很简单: 它们只需要 WATCH Etcd 里的宿主机和对应网段的变化信息, 然后把这些信息通过 BGP 协议分发给网关即可.</p> <blockquote><p>总结</p></blockquote> <p>本节讲述了 Fannel host-gw 模式和 Calico 这两种纯三层网络方案的工作原理.</p> <p>需要注意的是, <strong>在大规模集群里, 三层网络方案在宿主机上的路由规则可能会非常多, 这会导致错误排查变得困难</strong>. 此外, 在系统故障的时候, 路由规则出现重叠冲突的概率也会变大.</p> <p>基于上述原因, 如果是<strong>在公有云上, 由于宿主机网络本身比较&quot;直白&quot;, 一般会推荐更加简单的 Flannel host-gw 模式</strong>. 但不难看到, 在私有部署环境里, Calico 项目才能够覆盖更多的场景, 并可以供更加可靠的组网方案和架构思路.</p> <h4 id="_36-为什么说kubernetes只有soft-multi-tenancy"><a href="#_36-为什么说kubernetes只有soft-multi-tenancy" class="header-anchor">#</a> 36 | 为什么说Kubernetes只有soft multi-tenancy?</h4> <p>前面讲解了 Kubernetes 生态里, 主流容器网络方案的工作原理. 不难发现, Kubernetes 的网络模型, 以及前面这些网络方案的实现, 都只<strong>关注容器之间网络的 &quot;连通&quot;, 却并不关心容器之间网络的 &quot;隔离&quot;</strong> . 这跟传统的 IaaS 层的网络方案, 区别非常明显.</p> <p>你肯定会问了, Kubernetes 的网络方案<strong>对 &quot;隔离&quot; 到底是如何考虑</strong>的呢? 难道 Kubernetes 就不管网络 &quot;多租户&quot; 的需求吗?</p> <p>本节就来回答这些问题.</p> <p>在 Kubernetes 里, **网络隔离能力的定义, 是依靠一种专门的 API 对象来描述的, 即: **​<mark><strong>NetworkPolicy</strong></mark>.</p> <p>一个完整的 NetworkPolicy 对象的示例, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> NetworkPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>network<span class="token punctuation">-</span>policy
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">role</span><span class="token punctuation">:</span> db
  <span class="token key atrule">policyTypes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> Ingress
  <span class="token punctuation">-</span> Egress
  <span class="token key atrule">ingress</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">ipBlock</span><span class="token punctuation">:</span>
        <span class="token key atrule">cidr</span><span class="token punctuation">:</span> 172.17.0.0/16
        <span class="token key atrule">except</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> 172.17.1.0/24
    <span class="token punctuation">-</span> <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
          <span class="token key atrule">project</span><span class="token punctuation">:</span> myproject
    <span class="token punctuation">-</span> <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
          <span class="token key atrule">role</span><span class="token punctuation">:</span> frontend
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">6379</span>
  <span class="token key atrule">egress</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">to</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">ipBlock</span><span class="token punctuation">:</span>
        <span class="token key atrule">cidr</span><span class="token punctuation">:</span> 10.0.0.0/24
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">5978</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br></div></div><p>我前面已经说过, <strong>Kubernetes 里的 Pod 默认都是 &quot;允许所有&quot;(Accept All)的</strong>, 即: <strong>Pod 可以接收来自任何发送方的请求; 或者, 向任何接收方发送请求. 而如果要对这个情况作出限制, 就必须通过 NetworkPolicy 对象来指定</strong>.</p> <p>而在上面这个例子里, 首先会看到 <strong>podSelector</strong> 字段. 它的作用就是定义<strong>这个 NetworkPolicy 的限制范围</strong>, 比如: 当前 Namespace 里携带了 role=db 标签的 Pod.</p> <p>而如果把 podSelector 字段留空:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span> 
    <span class="token key atrule">podSelector</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>那么这个 NetworkPolicy 就会作用于当前 Namespace 下的<strong>所有 Pod</strong>.</p> <p>而一旦 Pod 被 NetworkPolicy 选中, <strong>那么这个 Pod 就会进入 &quot;拒绝所有&quot; (Deny All)的状态</strong>, 即: 这个 Pod 既不允许被外界访问, 也不允许对外界发起访问. <mark><strong>而 NetworkPolicy 定义的规则, 其实就是 &quot;白名单&quot;</strong></mark>​ **. **</p> <p>例如在上面这个例子里, 在 policyTypes 字段定义了这个 NetworkPolicy 的类型是 <strong>ingress 和 egress</strong>, 即: <strong>它既会影响流入(ingress)请求, 也会影响流出(egress)请求</strong>.</p> <p>然后在 ingress 字段里, 定义了 from 和 ports, 即: <strong>允许流入的 &quot;白名单&quot; 和端口</strong>. 其中这个允许流入的 &quot;白名单 &quot;里, 指定了<strong>三种并列的情况</strong>, 分别是: <strong>ipBlock, namespaceSelector 和 podSelector</strong>.</p> <p>而在 egress 字段里, 则定义了 to 和 ports, 即: <strong>允许流出的 &quot;白名单&quot; 和端口</strong>. 这里允许流出的 &quot;白名单&quot; 的定义方法与 ingress 类似. 只不过这一次 ipblock 字段指定的, 是目的地址的网段.</p> <p>综上所述, 这个 NetworkPolicy 对象, 指定的隔离规则如下所示:</p> <ol><li>该隔离规则只对 <strong>default Namespace 下的, 携带了 role=db 标签的 Pod 有效</strong>. 限制的请求类型包括 ingress(流入)和 egress(流出).</li> <li>Kubernetes 会<strong>拒绝任何访问被隔离 Pod 的请求</strong>, 除非这个请求来自于以下 &quot;白名单&quot; 里的对象, 并且访问的是被隔离 Pod 的 6379 端口. 这些 &quot;白名单&quot; 对象包括:</li> <li>default Namespace 里的, 携带了 role=fronted 标签的 Pod;</li> <li>任何 Namespace 里的, 携带了 project=myproject 标签的 Pod;</li> <li>任何源地址属于 172.17.0.0/16 网段, 且不属于 172.17.1.0/24 网段的请求.</li> <li><strong>Kubernetes 会拒绝被隔离 Pod 对外发起任何请求</strong>, 除非请求的目的地址属于 10.0.0.0/24 网段, 并且访问的是该网段地址的 5978 端口.</li></ol> <p>需要注意的是, 定义一个 NetworkPolicy 对象的过程, 容易犯错的是 &quot;白名单&quot; 部分(from 和 to 字段).</p> <p>举个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
  <span class="token key atrule">ingress</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
          <span class="token key atrule">user</span><span class="token punctuation">:</span> alice
    <span class="token punctuation">-</span> <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
          <span class="token key atrule">role</span><span class="token punctuation">:</span> client
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>像上面这样**定义的 namespaceSelector 和 podSelector, 是 &quot;**​<mark><strong>或</strong></mark>​ <strong>&quot; (OR)的关系</strong>. 所以说这个 from 字段定义了两种情况, 无论是 Namespace 满足条件, 还是 Pod 满足条件, 这个 NetworkPolicy 都会生效.</p> <p>而下面这个例子, 虽然看起来类似, 但是它定义的规则却完全不同:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
  <span class="token key atrule">ingress</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
          <span class="token key atrule">user</span><span class="token punctuation">:</span> alice
      <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
          <span class="token key atrule">role</span><span class="token punctuation">:</span> client
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>注意看, 这样**定义的 namespaceSelector 和 podSelector, 其实是 &quot;**​<mark><strong>与</strong></mark>​ <strong>&quot; (AND)的关系</strong>. 所以说这个 from 字段只定义了一种情况, 只有 Namespace 和 Pod 同时满足条件, 这个 NetworkPolicy 才会生效.</p> <p>**这两种定义方式的区别, 请一定要分清楚. **</p> <p>此外, 如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用, 你的 <strong>CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的</strong>. 在具体实现上, 凡是支持 NetworkPolicy 的 CNI 网络插件, 都维护着一个 <strong>NetworkPolicy Controller</strong>, 通过控制循环的方式对 NetworkPolicy 对象的增删改查做出响应, 然后在宿主机上完成 iptables 规则的配置工作.</p> <p>在 Kubernetes 生态里, 目前已经实现了 NetworkPolicy 的网络插件包括 Calico, Weave 和 kube-router 等多个项目, 但是<strong>并不包括 Flannel 项目</strong>. 所以如果想要在使用 Flannel 的同时还使用 NetworkPolicy 的话, 就需要再额外安装一个网络插件, 比如 Calico 项目, 来负责执行 NetworkPolicy. 安装 Flannel + Calico 的流程非常简单, 直接参考这个文档<a href="https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/flannel" target="_blank" rel="noopener noreferrer">一键安装<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>即可.</p> <p>那么这些网络插件, 又是<strong>如何根据 NetworkPolicy 对 Pod 进行隔离</strong>的呢?</p> <p>接下来就以<strong>三层网络插件</strong>为例(比如 Calico 和 kube-router), 来分析一下这部分的原理.</p> <p>为了方便讲解, 这一次编写了一个比较简单的 NetworkPolicy 对象, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> NetworkPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>network<span class="token punctuation">-</span>policy
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">role</span><span class="token punctuation">:</span> db
  <span class="token key atrule">ingress</span><span class="token punctuation">:</span>
   <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
     <span class="token punctuation">-</span> <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span>
         <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
           <span class="token key atrule">project</span><span class="token punctuation">:</span> myproject
     <span class="token punctuation">-</span> <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>
         <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
           <span class="token key atrule">role</span><span class="token punctuation">:</span> frontend
     <span class="token key atrule">ports</span><span class="token punctuation">:</span>
       <span class="token punctuation">-</span> <span class="token key atrule">protocol</span><span class="token punctuation">:</span> tcp
         <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">6379</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>可以看到, 指定的 <strong>ingress &quot;白名单&quot;</strong> , 是任何 Namespace 里, 携带 project=myproject 标签的 Pod; 以及 default Namespace 里, 携带了 role=frontend 标签的 Pod. 允许被访问的端口是: 6379.</p> <p>而被隔离的对象, 是所有携带了 role=db 标签的 Pod.</p> <p>那么这个时候, <mark><strong>Kubernetes 的网络插件就会使用这个 NetworkPolicy 的定义, 在宿主机上生成 iptables 规则</strong></mark>. 这个过程, 可以通过如下所示的一段 Go 语言风格的伪代码来描述:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">for</span> dstIP <span class="token operator">:=</span> <span class="token keyword">range</span> 所有被 networkpolicy<span class="token punctuation">.</span>spec<span class="token punctuation">.</span>podSelector 选中的 Pod 的 IP 地址
  <span class="token keyword">for</span> srcIP <span class="token operator">:=</span> <span class="token keyword">range</span> 所有被 ingress<span class="token punctuation">.</span>from<span class="token punctuation">.</span>podSelector 选中的 Pod 的 IP 地址
    <span class="token keyword">for</span> port<span class="token punctuation">,</span> protocol <span class="token operator">:=</span> <span class="token keyword">range</span> ingress<span class="token punctuation">.</span>ports <span class="token punctuation">{</span>
      iptables <span class="token operator">-</span>A KUBE<span class="token operator">-</span>NWPLCY<span class="token operator">-</span>CHAIN <span class="token operator">-</span>s $srcIP <span class="token operator">-</span>d $dstIP <span class="token operator">-</span>p $protocol <span class="token operator">-</span>m $protocol <span class="token operator">--</span>dport $port <span class="token operator">-</span>j ACCEPT 
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span> 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可以看到, 这是一条最基本的, <strong>通过匹配条件决定下一步动作的 iptables 规则</strong>.</p> <p>这条规则的名字是 KUBE-NWPLCY-CHAIN, 含义是: 当 IP 包的源地址是 srcIP, 目的地址是 dstIP, 协议是 protocol, 目的端口是 port 的时候, 就<strong>允许</strong>它通过(ACCEPT).</p> <p>而正如这段伪代码所示, 匹配这条规则所需的这<strong>四个参数, 都是从 NetworkPolicy 对象里读取</strong>出来的.</p> <p><mark>**可以看到, Kubernetes 网络插件对 Pod 进行隔离, 其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的.  **</mark> 此外, 在设置好上述 &quot;隔离&quot; 规则之后, 网络插件还需要想办法, 将所有对被隔离 Pod 的访问请求, 都转发到上述 KUBE-NWPLCY-CHAIN 规则上去进行匹配. 并且如果匹配不通过, 这个请求应该被 &quot;拒绝&quot;.</p> <p>在 CNI 网络插件中, 上述需求可以通过<strong>设置两组 iptables 规则</strong>来实现.</p> <p><strong>第一组规则, 负责 &quot;拦截&quot; 对被隔离 Pod 的访问请求</strong>. 生成这一组规则的伪代码, 如下所示:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">for</span> pod <span class="token operator">:=</span> <span class="token keyword">range</span> 该 Node 上的所有 Pod <span class="token punctuation">{</span>
    <span class="token keyword">if</span> pod 是 networkpolicy<span class="token punctuation">.</span>spec<span class="token punctuation">.</span>podSelector 选中的 <span class="token punctuation">{</span>
        iptables <span class="token operator">-</span>A FORWARD <span class="token operator">-</span>d $podIP <span class="token operator">-</span>m physdev <span class="token operator">--</span>physdev<span class="token operator">-</span>is<span class="token operator">-</span>bridged <span class="token operator">-</span>j KUBE<span class="token operator">-</span>POD<span class="token operator">-</span>SPECIFIC<span class="token operator">-</span>FW<span class="token operator">-</span>CHAIN
        iptables <span class="token operator">-</span>A FORWARD <span class="token operator">-</span>d $podIP <span class="token operator">-</span>j KUBE<span class="token operator">-</span>POD<span class="token operator">-</span>SPECIFIC<span class="token operator">-</span>FW<span class="token operator">-</span>CHAIN
        <span class="token operator">...</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可以看到, 这里的的 iptables 规则使用到了内置链: <strong>FORWARD</strong>. 它是什么意思呢? 实际上, <strong>iptables 只是一个操作 Linux 内核 Netfilter 子系统的 &quot;界面&quot;</strong> . 顾名思义, Netfilter 子系统的作用, 就是 <strong>Linux 内核里挡在 &quot;网卡&quot; 和 &quot;用户态进程&quot; 之间的一道 &quot;防火墙&quot;</strong> . 它们的关系, 可以用如下的示意图来表示:</p> <p><img src="/img/9d16fccf2a5712b11c0d384f25bbf20b-20230731162150-ejs7igs.png" alt=""></p> <p>可以看到, 这幅示意图中, IP 包 &quot;一进一出&quot; 的两条路径上, <strong>有几个关键的 &quot;检查点&quot;, 它们正是 Netfilter 设置 &quot;防火墙&quot; 的地方</strong>. <strong>在 iptables 中, 这些 &quot;检查点&quot; 被称为: 链(Chain)</strong> . 这是因为这些 &quot;检查点&quot; 对应的 <strong>iptables 规则</strong>, 是按照定义顺序依次进行匹配的. 这些 &quot;检查点&quot; 的具体工作原理, 可以用如下所示的示意图进行描述:</p> <p><img src="/img/2ffaa05cbf2419d380e44a8397894b37-20230731162150-2rppcr7.png" alt=""></p> <p>可以看到, 当一个 IP 包通过网卡进入主机之后, 它就<strong>进入了 Netfilter 定义的流入路径(Input Path)</strong> 里. 在这个路径中, IP 包要经过<strong>路由表路由</strong>来决定下一步的去向. 而在这次路由之前, Netfilter 设置了一个名叫 PREROUTING 的 &quot;检查点&quot;. 在 Linux 内核的实现里, 所谓 &quot;检查点&quot; 实际上就是<strong>内核网络协议栈代码里的 Hook</strong>(比如在执行路由判断的代码之前, 内核会先调用 PREROUTING 的 Hook).</p> <p>而在经过路由之后, IP 包的去向就分为了两种:</p> <ul><li>第一种, 继续在<strong>本机</strong>处理;</li> <li>第二种, <strong>被转发到其他目的地</strong>.</li></ul> <p><strong>先说一下 IP 包的第一种去向</strong>. 这时候, IP 包将继续向上层协议栈流动. 在它进入传输层之前, Netfilter 会设置一个名叫 INPUT 的 &quot;检查点&quot;. 到这里, IP 包流入路径(Input Path)结束. 接下来这个 IP 包通过传输层进入<strong>用户空间</strong>, 交给用户进程处理. 而处理完成后, 用户进程会通过本机发出返回的 IP 包. 这时候, 这个 IP 包就进入了流出路径(Output Path). 此时 IP 包首先还是会经过主机的路由表进行路由. 路由结束后, Netfilter 就会设置一个名叫 OUTPUT 的 &quot;检查点&quot;. 然后在 OUTPUT 之后, 再设置一个名叫 POSTROUTING &quot;检查点&quot;.</p> <p>你可能会觉得奇怪, 为什么在流出路径结束后, Netfilter 会连着设置<strong>两个</strong> &quot;检查点&quot; 呢?</p> <p>这就要说到在流入路径里, <strong>路由判断后的第二种去向</strong>了.</p> <p>在这种情况下, 这个 IP 包不会进入传输层, 而是会继续在<strong>网络层流动</strong>, 从而进入到转发路径(Forward Path). 在转发路径中, Netfilter 会设置一个名叫 FORWARD 的 &quot;检查点&quot;. 而在 FORWARD &quot;检查点&quot; 完成后, IP 包就会来到流出路径. 而转发的 IP 包由于目的地已经确定, 它就不会再经过路由, 也自然不会经过 OUTPUT, 而是会直接来到 <strong>POSTROUTING</strong> &quot;检查点&quot;.</p> <p>所以说, <strong>POSTROUTING 的作用, 其实就是上述两条路径, 最终汇聚在一起的 &quot;最终检查点&quot;</strong> .</p> <p>需要注意的是, 在有网桥参与的情况下, 上述 Netfilter 设置 &quot;检查点&quot; 的流程, 实际上也会出现在链路层(二层), 并且会跟在上面讲述的网络层(三层)的流程有交互.</p> <p>这些链路层的 &quot;检查点&quot; 对应的操作界面叫作 <strong>ebtables</strong>. 所以准确地说, 数据包在 Linux Netfilter 子系统里完整的流动过程, 其实应该如下所示(这是一幅来自 Netfilter 官方的原理图, 建议点击图片以查看大图):</p> <p>​<img src="/img/f369fd9e40285235b96901b77dfd6770-20230731162150-bbdw8vx.png" alt="">
可以看到, 前面讲述的, 正是上图中绿色部分, 也就是<strong>网络层的 iptables 链</strong>的工作流程.</p> <p>另外应该还能看到, 每一个白色的 &quot;检查点&quot; 上, 还有一个绿色的 &quot;标签&quot;, 比如: raw, nat, filter 等等. 在 iptables 里, 这些标签叫作: <strong>表</strong>. 比如同样是 OUTPUT 这个 &quot;检查点&quot;, filter Output 和 nat Output 在 iptables 里的语法和参数, 就完全不一样, 实现的功能也完全不同.</p> <p>所以说, <strong>iptables 表的作用, 就是在某个具体的 &quot;检查点&quot;(比如 Output)上, 按顺序执行几个不同的检查动作(比如先执行 nat, 再执行 filter)</strong> .</p> <p>在理解了 iptables 的工作原理之后, 再回到 NetworkPolicy 上来. 这时候前面由网络插件设置的, 负责 &quot;拦截&quot; 进入 Pod 的请求的三条 iptables 规则, 就很容易读懂了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>iptables <span class="token parameter variable">-A</span> FORWARD <span class="token parameter variable">-d</span> <span class="token variable">$podIP</span> <span class="token parameter variable">-m</span> physdev --physdev-is-bridged <span class="token parameter variable">-j</span> KUBE-POD-SPECIFIC-FW-CHAIN
iptables <span class="token parameter variable">-A</span> FORWARD <span class="token parameter variable">-d</span> <span class="token variable">$podIP</span> <span class="token parameter variable">-j</span> KUBE-POD-SPECIFIC-FW-CHAIN
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>其中 <strong>第一条 FORWARD 链 &quot;拦截&quot; 的是一种特殊情况</strong>: 它对应的是同一台宿主机上容器之间经过 CNI 网桥进行通信的流入数据包. 其中 –physdev-is-bridged 的意思就是, 这个 FORWARD 链匹配的是通过本机上的网桥设备, 发往目的地址是 podIP 的 IP 包.</p> <p>当然如果是像 Calico 这样的非网桥模式的 CNI 插件, 就不存在这个情况了. kube-router 其实是一个简化版的 Calico, 它也使用 BGP 来维护路由信息, 但是使用 CNI bridge 插件负责跟 Kubernetes 进行交互.</p> <p>而<strong>第二条 FORWARD 链 &quot;拦截&quot; 的则是最普遍的情况, 即: 容器跨主通信</strong>. 这时候流入容器的数据包都是经过<strong>路由转发</strong>(FORWARD 检查点)来的.</p> <p>不难看到, 这些规则最后都跳转(即: -j)到了名叫 <strong>KUBE-POD-SPECIFIC-FW-CHAIN 的规则</strong>上. 它正是网络插件为 NetworkPolicy 设置的第二组规则. 而这个 KUBE-POD-SPECIFIC-FW-CHAIN 的作用, 就是做出 &quot;允许&quot; 或者 &quot;拒绝&quot; 的判断. 这部分功能的实现, 可以简单描述为下面这样的 iptables 规则:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>iptables <span class="token parameter variable">-A</span> KUBE-POD-SPECIFIC-FW-CHAIN <span class="token parameter variable">-j</span> KUBE-NWPLCY-CHAIN
iptables <span class="token parameter variable">-A</span> KUBE-POD-SPECIFIC-FW-CHAIN <span class="token parameter variable">-j</span> REJECT --reject-with icmp-port-unreachable
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以看到, 首先在第一条规则里, 会把 IP 包<strong>转交给前面定义的 KUBE-NWPLCY-CHAIN 规则去进行匹配</strong>. 按照之前的讲述, 如果匹配成功, 那么 IP 包就会被 &quot;允许通过&quot;. 而如果匹配失败, IP 包就会来到第二条规则上. 可以看到它是一条 REJECT 规则. <strong>通过这条规则, 不满足 NetworkPolicy 定义的请求就会被拒绝掉, 从而实现了对该容器的 &quot;隔离&quot;</strong> .</p> <p>以上就是 CNI 网络插件实现 NetworkPolicy 的基本方法了. 当然对于不同的插件来说, 上述实现过程可能有不同的手段, 但根本原理是不变的.</p> <blockquote><p>总结</p></blockquote> <p>本节分享了 Kubernetes 对 Pod 进行 &quot;隔离&quot; 的手段, 即: <strong>NetworkPolicy</strong>. 可以看到, <strong>NetworkPolicy 实际上只是宿主机上的一系列 iptables 规则</strong>. 这跟传统 IaaS 里面的安全组(Security Group)其实是非常类似的.</p> <p>而基于上述讲述, 就会发现这样一个事实:</p> <p><mark><strong>Kubernetes 的网络模型以及大多数容器网络实现, 其实既不会保证容器之间二层网络的互通, 也不会实现容器之间的二层网络隔离. 这跟 IaaS 项目管理虚拟机的方式, 是完全不同的</strong></mark>.</p> <p>所以说, Kubernetes 从底层的设计和实现上, 更倾向于假设已经有了一套完整的物理基础设施. 然后 Kubernetes 负责在此基础上提供一种 &quot;弱多租户&quot; (soft multi-tenancy)的能力.</p> <p>并且基于上述思路, Kubernetes 将来也不大可能把 Namespace 变成一个具有实质意义的隔离机制, 或者把它映射成为 &quot;子网&quot; 或者 &quot;租户&quot;. 毕竟可以看到, NetworkPolicy 对象的描述能力, 要比基于 Namespace 的划分丰富得多. 这也是为什么, 到目前为止, Kubernetes 项目在云计算生态里的定位, 其实是<strong>基础设施与 PaaS 之间的中间层</strong>. 这是非常符合 &quot;容器&quot; 这个本质上就是进程的抽象粒度的.</p> <p>当然, 随着 Kubernetes 社区以及 CNCF 生态的不断发展, Kubernetes 项目也已经开始逐步下探, &quot;吃&quot; 掉了基础设施领域的很多 &quot;蛋糕&quot;. 这也正是容器生态继续发展的一个必然方向.</p> <h4 id="_37-找到容器不容易-service-dns与服务发现"><a href="#_37-找到容器不容易-service-dns与服务发现" class="header-anchor">#</a> 37 | 找到容器不容易:Service,DNS与服务发现</h4> <p>前面已经多次使用到了 <strong>Service</strong> 这个 Kubernetes 里重要的服务对象. 而 Kubernetes 之所以需要 Service, <strong>一方面是因为 Pod 的 IP 不是固定的, 另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求</strong>.</p> <p>一个最典型的 Service 定义, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> hostnames
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> hostnames
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> default
    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">9376</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>这个 Service 的例子使用了 <strong>selector</strong> 字段来声明这个 Service 只代理携带了 <strong>app=hostnames 标签的 Pod</strong>. 并且这个 Service 的 80 端口, 代理的是 Pod 的 9376 端口.</p> <p>然后应用的 Deployment, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> hostnames
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> hostnames
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> hostnames
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> hostnames
        <span class="token key atrule">image</span><span class="token punctuation">:</span> k8s.gcr.io/serve_hostname
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">9376</span>
          <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>这个应用的作用, 就是每次访问 9376 端口时, 返回它自己的 hostname. <strong>而被 selector 选中的 Pod, 就称为 Service 的 Endpoints</strong>, 可以使用 kubectl get ep 命令看到它们, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get endpoints hostnames
NAME        ENDPOINTS
hostnames   <span class="token number">10.244</span>.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>需要注意的是, 只有处于 <strong>Running</strong> 状态, 且 <strong>readinessProbe</strong> 检查通过的 Pod, 才会出现在 Service 的 Endpoints 列表里. 并且当某一个 Pod 出现问题时, Kubernetes 会自动把它从 Service 里摘除掉.</p> <p>而此通过该 Service 的 VIP 地址 10.0.1.175, 就可以<strong>访问到它所代理的 Pod</strong> 了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc hostnames
NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>   AGE
hostnames   ClusterIP   <span class="token number">10.0</span>.1.175   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">80</span>/TCP    5s

$ <span class="token function">curl</span> <span class="token number">10.0</span>.1.175:80
hostnames-0uton

$ <span class="token function">curl</span> <span class="token number">10.0</span>.1.175:80
hostnames-yp2kp

$ <span class="token function">curl</span> <span class="token number">10.0</span>.1.175:80
hostnames-bvc05
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>这个 VIP 地址是 Kubernetes 自动为 Service 分配的. 而像上面这样, 通过三次<strong>连续不断</strong>地访问 Service 的 VIP 地址和代理端口 80, 它就为我们依次返回了三个 Pod 的 hostname. 这也正印证了 Service 提供的是 <strong>Round Robin 方式的负载均衡</strong>. 对于这种方式, 可以称为: <strong>ClusterIP 模式的 Service</strong>.</p> <p>你可能一直比较好奇, Kubernetes 里的 Service 究竟是如何工作的呢?</p> <p>实际上, <mark><strong>Service 是由 kube-proxy 组件, 加上 iptables 来共同实现的</strong></mark>​ **. **</p> <p>举个例子, 对于前面创建的名叫 hostnames 的 Service 来说, 一旦它被提交给 Kubernetes, 那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 <strong>Service 对象</strong>的添加. 而作为对这个事件的响应, 它就会<strong>在宿主机上创建这样一条 iptables 规则</strong>(可以通过 iptables-save 看到它), 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token parameter variable">-A</span> KUBE-SERVICES <span class="token parameter variable">-d</span> <span class="token number">10.0</span>.1.175/32 <span class="token parameter variable">-p</span> tcp <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames: cluster IP&quot;</span> <span class="token parameter variable">-m</span> tcp <span class="token parameter variable">--dport</span> <span class="token number">80</span> <span class="token parameter variable">-j</span> KUBE-SVC-NWV5X2332I4OT4T3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到这条 iptables 规则的含义是: <strong>凡是目的地址是 10.0.1.175, 目的端口是 80 的 IP 包</strong>, 都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理.</p> <p>而前面已经看到, 10.0.1.175 正是这个 Service 的 VIP. 所以这一条规则, <strong>就为这个 Service 设置了一个固定的入口地址</strong>. 并且由于 10.0.1.175 只是一条 iptables 规则上的配置, <strong>并没有真正的网络设备</strong>, 所以 ping 这个地址, 是不会有任何响应的.</p> <p>那么, 即将跳转到的 KUBE-SVC-NWV5X2332I4OT4T3 规则, 又有什么作用呢? 实际上, 它是一组规则的集合, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token parameter variable">-A</span> KUBE-SVC-NWV5X2332I4OT4T3 <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-m</span> statistic <span class="token parameter variable">--mode</span> random <span class="token parameter variable">--probability</span> <span class="token number">0.33332999982</span> <span class="token parameter variable">-j</span> KUBE-SEP-WNBA2IHDGP2BOBGZ
<span class="token parameter variable">-A</span> KUBE-SVC-NWV5X2332I4OT4T3 <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-m</span> statistic <span class="token parameter variable">--mode</span> random <span class="token parameter variable">--probability</span> <span class="token number">0.50000000000</span> <span class="token parameter variable">-j</span> KUBE-SEP-X3P2623AGDH6CDF3
<span class="token parameter variable">-A</span> KUBE-SVC-NWV5X2332I4OT4T3 <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-j</span> KUBE-SEP-57KPRZ3JQVENLNBR
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到, 这一组规则, 实际上是一组<strong>随机模式</strong>(–mode random)的 iptables 链. 而<strong>随机转发</strong>的目的地, 分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ, KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR.</p> <p>而这三条链指向的<strong>最终目的地</strong>, 其实就是<strong>这个 Service 代理的三个 Pod</strong>. 所以<strong>这一组规则, 就是 Service 实现负载均衡的位置</strong>.</p> <p>需要注意的是, iptables 规则的匹配是<strong>从上到下</strong>逐条进行的, 所以为了保证上述三条规则每条被选中的概率都相同, 应该将它们的 <strong>probability</strong> 字段的值分别设置为 1/3(0.333…), 1/2 和 1. 这么设置的原理很简单: <strong>第一条规则被选中的概率就是 1/3; 而如果第一条规则没有被选中, 那么这时候就只剩下两条规则了, 所以第二条规则的 probability 就必须设置为 1/2; 类似地, 最后一条就必须设置为 1</strong>.</p> <p>可以想一下, 如果把这三条规则的 probability 字段的值都设置成 1/3, 最终每条规则被选中的概率会变成多少.</p> <p>通过查看上述三条链的明细, 就很容易理解 Service 进行转发的具体原理了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token parameter variable">-A</span> KUBE-SEP-57KPRZ3JQVENLNBR <span class="token parameter variable">-s</span> <span class="token number">10.244</span>.3.6/32 <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-j</span> MARK --set-xmark 0x00004000/0x00004000
<span class="token parameter variable">-A</span> KUBE-SEP-57KPRZ3JQVENLNBR <span class="token parameter variable">-p</span> tcp <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-m</span> tcp <span class="token parameter variable">-j</span> DNAT --to-destination <span class="token number">10.244</span>.3.6:9376

<span class="token parameter variable">-A</span> KUBE-SEP-WNBA2IHDGP2BOBGZ <span class="token parameter variable">-s</span> <span class="token number">10.244</span>.1.7/32 <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-j</span> MARK --set-xmark 0x00004000/0x00004000
<span class="token parameter variable">-A</span> KUBE-SEP-WNBA2IHDGP2BOBGZ <span class="token parameter variable">-p</span> tcp <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-m</span> tcp <span class="token parameter variable">-j</span> DNAT --to-destination <span class="token number">10.244</span>.1.7:9376

<span class="token parameter variable">-A</span> KUBE-SEP-X3P2623AGDH6CDF3 <span class="token parameter variable">-s</span> <span class="token number">10.244</span>.2.3/32 <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-j</span> MARK --set-xmark 0x00004000/0x00004000
<span class="token parameter variable">-A</span> KUBE-SEP-X3P2623AGDH6CDF3 <span class="token parameter variable">-p</span> tcp <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/hostnames:&quot;</span> <span class="token parameter variable">-m</span> tcp <span class="token parameter variable">-j</span> DNAT --to-destination <span class="token number">10.244</span>.2.3:9376
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到这三条链, 其实是<strong>三条 DNAT 规则</strong>. 但在 DNAT 规则之前, iptables 对流入的 IP 包还设置了一个 &quot;标志&quot;(–set-xmark). 这个 &quot;标志&quot; 的作用, 会在下一节讲解. 而 DNAT 规则的作用, 就是在 PREROUTING 检查点之前, 也就是<strong>在路由之前, 将流入 IP 包的目的地址和端口, 改成–to-destination 所指定的新的目的地址和端口</strong>. 可以看到, 这个目的地址和端口, 正是被代理 Pod 的 IP 地址和端口.</p> <p>这样访问 Service VIP 的 IP 包经过上述 iptables 处理之后, 就<strong>已经变成了访问具体某一个后端 Pod 的 IP 包了</strong>. 不难理解, <strong>这些 Endpoints 对应的 iptables 规则, 正是 kube-proxy 通过监听 Pod 的变化事件, 在宿主机上生成并维护的</strong>.</p> <p>以上就是 Service 最基本的工作原理.</p> <p>此外, 你可能已经听说过, Kubernetes 的 kube-proxy 还支持一种叫作 <strong>IPVS 的模式</strong>. 这又是怎么一回事儿呢?</p> <p>通过上面的讲解可以看到, kube-proxy 通过 iptables 处理 Service 的过程, 其实<strong>需要在宿主机上设置相当多的 iptables 规则. 而且 kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的</strong>.</p> <p>不难想到, 当宿主机上有大量 Pod 的时候, 成百上千条 iptables 规则不断地被刷新, 会大量占用该宿主机的 CPU 资源, 甚至会让宿主机 &quot;卡&quot; 在这个过程中. 所以<mark><strong>一直以来, 基于 iptables 的 Service 实现, 都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍</strong></mark>​ <strong>. 而 IPVS 模式的 Service, 就是解决这个问题的一个行之有效的方法</strong>.</p> <p>IPVS 模式的工作原理, 其实跟 iptables 模式类似. 当创建了前面的 Service 之后, kube-proxy 首先会<strong>在宿主机上创建一个虚拟网卡(叫作: kube-ipvs0)</strong> , 并为它分配 Service VIP 作为 IP 地址, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># ip addr</span>
  <span class="token punctuation">..</span>.
  <span class="token number">73</span>: kube-ipvs0: <span class="token operator">&lt;</span>BROADCAST,NOARP<span class="token operator">&gt;</span>  mtu <span class="token number">1500</span> qdisc noop state DOWN qlen <span class="token number">1000</span>
  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
  inet <span class="token number">10.0</span>.1.175/32  scope global kube-ipvs0
  valid_lft forever  preferred_lft forever
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>而接下来, kube-proxy 就会<strong>通过 Linux 的 IPVS 模块, 为这个 IP 地址设置三个 IPVS 虚拟主机, 并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略</strong>. 可以通过 ipvsadm 查看到这个设置, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># ipvsadm -ln</span>
 IP Virtual Server version <span class="token number">1.2</span>.1 <span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token number">4096</span><span class="token punctuation">)</span>
  Prot LocalAddress:Port Scheduler Flags
    -<span class="token operator">&gt;</span>  RemoteAddress:Port           Forward  Weight ActiveConn InActConn   
  TCP  <span class="token number">10.102</span>.128.4:80 rr
    -<span class="token operator">&gt;</span>  <span class="token number">10.244</span>.3.6:9376    Masq    <span class="token number">1</span>       <span class="token number">0</span>          <span class="token number">0</span>       
    -<span class="token operator">&gt;</span>  <span class="token number">10.244</span>.1.7:9376    Masq    <span class="token number">1</span>       <span class="token number">0</span>          <span class="token number">0</span>
    -<span class="token operator">&gt;</span>  <span class="token number">10.244</span>.2.3:9376    Masq    <span class="token number">1</span>       <span class="token number">0</span>          <span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 这三个 IPVS 虚拟主机的 IP 地址和端口, 对应的正是三个被代理的 Pod. 这时候, 任何发往 10.102.128.4:80 的请求, 就都会<strong>被 IPVS 模块转发到某一个后端 Pod 上</strong>了.</p> <p>而相比于 iptables, IPVS 在内核中的实现其实也是<strong>基于 Netfilter 的 NAT 模式</strong>, 所以在转发这一层上, 理论上 IPVS 并没有显著的性能提升. <strong>但是 IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则, 而是把对这些 &quot;规则&quot; 的处理放到了内核态, 从而极大地降低了维护这些规则的代价. 这也正印证了前面提到过的, &quot;将重要操作放入内核态&quot; 是提高性能的重要手段</strong>.</p> <p>不过需要注意的是, IPVS 模块只负责上述的<strong>负载均衡和代理功能</strong>. 而一个完整的 Service 流程正常工作所需要的<strong>包过滤, SNAT</strong> 等操作, 还是要靠 <strong>iptables</strong> 来实现. 只不过这些辅助性的 iptables 规则数量有限, 也不会随着 Pod 数量的增加而增加.</p> <p><strong>所以在大规模集群里, 非常建议为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能. 它为 Kubernetes 集群规模带来的提升还是非常巨大的.</strong></p> <p>**此外, 前面还介绍过 Service 与 DNS 的关系. **</p> <p>在 Kubernetes 中, <strong>Service 和 Pod 都会被分配对应的 DNS A 记录(从域名解析 IP 的记录)</strong> .</p> <p>对于 ClusterIP 模式的 Service 来说(比如上面的例子), 它的 A 记录的格式是:  <strong>..svc.cluster.local</strong>. 当访问这条 A 记录的时候, 它解析到的就是该 Service 的 VIP 地址.</p> <p>而对于指定了 <strong>clusterIP=None</strong> 的 Headless Service 来说, 它的 A 记录的格式也是: <code>..svc.cluster.local</code>​. 但是, 当访问这条 A 记录的时候, 它返回的是<strong>所有被代理的 Pod 的 IP 地址的集合</strong>. 当然, 如果客户端没办法解析这个集合的话, 它可能会只会拿到第一个 Pod 的 IP 地址.</p> <p>此外, 对于 ClusterIP 模式的 Service 来说, 它代理的 Pod 被自动分配的 A 记录的格式是: <code>..pod.cluster.local</code>​. 这条记录指向 Pod 的 IP 地址.</p> <p>而对 <strong>Headless Service</strong> 来说, 它代理的 Pod 被自动分配的 A 记录的格式是: <code>...svc.cluster.local</code>​. 这条记录也指向 Pod 的 IP 地址.</p> <p>但如果为 Pod 指定了 Headless Service, 并且 Pod 本身声明了 hostname 和 subdomain 字段, 那么这时候 Pod 的 A 记录就会变成: <code>&lt;pod 的 hostname&gt;...svc.cluster.local</code>​, 比如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo
    port: <span class="token number">1234</span>
    targetPort: <span class="token number">1234</span>
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox
    command:
      - <span class="token function">sleep</span>
      - <span class="token string">&quot;3600&quot;</span>
    name: busybox
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><p>在上面这个 Service 和 Pod 被创建之后, 就可以通过 <strong>busybox-1.default-subdomain.default.svc.cluster.local</strong> 解析到这个 Pod 的 IP 地址了.</p> <p>需要注意的是, 在 Kubernetes 里,  <strong>/etc/hosts 文件是单独挂载</strong>的, 这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因. 这跟 Docker 的 Init 层是一个原理.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Service 的工作原理. 实际上, Service 机制, 以及 Kubernetes 里的 DNS 插件, 都是在解决同样一个问题, 即: <strong>如何找到我的某一个容器</strong>?</p> <p>这个问题在平台级项目中, 往往就被称作<mark><strong>服务发现</strong></mark>, 即: <strong>当一个服务(Pod)的 IP 地址是不固定的且没办法提前获知时, 该如何通过一个固定的方式访问到这个 Pod 呢</strong>?</p> <p>而在这里讲解的, <mark><strong>ClusterIP 模式的 Service 为你提供的, 就是一个 Pod 的稳定的 IP 地址, 即 VIP. 并且, 这里 Pod 和 Service 的关系是可以通过 Label 确定的</strong></mark>.</p> <p>而 <strong>Headless Service</strong> 为你提供的, 则是<strong>一个 Pod 的稳定的 DNS 名字</strong>, 并且这个名字是可以通过 Pod 名字和 Service 名字拼接出来的.</p> <p>在实际的场景里, 应该根据自己的具体需求进行合理选择.</p> <h4 id="_38-从外界连通service与service调试-三板斧"><a href="#_38-从外界连通service与service调试-三板斧" class="header-anchor">#</a> 38 | 从外界连通Service与Service调试&quot;三板斧&quot;</h4> <p>上一节介绍了 Service 机制的工作原理. 通过这些讲解, 你应该能够明白这样一个事实: <strong>Service 的访问信息在 Kubernetes 集群之外, 其实是无效的</strong>. 这其实也容易理解: <mark><strong>所谓 Service 的访问入口, 其实就是每台宿主机上由 kube-proxy 生成的 iptables 规则, 以及 kube-dns 生成的 DNS 记录. 而一旦离开了这个集群, 这些信息对用户来说, 也就自然没有作用了</strong></mark>.</p> <p>所以在使用 Kubernetes 的 Service 时, 一个必须要面对和解决的问题就是: **如何从外部(Kubernetes 集群之外), 访问到 Kubernetes 里创建的 Service? **</p> <p>这里最常用的一种方式就是: <strong>NodePort</strong>. 举个例子.</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>nginx
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">run</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    <span class="token key atrule">name</span><span class="token punctuation">:</span> http
  <span class="token punctuation">-</span> <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">443</span>
    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    <span class="token key atrule">name</span><span class="token punctuation">:</span> https
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">run</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>在这个 Service 的定义里, 声明它的类型是 type=<strong>NodePort</strong>. 然后在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口, Service 的 443 端口代理 Pod 的 443 端口.</p> <p>当然, 如果不显式地声明 nodePort 字段, Kubernetes 就会分配随机的可用端口来设置代理. 这个端口的范围默认是 30000-32767, 可以通过 kube-apiserver 的–service-node-port-range 参数来修改它.</p> <p>那么这时候, 要访问这个 Service, 只需要访问:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>&lt;任何一台宿主机的 IP 地址<span class="token punctuation">&gt;</span><span class="token punctuation">:</span><span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>就可以访问到<strong>某一个被代理的 Pod 的 80 端口</strong>了.</p> <p>而在理解了上一节讲解的 Service 的工作原理之后, NodePort 模式也就非常容易理解了. 显然, kube-proxy 要做的, 就是<strong>在每台宿主机上生成这样一条 iptables 规则</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token parameter variable">-A</span> KUBE-NODEPORTS <span class="token parameter variable">-p</span> tcp <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;default/my-nginx: nodePort&quot;</span> <span class="token parameter variable">-m</span> tcp <span class="token parameter variable">--dport</span> <span class="token number">8080</span> <span class="token parameter variable">-j</span> KUBE-SVC-67RL4FN6JRUPOJYM
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>而前面已经讲到, KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组<strong>随机模式的 iptables 规则</strong>. 所以接下来的流程, 就跟 ClusterIP 模式完全一样了.</p> <p>需要注意的是, 在 NodePort 方式下, Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时, 对这个 IP 包做一次 SNAT 操作, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token parameter variable">-A</span> KUBE-POSTROUTING <span class="token parameter variable">-m</span> comment <span class="token parameter variable">--comment</span> <span class="token string">&quot;kubernetes service traffic requiring SNAT&quot;</span> <span class="token parameter variable">-m</span> mark <span class="token parameter variable">--mark</span> 0x4000/0x4000 <span class="token parameter variable">-j</span> MASQUERADE
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到, 这条规则<strong>设置在 POSTROUTING 检查点</strong>, 也就是说, 它<strong>给即将离开这台主机的 IP 包, 进行了一次 SNAT 操作, 将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址, 或者宿主机本身的 IP 地址</strong>(如果 CNI 网桥不存在的话).</p> <p>当然这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行(否则普通的 IP 包就被影响了). 而 iptables 做这个判断的依据, 就是查看该 IP 包是否有一个 &quot;0x4000&quot; 的 &quot;标志&quot;. 前面提到过, 这个标志正是<strong>在 IP 包被执行 DNAT 操作之前被打上去</strong>的.</p> <p>可是<strong>为什么一定要对流出的包做 SNAT</strong>​******操作****<strong>​</strong>呢? ** 这里的原理其实很简单, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>           client
             <span class="token punctuation">\</span> ^
              <span class="token punctuation">\</span> <span class="token punctuation">\</span>
               <span class="token function">v</span> <span class="token punctuation">\</span>
   <span class="token function">node</span> <span class="token number">1</span> <span class="token operator">&lt;</span>--- <span class="token function">node</span> <span class="token number">2</span>
    <span class="token operator">|</span> ^   SNAT
    <span class="token operator">|</span> <span class="token operator">|</span>   ---<span class="token operator">&gt;</span>
    <span class="token function">v</span> <span class="token operator">|</span>
 endpoint
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>当一个外部的 client 通过 <strong>node 2 的地址访问一个 Service</strong> 的时候, node 2 上的负载均衡规则, 就可能把这个 IP 包转发给一个在 node 1 上的 Pod. 这里没有任何问题. 而当 node 1 上的这个 Pod 处理完请求之后, 它就会按照这个 IP 包的源地址发出回复.</p> <p>可是, 如果没有做 SNAT 操作的话, 这时候被转发来的 IP 包的源地址就是 client 的 IP 地址. 所以此时, Pod 就会直接将回复发给 **** **** client. 对于 client 来说, 它的请求明明发给了 node 2, 收到的回复却来自 node 1, 这个 client 很可能会报错.</p> <p>所以在上图中, 当 IP 包离开 node 2 之后, <strong>它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址</strong>. 这样 Pod 在处理完成之后就会先回复给 node 2(而不是 client), 然后再由 node 2 发送给 client.</p> <p>当然这也就意味着这个 Pod 只知道该 IP 包来自于 node 2, 而不是外部的 client. 对于 Pod 需要明确知道所有请求来源的场景来说, 这是不可以的.</p> <p>所以这时候, 就可以将 Service 的 <strong>spec.externalTrafficPolicy 字段设置为 local</strong>, 这就<strong>保证了所有 Pod 通过 Service 收到请求之后, 一定可以看到真正的, 外部 client 的源地址</strong>.</p> <p>而这个机制的实现原理也非常简单: <mark><strong>这时候一台宿主机上的 iptables 规则, 会设置为只将 IP 包转发给运行在这台宿主机上的 Pod</strong></mark>. 所以这时候, Pod 就可以<strong>直接使用源地址将回复包发出, 不需要事先进行 SNAT</strong> 了. 这个流程如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>       client
       ^ /   <span class="token punctuation">\</span>
      / /     <span class="token punctuation">\</span>
     / <span class="token function">v</span>       X
   <span class="token function">node</span> <span class="token number">1</span>     <span class="token function">node</span> <span class="token number">2</span>
    ^ <span class="token operator">|</span>
    <span class="token operator">|</span> <span class="token operator">|</span>
    <span class="token operator">|</span> <span class="token function">v</span>
 endpoint
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>当然这也就意味着如果在一台宿主机上, <strong>没有任何一个被代理的 Pod 存在</strong>, 比如上图中的 node 2, 那么使用 node 2 的 IP 地址访问这个 Service, 就是无效的. 此时请求会直接被 DROP 掉.</p> <p>从<strong>外部访问 Service 的第二种方式, 适用于公有云上的 Kubernetes 服务</strong>. 这时候可以指定一个 <strong>LoadBalancer 类型</strong>的 Service, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">---</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>service
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8765</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">9376</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> example
  <span class="token key atrule">type</span><span class="token punctuation">:</span> LoadBalancer
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>在公有云提供的 Kubernetes 服务里, 都使用了一个叫作 <strong>CloudProvider</strong> 的转接层, 来跟<strong>公有云本身的 API 进行对接</strong>. 所以在上述 LoadBalancer 类型的 Service 被提交后, <strong>Kubernetes 就会调用 CloudProvider 在公有云上创建一个负载均衡服务, 并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端</strong>.</p> <p>而第三种方式, 是 Kubernetes 在 1.7 之后支持的一个新特性, 叫作 <strong>ExternalName</strong>. 举个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>service
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> ExternalName
  <span class="token key atrule">externalName</span><span class="token punctuation">:</span> my.database.example.com
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>在上述 Service 的 YAML 文件中, 指定了一个 externalName=my.database.example.com 的字段. 可以看到这个 YAML 文件里不需要指定 selector.</p> <p>这时候, 当通过 Service 的 DNS 名字访问它的时候, 比如访问: my-service.default.svc.cluster.local. 那么, Kubernetes 返回的就是 <code>my.database.example.com</code>​. 所以 ExternalName 类型的 Service, 其实是<strong>在 kube-dns 里添加了一条 CNAME 记录</strong>. 这时访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了.</p> <p>此外 Kubernetes 的 Service 还可以为 Service 分配公有 IP 地址, 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>service
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> MyApp
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">9376</span>
  <span class="token key atrule">externalIPs</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> 80.11.12.10
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>在上述 Service 中, 为它指定的 externalIPs=80.11.12.10, 那么此时就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了. 不过在这里 Kubernetes 要求 externalIPs <strong>必须是至少能够路由到一个 Kubernetes 的节点</strong>. 可以想一想这是为什么.</p> <p>在理解了 Kubernetes Service 机制的工作原理之后, 很多与 Service 相关的问题, 其实都可以<strong>通过分析 Service 在宿主机上对应的 iptables 规则(或者 IPVS 配置)得到解决</strong>.</p> <p>比如, 当你的 Service 没办法通过 DNS 访问到的时候. 就需要<strong>区分到底是 Service 本身的配置问题, 还是集群的 DNS 出了问题. 一个行之有效的方法, 就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 在一个 Pod 里执行</span>
$ <span class="token function">nslookup</span> kubernetes.default
Server:    <span class="token number">10.0</span>.0.10
Address <span class="token number">1</span>: <span class="token number">10.0</span>.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address <span class="token number">1</span>: <span class="token number">10.0</span>.0.1 kubernetes.default.svc.cluster.local
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>如果上面访问 kubernetes.default 返回的值都有问题, 那就需要检查 kube-dns 的运行状态和日志了. 否则的话, 应该去检查自己的 Service 定义是不是有问题.</p> <p>而如果你的 Service 没办法通过 <strong>ClusterIP</strong> 访问到的时候, 首先应该检查的是这个 Service 是否有 <strong>Endpoints</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get endpoints hostnames
NAME        ENDPOINTS
hostnames   <span class="token number">10.244</span>.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>需要注意的是, 如果你的 Pod 的 readniessProbe 没通过, 它也不会出现在 Endpoints 列表里. 而如果 Endpoints 正常, 那么就需要<strong>确认 kube-proxy 是否在正确运行</strong>. 在通过 kubeadm 部署的集群里, 应该看到 kube-proxy 输出的日志如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>I1027 <span class="token number">22</span>:14:53.995134    <span class="token number">5063</span> server.go:200<span class="token punctuation">]</span> Running <span class="token keyword">in</span> resource-only container <span class="token string">&quot;/kube-proxy&quot;</span>
I1027 <span class="token number">22</span>:14:53.998163    <span class="token number">5063</span> server.go:247<span class="token punctuation">]</span> Using iptables Proxier.
I1027 <span class="token number">22</span>:14:53.999055    <span class="token number">5063</span> server.go:255<span class="token punctuation">]</span> Tearing down userspace rules. Errors here are acceptable.
I1027 <span class="token number">22</span>:14:54.038140    <span class="token number">5063</span> proxier.go:352<span class="token punctuation">]</span> Setting endpoints <span class="token keyword">for</span> <span class="token string">&quot;kube-system/kube-dns:dns-tcp&quot;</span> to <span class="token punctuation">[</span><span class="token number">10.244</span>.1.3:53<span class="token punctuation">]</span>
I1027 <span class="token number">22</span>:14:54.038164    <span class="token number">5063</span> proxier.go:352<span class="token punctuation">]</span> Setting endpoints <span class="token keyword">for</span> <span class="token string">&quot;kube-system/kube-dns:dns&quot;</span> to <span class="token punctuation">[</span><span class="token number">10.244</span>.1.3:53<span class="token punctuation">]</span>
I1027 <span class="token number">22</span>:14:54.038209    <span class="token number">5063</span> proxier.go:352<span class="token punctuation">]</span> Setting endpoints <span class="token keyword">for</span> <span class="token string">&quot;default/kubernetes:https&quot;</span> to <span class="token punctuation">[</span><span class="token number">10.240</span>.0.2:443<span class="token punctuation">]</span>
I1027 <span class="token number">22</span>:14:54.038238    <span class="token number">5063</span> proxier.go:429<span class="token punctuation">]</span> Not syncing iptables <span class="token keyword">until</span> Services and Endpoints have been received from master
I1027 <span class="token number">22</span>:14:54.040048    <span class="token number">5063</span> proxier.go:294<span class="token punctuation">]</span> Adding new <span class="token function">service</span> <span class="token string">&quot;default/kubernetes:https&quot;</span> at <span class="token number">10.0</span>.0.1:443/TCP
I1027 <span class="token number">22</span>:14:54.040154    <span class="token number">5063</span> proxier.go:294<span class="token punctuation">]</span> Adding new <span class="token function">service</span> <span class="token string">&quot;kube-system/kube-dns:dns&quot;</span> at <span class="token number">10.0</span>.0.10:53/UDP
I1027 <span class="token number">22</span>:14:54.040223    <span class="token number">5063</span> proxier.go:294<span class="token punctuation">]</span> Adding new <span class="token function">service</span> <span class="token string">&quot;kube-system/kube-dns:dns-tcp&quot;</span> at <span class="token number">10.0</span>.0.10:53/TCP
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>如果 kube-proxy 一切正常, 就应该仔细查看宿主机上的 <strong>iptables</strong> 了. 而**一个 iptables 模式的 Service 对应的规则, 之前已经全部介绍到了, 它们包括: **</p> <ol><li>KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链, 这个规则应该与 VIP 和 Service 端口一一对应;</li> <li>KUBE-SEP-(hash) 规则对应的 DNAT 链, 这些规则应该与 Endpoints 一一对应;</li> <li>KUBE-SVC-(hash) 规则对应的负载均衡链, 这些规则的数目应该与 Endpoints 数目一致;</li> <li>如果是 NodePort 模式的话, 还有 POSTROUTING 处的 SNAT 链.</li></ol> <p>通过查看这些<strong>链的数量, 转发目的地址, 端口, 过滤条件等信息</strong>, 就能很容易发现一些异常的蛛丝马迹.</p> <p>当然, <strong>还有一种典型问题, 就是 Pod 没办法通过 Service 访问到自己</strong>. 这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置. 关于 Hairpin 的原理前面已经介绍过. 只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可.</p> <p>其中, 在 hairpin-veth 模式下, 应该能看到 CNI 网桥对应的各个 VETH 设备, 都将 Hairpin 模式设置为了 1, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token keyword">for</span> <span class="token for-or-select variable">d</span> <span class="token keyword">in</span> /sys/devices/virtual/net/cni0/brif/veth*/hairpin_mode<span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$d</span> = <span class="token variable"><span class="token variable">$(</span><span class="token function">cat</span> $d<span class="token variable">)</span></span>&quot;</span><span class="token punctuation">;</span> <span class="token keyword">done</span>
/sys/devices/virtual/net/cni0/brif/veth4bfbfe74/hairpin_mode <span class="token operator">=</span> <span class="token number">1</span>
/sys/devices/virtual/net/cni0/brif/vethfc2a18c5/hairpin_mode <span class="token operator">=</span> <span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>而如果是 promiscuous-bridge 模式的话, 应该看到 CNI 网桥的混杂模式(PROMISC)被开启, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">ifconfig</span> cni0 <span class="token operator">|</span><span class="token function">grep</span> PROMISC
UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1460  Metric:1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>总结</p></blockquote> <p>本节讲解了<mark><strong>从外部访问 Service 的三种方式(NodePort, LoadBalancer 和 External Name)和具体的工作原理</strong></mark>. 然后还讲述了当 Service 出现故障的时候, 如何根据它的工作原理, 按照一定的思路去定位问题的可行之道.</p> <p>通过上述讲解不难看出, <mark><strong>所谓 Service, 其实就是 Kubernetes 为 Pod 分配的, 固定的, 基于 iptables(或者 IPVS)的访问入口. 而这些访问入口代理的 Pod 信息, 则来自于 Etcd, 由 kube-proxy 通过控制循环来维护</strong></mark>.</p> <p>并且可以看到, Kubernetes 里面的 Service 和 DNS 机制, 也都不具备强多租户能力. 比如在多租户情况下, 每个租户应该拥有一套独立的 Service 规则(Service 只应该看到和代理同一个租户下的 Pod). 再比如 DNS, 在多租户情况下, 每个租户应该拥有自己的 kube-dns(kube-dns 只应该为同一个租户下的 Service 和 Pod 创建 DNS Entry).</p> <p>当然, 在 Kubernetes 中, kube-proxy 和 kube-dns 其实也是普通的<strong>插件</strong>而已. 你完全可以根据自己的需求, 实现符合自己预期的 Service.</p> <h4 id="_39-谈谈service与ingress"><a href="#_39-谈谈service与ingress" class="header-anchor">#</a> 39 | 谈谈Service与Ingress</h4> <p>上一节讲解了<strong>将 Service 暴露给外界的三种方法</strong>. 其中有一个叫作 <strong>LoadBalancer</strong> 类型的 Service, 它会在 Cloud Provider(比如: Google Cloud 或者 OpenStack)里<strong>创建一个与该 Service 对应的负载均衡服务</strong>.</p> <p>相信你也应该能感受到, 由于每个 Service 都要有一个负载均衡服务, 所以这个做法实际上<strong>既浪费成本又高</strong>. 作为用户, 其实更希望看到 Kubernetes 为我内置一个<strong>全局的负载均衡器</strong>. 然后通过访问的 URL, 把请求转发给不同的后端 Service.</p> <p><mark>**这种全局的, 为了代理不同后端 Service 而设置的负载均衡服务, 就是 Kubernetes 里的 Ingress 服务. **</mark></p> <p>所以 Ingress 的功能其实很容易理解: **所谓 Ingress, 就是 Service 的 &quot;Service&quot;. **</p> <p>举个例子, 假如现在有这样一个站点: <code>https://cafe.example.com</code>​. 其中 <code>https://cafe.example.com/coffee</code>​, 对应的是 &quot;咖啡点餐系统&quot;. 而 <code>https://cafe.example.com/tea</code>​, 对应的则是 &quot;茶水点餐系统&quot;. 这两个系统, 分别由名叫 coffee 和 tea 这样两个 <strong>Deployment</strong> 来提供服务. 那如何能使用 Kubernetes 的 <strong>Ingress 来创建一个统一的负载均衡器, 从而实现当用户访问不同的域名时, 能够访问到不同的 Deployment 呢</strong>?</p> <p>上述功能, 在 Kubernetes 里就需要通过 Ingress 对象来描述, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> cafe<span class="token punctuation">-</span>ingress
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">tls</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">hosts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> cafe.example.com
    <span class="token key atrule">secretName</span><span class="token punctuation">:</span> cafe<span class="token punctuation">-</span>secret
  <span class="token key atrule">rules</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> cafe.example.com
    <span class="token key atrule">http</span><span class="token punctuation">:</span>
      <span class="token key atrule">paths</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /tea
        <span class="token key atrule">backend</span><span class="token punctuation">:</span>
          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> tea<span class="token punctuation">-</span>svc
          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /coffee
        <span class="token key atrule">backend</span><span class="token punctuation">:</span>
          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> coffee<span class="token punctuation">-</span>svc
          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>在上面这个名叫 cafe-ingress.yaml 文件中, 最值得关注的, 是 <strong>rules 字段</strong>. 在 Kubernetes 里, 这个字段叫作: <strong>IngressRule</strong>.</p> <p>IngressRule 的 Key, 就叫做: <strong>host</strong>. 它必须是一个<strong>标准的域名格式</strong>(Fully Qualified Domain Name)的字符串, 而不能是 IP 地址.</p> <p><strong>而 host 字段定义的值, 就是这个 Ingress 的入口</strong>. 这也就意味着, 当用户访问 cafe.example.com 的时候, <strong>实际上访问到的是这个 Ingress 对象. 这样 Kubernetes 就能使用 IngressRule 来对请求进行下一步转发</strong>.</p> <p>而接下来 IngressRule 规则的定义, 则<strong>依赖于 path 字段</strong>. 可以简单地理解为, 这里的<strong>每一个 path 都对应一个后端 Service</strong>. 所以在这个例子里, 定义了两个 path, 它们分别对应 coffee 和 tea 这两个 <strong>Deployment 的 Service</strong>(即: coffee-svc 和 tea-svc).</p> <p>**通过上面的讲解可以看到, <strong>​<mark><strong>所谓 Ingress 对象, 其实就是 Kubernetes 项目对 &quot;反向代理&quot; 的一种抽象</strong></mark>​ <strong>. ** 一个 Ingress 对象的主要内容, 实际上就是一个 &quot;<strong>反向代理</strong>&quot; 服务(比如: Nginx)的配置文件的描述. 而这个代理服务对应的</strong>转发规则</strong>, 就是 <strong>IngressRule</strong>.</p> <p>这就是为什么<strong>在每条 IngressRule 里, 需要有一个 host 字段来作为这条 IngressRule 的入口, 然后还需要有一系列 path 字段来声明具体的转发策略</strong>. 这其实跟 Nginx, HAproxy 等项目的配置文件的写法是一致的.</p> <p>而有了 Ingress 这样一个统一的抽象, Kubernetes 的用户就无需关心 Ingress 的具体细节了.</p> <p>在实际的使用中, 只需要从社区里选择一个具体的 <strong>Ingress Controller, 把它部署在 Kubernetes 集群</strong>里即可. 然后这个 Ingress Controller 会<strong>根据定义的 Ingress 对象, 提供对应的代理能力</strong>. 目前业界常用的各种<strong>反向代理项目</strong>, 比如 Nginx, HAProxy, Envoy, Traefik 等, 都已经为 Kubernetes 专门维护了对应的 Ingress Controller.</p> <p>接下来就以最常用的 <strong>Nginx Ingress Controller</strong> 为例, 在前面用 kubeadm 部署的 Bare-metal 环境中, 实践一下 Ingress 机制的使用过程.</p> <p>部署 Nginx Ingress Controller 的方法非常简单, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中, 在 <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml" target="_blank" rel="noopener noreferrer">mandatory.yaml<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 这个文件里, 正是 Nginx 官方维护的 Ingress Controller 的定义. 来看一下它的内容:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>configuration
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
    <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
<span class="token punctuation">---</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>controller
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
    <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
      <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
        <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
      <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
        <span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>serviceaccount
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>controller
          <span class="token key atrule">image</span><span class="token punctuation">:</span> quay.io/kubernetes<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>controller/nginx<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>controller<span class="token punctuation">:</span>0.20.0
          <span class="token key atrule">args</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> /nginx<span class="token punctuation">-</span>ingress<span class="token punctuation">-</span>controller
            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>configmap=$(POD_NAMESPACE)/nginx<span class="token punctuation">-</span>configuration
            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>publish<span class="token punctuation">-</span>service=$(POD_NAMESPACE)/ingress<span class="token punctuation">-</span>nginx
            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>annotations<span class="token punctuation">-</span>prefix=nginx.ingress.kubernetes.io
          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>
            <span class="token key atrule">capabilities</span><span class="token punctuation">:</span>
              <span class="token key atrule">drop</span><span class="token punctuation">:</span>
                <span class="token punctuation">-</span> ALL
              <span class="token key atrule">add</span><span class="token punctuation">:</span>
                <span class="token punctuation">-</span> NET_BIND_SERVICE
            <span class="token comment"># www-data -&gt; 33</span>
            <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">33</span>
          <span class="token key atrule">env</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> POD_NAME
              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.name
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> POD_NAMESPACE
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.namespace
          <span class="token key atrule">ports</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
              <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https
              <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">443</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br></div></div><p>可以看到, 在上述 YAML 文件中, 定义了一个<strong>使用 nginx-ingress-controller 镜像的 Pod</strong>. 需要注意的是, 这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数. 而这个信息, 当然是通过 Downward API 拿到的, 即: <strong>Pod 的 env 字段里的定义</strong>(env.valueFrom.fieldRef.fieldPath).</p> <p>**而这个 Pod 本身, 就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器. **</p> <p>**当一个新的 Ingress 对象由用户创建后, nginx-ingress-controller 就会根据 Ingress 对象里定义的内容, 生成一份对应的 Nginx 配置文件(/etc/nginx/nginx.conf), 并使用这个配置文件启动一个 Nginx 服务. **</p> <p>而一旦 Ingress 对象被更新, nginx-ingress-controller 就会<strong>更新</strong>这个配置文件. 需要注意的是, 如果这里只是被代理的 Service 对象被更新, nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载(reload)的. 这当然是因为 nginx-ingress-controller 通过 <a href="https://github.com/openresty/lua-nginx-module" target="_blank" rel="noopener noreferrer">Nginx Lua<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 方案实现了 Nginx Upstream 的<strong>动态配置</strong>.</p> <p>此外, nginx-ingress-controller 还可以通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制. 这个 ConfigMap 的名字, 需要以参数的方式传递给 nginx-ingress-controller. 而在这个 ConfigMap 里添加的字段, 将会被合并到最后生成的 Nginx 配置文件当中.</p> <p><mark>**可以看到, 一个 Nginx Ingress Controller 为你提供的服务, 其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化, 来自动进行更新的 Nginx 负载均衡器. **</mark></p> <p>当然为了让用户能够用到这个 Nginx, 就需要<strong>创建一个 Service</strong> 来把 Nginx Ingress Controller 管理的 Nginx 服务暴露出去, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>由于使用的是 Bare-metal 环境, 所以 service-nodeport.yaml 文件里的内容, 就是一个 <strong>NodePort</strong> 类型的 Service, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
    <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https
      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>
      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">443</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app.kubernetes.io/name</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
    <span class="token key atrule">app.kubernetes.io/part-of</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>可以看到, 这个 Service 的<strong>唯一工作, 就是将所有携带 ingress-nginx 标签的 Pod 的 80 和 433 端口暴露出去</strong>. 而如果是公有云上的环境, 需要创建的就是 <strong>LoadBalancer</strong> 类型的 Service 了.</p> <p>**上述操作完成后, **​<mark><strong>一定要记录下这个 Service 的访问入口, 即: 宿主机的地址和 NodePort 的端口</strong></mark>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc <span class="token parameter variable">-n</span> ingress-nginx
NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>                      AGE
ingress-nginx   NodePort   <span class="token number">10.105</span>.72.96   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">80</span>:30044/TCP,443:31453/TCP   3h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>为了后面方便使用, 会把上述访问入口设置为<strong>环境变量</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token assign-left variable">IC_IP</span><span class="token operator">=</span><span class="token number">10.168</span>.0.2 <span class="token comment"># 任意一台宿主机的地址</span>
$ <span class="token assign-left variable">IC_HTTPS_PORT</span><span class="token operator">=</span><span class="token number">31453</span> <span class="token comment"># NodePort 端口</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在 Ingress Controller 和它所需要的 Service 部署完成后, 就可以使用它了. 备注: 这个 &quot;咖啡厅&quot; Ingress 的所有示例文件, 都在<a href="https://github.com/resouer/kubernetes-ingress/tree/master/examples/complete-example" target="_blank" rel="noopener noreferrer">这里<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>首先要在集群里部署应用 Pod 和它们对应的 Service, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> cafe.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后需要<strong>创建 Ingress 所需的 SSL 证书(tls.crt)和密钥(tls.key), 这些信息都是通过 Secret 对象定义好的</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> cafe-secret.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这一步完成后, 就可以创建在本节一开始定义的 Ingress 对象了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> cafe-ingress.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这时候就可以查看一下这个 Ingress 对象的信息, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get ingress
NAME           HOSTS              ADDRESS   PORTS     AGE
cafe-ingress   cafe.example.com             <span class="token number">80</span>, <span class="token number">443</span>   2h

$ kubectl describe ingress cafe-ingress
Name:             cafe-ingress
Namespace:        default
Address:        
Default backend:  default-http-backend:80 <span class="token punctuation">(</span><span class="token operator">&lt;</span>none<span class="token operator">&gt;</span><span class="token punctuation">)</span>
TLS:
  cafe-secret terminates cafe.example.com
Rules:
  Host              Path  Backends
  ----              ----  --------
  cafe.example.com  
                    /tea      tea-svc:80 <span class="token punctuation">(</span><span class="token operator">&lt;</span>none<span class="token operator">&gt;</span><span class="token punctuation">)</span>
                    /coffee   coffee-svc:80 <span class="token punctuation">(</span><span class="token operator">&lt;</span>none<span class="token operator">&gt;</span><span class="token punctuation">)</span>
Annotations:
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  CREATE  4m    nginx-ingress-controller  Ingress default/cafe-ingress
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>可以看到, 这个 Ingress 对象最核心的部分, 正是 <strong>Rules 字段</strong>. 其中定义的 Host 是 <code>cafe.example.com</code>​, 它有两条转发规则(Path), <strong>分别转发给 tea-svc 和 coffee-svc</strong>.</p> <p>当然, 在 Ingress 的 YAML 文件里, 还可以定义多个 Host, 比如 <code>restaurant.example.com</code>​, <code>movie.example.com</code>​ 等等, 来为更多的<strong>域名</strong>提供负载均衡服务.</p> <p>接下来就可以通过访问这个 Ingress 的地址和端口, 访问到前面部署的应用了, 比如, 当访问 <code>https://cafe.example.com:443/coffee</code>​ 时, 应该是 coffee 这个 <strong>Deployment</strong> 负责响应请求. 可以来尝试一下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">--resolve</span> cafe.example.com:<span class="token variable">$IC_HTTPS_PORT</span><span class="token builtin class-name">:</span><span class="token variable">$IC_IP</span> https://cafe.example.com:<span class="token variable">$IC_HTTPS_PORT</span>/coffee <span class="token parameter variable">--insecureServer</span> address: <span class="token number">10.244</span>.1.56:80
Server name: coffee-7dbb5795f6-vglbv
Date: 03/Nov/2018:03:55:32 +0000
URI: /coffee
Request ID: e487e672673195c573147134167cf898
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看到, 访问这个 URL 得到的返回信息是: Server name: coffee-7dbb5795f6-vglbv. 这正是 coffee 这个 Deployment 的名字.</p> <p>而当访问 <code>https://cafe.example.com:433/tea</code>​ 的时候, 则应该是 tea 这个 Deployment 负责响应请求(Server name: tea-7d57856c44-lwbnp), 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">--resolve</span> cafe.example.com:<span class="token variable">$IC_HTTPS_PORT</span><span class="token builtin class-name">:</span><span class="token variable">$IC_IP</span> https://cafe.example.com:<span class="token variable">$IC_HTTPS_PORT</span>/tea <span class="token parameter variable">--insecure</span>
Server address: <span class="token number">10.244</span>.1.58:80
Server name: tea-7d57856c44-lwbnp
Date: 03/Nov/2018:03:55:52 +0000
URI: /tea
Request ID: 32191f7ea07cb6bb44a1f43b8299415c
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, <strong>Nginx Ingress Controller 创建的 Nginx 负载均衡器, 已经成功地将请求转发给了对应的后端 Service</strong>.</p> <p>以上就是 Kubernetes 里 Ingress 的设计思想和使用方法了.</p> <p>不过你可能会有一个疑问, **如果请求没有匹配到任何一条 IngressRule, 那么会发生什么呢? **</p> <p>首先, 既然 Nginx Ingress Controller 是用 Nginx 实现的, 那么它当然会<strong>返回一个 Nginx 的 404 页面</strong>.</p> <p>不过, Ingress Controller 也可以通过 Pod 启动命令里的 –default-backend-service 参数, 设置一条<strong>默认规则</strong>, 比如: –default-backend-service=nginx-default-backend.</p> <p>这样任何匹配失败的请求, 就都会被转发到这个名叫 nginx-default-backend 的 Service. 所以就可以通过部署一个专门的 Pod, 来为用户返回<strong>自定义的 404 页面</strong>了.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Ingress 这个概念在 Kubernetes 里到底是怎么一回事儿. <mark><strong>Ingress 实际上就是 Kubernetes 对 &quot;反向代理&quot; 的抽象</strong></mark>.</p> <p><mark><strong>目前, Ingress 只能工作在七层, 而 Service 只能工作在四层</strong></mark>. 所以当想要在 Kubernetes 里为应用进行 TLS 配置等 HTTP 相关的操作时, 都必须通过 <strong>Ingress</strong> 来进行.</p> <p>当然, 正如同很多负载均衡项目可以同时提供七层和四层代理一样, 将来 Ingress 的进化中, 也会加入四层代理的能力. 这样一个比较完善的 &quot;反向代理&quot; 机制就比较成熟了.</p> <p>而 Kubernetes 提出 Ingress 概念的原因其实也非常容易理解, <strong>有了 Ingress 这个抽象, 用户就可以根据自己的需求来自由选择 Ingress Controller</strong>. 比如, 如果你的应用<strong>对代理服务的中断非常敏感, 那么你就应该考虑选择类似于 Traefik 这样支持 &quot;热加载&quot; 的 Ingress Controller 实现</strong>.</p> <p>更重要的是, 一旦你对社区里现有的 Ingress 方案感到不满意, 或者你已经有了自己的负载均衡方案时, 只需要做很少的编程工作, 就可以实现一个自己的 Ingress Controller. 在生产环境中, Ingress 带来的灵活度和自由度, 对于使用容器的用户来说, 其实是非常有意义的. 要知道当年在 Cloud Foundry 项目里, 不知道有多少人为了给 Gorouter 组件配置一个 TLS 而伤透了脑筋.</p> <h3 id="kubernetes作业调度与资源管理"><a href="#kubernetes作业调度与资源管理" class="header-anchor">#</a> Kubernetes作业调度与资源管理</h3> <h4 id="_40-kubernetes的资源模型与资源管理"><a href="#_40-kubernetes的资源模型与资源管理" class="header-anchor">#</a> 40 | Kubernetes的资源模型与资源管理</h4> <p>作为一个容器集群编排与管理项目, Kubernetes 为用户提供的基础设施能力, 不仅包括了前面讲述的应用定义和描述的部分, 还包括了<strong>对应用的资源管理和调度的处理</strong>. 本节开始就来详细讲解一下后面这部分内容.</p> <p>而作为 Kubernetes 的资源管理与调度部分的基础, 要从它的<strong>资源模型</strong>开始说起.</p> <p>前面已经提到过, 在 Kubernetes 里, <strong>Pod 是最小的原子调度单位. 这也就意味着所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段</strong>. 而这其中最重要的部分, 就是 Pod 的 <strong>CPU 和内存配置</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> db
    <span class="token key atrule">image</span><span class="token punctuation">:</span> mysql
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> MYSQL_ROOT_PASSWORD
      <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;password&quot;</span>
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;64Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;250m&quot;</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;128Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;500m&quot;</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> wp
    <span class="token key atrule">image</span><span class="token punctuation">:</span> wordpress
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;64Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;250m&quot;</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;128Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;500m&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p>在 Kubernetes 中, 像 CPU 这样的资源被称作 &quot;<strong>可压缩资源</strong>&quot;(compressible resources). 它的典型特点是, <strong>当可压缩资源不足时, Pod 只会 &quot;饥饿&quot;, 但不会退出</strong>. 而像<strong>内存</strong>这样的资源, 则被称作 &quot;<strong>不可压缩资源</strong>(incompressible resources). <strong>当不可压缩资源不足时, Pod 就会因为 OOM(Out-Of-Memory)被内核杀掉</strong>.</p> <p>而由于 Pod 可以由多个 Container 组成, 所以 CPU 和内存资源的限额, 是要配置在<strong>每个 Container</strong> 的定义上的. 这样 Pod 整体的资源配置, 就由这些 Container 的配置值<strong>累加</strong>得到.</p> <p>其中 Kubernetes 里为 CPU 设置的单位是 &quot;<strong>CPU 的个数</strong>&quot;. 比如 cpu=1 指的就是, 这个 Pod 的 CPU 限额是 1 个 CPU. 当然, 具体 &quot;1 个 CPU&quot; 在宿主机上如何解释, 是 1 个 CPU 核心, 还是 1 个 vCPU, 还是 1 个 CPU 的超线程(Hyperthread), 完全<strong>取决于宿主机的 CPU 实现方式</strong>. <strong>Kubernetes 只负责保证 Pod 能够使用到 &quot;1 个 CPU&quot; 的计算能力</strong>.</p> <p>此外, Kubernetes 允许将 CPU 限额设置为<strong>分数</strong>, 比如在上面的例子里, CPU limits 的值就是 500m. 所谓 500m, 指的就是 500 millicpu, 也就是 0.5 个 CPU 的意思. 这样这个 Pod 就会被分配到 1 个 CPU <strong>一半</strong>的计算能力.</p> <p>当然**也可以直接把这个配置写成 cpu=0.5. 但在实际使用时, 还是推荐使用 500m 的写法, 毕竟这才是 Kubernetes 内部通用的 CPU 表示方式. **</p> <p>而对于<strong>内存</strong>资源来说, 它的单位自然就是 <strong>bytes</strong>. Kubernetes 支持使用 Ei, Pi, Ti, Gi, Mi, Ki(或者 E, P, T, G, M, K)的方式来作为 bytes 的值. 比如在这个例子里, Memory requests 的值就是 64MiB (2 的 26 次方 bytes). 这里要注意区分 MiB(mebibyte)和 MB(megabyte)的区别. 备注: 1Mi=1024*1024; 1M=1000*1000.</p> <p>此外不难看到, <strong>Kubernetes 里 Pod 的 CPU 和内存资源, 实际上还要分为 limits 和 requests 两种情况</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>spec.containers<span class="token punctuation">[</span><span class="token punctuation">]</span>.resources.limits.cpu
spec.containers<span class="token punctuation">[</span><span class="token punctuation">]</span>.resources.limits.memory
spec.containers<span class="token punctuation">[</span><span class="token punctuation">]</span>.resources.requests.cpu
spec.containers<span class="token punctuation">[</span><span class="token punctuation">]</span>.resources.requests.memory
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这两者的区别其实非常简单: 在调度的时候, <strong>kube-scheduler 只会按照 requests 的值进行计算. 而在真正设置 Cgroups 限制的时候, kubelet 则会按照 limits 的值来进行设置</strong>.</p> <p>更确切地说, 当指定了 requests.cpu=250m 之后, 相当于将 Cgroups 的 cpu.shares 的值设置为 (250/1000)*1024. 而当没有设置 requests.cpu 的时候, cpu.shares 默认则是 1024. 这样 Kubernetes 就<strong>通过 cpu.shares 完成了对 CPU 时间的按比例分配</strong>.</p> <p>而如果指定了 limits.cpu=500m 之后, 则相当于将 Cgroups 的 cpu.cfs_quota_us 的值设置为 (500/1000)*100ms, 而 cpu.cfs_period_us 的值始终是 100ms. 这样 Kubernetes 就为设置了这个容器只能用到 CPU 的 50%.</p> <p>而对于<strong>内存</strong>来说, 当指定了 limits.memory=128Mi 之后, 相当于将 Cgroups 的 memory.limit_in_bytes 设置为 128 * 1024 * 1024. 而需要注意的是, 在调度的时候, 调度器只会使用 requests.memory=64Mi 来进行判断.</p> <p><mark><strong>Kubernetes 这种对 CPU 和内存资源限额的设计, 实际上参考了 Borg 论文中对 &quot;动态资源边界&quot; 的定义</strong></mark>, 既: <strong>容器化作业在提交时所设置的资源边界, 并不一定是调度系统所必须严格遵守的, 这是因为在实际场景中, 大多数作业使用到的资源其实远小于它所请求的资源限额</strong>.</p> <p>基于这种假设, Borg 在作业被提交后, 会主动减小它的资源限额配置, 以便容纳更多的作业, 提升资源利用率. 而当作业资源使用量增加到一定阈值时, Borg 会通过 &quot;快速恢复&quot; 过程, 还原作业原始的资源限额, 防止出现异常情况.</p> <p>而 Kubernetes 的 <strong>requests+limits</strong> 的做法, 其实就是上述思路的一个简化版: <strong>用户在提交 Pod 时, 可以声明一个相对较小的 requests 值供调度器使用, 而 Kubernetes 真正设置给容器 Cgroups 的, 则是相对较大的 limits 值. 不难看到, 这跟 Borg 的思路相通的</strong>.</p> <p>在理解了 Kubernetes 资源模型的设计之后, 再来谈谈 Kubernetes 里的 <strong>QoS 模型</strong>. 在 Kubernetes 中, 不同的 requests 和 limits 的设置方式, 其实会将这个 Pod 划分到不同的 QoS 级别当中.</p> <p><strong>当 Pod 里的每一个 Container 都同时设置了 requests 和 limits, 并且 requests 和 limits 值相等的时候, 这个 Pod 就属于 Guaranteed 类别</strong>, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>demo
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>example
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>demo<span class="token punctuation">-</span>ctr
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;200Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;700m&quot;</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;200Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;700m&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>当这个 Pod 创建之后, 它的 qosClass 字段就会被 Kubernetes 自动设置为 Guaranteed. 需要注意的是, 当 Pod 仅设置了 limits 没有设置 requests 的时候, Kubernetes 会自动为它设置与 limits 相同的 requests 值, 所以这也属于 Guaranteed 情况.</p> <p><strong>而当 Pod 不满足 Guaranteed 的条件, 但至少有一个 Container 设置了 requests. 那么这个 Pod 就会被划分到 Burstable 类别</strong>. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>demo<span class="token punctuation">-</span><span class="token number">2</span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>example
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>demo<span class="token punctuation">-</span>2<span class="token punctuation">-</span>ctr
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      limits
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;200Mi&quot;</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;100Mi&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p><strong>而如果一个 Pod 既没有设置 requests, 也没有设置 limits, 那么它的 QoS 类别就是 BestEffort</strong>. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>demo<span class="token punctuation">-</span><span class="token number">3</span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>example
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> qos<span class="token punctuation">-</span>demo<span class="token punctuation">-</span>3<span class="token punctuation">-</span>ctr
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>那么, Kubernetes 为 Pod 设置这样<strong>三种 QoS 类别</strong>, 具体有什么作用呢?</p> <p>实际上, <mark><strong>QoS 划分的主要应用场景, 是当宿主机资源紧张的时候, kubelet 对 Pod 进行 Eviction(即资源回收)时需要用到的</strong></mark>​ <strong>.  ** 具体地说, 当 Kubernetes 所管理的宿主机上</strong>不可压缩资源短缺时, 就有可能触发 Eviction**. 比如, 可用内存(memory.available), 可用的宿主机磁盘空间(nodefs.available), 以及容器运行时镜像存储空间(imagefs.available)等等.</p> <p>目前, Kubernetes 设置的 Eviction 的默认阈值如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>memory.available<span class="token operator">&lt;</span>100Mi
nodefs.available<span class="token operator">&lt;</span><span class="token number">10</span>%
nodefs.inodesFree<span class="token operator">&lt;</span><span class="token number">5</span>%
imagefs.available<span class="token operator">&lt;</span><span class="token number">15</span>%
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>当然, 上述各个触发条件在 kubelet 里都是可配置的. 比如下面这个例子:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>kubelet --eviction-hard<span class="token operator">=</span>imagefs.available<span class="token operator">&lt;</span><span class="token number">10</span>%,memory.available<span class="token operator">&lt;</span>500Mi,nodefs.available<span class="token operator">&lt;</span><span class="token number">5</span>%,nodefs.inodesFree<span class="token operator">&lt;</span><span class="token number">5</span>% --eviction-soft<span class="token operator">=</span>imagefs.available<span class="token operator">&lt;</span><span class="token number">30</span>%,nodefs.available<span class="token operator">&lt;</span><span class="token number">10</span>% --eviction-soft-grace-period<span class="token operator">=</span>imagefs.available<span class="token operator">=</span>2m,nodefs.available<span class="token operator">=</span>2m --eviction-max-pod-grace-period<span class="token operator">=</span><span class="token number">600</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在这个配置中, 可以看到 <strong>Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式</strong>.</p> <p>其中 <strong>Soft Eviction</strong> 允许为 Eviction 过程设置一段 &quot;<strong>优雅时间</strong>&quot;, 比如上面例子里的 imagefs.available=2m, 就意味着当 imagefs 不足的阈值<strong>达到 2 分钟</strong>之后, kubelet 才会开始 Eviction 的过程. 而 <strong>Hard Eviction</strong> 模式下, Eviction 过程就会在阈值达到之后<strong>立刻开始</strong>. Kubernetes 计算 Eviction 阈值的数据来源, 主要依赖于从 Cgroups 读取到的值, 以及使用 cAdvisor 监控到的数据.</p> <p>当宿主机的 Eviction 阈值达到后, 就会进入 <strong>MemoryPressure</strong> 或者 <strong>DiskPressure</strong> 状态, 从而避免新的 Pod 被调度到这台宿主机上.</p> <p>而当 Eviction 发生的时候, kubelet 具体会<strong>挑选哪些 Pod 进行删除操作</strong>, 就需要参考这些 Pod 的 <strong>QoS 类别</strong>了.</p> <ul><li>首当其冲的, 自然是 <strong>BestEffort</strong> 类别的 Pod.</li> <li>其次是属于 <strong>Burstable</strong> 类别, 并且发生 &quot;饥饿&quot; 的资源使用量已经超出了 requests 的 Pod.</li> <li>最后才是 <strong>Guaranteed</strong> 类别. 并且 Kubernetes 会保证只有当 Guaranteed 类别的 Pod 的资源使用量超过了其 limits 的限制, 或者宿主机本身正处于 Memory Pressure 状态时, Guaranteed 的 Pod 才可能被选中进行 Eviction 操作.</li></ul> <p>当然对于同 QoS 类别的 Pod 来说, Kubernetes 还会根据 Pod 的优先级来进行进一步地排序和选择. 在理解了 Kubernetes 里的 QoS 类别的设计之后, 再来讲解一下 Kubernetes 里一个非常有用的特性: <strong>cpuset 的设置</strong>.</p> <p>在使用容器的时候, 可以通过<strong>设置 cpuset 把容器绑定到某个 CPU 的核上</strong>, 而不是像 cpushare 那样共享 CPU 的计算能力. 这种情况下, 由于操作系统在 CPU 之间进行<strong>上下文切换的次数大大减少</strong>, 容器里应用的性能会得到大幅提升. 事实上 **cpuset 方式是生产环境里部署在线应用类型的 Pod 时, 非常常用的一种方式. **</p> <p>可这样的需求在 Kubernetes 里又该如何实现呢? 其实非常简单.</p> <ul><li>首先, Pod 必须是 <strong>Guaranteed</strong> 的 QoS 类型;</li> <li>然后, 只需要<strong>将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的整数值</strong>即可.</li></ul> <p>比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;200Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;2&quot;</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token string">&quot;200Mi&quot;</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token string">&quot;2&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>这时候, 该 Pod 就会<strong>被绑定在 2 个独占的 CPU 核上</strong>. 当然具体是哪两个 CPU 核, 是由 kubelet 来分配的.</p> <p>以上就是 Kubernetes 的<strong>资源模型和 QoS 类别</strong>相关的主要内容.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Kubernetes 里<strong>对资源的定义方式和资源模型的设计. 然后讲述了 Kubernetes 里对 Pod 进行 Eviction 的具体策略和实践方式</strong>.</p> <p>在实际的使用中, <mark><strong>强烈建议将 DaemonSet 的 Pod 都设置为 Guaranteed 的 QoS 类型</strong></mark>. 否则一旦 DaemonSet 的 Pod 被回收, 它又会<strong>立即在原宿主机上被重建出来</strong>, 这就使得前面资源回收的动作, 完全没有意义了.</p> <h4 id="_41-十字路口上的kubernetes默认调度器"><a href="#_41-十字路口上的kubernetes默认调度器" class="header-anchor">#</a> 41 | 十字路口上的Kubernetes默认调度器</h4> <p>上一节介绍了 Kubernetes 里关于<strong>资源模型和资源管理</strong>的设计方法. 本节就来介绍一下 Kubernetes 的<strong>默认调度器</strong>(default scheduler). **在 Kubernetes 项目中, **​<mark><strong>默认调度器的主要职责, 就是为一个新创建出来的 Pod, 寻找一个最合适的节点(Node)</strong></mark>​ **. **</p> <p>而这里 &quot;最合适&quot; 的含义, 包括两层:</p> <ol><li>从集群所有的节点中, 根据调度算法挑选出<strong>所有可以运行该 Pod 的节点</strong>;</li> <li>从第一步的结果中, 再根据调度算法挑选一个最<strong>符合条件的节点作为最终结果</strong>.</li></ol> <p>所以在具体的调度流程中, 默认调度器会首先调用一组叫作 <strong>Predicate</strong> 的调度算法, 来检查每个 Node. 然后再调用一组叫作 <strong>Priority</strong> 的调度算法, 来给上一步得到的结果里的每个 Node 打分. 最终的调度结果, 就是得分最高的那个 Node.</p> <p>前面曾经介绍过, <strong>调度器对一个 Pod 调度成功, 实际上就是将它的 spec.nodeName 字段填上调度结果的节点名字</strong>.</p> <p>在 Kubernetes 中, 上述调度机制的工作原理, 可以用如下所示的一幅示意图来表示.</p> <p><img src="/img/5be6594a40aaa28642a1373715bdcf35-20230731162150-yegvxzb.png" alt="">
可以看到, Kubernetes 的调度器的核心, 实际上就是<strong>两个相互独立的控制循环</strong>.</p> <p>其中, <strong>第一个控制循环可以称之为 Informer Path</strong>. 它的主要目的, 是<strong>启动一系列 Informer, 用来监听(Watch)Etcd 中 Pod, Node, Service 等与调度相关的 API 对象的变化</strong>. 比如当一个待调度 Pod(即: 它的 nodeName 字段是空的)被创建出来之后, 调度器就会通过 Pod Informer 的 Handler, 将这个待调度 Pod 添加进调度队列.</p> <p>在默认情况下, Kubernetes 的调度队列是一个 <strong>PriorityQueue</strong>(优先级队列), 并且当某些集群信息发生变化的时候, 调度器还会对调度队列里的内容进行一些特殊操作. 这里的设计, 主要是出于<strong>调度优先级和抢占</strong>的考虑, 后面会详细介绍这部分内容.</p> <p>此外, Kubernetes 的默认调度器还要负责对<strong>调度器缓存</strong>(即: scheduler cache)进行更新. 事实上, <mark><strong>Kubernetes 调度部分进行性能优化的一个最根本原则, 就是尽最大可能将集群信息 Cache 化, 以便从根本上提高 Predicate 和 Priority 调度算法的执行效率</strong></mark>.</p> <p>而**第二个控制循环, 是调度器负责 Pod 调度的主循环, 可以称之为 Scheduling Path. **</p> <p>Scheduling Path 的主要逻辑, 就是不断地从调度队列里出队一个 Pod. 然后调用 Predicates 算法进行 &quot;过滤&quot;. 这一步 &quot;过滤&quot; 得到的一组 Node, 就是<strong>所有可以运行这个 Pod 的宿主机列表</strong>. 当然 Predicates 算法需要的 Node 信息, 都是从 Scheduler Cache 里直接拿到的, 这是调度器保证算法执行效率的主要手段之一. 接下来调度器就会再调用 <strong>Priorities</strong> 算法为上述列表里的 Node 打分, 分数从 0 到 10. 得分最高的 Node, 就会作为这次调度的结果.</p> <p>调度算法执行完成后, 调度器就需要<strong>将 Pod 对象的 nodeName 字段的值, 修改为上述 Node 的名字</strong>. **这个步骤在 Kubernetes 里面被称作 Bind. ** 但是为了不在关键调度路径里远程访问 APIServer, Kubernetes 的默认调度器在 Bind 阶段, 只会更新 Scheduler Cache 里的 Pod 和 Node 的信息. **这种基于 &quot;乐观&quot; 假设的 API 对象更新方式, 在 Kubernetes 里被称作 Assume. **</p> <p>Assume 之后, 调度器才会<strong>创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求, 来真正完成 Bind 操作</strong>. 如果这次异步的 Bind 过程失败了, 其实也没有太大关系, 等 Scheduler Cache 同步之后一切就会恢复正常.</p> <p>当然, 正是由于上述 Kubernetes 调度器的 &quot;乐观&quot; 绑定的设计, 当一个新的 Pod 完成调度需要在某个节点上运行起来之前, 该节点上的 kubelet 还会<strong>通过一个叫作 Admit 的操作来再次验证该 Pod 是否确实能够运行在该节点上</strong>. 这一步 Admit 操作, 实际上就是把一组叫作 GeneralPredicates 的, 最基本的调度算法, 比如: &quot;资源是否可用&quot;, &quot;端口是否冲突&quot;等再执行一遍, 作为 kubelet 端的二次确认. 关于 Kubernetes 默认调度器的<strong>调度算法</strong>会在下一节讲解.</p> <p>**除了上述的 &quot;Cache 化&quot; 和 &quot;乐观绑定&quot;, Kubernetes 默认调度器还有一个重要的设计, 那就是 &quot;无锁化&quot;. **</p> <p>在 Scheduling Path 上, 调度器会启动多个 Goroutine 以<strong>节点为粒度</strong>并发执行 Predicates 算法, 从而提高这一阶段的执行效率. 而与之类似的, Priorities 算法也会以 MapReduce 的方式并行计算然后再进行汇总. 而在这些所有需要并发的路径上, 调度器会<strong>避免设置任何全局的竞争资源</strong>, 从而免去了使用锁进行同步带来的巨大的性能损耗. 所以在这种思想的指导下, 如果再去查看一下前面的调度器原理图, 就会发现, Kubernetes 调度器<strong>只有对调度队列和 Scheduler Cache 进行操作时, 才需要加锁. 而这两部分操作, 都不在 Scheduling Path 的算法执行路径上</strong>.</p> <p>当然, Kubernetes 调度器的上述设计思想, 也是在集群规模不断增长的演进过程中逐步实现的. 尤其是  **&quot;Cache 化&quot;, 这个变化其实是最近几年 Kubernetes 调度器性能得以提升的一个关键演化. **</p> <p>不过, 随着 Kubernetes 项目发展到今天, 它的默认调度器也已经来到了一个关键的十字路口. 事实上, Kubernetes 现今发展的主旋律, 是整个开源项目的 &quot;民主化&quot;. 也就是说, Kubernetes 下一步发展的方向, 是<strong>组件的轻量化, 接口化和插件化</strong>. 所以才有了 CRI, CNI, CSI, CRD, Aggregated APIServer, Initializer, Device Plugin 等各个层级的可扩展能力. 可是, 默认调度器, 却成了 Kubernetes 项目里最后一个没有对外暴露出良好定义过的, 可扩展接口的组件.</p> <p>当然这是有一定的历史原因的. 在过去几年, Kubernetes 发展的重点, 都是以功能性需求的实现和完善为核心. 在这个过程中, 它的很多决策, 还是<strong>以优先服务公有云的需求为主, 而性能和规模则居于相对次要的位置</strong>.</p> <p>而现在, 随着 Kubernetes 项目逐步趋于稳定, 越来越多的用户开始把 Kubernetes 用在规模更大, 业务更加复杂的私有集群当中. 很多以前的 Mesos 用户, 也开始尝试使用 Kubernetes 来替代其原有架构. 在这些场景下, 对默认调度器进行扩展和重新实现, 就成了社区对 Kubernetes 项目最主要的一个诉求.</p> <p>所以, Kubernetes 的默认调度器, 是目前这个项目里为数不多的, 正在经历大量重构的核心组件之一. 这些正在进行的重构的目的, 一方面是将默认调度器里大量的 &quot;技术债&quot; 清理干净; 另一方面, 就是为默认调度器的可扩展性设计进行铺垫.</p> <p>而 Kubernetes 默认调度器的可扩展性设计, 可以用如下所示的一幅示意图来描述:</p> <p><img src="/img/a5a451c3e4271d057a05f31be6bb92c6-20230731162150-sug0pzt.png" alt="">​可以看到, 默认调度器的可扩展机制, 在 Kubernetes 里面叫作 <strong>Scheduler Framework</strong>. 顾名思义, 这个设计的主要目的, 就是在调度器生命周期的各个关键点上, <strong>为用户暴露出可以进行扩展和实现的接口, 从而实现由用户自定义调度器的能力</strong>.</p> <p>上图中每一个绿色的箭头都是一个可以插入自定义逻辑的接口. 比如上面的 Queue 部分, 就意味着可以在这一部分提供一个自己的<strong>调度队列</strong>的实现, 从而控制每个 Pod 开始被调度(出队)的时机. 而 Predicates 部分, 则意味着可以提供自己的<strong>过滤算法</strong>实现, 根据自己的需求, 来决定选择哪些机器.</p> <p><strong>需要注意的是, 上述这些可插拔式逻辑, 都是标准的 Go 语言插件机制(Go plugin 机制)</strong> , 也就是说需要在编译的时候选择把哪些插件编译进去.</p> <p>有了上述设计之后, 扩展和自定义 Kubernetes 的默认调度器就变成了一件非常容易实现的事情. 这也意味着默认调度器在后面的发展过程中, 必然不会在现在的实现上再添加太多的功能, 反而还会对现在的实现进行精简, 最终成为 Scheduler Framework 的一个最小实现. 而调度领域更多的创新和工程工作, 就可以交给整个社区来完成了. 这个思路是完全符合前面提到的 Kubernetes 的 &quot;民主化&quot; 设计的.</p> <p>不过这样的 Scheduler Framework 也有一个不小的问题, 那就是一旦这些插入点的接口设计不合理, 就会导致整个生态没办法很好地把这个插件机制使用起来. 而与此同时, 这些接口本身的变更又是一个费时费力的过程, 一旦把控不好, 就很可能会把社区推向另一个极端, 即: Scheduler Framework 没法实际落地, 大家只好都再次 fork kube-scheduler.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Kubernetes 里默认调度器的设计与实现, 分析了它现在正在经历的重构, 以及未来的走向. 不难看到, 在 Kubernetes 的整体架构中, kube-scheduler 的责任虽然重大, 但其实它却是在社区里最少受到关注的组件之一. 这里的原因也很简单, 调度这个事情, 在不同的公司和团队里的实际需求一定是大相径庭的, 上游社区不可能提供一个大而全的方案出来. 所以, <strong>将默认调度器进一步做轻做薄, 并且插件化, 才是 kube-scheduler 正确的演进方向</strong>.</p> <h4 id="_42-kubernetes默认调度器调度策略解析"><a href="#_42-kubernetes默认调度器调度策略解析" class="header-anchor">#</a> 42 | Kubernetes默认调度器调度策略解析</h4> <p>上一节讲解了 Kubernetes 默认调度器的设计原理和架构. 本节就专注在<strong>调度过程中 Predicates 和 Priorities 这两个调度策略</strong>主要发生作用的阶段.</p> <p>首先看看 Predicates.</p> <p><strong>Predicates 在调度过程中的作用, 可以理解为 Filter</strong>, 即: <mark><strong>它按照调度策略, 从当前集群的所有节点中, &quot;过滤&quot; 出一系列符合条件的节点</strong></mark>. <strong>这些节点都是可以运行待调度 Pod 的宿主机</strong>.</p> <p>而在 Kubernetes 中, 默认的调度策略有如下三种.</p> <p>**第一种类型, 叫作 GeneralPredicates. **</p> <p>顾名思义, 这一组过滤规则负责的是<strong>最基础</strong>的调度策略. 比如 <strong>PodFitsResources</strong> 计算的就是宿主机的 CPU 和内存资源等是否够用. 当然前面已经提到过, PodFitsResources 检查的只是 Pod 的 requests 字段. 需要注意的是, Kubernetes 的调度器并没有为 GPU 等硬件资源定义具体的资源类型, 而是统一用一种名叫 Extended Resource 的, Key-Value 格式的扩展字段来描述的. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> extended<span class="token punctuation">-</span>resource<span class="token punctuation">-</span>demo
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> extended<span class="token punctuation">-</span>resource<span class="token punctuation">-</span>demo<span class="token punctuation">-</span>ctr
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">alpha.kubernetes.io/nvidia-gpu</span><span class="token punctuation">:</span> <span class="token number">2</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">alpha.kubernetes.io/nvidia-gpu</span><span class="token punctuation">:</span> <span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>可以看到, 这个 Pod 通过 <code>alpha.kubernetes.io/nvidia-gpu=2</code>​ 这样的定义方式, 声明使用了两个 NVIDIA 类型的 GPU.</p> <p>而在 PodFitsResources 里面, 调度器其实并不知道这个字段 Key 的含义是 GPU, 而是直接使用后面的 Value 进行计算. 当然, 在 Node 的 Capacity 字段里, 也得相应地加上这台宿主机上 GPU 的总数, 比如: <code>alpha.kubernetes.io/nvidia-gpu=4</code>​. 这些流程在后面讲解 Device Plugin 的时候会详细介绍到.</p> <p>而 <strong>PodFitsHost</strong> 检查的是宿主机的名字是否跟 Pod 的 spec.nodeName 一致. <strong>PodFitsHostPorts</strong> 检查的是 Pod 申请的宿主机端口(spec.nodePort)是不是跟已经被使用的端口有冲突. <strong>PodMatchNodeSelector</strong> 检查的是 Pod 的 nodeSelector 或者 nodeAffinity 指定的节点, 是否与待考察节点匹配, 等等.</p> <p>可以看到, 像上面这样一组 <strong>GeneralPredicates</strong>, 正是 Kubernetes 考察一个 Pod 能不能运行在一个 Node 上<strong>最基本的过滤条件</strong>. 所以 GeneralPredicates 也会被其他组件(比如 kubelet)<strong>直接调用</strong>.</p> <p>上一节已经提到过, kubelet 在启动 Pod 前, 会执行一个 Admit 操作来进行二次确认. 这里<strong>二次确认的规则, 就是执行一遍 GeneralPredicates</strong>.</p> <p>**第二种类型, 是与 Volume 相关的过滤规则. **</p> <p>这一组过滤规则负责的是跟<strong>容器持久化 Volume 相关的调度策略</strong>.</p> <p>其中, <strong>NoDiskConflict</strong> 检查的条件, 是多个 Pod 声明挂载的持久化 Volume 是否有冲突. 比如 AWS EBS 类型的 Volume, 是不允许被两个 Pod 同时使用的. 所以当一个名叫 A 的 EBS Volume 已经被挂载在了某个节点上时, 另一个同样声明使用这个 A Volume 的 Pod, 就不能被调度到这个节点上了.</p> <p>而 <strong>MaxPDVolumeCountPredicate</strong> 检查的条件, 则是一个节点上某种类型的持久化 Volume 是不是已经超过了一定数目, 如果是的话, 那么声明使用该类型持久化 Volume 的 Pod 就不能再调度到这个节点了.</p> <p>而 <strong>VolumeZonePredicate</strong>, 则是检查持久化 Volume 的 Zone(高可用域)标签, 是否与待考察节点的 Zone 标签相匹配.</p> <p>此外, 这里还有一个叫作 <strong>VolumeBindingPredicate</strong> 的规则. 它负责检查的是该 Pod 对应的 PV 的 nodeAffinity 字段, 是否跟某个节点的标签相匹配.</p> <p>前面<a href="https://time.geekbang.org/column/article/42819" target="_blank" rel="noopener noreferrer">《PV, PVC 体系是不是多此一举? 从本地持久化卷谈起》<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>中曾经讲过, Local Persistent Volume(本地持久化卷), 必须使用 nodeAffinity 来跟某个具体的节点绑定. 这其实也就意味着, 在 Predicates 阶段, Kubernetes 就必须能够根据 Pod 的 Volume 属性来进行调度. 此外, 如果该 Pod 的 PVC 还没有跟具体的 PV 绑定的话, 调度器还要负责检查所有待绑定 PV, 当有可用的 PV 存在并且该 PV 的 nodeAffinity 与待考察节点一致时, 这条规则才会返回 &quot;成功&quot;. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token punctuation">-</span>local<span class="token punctuation">-</span>pv
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>
    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 500Gi
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token key atrule">persistentVolumeReclaimPolicy</span><span class="token punctuation">:</span> Retain
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> local<span class="token punctuation">-</span>storage
  <span class="token key atrule">local</span><span class="token punctuation">:</span>
    <span class="token key atrule">path</span><span class="token punctuation">:</span> /mnt/disks/vol1
  <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>
    <span class="token key atrule">required</span><span class="token punctuation">:</span>
      <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> kubernetes.io/hostname
          <span class="token key atrule">operator</span><span class="token punctuation">:</span> In
          <span class="token key atrule">values</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> my<span class="token punctuation">-</span>node
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>可以看到, 这个 PV 对应的<strong>持久化目录</strong>, 只会出现在名叫 my-node 的宿主机上. 所以任何一个通过 PVC 使用这个 PV 的 Pod, 都必须被调度到 my-node 上才可以正常工作. VolumeBindingPredicate, 正是调度器里完成这个决策的位置.</p> <p>**第三种类型, 是宿主机相关的过滤规则. **</p> <p>这一组规则主要考察<strong>待调度 Pod 是否满足 Node 本身的某些条件</strong>.</p> <p>比如 <strong>PodToleratesNodeTaints</strong>, 负责检查的就是前面经常用到的 Node 的 &quot;污点&quot; 机制. 只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配的时候, 这个 Pod 才能被调度到该节点上.</p> <p>而 <strong>NodeMemoryPressurePredicate</strong> 检查的是当前节点的内存是不是已经不够充足, 如果是的话, 那么待调度 Pod 就不能被调度到该节点上.</p> <p>**第四种类型, 是 Pod 相关的过滤规则. **</p> <p>这一组规则跟 GeneralPredicates 大多数是重合的. 而比较特殊的, 是 <strong>PodAffinityPredicate,</strong>  这个规则的作用是检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密(affinity)和反亲密(anti-affinity)关系. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> with<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>antiaffinity
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
    <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span> 
      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span> 
      <span class="token punctuation">-</span> <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">100</span>  
        <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>
          <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>
            <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> security 
              <span class="token key atrule">operator</span><span class="token punctuation">:</span> In 
              <span class="token key atrule">values</span><span class="token punctuation">:</span>
              <span class="token punctuation">-</span> S2
          <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> with<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>affinity
    <span class="token key atrule">image</span><span class="token punctuation">:</span> docker.io/ocpqe/hello<span class="token punctuation">-</span>pod
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>这个例子里的 podAntiAffinity 规则, 就指定了这个 Pod 不希望跟任何携带了 security=S2 标签的 Pod 存在于同一个 Node 上. 需要注意的是, PodAffinityPredicate 是有作用域的, 比如上面这条规则, 就仅对携带了 Key 是 <code>kubernetes.io/hostname</code>​ 标签的 Node 有效. 这正是 topologyKey 这个关键词的作用.</p> <p>而与 podAntiAffinity 相反的, 就是 podAffinity, 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> with<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>affinity
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
    <span class="token key atrule">podAffinity</span><span class="token punctuation">:</span> 
      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span> 
      <span class="token punctuation">-</span> <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>
          <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> security 
            <span class="token key atrule">operator</span><span class="token punctuation">:</span> In 
            <span class="token key atrule">values</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> S1 
        <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> failure<span class="token punctuation">-</span>domain.beta.kubernetes.io/zone
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> with<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>affinity
    <span class="token key atrule">image</span><span class="token punctuation">:</span> docker.io/ocpqe/hello<span class="token punctuation">-</span>pod
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>这个例子里的 Pod, 就只会被调度到已经有携带了 security=S1 标签的 Pod 运行的 Node 上. 而这条规则的作用域, 则是所有携带 Key 是 <code>failure-domain.beta.kubernetes.io/zone</code>​ 标签的 Node.</p> <p>此外, 上面这两个例子里的 requiredDuringSchedulingIgnoredDuringExecution 字段的含义是: <strong>这条规则必须在 Pod 调度时进行检查</strong>(requiredDuringScheduling); 但是如果是已经在运行的 Pod 发生变化, 比如 Label 被修改, 造成了该 Pod 不再适合运行在这个 Node 上的时候, Kubernetes 不会进行主动修正(IgnoredDuringExecution).</p> <p>上面这四种类型的 Predicates, 就构成了调度器确定一个 Node 可以运行待调度 Pod 的基本策略.</p> <p><mark>**在具体执行的时候, 当开始调度一个 Pod 时, Kubernetes 调度器会同时启动 16 个 Goroutine, 来并发地为集群里的所有 Node 计算 Predicates, 最后返回可以运行这个 Pod 的宿主机列表. **</mark></p> <p>需要注意的是, 在为每个 Node 执行 Predicates 时, 调度器会按照<strong>固定的顺序</strong>来进行检查. 这个顺序是按照 Predicates 本身的含义来确定的. 比如宿主机相关的 Predicates 会被放在相对靠前的位置进行检查. 要不然的话, 在一台资源已经严重不足的宿主机上, 上来就开始计算 PodAffinityPredicate, 是没有实际意义的.</p> <p>接下来再来看一下 <strong>Priorities</strong>.</p> <p><mark><strong>在 Predicates 阶段完成了节点的 &quot;过滤&quot; 之后, Priorities 阶段的工作就是为这些节点打分. 这里打分的范围是 0-10 分, 得分最高的节点就是最后被 Pod 绑定的最佳节点.</strong></mark></p> <p>Priorities 里最常用到的一个打分规则, 是 <strong>LeastRequestedPriority</strong>. 它的计算方法, 可以简单地总结为如下所示的公式:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>score = (cpu((capacity<span class="token punctuation">-</span>sum(requested))10/capacity) + memory((capacity<span class="token punctuation">-</span>sum(requested))10/capacity))/2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到, 这个算法实际上就是在<strong>选择空闲资源(CPU 和 Memory)最多的宿主机</strong>.</p> <p>而与 LeastRequestedPriority 一起发挥作用的, 还有 <strong>BalancedResourceAllocation</strong>. 它的计算公式如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>score = 10 <span class="token punctuation">-</span> variance(cpuFraction<span class="token punctuation">,</span>memoryFraction<span class="token punctuation">,</span>volumeFraction)<span class="token important">*10</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中每种资源的 Fraction 的定义是 : <strong>Pod 请求的资源 / 节点上的可用资源</strong>. 而 <strong>variance</strong> 算法的作用, 则是计算每两种资源 Fraction 之间的 &quot;距离&quot;. 而最后选择的, 则是资源 Fraction 差距最小的节点.</p> <p>所以 BalancedResourceAllocation 选择的, 其实是<strong>调度完成后, 所有节点里各种资源分配最均衡的那个节点, 从而避免一个节点上 CPU 被大量分配, 而 Memory 大量剩余的情况</strong>.</p> <p>此外, 还有 NodeAffinityPriority, TaintTolerationPriority 和 InterPodAffinityPriority 这三种 Priority. 顾名思义, 它们与前面的 PodMatchNodeSelector, PodToleratesNodeTaints 和 PodAffinityPredicate 这三个 Predicate 的含义和计算方法是类似的. 但是作为 Priority, <strong>一个 Node 满足上述规则的字段数目越多, 它的得分就会越高</strong>.</p> <p>在默认 Priorities 里, 还有一个叫作 <strong>ImageLocalityPriority</strong> 的策略. 它是在 Kubernetes v1.12 里新开启的调度规则, 即: 如果待调度 Pod 需要使用的镜像很大, 并且已经存在于某些 Node 上, 那么这些 Node 的得分就会比较高. 当然, 为了避免这个算法引发调度堆叠, 调度器在计算得分的时候还会根据镜像的分布进行优化, 即: 如果大镜像分布的节点数目很少, 那么这些节点的权重就会被调低, 从而 &quot;对冲&quot; 掉引起调度堆叠的风险.</p> <p>以上, 就是 Kubernetes 调度器的 Predicates 和 Priorities 里默认调度规则的主要工作原理了.</p> <p><mark><strong>在实际的执行过程中, 调度器里关于集群和 Pod 的信息都已经缓存化, 所以这些算法的执行过程还是比较快的.</strong></mark></p> <p>此外, 对于比较复杂的调度算法来说, 比如 PodAffinityPredicate, 它们在计算的时候不只关注待调度 Pod 和待考察 Node, 还需要关注<strong>整个集群的信息</strong>, 比如遍历所有节点, 读取它们的 Labels. 这时候, Kubernetes 调度器会在为每个待调度 Pod 执行该调度算法之前, 先将算法需要的集群信息初步计算一遍, 然后缓存起来. 这样在真正执行该算法的时候, 调度器只需要读取缓存信息进行计算即可, 从而避免了为每个 Node 计算 Predicates 的时候反复获取和计算整个集群的信息.</p> <blockquote><p>总结</p></blockquote> <p>本节讲述了 Kubernetes 默认调度器里的主要调度算法. 需要注意的是, 除了本篇讲述的这些规则, Kubernetes 调度器里其实还有一些默认不会开启的策略. 可以通过为 kube-scheduler 指定一个配置文件或者创建一个 ConfigMap, 来配置哪些规则需要开启, 哪些规则需要关闭. 并且可以通过为 Priorities 设置<strong>权重</strong>, 来控制调度器的调度行为.</p> <h4 id="_43-kubernetes默认调度器的优先级与抢占机制"><a href="#_43-kubernetes默认调度器的优先级与抢占机制" class="header-anchor">#</a> 43 | Kubernetes默认调度器的优先级与抢占机制</h4> <p>上一节讲解了 Kubernetes 默认调度器的主要调度算法的工作原理. 本节讲解一下 Kubernetes 调度器里的另一个重要机制, 即: <strong>优先级(Priority )和抢占(Preemption)机制</strong>. 首先需要明确的是, <mark><strong>优先级和抢占机制, 解决的是 Pod 调度失败时该怎么办的问题</strong></mark>.</p> <p>正常情况下, 当一个 Pod 调度失败后, 它就会被暂时 &quot;搁置&quot; 起来, 直到 Pod 被更新, 或者集群状态发生变化, 调度器才会对这个 Pod 进行重新调度.</p> <p>但在有时候希望的是这样一个场景. 当一个高优先级的 Pod 调度失败后, 该 Pod 并不会被 &quot;搁置&quot;, 而是会 &quot;挤走&quot; 某个 Node 上的一些低优先级的 Pod. 这样就可以保证这个高优先级 Pod 的调度成功. 这个特性其实也是一直以来就存在于 Borg 以及 Mesos 等项目里的一个基本功能.</p> <p>而在 Kubernetes 里, 优先级和抢占机制是在 1.10 版本后才逐步可用的. 要使用这个机制, 首先需要在 Kubernetes 里提交一个 <strong>PriorityClass</strong> 的定义, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> scheduling.k8s.io/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PriorityClass
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> high<span class="token punctuation">-</span>priority
<span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token number">1000000</span>
<span class="token key atrule">globalDefault</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
<span class="token key atrule">description</span><span class="token punctuation">:</span> <span class="token string">&quot;This priority class should be used for high priority service pods only.&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>上面这个 YAML 文件, 定义的是一个名叫 high-priority 的 PriorityClass, 其中 value 的值是 1000000 (一百万).</p> <p>**Kubernetes 规定, 优先级是一个 32 bit 的整数, 最大值不超过 1000000000(10 亿, 1 billion), 并且值越大代表优先级越高. ** 而超出 10 亿的值, 其实是被 Kubernetes 保留下来分配给系统 Pod 使用的. 显然这样做的目的就是保证系统 Pod 不会被用户抢占掉.</p> <p>而一旦上述 YAML 文件里的 globalDefault 被设置为 true 的话, 那就意味着这个 PriorityClass 的值会成为系统的<strong>默认值</strong>. 而如果这个值是 false, 就表示只希望声明使用该 PriorityClass 的 Pod 拥有值为 1000000 的优先级, 而对于没有声明 PriorityClass 的 Pod 来说, 它们的优先级就是 0.</p> <p>在创建了 PriorityClass 对象之后, Pod 就可以声明使用它了, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">env</span><span class="token punctuation">:</span> test
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent
  <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> high<span class="token punctuation">-</span>priority
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>可以看到, 这个 Pod 通过 priorityClassName 字段, 声明了要使用名叫 high-priority 的 PriorityClass. 当这个 Pod 被提交给 Kubernetes 之后, Kubernetes 的 PriorityAdmissionController 就会自动将这个 Pod 的 <strong>spec.priority</strong> 字段设置为 1000000.</p> <p>前面介绍过, 调度器里维护着一个调度队列. 所以当 Pod 拥有了优先级之后, 高优先级的 Pod 就可能会比低优先级的 Pod 提前出队, 从而尽早完成调度过程. **这个过程就是 &quot;优先级&quot; 这个概念在 Kubernetes 里的主要体现. **</p> <p>而当一个高优先级的 Pod 调度失败的时候, 调度器的<strong>抢占能力就会被触发</strong>. 这时调度器就会试图从当前集群里寻找一个节点, 使得当这个节点上的一个或者多个低优先级 Pod 被删除后, 待调度的高优先级 Pod 就可以被调度到这个节点上. **这个过程就是 &quot;抢占&quot; 这个概念在 Kubernetes 里的主要体现. **</p> <p>为了方便叙述, 接下来会把待调度的高优先级 Pod 称为 &quot;<strong>抢占者</strong>&quot;(Preemptor). 当上述抢占过程发生时, <strong>抢占者并不会立刻被调度到被抢占的 Node 上</strong>. 事实上, 调度器只会将抢占者的 spec.nominatedNodeName 字段设置为被抢占的 Node 的名字. 然后抢占者会<strong>重新进入下一个调度周期, 然后在新的调度周期里来决定是不是要运行在被抢占的节点上</strong>. 这当然也就意味着, 即使在下一个调度周期, 调度器也不会保证抢占者一定会运行在被抢占的节点上.</p> <p>这样设计的一个重要原因是, 调度器只会通过标准的 DELETE API 来删除被抢占的 Pod, 所以这些 Pod 必然是有一定的 &quot;优雅退出&quot; 时间(默认是 30s)的. 而在这段时间里, 其他的节点也是有可能变成可调度的, 或者直接有新的节点被添加到这个集群中来. 所以, 鉴于优雅退出期间, 集群的可调度性可能会发生的变化, **把抢占者交给下一个调度周期再处理, 是一个非常合理的选择. **</p> <p>而在抢占者等待被调度的过程中, 如果有其他<strong>更高优先级</strong>的 Pod 也要抢占同一个节点, 那么调度器就会清空原抢占者的 spec.nominatedNodeName 字段, 从而允许更高优先级的抢占者执行抢占, 并且这也就是得原抢占者本身, 也有机会去重新抢占其他节点. 这些都是设置 nominatedNodeName 字段的主要目的.</p> <p>那么, Kubernetes 调度器里的抢占机制, 又是如何设计的呢? 接下来就详细讲述一下这其中的原理.</p> <p>前面提到过, <mark><strong>抢占发生的原因, 一定是一个高优先级的 Pod 调度失败</strong></mark>. 这一次还是称这个 Pod 为 &quot;抢占者&quot;, 称被抢占的 Pod 为 &quot;<strong>牺牲者</strong>&quot;(victims).</p> <p>而 Kubernetes 调度器实现抢占算法的一个最重要的设计, 就是<strong>在调度队列的实现里, 使用了两个不同的队列</strong>.</p> <p><strong>第一个队列, 叫作 activeQ. ** 凡是在 activeQ 里的 Pod, 都是</strong>下一个调度周期需要调度的对象**. 所以当在 Kubernetes 集群里新创建一个 Pod 的时候, 调度器会将这个 Pod 入队到 activeQ 里面. 前面提到过, 调度器不断从队列里出队(Pop)一个 Pod 进行调度, 实际上都是<strong>从 activeQ 里出队</strong>的.</p> <p><strong>第二个队列, 叫作 unschedulableQ</strong>, 专门用来存放<strong>调度失败的 Pod</strong>. 而这里的一个关键点就在于, 当一个 unschedulableQ 里的 Pod 被更新之后, 调度器会<strong>自动把这个 Pod 移动到 activeQ 里</strong>, 从而给这些调度失败的 Pod &quot;重新做人&quot; 的机会.</p> <p>现在回到抢占者调度失败这个时间点上来.</p> <p>调度失败之后, 抢占者就会被放进 <strong>unschedulableQ</strong> 里面. 然后这次失败事件就会触发<strong>调度器为抢占者寻找牺牲者的流程</strong>.</p> <p><strong>第一步</strong>, 调度器会检查这次失败事件的原因, 来确认抢占是不是可以帮助抢占者找到一个新节点. 这是因为有很多 Predicates 的失败是不能通过抢占来解决的. 比如 PodFitsHost 算法(负责的是, 检查 Pod 的 nodeSelector 与 Node 的名字是否匹配), 这种情况下, 除非 Node 的名字发生变化, 否则你即使删除再多的 Pod, 抢占者也不可能调度成功.</p> <p><strong>第二步</strong>, 如果<strong>确定抢占可以发生</strong>, 那么调度器就会把自己缓存的所有节点信息复制一份, 然后使用这个副本来<strong>模拟抢占过程</strong>. 这里的抢占过程很容易理解. 调度器会检查缓存副本里的每一个节点, 然后从该节点上最低优先级的 Pod 开始, 逐一 &quot;删除&quot; 这些 Pod. 而每删除一个低优先级 Pod, 调度器都会检查一下抢占者是否能够运行在该 Node 上. 一旦<strong>可以运行, 调度器就记录下这个 Node 的名字和被删除 Pod 的列表, 这就是一次抢占过程的结果</strong>了.</p> <p>当遍历完所有的节点之后, 调度器会在上述模拟产生的所有抢占结果里做一个选择, 找出<strong>最佳</strong>结果. 而这一步的<strong>判断原则, 就是尽量减少抢占对整个系统的影响</strong>. 比如需要抢占的 Pod 越少越好, 需要抢占的 Pod 的优先级越低越好, 等等. 在得到了最佳的抢占结果之后, 这个结果里的 Node 就是即将被抢占的 Node; 被删除的 Pod 列表, 就是牺牲者. 所以接下来, <strong>调度器就可以真正开始抢占的操作</strong>了, 这个过程, 可以分为三步.</p> <p><strong>第一步</strong>, 调度器会检查牺牲者列表, 清理这些 Pod 所携带的 nominatedNodeName 字段.</p> <p><strong>第二步</strong>, 调度器会把抢占者的 <strong>nominatedNodeName</strong>, 设置为被抢占的 Node 的名字.</p> <p><strong>第三步</strong>, 调度器会开启一个 <strong>Goroutine</strong>, 同步地删除牺牲者.</p> <p>而第二步对抢占者 Pod 的更新操作, 就会触发前面提到的 &quot;重新做人&quot; 的流程, 从而<strong>让抢占者在下一个调度周期重新进入调度流程</strong>.  所以<strong>接下来, 调度器就会通过正常的调度流程把抢占者调度成功</strong>. 这也是为什么前面会说调度器并不保证抢占的结果: 在这个正常的调度流程里, 是<strong>一切皆有可能</strong>的.</p> <p>不过, 对于任意一个待调度 Pod 来说, 因为有上述抢占者的存在, 它的调度过程其实是有一些<strong>特殊情况</strong>需要特殊处理的.</p> <p>具体来说, 在为某一对 Pod 和 Node 执行 Predicates 算法的时候, 如果待检查的 Node 是一个<strong>即将被抢占</strong>的节点, 即: 调度队列里有 nominatedNodeName 字段值是该 Node 名字的 Pod 存在(可以称之为: &quot;潜在的抢占者&quot;). 那么**调度器就会对这个 Node, 将同样的 Predicates 算法运行两遍. **</p> <p><strong>第一遍</strong>, 调度器会假设上述 &quot;潜在的抢占者&quot; 已经运行在这个节点上, 然后执行 Predicates 算法;</p> <p><strong>第二遍</strong>, 调度器会正常执行 Predicates 算法, 即: 不考虑任何 &quot;潜在的抢占者&quot;.</p> <p>而只有<strong>这两遍 Predicates 算法都能通过时, 这个 Pod 和 Node 才会被认为是可以绑定(bind)的</strong>.</p> <p>不难想到, 这里需要执行第一遍 Predicates 算法的原因, 是由于 InterPodAntiAffinity 规则的存在. 由于 InterPodAntiAffinity 规则关心待考察节点上所有 Pod 之间的互斥关系, 所以在执行调度算法时必须考虑, 如果抢占者已经存在于待考察 Node 上时, 待调度 Pod 还能不能调度成功. 当然这也就意味着, 在这一步<strong>只需要考虑那些优先级等于或者大于待调度 Pod 的抢占者</strong>. 毕竟对于其他较低优先级 Pod 来说, 待调度 Pod 总是可以通过抢占运行在待考察 Node 上.</p> <p>而需要执行第二遍 Predicates 算法的原因, 则是因为 &quot;潜在的抢占者&quot; 最后<strong>不一定</strong>会运行在待考察的 Node 上. 关于这一点前面已经讲解过了: Kubernetes 调度器<strong>并不保证</strong>抢占者一定会运行在当初选定的被抢占的 Node 上.</p> <p>以上就是 Kubernetes 默认调度器里优先级和抢占机制的实现原理了.</p> <blockquote><p>总结</p></blockquote> <p>本节详细讲述了 Kubernetes 里关于 Pod 的优先级和抢占机制的设计与实现. 这个特性在 v1.11 之后已经是 Beta 了, 意味着比较稳定了. 所以建议在 Kubernetes 集群中开启这两个特性, 以便实现更高的<strong>资源使用率</strong>.</p> <h4 id="_44-kubernetes-gpu管理与device-plugin机制"><a href="#_44-kubernetes-gpu管理与device-plugin机制" class="header-anchor">#</a> 44 | Kubernetes GPU管理与Device Plugin机制</h4> <p>2016 年, 随着 AlphaGo 的走红和 TensorFlow 项目的异军突起, 一场名为 AI 的技术革命迅速从学术界蔓延到了工业界, 所谓的 AI 元年, 就此拉开帷幕. 而这次热潮的背后, <strong>云计算服务的普及与成熟, 以及算力的巨大提升</strong>, 其实正是将人工智能从象牙塔带到工业界的一个重要推手.</p> <p>而与之相对应的, 从 2016 年开始, Kubernetes 社区就不断收到来自不同渠道的大量诉求, 希望能够在 Kubernetes 集群上运行 <strong>TensorFlow</strong> 等机器学习框架所创建的训练(Training)和服务(Serving)任务. 而这些诉求中, 除了前面讲解过的 Job, Operator 等离线作业管理需要用到的编排概念之外, 还有一个亟待实现的功能, 就是<strong>对 GPU 等硬件加速设备管理</strong>的支持.</p> <p>不过, 正如同 TensorFlow 之于 Google 的战略意义一样, <strong>GPU 支持对于 Kubernetes 项目来说, 其实也有着超过技术本身的考虑</strong>. 所以尽管在硬件加速器这个领域里, Kubernetes 上游有着不少来自 NVIDIA 和 Intel 等芯片厂商的工程师, 但这个特性本身, 却从一开始就是以 Google Cloud 的需求为主导来推进的.</p> <p>而对于云的用户来说, 在 GPU 的支持上, 他们最基本的诉求其实非常简单: <strong>只要在 Pod 的 YAML 里面, 声明某容器需要的 GPU 个数, 那么 Kubernetes 创建的容器里就应该出现对应的 GPU 设备, 以及它对应的驱动目录</strong>.</p> <p>以 NVIDIA 的 GPU 设备为例, 上面的需求就意味着当用户的容器被创建之后, 这个容器里必须出现如下两部分设备和目录:</p> <ol><li><strong>GPU 设备</strong>, 比如 /dev/nvidia0;</li> <li><strong>GPU 驱动目录</strong>, 比如 /usr/local/nvidia/*.</li></ol> <p>其中, <strong>GPU 设备路径, 正是该容器启动时的 Devices 参数; 而驱动目录, 则是该容器启动时的 Volume 参数</strong>. 所以, 在 Kubernetes 的 GPU 支持的实现里, kubelet 实际上就是将上述两部分内容, 设置在了创建该容器的 <strong>CRI</strong> (Container Runtime Interface)参数里面. 这样等到该容器启动之后, 对应的容器里就会出现 GPU 设备和驱动的路径了.</p> <p>不过, Kubernetes 在 Pod 的 API 对象里, 并没有为 GPU 专门设置一个资源类型字段, 而是使用了一种叫作 <strong>Extended Resource(ER)</strong> 的特殊字段来负责传递 GPU 的信息. 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> cuda<span class="token punctuation">-</span>vector<span class="token punctuation">-</span>add
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> cuda<span class="token punctuation">-</span>vector<span class="token punctuation">-</span>add
      <span class="token key atrule">image</span><span class="token punctuation">:</span> <span class="token string">&quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;</span>
      <span class="token key atrule">resources</span><span class="token punctuation">:</span>
        <span class="token key atrule">limits</span><span class="token punctuation">:</span>
          <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>可以看到, 在上述 Pod 的 limits 字段里, 这个资源的名称是 <code>nvidia.com/gpu</code>​, 它的值是 1. 也就是说, 这个 Pod 声明了自己要使用一个 NVIDIA 类型的 GPU.</p> <p><strong>而在 kube-scheduler 里面, 它其实并不关心这个字段的具体含义, 只会在计算的时候, 一律将调度器里保存的该类型资源的可用量, 直接减去 Pod 声明的数值即可. 所以说 Extended Resource, 其实是 Kubernetes 为用户设置的一种对自定义资源的支持.</strong></p> <p>当然为了能够让调度器知道这个自定义类型的资源在每台宿主机上的可用量, 宿主机节点本身, 就必须能够向 API Server 汇报该类型资源的可用数量. 在 Kubernetes 里, 各种类型的资源可用量, 其实是 Node 对象 Status 字段的内容, 比如下面这个例子:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Node
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span><span class="token number">1</span>
<span class="token punctuation">...</span>
<span class="token key atrule">Status</span><span class="token punctuation">:</span>
  <span class="token key atrule">Capacity</span><span class="token punctuation">:</span>
   <span class="token key atrule">cpu</span><span class="token punctuation">:</span>  <span class="token number">2</span>
   <span class="token key atrule">memory</span><span class="token punctuation">:</span>  2049008Ki
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>而为了能够在上述 Status 字段里添加自定义资源的数据, 就必须使用 <strong>PATCH</strong> API 来对该 Node 对象进行更新, 加上自定义资源的数量. 这个 PATCH 操作, 可以简单地使用 curl 命令来发起, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 启动 Kubernetes 的客户端 proxy, 这样你就可以直接使用 curl 来跟 Kubernetes  的 API Server 进行交互了</span>
$ kubectl proxy

<span class="token comment"># 执行 PACTH 操作</span>
$ <span class="token function">curl</span> <span class="token parameter variable">--header</span> <span class="token string">&quot;Content-Type: application/json-patch+json&quot;</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--request</span> PATCH <span class="token punctuation">\</span>
<span class="token parameter variable">--data</span> <span class="token string">'[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/nvidia.com/gpu&quot;, &quot;value&quot;: &quot;1&quot;}]'</span> <span class="token punctuation">\</span>
http://localhost:8001/api/v1/nodes/<span class="token operator">&lt;</span>your-node-name<span class="token operator">&gt;</span>/status
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>PATCH 操作完成后, 就可以看到 Node 的 Status 变成了如下所示的内容:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Node
<span class="token punctuation">...</span>
<span class="token key atrule">Status</span><span class="token punctuation">:</span>
  <span class="token key atrule">Capacity</span><span class="token punctuation">:</span>
   <span class="token key atrule">cpu</span><span class="token punctuation">:</span>  <span class="token number">2</span>
   <span class="token key atrule">memory</span><span class="token punctuation">:</span>  2049008Ki
   <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>这样在调度器里, 它就能够在缓存里记录下 node-1 上的 <code>nvidia.com/gpu</code>​ 类型的资源的数量是 1.</p> <p>当然在 Kubernetes 的 GPU 支持方案里, 并不需要真正去做上述关于 Extended Resource 的这些操作. 在 Kubernetes 中, <strong>对所有硬件加速设备进行管理的功能, 都是由一种叫作 Device Plugin 的插件来负责的</strong>. 这其中当然也就包括了对该硬件的 Extended Resource 进行汇报的逻辑.</p> <p>Kubernetes 的 Device Plugin 机制, 可以用如下所示的一幅示意图来解释清楚.</p> <p><img src="/img/32e8f979fea57f2b42916a09315d39f6-20230731162150-jo6fcws.png" alt=""></p> <p>先从这幅示意图的右侧开始看起.</p> <p>首先, 对于每一种<strong>硬件设备</strong>, 都需要有它所对应的 <strong>Device Plugin</strong> 进行管理, 这些 Device Plugin, 都通过 <strong>gRPC</strong> 的方式, 同 kubelet 连接起来. 以 NVIDIA GPU 为例, 它对应的插件叫作 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="noopener noreferrer"><code>NVIDIA GPU device plugin</code><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>​.</p> <p>这个 Device Plugin 会通过一个叫作 <strong>ListAndWatch</strong> 的 API, 定期向 kubelet 汇报该 Node 上 GPU 的列表. 比如在这个例子里, 一共有三个 GPU(GPU0, GPU1 和 GPU2). 这样 kubelet 在拿到这个列表之后, 就可以直接在它向 APIServer 发送的心跳里, 以 <strong>Extended Resource</strong> 的方式, 加上这些 GPU 的数量, 比如 <code>nvidia.com/gpu=3</code>​. 所以用户在这里是不需要关心 GPU 信息向上的汇报流程的.</p> <p>需要注意的是, ListAndWatch 向上汇报的信息, 只有<strong>本机上 GPU 的 ID 列表</strong>, 而不会有任何关于 GPU 设备本身的信息. 而且 kubelet 在向 API Server 汇报的时候, 只会汇报该 GPU 对应的 Extended Resource 的数量. 当然 kubelet 本身, 会将这个 GPU 的 ID 列表保存在自己的内存里, 并通过 ListAndWatch API 定时更新.</p> <p>而当一个 Pod 想要使用一个 GPU 的时候, 它只需要像本节一开始给出的例子一样, 在 Pod 的 limits 字段声明 <code>nvidia.com/gpu: 1</code>​. 那么接下来, Kubernetes 的调度器就会从它的缓存里, 寻找 GPU 数量满足条件的 Node, 然后将缓存里的 GPU 数量减 1, 完成 Pod 与 Node 的绑定.</p> <p>这个调度成功后的 Pod 信息, 自然就会被对应的 kubelet 拿来进行容器操作. 而当 kubelet 发现这个 Pod 的容器请求一个 GPU 的时候, kubelet 就会从自己持有的 GPU 列表里, 为这个容器分配一个 GPU. 此时 kubelet 就会向本机的 Device Plugin 发起一个 Allocate() 请求. 这个请求携带的参数, 正是即将分配给该容器的设备 ID 列表.</p> <p>当 Device Plugin 收到 Allocate 请求之后, 它就会根据 kubelet 传递过来的设备 ID, 从 Device Plugin 里找到这些设备对应的设备路径和驱动目录. 当然这些信息, 正是 Device Plugin 周期性的从本机查询到的. 比如, 在 NVIDIA Device Plugin 的实现里, 它会定期访问 nvidia-docker 插件, 从而获取到本机的 GPU 信息.</p> <p><strong>而被分配 GPU 对应的设备路径和驱动目录信息被返回给 kubelet 之后, kubelet 就完成了为一个容器分配 GPU 的操作</strong>. 接下来, kubelet 会把这些信息追加在创建该容器所对应的 CRI 请求当中. 这样当这个 CRI 请求发给 Docker 之后, Docker 创建出来的容器里, 就会出现这个 GPU 设备, 并把它所需要的驱动目录挂载进去.</p> <p>至此 Kubernetes 为一个 Pod 分配一个 GPU 的流程就完成了.</p> <p>对于其他类型<strong>硬件</strong>来说, <strong>要想在 Kubernetes 所管理的容器里使用这些硬件的话, 也需要遵循上述 Device Plugin 的流程来实现如下所示的 Allocate 和 ListAndWatch API</strong>:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code>service DevicePlugin <span class="token punctuation">{</span>
    <span class="token comment">// ListAndWatch returns a stream of List of Devices</span>
    <span class="token comment">// Whenever a Device state change or a Device disappears, ListAndWatch</span>
    <span class="token comment">// returns the new list</span>
    rpc <span class="token function">ListAndWatch</span><span class="token punctuation">(</span>Empty<span class="token punctuation">)</span> returns <span class="token punctuation">(</span>stream ListAndWatchResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    <span class="token comment">// Allocate is called during container creation so that the Device</span>
    <span class="token comment">// Plugin can run device specific operations and instruct Kubelet</span>
    <span class="token comment">// of the steps to make the Device available in the container</span>
    rpc <span class="token function">Allocate</span><span class="token punctuation">(</span>AllocateRequest<span class="token punctuation">)</span> returns <span class="token punctuation">(</span>AllocateResponse<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>目前, Kubernetes 社区里已经实现了很多硬件插件, 比如 <a href="https://github.com/intel/intel-device-plugins-for-kubernetes" target="_blank" rel="noopener noreferrer">FPGA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <a href="https://github.com/intel/sriov-network-device-plugin" target="_blank" rel="noopener noreferrer">SRIOV<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <a href="https://github.com/hustcat/k8s-rdma-device-plugin" target="_blank" rel="noopener noreferrer">RDMA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 等等. 感兴趣的话可以看看这些 Device Plugin 的实现.</p> <blockquote><p>总结</p></blockquote> <p>本节讲述了 Kubernetes 对 GPU 的管理方式, 以及它所需要使用的 Device Plugin 机制.</p> <p>需要指出的是, Device Plugin 的设计, 长期以来都是以 Google Cloud 的用户需求为主导的, 所以它的整套工作机制和流程上, 实际上跟学术界和工业界的真实场景还有着不小的差异.</p> <p>这里最大的问题在于, <strong>GPU 等硬件设备的调度工作, 实际上是由 kubelet 完成的</strong>. 即 <strong>kubelet 会负责从它所持有的硬件设备列表中, 为容器挑选一个硬件设备, 然后调用 Device Plugin 的 Allocate API 来完成这个分配操作</strong>. 可以看到, 在整条链路中, 调度器扮演的角色, 仅仅是为 Pod 寻找到可用的, 支持这种硬件设备的节点而已.</p> <p>这就使得, Kubernetes 里对硬件设备的管理, <strong>只能处理 &quot;设备个数&quot; 这唯一一种情况</strong>. 一旦设备是异构的, 不能简单地用 &quot;数目&quot; 去描述具体使用需求的时候, 比如, &quot;目标 Pod 想要运行在计算能力最强的那个 GPU 上&quot;, Device Plugin 就完全不能处理了. 更不用说, 在很多场景下, 其实希望在调度器进行调度的时候, 就可以根据整个集群里的某种硬件设备的全局分布, 做出一个最佳的调度选择.</p> <p>此外, 上述 Device Plugin 的设计, 也使得 Kubernetes 里, <strong>缺乏一种能够对 Device 进行描述的 API 对象</strong>. 这就使得如果硬件设备本身的属性比较复杂, 并且 Pod 也关心这些硬件的属性的话, 那么 Device Plugin 也是完全没有办法支持的. 更为棘手的是, 在 Device Plugin 的设计和实现中, Google 的工程师们一直不太愿意为 Allocate 和 ListAndWatch API 添加可扩展性的参数. 这就使得, 当确实需要处理一些比较复杂的硬件设备使用需求时, 是没有办法通过扩展 Device Plugin 的 API 来实现的.</p> <p>针对这些问题, RedHat 在社区里曾经大力推进过 <a href="https://github.com/kubernetes/community/pull/2265" target="_blank" rel="noopener noreferrer">ResourceClass<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 的设计, 试图将硬件设备的管理功能上浮到 API 层和调度层. 但是由于各方势力的反对, 这个提议最后不了了之了.</p> <p>所以目前 Kubernetes 本身的 Device Plugin 的设计, 实际上能覆盖的场景是非常单一的, 属于 &quot;可用&quot; 但是 &quot;不好用&quot; 的状态. 并且 Device Plugin 的 API 本身的可扩展性也不是很好. 这也就解释了为什么像 NVIDIA 这样的硬件厂商, 实际上并没有完全基于上游的 Kubernetes 代码来实现自己的 GPU 解决方案, 而是做了一定的改动, 也就是 fork. 这实属不得已而为之.</p> <h3 id="kubernetes容器运行时"><a href="#kubernetes容器运行时" class="header-anchor">#</a> Kubernetes容器运行时</h3> <h4 id="_45-幕后英雄-sig-node与cri"><a href="#_45-幕后英雄-sig-node与cri" class="header-anchor">#</a> 45 | 幕后英雄:SIG-Node与CRI</h4> <p>前面讲解了关于 Kubernetes 调度和资源管理相关的内容. 实际上, 在调度这一步完成后, Kubernetes 就需要负责<strong>将这个调度成功的 Pod, 在宿主机上创建出来, 并把它所定义的各个容器启动起来</strong>. 这些都是 kubelet 这个核心组件的主要功能. 下面三节就深入到 kubelet 里面来详细剖析一下 <strong>Kubernetes 对容器运行时的管理能力</strong>.</p> <p>在 Kubernetes 社区里, 与 kubelet 以及容器运行时管理相关的内容, 都属于 <strong>SIG-Node</strong> 的范畴. 如果你经常参与社区的话, 你可能会觉得, 相比于其他每天都热闹非凡的 SIG 小组, SIG-Node 是 Kubernetes 里相对沉寂也不太发声的一个小组, 小组里的成员也很少在外面公开宣讲. 不过正如前面所介绍的, SIG-Node 以及 kubelet, 其实是 Kubernetes 整套体系里<strong>非常核心</strong>的一个部分. 毕竟它们才是 Kubernetes 这样一个容器编排与管理系统, 跟容器打交道的主要 &quot;场所&quot;.</p> <p>而 kubelet 这个组件本身, 也是 Kubernetes 里面第二个不可被替代的组件(第一个不可被替代的组件当然是 kube-apiserver). 也就是说, <mark><strong>无论如何都不太建议对 kubelet 的代码进行大量的改动. 保持 kubelet 跟上游基本一致的重要性, 就跟保持 kube-apiserver 跟上游一致是一个道理</strong></mark>​ **. **</p> <p>当然, kubelet 本身, 也是按照  <strong>&quot;控制器&quot; 模式</strong>来工作的. 它实际的工作原理, 可以用如下所示的一幅示意图来表示清楚.</p> <p><img src="/img/d3836d2ab8758de89929314e6338f5b2-20230731162150-ccp9xty.png" alt="">​</p> <p>可以看到, kubelet 的工作核心, 就是一个<strong>控制循环</strong>, 即: <strong>SyncLoop</strong>(图中的大圆圈). 而驱动这个控制循环运行的事件, 包括四种:</p> <ol><li>**Pod 更新事件; **</li> <li>**Pod 生命周期变化; **</li> <li>**kubelet 本身设置的执行周期; **</li> <li>**定时的清理事件. **</li></ol> <p>所以跟其他控制器类似, kubelet 启动的时候, 要做的第一件事情就是<strong>设置 Listers</strong>, 也就是<strong>注册它所关心的各种事件的 Informer.</strong>  这些 Informer 就是 SyncLoop 需要处理的<strong>数据的来源</strong>.</p> <p>此外 kubelet 还负责维护着很多很多其他的<strong>子控制循环</strong>(也就是图中的小圆圈). 这些控制循环的名字, 一般被称作某某 <strong>Manager</strong>, 比如 Volume Manager, Image Manager, Node Status Manager 等等. 不难想到, 这些控制循环的责任, 就是<strong>通过控制器模式, 完成 kubelet 的某项具体职责</strong>. 比如 Node Status Manager, 就负责响应 Node 的状态变化, 然后将 Node 的状态收集起来, 并通过 Heartbeat 的方式上报给 APIServer. 再比如 CPU Manager, 就负责维护该 Node 的 CPU 核的信息, 以便在 Pod 通过 cpuset 的方式请求 CPU 核的时候, 能够正确地管理 CPU 核的使用量和可用量.</p> <p>那么这个 **SyncLoop, 又是如何根据 Pod 对象的变化, 来进行容器操作的呢? **</p> <p>实际上, kubelet 也是<mark><strong>通过 Watch 机制, 监听了与自己相关的 Pod 对象的变化</strong></mark>. 当然这个 Watch 的过滤条件是该 Pod 的 nodeName 字段与自己相同. <strong>kubelet 会把这些 Pod 的信息缓存在自己的内存里</strong>.</p> <p>而当一个 Pod <strong>完成调度</strong>, 与一个 Node 绑定起来之后,  <strong>这个 Pod 的变化就会触发 kubelet 在控制循环里注册的 Handler</strong>, 也就是上图中的 HandlePods 部分. 此时通过检查该 Pod 在 kubelet 内存里的状态, kubelet 就能够判断出这是一个新调度过来的 Pod, 从而触发 Handler 里 ADD 事件对应的处理逻辑.</p> <p>在具体的处理过程当中, kubelet 会启动一个名叫 Pod Update Worker 的, 单独的 Goroutine 来完成对 Pod 的处理工作. 比如如果是 ADD 事件的话, kubelet 就会为这个新的 Pod 生成对应的 Pod Status, 检查 Pod 所声明使用的 Volume 是不是已经准备好. 然后调用下层的容器运行时(比如 Docker), 开始创建这个 Pod 所定义的容器. 而如果是 UPDATE 事件的话, kubelet 就会根据 Pod 对象具体的变更情况, 调用下层容器运行时进行容器的重建工作.</p> <p>在这里需要注意的是, <mark><strong>kubelet 调用下层容器运行时的执行过程, 并不会直接调用 Docker 的 API, 而是通过一组叫作 CRI(Container Runtime Interface, 容器运行时接口)的 gRPC 接口来间接执行的</strong></mark>​ **. **</p> <p>Kubernetes 项目之所以要在 kubelet 中引入这样一层<strong>单独的抽象</strong>, 当然是为了对 Kubernetes 屏蔽下层容器运行时的差异. 实际上, 对于 1.6 版本之前的 Kubernetes 来说, 它就是直接调用 Docker 的 API 来创建和管理容器的.</p> <p>但是, 正如一开始介绍容器背景的时候提到过的, Docker 项目风靡全球后不久, CoreOS 公司就推出了 rkt 项目来与 Docker 正面竞争. 在这种背景下, Kubernetes 项目的默认容器运行时, 自然也就成了两家公司角逐的重要战场. 毋庸置疑, Docker 项目必然是 Kubernetes 项目最依赖的容器运行时. 但凭借与 Google 公司非同一般的关系, CoreOS 公司还是在 2016 年成功地将对 rkt 容器的支持, 直接添加进了 kubelet 的主干代码里. 不过这个 &quot;赶鸭子上架&quot; 的举动, 并没有为 rkt 项目带来更多的用户, 反而给 kubelet 的维护人员, 带来了巨大的负担. 不难想象, 在这种情况下, <strong>kubelet 任何一次重要功能的更新, 都不得不考虑 Docker 和 rkt 这两种容器运行时的处理场景, 然后分别更新 Docker 和 rkt 两部分代码. ** 更让人为难的是, 由于 rkt 项目实在太小众, kubelet 团队所有与 rkt 相关的代码修改, 都必须依赖于 CoreOS 的员工才能做到. 这不仅拖慢了 kubelet 的开发周期, 也给项目的稳定性带来了巨大的隐患. 与此同时, 在 2016 年, Kata Containers 项目的前身 runV 项目也开始逐渐成熟, 这种基于虚拟化技术的强隔离容器, 与 Kubernetes 和 Linux 容器项目之间具有良好的互补关系. 所以, <strong>在 Kubernetes 上游, 对虚拟化容器的支持很快就被提上了日程. ** 不过, 虽然虚拟化容器运行时有各种优点, 但它与 Linux 容器截然不同的实现方式, 使得它跟 Kubernetes 的集成工作, 比 rkt 要复杂得多. 如果此时再把对 runV 支持的代码也一起添加到 kubelet 当中, 那么接下来 kubelet 的维护工作就可以说完全没办法正常进行了. 所以在 2016 年, SIG-Node 决定开始动手解决上述问题. <strong>而解决办法也很容易想到, 那就是把 kubelet 对容器的操作, 统一地抽象成一个接口</strong>. 这样 kubelet 就只需要跟这个接口打交道了. 而作为具体的容器项目, 比如 Docker,  rkt, runV, 它们就</strong>只需要自己提供一个该接口的实现, 然后对 kubelet 暴露出 gRPC 服务即可</strong>. 这一层统一的容器操作接口, 就是 <strong>CRI</strong> 了. 下一节会讲解 CRI 的设计与具体的实现原理.</p> <p>而在有了 CRI 之后, Kubernetes 以及 kubelet 本身的架构, 就可以用如下所示的一幅示意图来描述.</p> <p><img src="/img/de00366b6c3d133eb8421b09df20a765-20230731162150-ul79mm3.png" alt=""></p> <p>​可以看到, 当 Kubernetes 通过编排能力创建了一个 Pod 之后, <strong>调度器会为这个 Pod 选择一个具体的节点来运行</strong>. 这时候, kubelet 当然就会通过前面讲解过的 SyncLoop 来判断需要执行的具体操作, 比如创建一个 Pod. 那么此时, kubelet 实际上就会调用一个叫作 <strong>GenericRuntime</strong> 的通用组件来<strong>发起创建 Pod 的 CRI 请求</strong>.</p> <p>那么**这个 CRI 请求, 又该由谁来响应呢? **</p> <p>如果使用的容器项目是 Docker 的话, 那么负责响应这个请求的就是一个叫作 <strong>dockershim</strong> 的组件. 它会把 CRI 请求里的内容拿出来, 然后<strong>组装成 Docker API 请求发给 Docker Daemon</strong>. 需要注意的是, 在 Kubernetes 目前的实现里, dockershim 依然是 kubelet 代码的一部分. 当然在将来, dockershim 肯定会被从 kubelet 里移出来, 甚至直接被废弃掉.</p> <p>而更普遍的场景, 就是<strong>需要在每台宿主机上单独安装一个负责响应 CRI 的组件</strong>, 这个组件一般被称作 CRI shim. 顾名思义, CRI shim 的工作, 就是扮演 kubelet 与容器项目之间的 &quot;垫片&quot;(shim). 所以它的<strong>作用非常单一, 那就是实现 CRI 规定的每个接口, 然后把具体的 CRI 请求 &quot;翻译&quot; 成对后端容器项目的请求或者操作</strong>.</p> <blockquote><p>总结</p></blockquote> <p>本节介绍了 SIG-Node 的职责, 以及 kubelet 这个组件的工作原理. 然后讲解了 kubelet 究竟是如何将 Kubernetes 对应用的定义, 一步步转换成最终对 Docker 或者其他容器项目的 API 请求的.</p> <p>不难看到, 在这个过程中, <strong>kubelet 的 SyncLoop 和 CRI 的设计, 是其中最重要的两个关键点</strong>. 也正是基于以上设计, SyncLoop 本身就要求这个控制循环是绝对不可以被阻塞的. 所以凡是在 kubelet 里有可能会耗费大量时间的操作, 比如准备 Pod 的 Volume, 拉取镜像等, SyncLoop 都会开启单独的 Goroutine 来进行操作.</p> <h4 id="_46-解读cri与容器运行时"><a href="#_46-解读cri与容器运行时" class="header-anchor">#</a> 46 | 解读CRI与容器运行时</h4> <p>上一节讲解了 kubelet 的工作原理和 CRI 的来龙去脉. 本节就来深入地了解一下 <strong>CRI 的设计与工作原理</strong>.</p> <p>首先先来简要回顾一下有了 CRI 之后, Kubernetes 的架构图, 如下所示.</p> <p><img src="/img/226cb4d0db2dec0b10a6b3305a5531a5-20230731162150-7qatr7v.png" alt=""></p> <p>在上一节也提到了, <mark><strong>CRI 机制能够发挥作用的核心, 就在于每一种容器项目现在都可以自己实现一个 CRI shim, 自行对 CRI 请求进行处理. 这样, Kubernetes 就有了一个统一的容器抽象层, 使得下层容器运行时可以自由地对接进入 Kubernetes 当中</strong></mark>. 所以说, 这里的 CRI shim, 就是容器项目的维护者们自由发挥的 &quot;场地&quot; 了. 而除了 dockershim 之外, 其他容器运行时的 CRI shim, 都是需要<strong>额外部署</strong>在宿主机上的.</p> <p>举个例子. CNCF 里的 containerd 项目, 就可以提供一个典型的 CRI shim 的能力, 即: <strong>将 Kubernetes 发出的 CRI 请求, 转换成对 containerd 的调用, 然后创建出 runC 容器</strong>. 而 runC 项目, 才是负责执行前面讲解过的设置容器 Namespace, Cgroups 和 chroot 等基础操作的组件. 所以这几层的组合关系, 可以用如下所示的示意图来描述.</p> <p><img src="/img/4b777b2dd037560f38a2d81113f9a740-20230731162150-hcgm9y0.png" alt=""></p> <p>**而作为一个 CRI shim, containerd 对 CRI 的具体实现, 又是怎样的呢? **</p> <p>先来看一下 CRI 这个接口的定义. 下面这幅示意图, 就展示了 CRI 里主要的待实现接口.</p> <p><img src="/img/0e5e09174a91f45d173442399827132a-20230731162150-vscwqhw.png" alt=""></p> <p>具体地说, **可以把 CRI 分为两组: **</p> <ul><li>第一组, 是 <strong>RuntimeService</strong>. 它提供的接口主要是跟<strong>容器相关</strong>的操作. 比如创建和启动容器, 删除容器, 执行 exec 命令等等.</li> <li>而第二组, 则是 <strong>ImageService</strong>. 它提供的接口主要是<strong>容器镜像相关</strong>的操作比如拉取镜像, 删除镜像等等.</li></ul> <p>关于容器镜像的操作比较简单, 所以就暂且略过. 接下来主要讲解一下 <strong>RuntimeService</strong> 部分.</p> <p>**在这一部分, **​<mark><strong>CRI 设计的一个重要原则, 就是确保这个接口本身, 只关注容器, 不关注 Pod</strong></mark>​ **. ** 这样做的原因, 也很容易理解.</p> <p><strong>第一</strong>, Pod 是 Kubernetes 的编排概念, 而不是容器运行时的概念. 所以就不能假设所有下层容器项目, 都能够暴露出可以直接映射为 Pod 的 API.</p> <p><strong>第二</strong>, 如果 CRI 里引入了关于 Pod 的概念, 那么接下来只要 Pod API 对象的字段发生变化, 那么 CRI 就很有可能需要变更. 而在 Kubernetes 开发的前期, Pod 对象的变化还是比较频繁的, 但对于 CRI 这样的标准接口来说, 这个变更频率就有点麻烦了.</p> <p>所以在 CRI 的设计里, <strong>并没有一个直接创建 Pod 或者启动 Pod 的接口</strong>.</p> <p>不过, 相信你也已经注意到了, CRI 里还是有一组叫作 <strong>RunPodSandbox</strong> 的接口的. 这个 PodSandbox, 对应的并不是 Kubernetes 里的 Pod API 对象, 而<strong>只是抽取了 Pod 里的一部分与容器运行时相关的字段</strong>, 比如 HostName, DnsConfig, CgroupParent 等. 所以 PodSandbox 这个接口描述的, 其实<strong>是 Kubernetes 将 Pod 这个概念映射到容器运行时层面所需要的字段, 或者说是一个 Pod 对象子集</strong>.</p> <p>而作为<strong>具体的容器项目, 就需要自己决定如何使用这些字段来实现一个 Kubernetes 期望的 Pod 模型</strong>. 这里的原理, 可以用如下所示的示意图来表示清楚.</p> <p><img src="/img/aeb67ef50b66c835070a2a1cc5fff098-20230731162150-lr95d83.png" alt=""></p> <p>比如, 当执行 kubectl run 创建了一个名叫 foo 的, 包括了 A, B 两个容器的 Pod 之后. 这个 Pod 的信息最后来到 <strong>kubelet</strong>, kubelet 就会按照图中所示的顺序来<strong>调用 CRI 接口</strong>.</p> <p>在具体的 CRI shim 中, 这些接口的实现是可以完全不同的. 比如, 如果是 Docker 项目, dockershim 就会创建出一个名叫 foo 的 Infra 容器(pause 容器), 用来 &quot;hold&quot; 住整个 Pod 的 Network Namespace. 而如果是基于<strong>虚拟化技术</strong>的容器, 比如 Kata Containers 项目, 它的 CRI 实现就会直接创建出一个轻量级虚拟机来充当 Pod.</p> <p>需要注意的是, 在 RunPodSandbox 这个接口的实现中, 还需要调用 networkPlugin.SetUpPod() 来为这个 Sandbox 设置网络. 这个 SetUpPod() 方法, 实际上就在执行 CNI 插件里的 add() 方法, 也就是前面讲过的 CNI 插件为 Pod 创建网络, 并且把 Infra 容器加入到网络中的操作.</p> <p>接下来, kubelet 继续调用 <strong>CreateContainer 和 StartContainer 接口来创建和启动容器 A, B</strong>. 对应到 dockershim 里, 就是<strong>直接启动 A, B 两个 Docker 容器</strong>. 所以最后宿主机上会出现<strong>三个</strong> Docker 容器组成这一个 Pod.</p> <p>而如果是 Kata Containers 的话, CreateContainer 和 StartContainer 接口的实现, 就只会在前面创建的轻量级虚拟机里创建两个 A, B 容器对应的 Mount Namespace. 所以最后在宿主机上, 只会有<strong>一个</strong>叫作 foo 的轻量级虚拟机在运行. 关于像 Kata Containers 或者 gVisor 这种所谓的安全容器项目, 会在下一节中详细介绍.</p> <p>除了上述对容器生命周期的实现之外, CRI shim 还有一个重要的工作, 就是如何<strong>实现 exec, logs 等接口</strong>. 这些接口跟前面的操作有一个很大的不同, 就是这些 gRPC 接口调用期间, kubelet 需要跟容器项目维护一个长连接来<strong>传输数据</strong>. 这种 API 就称之为 Streaming API.</p> <p>CRI shim 里对 Streaming API 的实现, 依赖于一套独立的 Streaming Server 机制. 这一部分原理, 可以用如下所示的示意图来描述.</p> <p>​<img src="/img/5e1e35439d90cccf937f7682ec44d9c2-20230731162150-foty2ot.png" alt="">
可以看到, 当对一个容器执行 kubectl exec 命令的时候, 这个请求首先交给 API Server, 然后 API Server 就会调用 kubelet 的 Exec API. 这时, kubelet 就会调用 CRI 的 Exec 接口, 而负责响应这个接口的, 自然就是具体的 CRI shim.</p> <p>但在这一步, CRI shim 并不会直接去调用后端的容器项目(比如 Docker )来进行处理, 而只会返回一个 URL 给 kubelet. 这个 URL, 就是该 CRI shim 对应的 <strong>Streaming Server 的地址和端口</strong>. 而 kubelet 在拿到这个 URL 之后, 就会把它以 Redirect 的方式返回给 API Server. 所以这时候, API Server 就会通过重定向来向 Streaming Server 发起真正的 /exec 请求, 与它建立<strong>长连接</strong>.</p> <p>当然, 这个 Streaming Server 本身, 是需要通过使用 SIG-Node 维护的 Streaming API 库来实现的. 并且 Streaming Server 会在 CRI shim 启动时就一起启动. 此外, Stream Server 这一部分具体怎么实现, 完全可以由 CRI shim 的维护者自行决定. 比如对于 Docker 项目来说, dockershim 就是直接调用 Docker 的 Exec API 来作为实现的.</p> <p>以上就是 CRI 的设计以及具体的工作原理了.</p> <blockquote><p>总结</p></blockquote> <p>本节解读了 CRI 的设计和具体工作原理, 并梳理了实现 CRI 接口的核心流程. 不难看出, CRI 这个接口的设计, 实际上还是比较宽松的. 这就意味着, 作为容器项目的维护者, 在实现 CRI 的具体接口时, 往往拥有着很高的自由度, 这个自由度不仅包括了容器的生命周期管理, 也包括了如何将 Pod 映射成为自己的实现, 还包括了如何调用 CNI 插件来为 Pod 设置网络的过程.</p> <p>所以说, 当<strong>对容器这一层有特殊的需求时, 一定优先建议你考虑实现一个自己的 CRI shim</strong>, 而不是修改 kubelet 甚至容器项目的代码. 这样<mark><strong>通过插件的方式定制 Kubernetes 的做法, 也是整个 Kubernetes 社区最鼓励和推崇的一个最佳实践</strong></mark>. 这也正是为什么像 Kata Containers, gVisor 甚至虚拟机这样的 &quot;非典型&quot; 容器, 都可以无缝接入到 Kubernetes 项目里的重要原因.</p> <h4 id="_47-绝不仅仅是安全-kata-containers与gvisor"><a href="#_47-绝不仅仅是安全-kata-containers与gvisor" class="header-anchor">#</a> 47 | 绝不仅仅是安全:Kata Containers与gVisor</h4> <p>上一节讲解了 kubelet 和 CRI 的设计和具体的工作原理. 而在讲解 CRI 的诞生背景时也提到过, 这其中的一个重要推动力, 就是<strong>基于虚拟化或者独立内核的安全容器项目的逐渐成熟</strong>.</p> <p>使用虚拟化技术来做一个像 Docker 一样的容器项目, 并不是一个新鲜的主意. 早在 Docker 项目发布之后, Google 公司就开源了一个实验性的项目, 叫作 novm. 这可以算是试图使用常规的虚拟化技术来运行 Docker 镜像的第一次尝试. 不过, novm 在开源后不久就被放弃了, 这对于 Google 公司来说或许不算是什么新鲜事, 但是 novm 的昙花一现, 还是激发出了很多内核开发者的灵感. 所以在 2015 年, 几乎在同一个星期, Intel OTC (Open Source Technology Center) 和国内的 HyperHQ 团队同时开源了两个基于虚拟化技术的容器实现, 分别叫做 Intel Clear Container 和 runV 项目.</p> <p>而在 2017 年, 借着 Kubernetes 的东风, 这两个相似的容器运行时项目在中立基金会的撮合下最终合并, 就成了现在大家耳熟能详的 Kata Containers 项目. 由于 Kata Containers 的本质就是一个精简后的轻量级虚拟机, 所以它的特点就是 &quot;像虚拟机一样安全, 像容器一样敏捷&quot;. 而在 2018 年, Google 公司则发布了一个名叫 gVisor 的项目. gVisor 项目给容器进程配置一个用 Go 语言实现的, 运行在用户态的, 极小的 &quot;独立内核&quot;. 这个内核对容器进程暴露 Linux 内核 ABI, 扮演着 &quot;Guest Kernel&quot; 的角色, 从而达到了将容器和宿主机隔离开的目的.</p> <p>不难看到, 无论是 Kata Containers, 还是 gVisor, 它们实现<strong>安全容器</strong>的方法其实是殊途同归的. <strong>这两种容器实现的本质, 都是给进程分配了一个独立的操作系统内核, 从而避免了让容器共享宿主机的内核. 这样容器进程能够看到的攻击面, 就从整个宿主机内核变成了一个极小的, 独立的, 以容器为单位的内核, 从而有效解决了容器进程发生 &quot;逃逸&quot; 或者夺取整个宿主机的控制权的问题</strong>. 这个原理, 可以用如下所示的示意图来表示清楚.</p> <p><img src="/img/5e40fb376bc52c8d5321375277dba519-20230731162150-3re1aph.png" alt="">​</p> <p>而它们的区别在于, Kata Containers 使用的是传统的虚拟化技术, 通过虚拟硬件模拟出了一台 &quot;小虚拟机&quot;, 然后在这个小虚拟机里安装了一个裁剪后的 Linux 内核来实现强隔离. 而 gVisor 的做法则更加激进, Google 的工程师直接用 Go 语言 &quot;模拟&quot; 出了一个运行在用户态的操作系统内核, 然后通过这个模拟的内核来代替容器进程向宿主机发起有限的, 可控的系统调用.</p> <p>接下来就详细解读一下 KataContainers 和 gVisor 具体的设计原理.</p> <p><strong>首先, 来看 KataContainers</strong>. 它的工作原理可以用如下所示的示意图来描述.</p> <p><img src="/img/e7d3b36c77524100a28d6064bee0e22c-20230731162150-g71wuqi.png" alt=""></p> <p>前面说过, Kata Containers 的本质, 就是一个<strong>轻量化虚拟机</strong>. 所以当启动一个 Kata Containers 之后, 其实就会看到一个正常的虚拟机在运行. 这也就意味着一个标准的虚拟机管理程序(Virtual Machine Manager, VMM)是运行 Kata Containers 必备的一个组件. 在上面图中, 使用的 VMM 就是 Qemu.</p> <p>而使用了<strong>虚拟机作为进程的隔离环境</strong>之后, Kata Containers 原生就带有了 Pod 的概念. 即: <strong>这个 Kata Containers 启动的虚拟机, 就是一个 Pod; 而用户定义的容器, 就是运行在这个轻量级虚拟机里的进程</strong>. 在具体实现上, Kata Containers 的虚拟机里会有一个特殊的 Init 进程负责管理虚拟机里面的用户容器, 并且只为这些容器开启 Mount Namespace. 所以这些用户容器之间, 原生就是共享 Network 以及其他 Namespace 的.</p> <p>此外, 为了跟上层编排框架比如 Kubernetes 进行对接, Kata Containers 项目会启动一系列跟<strong>用户容器对应的 shim 进程</strong>, 来负责操作这些用户容器的生命周期. 当然这些操作, 实际上还是要靠虚拟机里的 Init 进程来帮你做到.</p> <p>而在具体的架构上, Kata Containers 的实现方式同一个正常的虚拟机其实也非常类似. 这里的原理可以用如下所示的一幅示意图来表示.<span data-type="text" style="background-color:var(--b3-theme-background);font-family:var(--b3-font-family);">可以看到, 当 Kata Containers 运行起来之后, 虚拟机里的用户进程(容器), 实际上只能看到虚拟机里的, 被裁减过的 Guest Kernel, 以及通过 Hypervisor 虚拟出来的硬件设备.</span></p> <p><img src="/img/58a25840b69f5b552ea68037288d207e-20230731162150-9q0v57w.png" alt="">​可以看到, 当 Kata Containers 运行起来之后, <strong>虚拟机里的用户进程(容器), 实际上只能看到虚拟机里的, 被裁减过的 Guest Kernel, 以及通过 Hypervisor 虚拟出来的硬件设备</strong>.</p> <p>而为了能够对这个虚拟机的 I/O 性能进行优化, Kata Containers 也会通过 vhost 技术(比如: vhost-user)来实现 Guest 与 Host 之间的高效的网络通信, 并且使用 PCI Passthrough (PCI 穿透)技术来让 Guest 里的进程直接访问到宿主机上的物理设备. 这些架构设计与实现, 其实跟常规虚拟机的优化手段是基本一致的.</p> <p>相比之下, gVisor 的设计其实要更加 &quot;激进&quot; 一些. 它的原理, 可以用如下所示的示意图来表示清楚.</p> <p><img src="/img/7b928d3d6654d39dce445d14bb5f8c50-20230731162150-7q44gk8.png" alt="">
gVisor 工作的核心, 在于它为应用进程, 也就是用户容器, 启动了一个名叫 <strong>Sentry 的进程</strong>. 而 Sentry 进程的主要职责, 就是提供一个传统的操作系统内核的能力, 即: <strong>运行用户程序, 执行系统调用</strong>. 所以说, Sentry 并不是使用 Go 语言重新实现了一个完整的 Linux 内核, 而只是<strong>一个对应用进程 &quot;冒充&quot; 内核的系统组件</strong>. 在这种设计思想下, 就不难理解, <strong>Sentry 其实需要自己实现一个完整的 Linux 内核网络栈, 以便处理应用进程的通信请求</strong>. 然后把封装好的二层帧直接发送给 Kubernetes 设置的 Pod 的 Network Namespace 即可.</p> <p>此外 Sentry 对于 Volume 的操作, 则需要通过 9p 协议交给一个叫做 Gofer 的代理进程来完成. Gofer 会代替应用进程直接操作宿主机上的文件, 并依靠 seccomp 机制将自己的能力限制在最小集, 从而防止恶意应用进程通过 Gofer 来从容器中 &quot;逃逸&quot; 出去.</p> <p>而在具体的实现上, gVisor 的 Sentry 进程, 其实还分为两种不同的实现方式. 这里的工作原理, 可以用下面的示意图来描述清楚.</p> <p><img src="/img/01ffd3e080c93b4210adc9e68c0e5a30-20230731162150-d25atk8.png" alt=""></p> <p><strong>第一种实现方式</strong>, 是使用 Ptrace 机制来拦截用户应用的系统调用(System Call), 然后把这些系统调用交给 Sentry 来进行处理. 这个过程对于应用进程来说是完全透明的. 而 Sentry 接下来, 则会扮演操作系统的角色, 在用户态执行用户程序, 然后仅在需要的时候, 才向宿主机发起 Sentry 自己所需要执行的系统调用. 这就是 gVisor 对用户应用进程进行强隔离的主要手段. 不过 Ptrace 进行系统调用拦截的性能实在是太差, 仅能供 Demo 时使用.</p> <p>而<strong>第二种实现方式</strong>, 则更加具有普适性. 它的工作原理如下图所示.</p> <p><img src="/img/3be8e798411b0af214a6bb61dd01ee36-20230731162150-bvwimbf.png" alt="">
在这种实现里, Sentry 会使用 KVM 来进行系统调用的拦截, 这个性能比 Ptrace 就要好很多了. 当然, 为了能够做到这一点, Sentry 进程就必须扮演一个 Guest Kernel 的角色, 负责执行用户程序, 发起系统调用. 而这些系统调用被 KVM 拦截下来, 还是继续交给 Sentry 进行处理. 只不过在这时候, Sentry 就切换成了一个普通的宿主机进程的角色, 来向宿主机发起它所需要的系统调用.</p> <p>可以看到, **在这种实现里, Sentry 并不会真的像虚拟机那样去虚拟出硬件设备, 安装 Guest 操作系统. 它只是借助 KVM 进行系统调用的拦截, 以及处理地址空间切换等细节. ** 值得一提的是, 在 Google 内部也是使用的第二种基于 Hypervisor 的 gVisor 实现. 只不过 Google 内部有自己研发的 Hypervisor, 所以要比 KVM 实现的性能还要好.</p> <p>另外, 你可能还听说过 AWS 在 2018 年末发布的一个叫做 Firecracker 的安全容器项目. 这个项目的核心, 其实是一个用 Rust 语言重新编写的 VMM(即: 虚拟机管理器). 这就意味着 Firecracker 和 Kata Containers 的本质原理, 其实是一样的. 只不过, Kata Containers 默认使用的 VMM 是 Qemu, 而 Firecracker, 则使用自己编写的 VMM. 所以, 理论上 Kata Containers 也可以使用 Firecracker 运行起来.</p> <blockquote><p>总结</p></blockquote> <p>本节介绍了拥有独立内核的安全容器项目, 对比了 KataContainers 和 gVisor 的设计与实现细节.</p> <p>在性能上, KataContainers 和 KVM 实现的 gVisor 基本不分伯仲, 在启动速度和占用资源上, 基于用户态内核的 gVisor 还略胜一筹. 但是对于系统调用密集的应用, 比如重 I/O 或者重网络的应用, gVisor 就会因为需要频繁拦截系统调用而出现性能急剧下降的情况. 此外 gVisor 由于要自己使用 Sentry 去模拟一个 Linux 内核, 所以它能支持的系统调用是有限的, 只是 Linux 系统调用的一个子集. 不过 gVisor 虽然现在没有任何优势, 但是这种通过在用户态运行一个操作系统内核, 来为应用进程提供强隔离的思路, 的确是未来安全容器进一步演化的一个非常有前途的方向.</p> <p>值得一提的是, Kata Containers 团队在 gVisor 之前, 就已经 Demo 了一个名叫 Linuxd 的项目. 这个项目, 使用了 User Mode Linux (UML) 技术, 在用户态运行起了一个真正的 Linux Kernel 来为应用进程提供强隔离, 从而避免了重新实现 Linux Kernel 带来的各种麻烦.</p> <h3 id="kubernetes容器监控与日志"><a href="#kubernetes容器监控与日志" class="header-anchor">#</a> Kubernetes容器监控与日志</h3> <h4 id="_48-prometheus-metrics-server与kubernetes监控体系"><a href="#_48-prometheus-metrics-server与kubernetes监控体系" class="header-anchor">#</a> 48 | Prometheus, Metrics Server与Kubernetes监控体系</h4> <p>前面讲了 Kubernetes 的核心架构, 编排概念, 以及具体的设计与实现. 接下来会用 3 节来介绍 Kubernetes <strong>监控相关</strong>的一些核心技术. Kubernetes 项目的监控体系曾经非常繁杂, 在社区中也有很多方案. 但这套体系发展到今天, 已经完全演变成了以 <strong>Prometheus</strong> 项目为核心的一套统一的方案.</p> <p>实际上, Prometheus 项目是当年 CNCF 基金会起家时的 &quot;第二把交椅&quot;. 而这个项目发展到今天, 已经**全面接管了 Kubernetes 项目的整套监控体系. ** 比较有意思的是, Prometheus 项目与 Kubernetes 项目一样, 也来自于 Google 的 Borg 体系, 它的原型系统, 叫作 BorgMon, 是一个几乎与 Borg 同时诞生的内部监控系统. 而 Prometheus 项目的发起原因也跟 Kubernetes 很类似, 都是希望通过对用户更友好的方式, 将 Google 内部系统的设计理念, 传递给用户和开发者.</p> <p>作为一个监控系统, Prometheus 项目的作用和工作方式, 其实可以用如下所示的一张官方示意图来解释.</p> <p><img src="/img/3e3f4f2b0a711b5babeef15afa11ca8c-20230731162150-6tc2pet.png" alt="">​</p> <p><mark><strong>可以看到, Prometheus 项目工作的核心, 是使用 Pull (抓取)的方式去搜集被监控对象的 Metrics 数据(监控指标数据), 然后再把这些数据保存在一个 TSDB (时间序列数据库, 比如 OpenTSDB, InfluxDB 等)当中, 以便后续可以按照时间进行检索.</strong></mark></p> <p>有了这套核心监控机制, Prometheus 剩下的组件就是用来<strong>配合</strong>这套机制的运行. 比如 Pushgateway, 可以允许被监控对象以 <strong>Push</strong> 的方式向 Prometheus 推送 Metrics 数据. 而 Alertmanager, 则可以根据 Metrics 信息灵活地设置<strong>报警</strong>. 当然,  Prometheus 最受用户欢迎的功能, 还是通过 <strong>Grafana</strong> 对外暴露出的, 可以灵活配置的监控数据可视化界面.</p> <p>有了 Prometheus 之后, 就可以按照 Metrics 数据的来源, 来对 Kubernetes 的监控体系做一个汇总了.</p> <p>**第一种 Metrics, <strong>​<mark><strong>是宿主机的监控数据</strong></mark>​ <strong>. ** 这部分数据的提供, 需要借助一个由 Prometheus 维护的 <a href="https://github.com/prometheus/node_exporter" target="_blank" rel="noopener noreferrer">Node Exporter<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 工具. 一般来说, <strong>Node Exporter 会以 DaemonSet 的方式运行在宿主机上</strong>. 其实</strong>所谓的 Exporter, 就是代替被监控对象来对 Prometheus 暴露出可以被 &quot;抓取&quot; 的 Metrics 信息的一个辅助进程</strong>. 而 Node Exporter 可以暴露给 Prometheus 采集的 Metrics 数据, 也不单单是节点的负载(Load), CPU , 内存, 磁盘以及网络这样的常规信息, 它的 Metrics 指标可以说是 &quot;包罗万象&quot;, 可以查看<a href="https://github.com/prometheus/node_exporter#enabled-by-default" target="_blank" rel="noopener noreferrer">这个列表<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>来感受一下.</p> <p><strong>第二种 Metrics, 是</strong>​<mark><strong>来自于 Kubernetes 的 API Server, kubelet 等组件的 /metrics API</strong></mark>. 除了常规的 CPU, 内存的信息外, 这部分信息还主要包括了各个组件的核心监控指标. 比如对于 API Server 来说, 它就会在 /metrics API 里, 暴露出各个 Controller 的工作队列(Work Queue)的长度, 请求的 QPS 和延迟数据等等. 这些信息是检查 Kubernetes 本身工作情况的主要依据.</p> <p>**第三种 Metrics, **​<mark><strong>是 Kubernetes 相关的监控数据</strong></mark>​ **. ** 这部分数据, 一般叫作 Kubernetes 核心监控数据(core metrics). 这其中包括了 Pod, Node, 容器, Service 等主要 Kubernetes 核心概念的 Metrics.</p> <p>其中容器相关的 Metrics 主要来自于 <strong>kubelet 内置的 cAdvisor 服务</strong>. 在 kubelet 启动后, cAdvisor 服务也随之启动, 而它能够提供的信息, 可以细化到每一个容器的 CPU, 文件系统, 内存, 网络等资源的使用情况.</p> <p>需要注意的是, 这里提到的 Kubernetes 核心监控数据, 其实使用的是 Kubernetes 的一个非常重要的扩展能力, 叫作 <strong>Metrics Server</strong>.</p> <p>Metrics Server 在 Kubernetes 社区的定位, 其实是用来取代 Heapster 这个项目的. 在 Kubernetes 项目发展的初期, Heapster 是用户获取 Kubernetes 监控数据(比如 Pod 和 Node 的资源使用情况) 的主要渠道. 而后面提出来的 Metrics Server, 则把这些信息, 通过标准的 Kubernetes API 暴露了出来. 这样, Metrics 信息就跟 Heapster 完成了解耦, 允许 Heapster 项目慢慢退出舞台.</p> <p>而有了 Metrics Server 之后, 用户就可以通过<strong>标准的 Kubernetes API 来访问到这些监控数据</strong>了. 比如下面这个 URL:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code>http<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token number">127.0</span><span class="token number">.0</span><span class="token number">.1</span><span class="token punctuation">:</span><span class="token number">8001</span><span class="token operator">/</span>apis<span class="token operator">/</span>metrics<span class="token punctuation">.</span>k8s<span class="token punctuation">.</span>io<span class="token operator">/</span>v1beta1<span class="token operator">/</span>namespace<span class="token operator">/</span><span class="token operator">&lt;</span>namespace<span class="token operator">-</span>name<span class="token operator">&gt;</span><span class="token operator">/</span>pods<span class="token operator">/</span><span class="token operator">&lt;</span>pod<span class="token operator">-</span>name<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>当访问这个 Metrics API 时, 它就会返回一个 Pod 的监控数据, 而这些数据其实是从 kubelet 的 Summary API (即 <code>&lt;kubelet_ip&gt;:&lt;kubelet_port&gt;/stats/summary)</code>​ 采集而来的. Summary API 返回的信息, 既包括了 cAdVisor 的监控数据, 也包括了 kubelet 本身汇总的信息.</p> <p>需要指出的是, Metrics Server 并不是 kube-apiserver 的一部分, 而是<strong>通过 Aggregator 这种插件机制, 在独立部署的情况下同 kube-apiserver 一起统一对外服务的</strong>.</p> <p>这里, Aggregator APIServer 的工作原理, 可以用如下所示的一幅示意图来表示清楚:</p> <p><img src="/img/f5d3b0f5bcd2a9757813cfa0af39c20b-20230731162150-l4m6e2p.png" alt=""></p> <p>备注: 图片出处<a href="https://blog.jetstack.io/blog/resource-and-custom-metrics-hpa-v2/" target="_blank" rel="noopener noreferrer">https://blog.jetstack.io/blog/resource-and-custom-metrics-hpa-v2/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>可以看到, 当 Kubernetes 的 API Server 开启了 Aggregator 模式之后, 再访问 apis/metrics.k8s.io/v1beta1 的时候, 实际上访问到的是一个叫作 <strong>kube-aggregator 的代理</strong>. 而 kube-apiserver 正是这个代理的一个后端; 而 Metrics Server, 则是另一个后端.</p> <p>而且在这个机制下, 还可以添加更多的后端给这个 kube-aggregator. 所以 <mark><strong>kube-aggregator 其实就是一个根据 URL 选择具体的 API 后端的代理服务器</strong></mark>​ **. ** 通过这种方式就可以很方便地扩展 Kubernetes 的 API 了.</p> <p>而 Aggregator 模式的开启也非常简单:</p> <ul><li>如果是使用 kubeadm <a href="http://xn--kube-up-kk6mp69bbsie91cd3x.sh" target="_blank" rel="noopener noreferrer">或者官方的 kube-up.sh<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 脚本部署 Kubernetes 集群的话, Aggregator 模式就是默认开启的;</li> <li>如果是手动 DIY 搭建的话, 就需要在 kube-apiserver 的启动参数里加上如下所示的配置:</li></ul> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>--requestheader-client-ca-file<span class="token operator">=</span><span class="token operator">&lt;</span>path to aggregator CA cert<span class="token operator">&gt;</span>
--requestheader-allowed-names<span class="token operator">=</span>front-proxy-client
--requestheader-extra-headers-prefix<span class="token operator">=</span>X-Remote-Extra-
--requestheader-group-headers<span class="token operator">=</span>X-Remote-Group
--requestheader-username-headers<span class="token operator">=</span>X-Remote-User
--proxy-client-cert-file<span class="token operator">=</span><span class="token operator">&lt;</span>path to aggregator proxy cert<span class="token operator">&gt;</span>
--proxy-client-key-file<span class="token operator">=</span><span class="token operator">&lt;</span>path to aggregator proxy key<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>而这些配置的作用, 主要就是为 Aggregator 这一层<strong>设置对应的 Key 和 Cert 文件</strong>. 而这些文件的生成, 就需要自己手动完成了, 具体流程请参考这篇<a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener noreferrer">官方文档<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>Aggregator 功能开启之后, 只需要将 Metrics Server 的 YAML 文件部署起来, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">git</span> clone https://github.com/kubernetes-incubator/metrics-server
$ <span class="token builtin class-name">cd</span> metrics-server
$ kubectl create <span class="token parameter variable">-f</span> deploy/1.8+/
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>接下来就会看到 metrics.k8s.io 这个 API 出现在了 Kubernetes API 列表当中.</p> <p>在理解了 Prometheus 关心的三种监控数据源, 以及 Kubernetes 的核心 Metrics 之后, 作为用户其实要做的就是<strong>将 Prometheus Operator 在 Kubernetes 集群里部署起来</strong>. 然后按照本节一开始介绍的架构, 把<strong>上述 Metrics 源配置起来, 让 Prometheus 自己去进行采集即可</strong>.</p> <p>在后续的文章中, 会进一步剖析 Kubernetes 监控体系以及自定义 Metrics (自定义监控指标)的具体技术点.</p> <blockquote><p>总结</p></blockquote> <p>本节介绍了 Kubernetes 当前监控体系的设计, 介绍了 Prometheus 项目在这套体系中的地位, 讲解了以 Prometheus 为核心的监控系统的架构设计. 然后解读了 Kubernetes 核心监控数据的来源, 即: Metrics Server 的具体工作原理, 以及 Aggregator APIServer 的设计思路.</p> <p>通过以上讲述, 希望你能够对 Kubernetes 的监控体系形成一个整体的认知, 体会到 Kubernetes 社区在监控这个事情上, 全面以 Prometheus 项目为核心进行建设的大方向.</p> <p>最后, 在具体的监控指标规划上, 建议**遵循业界通用的 USE 原则和 RED 原则. **</p> <p>其中, <strong>USE 原则</strong>指的是, 按照如下三个维度来规划资源监控指标:</p> <ol><li><strong>利用率</strong>(Utilization), 资源被有效利用起来提供服务的平均时间占比;</li> <li><strong>饱和度</strong>(Saturation), 资源拥挤的程度, 比如工作队列的长度;</li> <li><strong>错误率</strong>(Errors), 错误的数量.</li></ol> <p>而 <strong>RED 原则</strong>指的是, 按照如下三个维度来规划服务监控指标:</p> <ol><li><strong>每秒请求数量</strong>(Rate);</li> <li><strong>每秒错误数量</strong>(Errors);</li> <li><strong>服务响应时间</strong>(Duration).</li></ol> <p>不难发现, <strong>USE 原则主要关注的是 &quot;资源&quot;, 比如节点和容器的资源使用情况, 而 RED 原则主要关注的是 &quot;服务&quot;, 比如 kube-apiserver 或者某个应用的工作情况</strong>. 这两种指标, 在本节讲解的 Kubernetes + Prometheus 组成的监控体系中, 都是可以完全覆盖到的.</p> <h4 id="_49-custom-metrics-让auto-scaling不再-食之无味"><a href="#_49-custom-metrics-让auto-scaling不再-食之无味" class="header-anchor">#</a> 49 | Custom Metrics:让Auto Scaling不再&quot;食之无味&quot;</h4> <p>上一节详细讲述了 Kubernetes 里的核心监控体系的架构. 不难看到, Prometheus 项目在其中占据了最为核心的位置. 实际上, 借助上述监控体系, Kubernetes 就可以为你提供一种非常有用的能力, 那就是 <strong>Custom Metrics, 自定义监控指标</strong>.</p> <p>在过去的很多 PaaS 项目中, 其实都有一种叫作 <strong>Auto Scaling, 即自动水平扩展的功能</strong>. 只不过, 这个功能往往只能依据某种指定的资源类型执行水平扩展, 比如 CPU 或者 Memory 的使用值. 而在真实的场景中, <strong>用户需要进行 Auto Scaling 的依据往往是自定义的监控指标</strong>. 比如某个应用的等待队列的长度, 或者某种应用相关资源的使用情况. 这些复杂多变的需求, 在传统 PaaS 项目和其他容器编排项目里, 几乎是不可能轻松支持的.</p> <p>而凭借强大的 API 扩展机制, <strong>Custom Metrics</strong> 已经成为了 Kubernetes 的一项标准能力. 并且, Kubernetes 的自动扩展器组件 Horizontal Pod Autoscaler (HPA), 也可以直接使用 Custom Metrics 来执行用户指定的扩展策略, 这里的整个过程都是非常灵活和可定制的.</p> <p>不难想到, Kubernetes 里的 Custom Metrics 机制, 也是借助 <strong>Aggregator APIServer</strong> 扩展机制来实现的. 这里的具体原理是, 当把 Custom Metrics APIServer 启动之后, Kubernetes 里就会出现一个叫作 <code>custom.metrics.k8s.io</code>​ 的 API. 而当访问这个 URL 时, Aggregator 就会把请求转发给 Custom Metrics APIServer .</p> <p>而 Custom Metrics APIServer 的实现, 其实就是<strong>一个 Prometheus 项目的 Adaptor</strong>.</p> <p>比如, 现在要<strong>实现一个根据指定 Pod 收到的 HTTP 请求数量来进行 Auto Scaling 的 Custom Metrics</strong>, 这个 Metrics 就可以通过访问如下所示的自定义监控 URL 获取到:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>https://<span class="token operator">&lt;</span>apiserver_ip<span class="token operator">&gt;</span>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/pods/sample-metrics-app/http_requests 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这里的工作原理是, 当访问这个 URL 的时候, Custom Metrics APIServer 就会<strong>去 Prometheus 里查询名叫 sample-metrics-app 这个 Pod 的 http_requests 指标的值, 然后按照固定的格式返回给访问者</strong>. 当然, http_requests 指标的值, 就需要由 Prometheus 按照上一节中讲到的核心监控体系, 从目标 Pod 上采集来.</p> <p>这里具体的做法有很多种, 最普遍的做法, 就是让 Pod 里的应用本身暴露出一个 /metrics API, 然后在这个 API 里返回自己收到的 HTTP 的请求的数量. 所以说, 接下来 HPA 只需要定时访问前面提到的自定义监控 URL, 然后根据这些值计算是否要执行 Scaling 即可.</p> <p>接下来, 通过一个具体的实例来讲解一下 Custom Metrics 具体的使用方式. 这个实例的 GitHub 库<a href="https://github.com/resouer/kubeadm-workshop" target="_blank" rel="noopener noreferrer">在这里<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 可以点击链接查看. 在这个例子中, 依然会假设集群是 kubeadm 部署出来的, 所以 Aggregator 功能已经默认开启了.</p> <p><strong>首先</strong>, 当然是先部署 Prometheus 项目. 这一步, 当然会使用 Prometheus Operator 来完成, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> demos/monitoring/prometheus-operator.yaml
clusterrole <span class="token string">&quot;prometheus-operator&quot;</span> created
serviceaccount <span class="token string">&quot;prometheus-operator&quot;</span> created
clusterrolebinding <span class="token string">&quot;prometheus-operator&quot;</span> created
deployment <span class="token string">&quot;prometheus-operator&quot;</span> created

$ kubectl apply <span class="token parameter variable">-f</span> demos/monitoring/sample-prometheus-instance.yaml
clusterrole <span class="token string">&quot;prometheus&quot;</span> created
serviceaccount <span class="token string">&quot;prometheus&quot;</span> created
clusterrolebinding <span class="token string">&quot;prometheus&quot;</span> created
prometheus <span class="token string">&quot;sample-metrics-prom&quot;</span> created
<span class="token function">service</span> <span class="token string">&quot;sample-metrics-prom&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p><strong>第二步</strong>, 需要把 Custom Metrics APIServer 部署起来, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> demos/monitoring/custom-metrics.yaml
namespace <span class="token string">&quot;custom-metrics&quot;</span> created
serviceaccount <span class="token string">&quot;custom-metrics-apiserver&quot;</span> created
clusterrolebinding <span class="token string">&quot;custom-metrics:system:auth-delegator&quot;</span> created
rolebinding <span class="token string">&quot;custom-metrics-auth-reader&quot;</span> created
clusterrole <span class="token string">&quot;custom-metrics-read&quot;</span> created
clusterrolebinding <span class="token string">&quot;custom-metrics-read&quot;</span> created
deployment <span class="token string">&quot;custom-metrics-apiserver&quot;</span> created
<span class="token function">service</span> <span class="token string">&quot;api&quot;</span> created
apiservice <span class="token string">&quot;v1beta1.custom-metrics.metrics.k8s.io&quot;</span> created
clusterrole <span class="token string">&quot;custom-metrics-server-resources&quot;</span> created
clusterrolebinding <span class="token string">&quot;hpa-controller-custom-metrics&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p><strong>第三步</strong>, 需要为 Custom Metrics APIServer 创建对应的 ClusterRoleBinding, 以便能够使用 curl 来直接访问 Custom Metrics 的 API:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrolebinding allowall-cm <span class="token parameter variable">--clusterrole</span> custom-metrics-server-resources <span class="token parameter variable">--user</span> system:anonymous
clusterrolebinding <span class="token string">&quot;allowall-cm&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>第四步</strong>, 就可以把待监控的应用和 HPA 部署起来了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> demos/monitoring/sample-metrics-app.yaml
deployment <span class="token string">&quot;sample-metrics-app&quot;</span> created
<span class="token function">service</span> <span class="token string">&quot;sample-metrics-app&quot;</span> created
servicemonitor <span class="token string">&quot;sample-metrics-app&quot;</span> created
horizontalpodautoscaler <span class="token string">&quot;sample-metrics-app-hpa&quot;</span> created
ingress <span class="token string">&quot;sample-metrics-app&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这里需要关注一下 HPA 的配置, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> HorizontalPodAutoscaler
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> autoscaling/v2beta1
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> sample<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>app<span class="token punctuation">-</span>hpa
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">scaleTargetRef</span><span class="token punctuation">:</span>
    <span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
    <span class="token key atrule">name</span><span class="token punctuation">:</span> sample<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>app
  <span class="token key atrule">minReplicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">maxReplicas</span><span class="token punctuation">:</span> <span class="token number">10</span>
  <span class="token key atrule">metrics</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Object
    <span class="token key atrule">object</span><span class="token punctuation">:</span>
      <span class="token key atrule">target</span><span class="token punctuation">:</span>
        <span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
        <span class="token key atrule">name</span><span class="token punctuation">:</span> sample<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>app
      <span class="token key atrule">metricName</span><span class="token punctuation">:</span> http_requests
      <span class="token key atrule">targetValue</span><span class="token punctuation">:</span> <span class="token number">100</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>可以看到, <mark><strong>HPA 的配置就是设置 Auto Scaling 规则的地方</strong></mark>​ **. **</p> <p>比如, <strong>scaleTargetRef</strong> 字段就指定了<strong>被监控的对象</strong>是名叫 sample-metrics-app 的 Deployment, 也就是上面部署的被监控应用. 并且它最小的实例数目是 2, 最大是 10.</p> <p>在 <strong>metrics</strong> 字段指定了这个 HPA 进行 Scale 的依据, 是名叫 http_requests 的 Metrics. 而获取这个 Metrics 的途径, 则是访问名叫 sample-metrics-app 的 Service.</p> <p>有了这些字段里的定义, HPA 就可以向如下所示的 URL 发起请求来获取 Custom Metrics 的值了:</p> <div class="language-http line-numbers-mode"><pre class="language-http"><code><span class="token header"><span class="token header-name keyword">https</span><span class="token punctuation">:</span><span class="token header-value">//&lt;apiserver_ip&gt;/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>需要注意的是, 上述这个 URL 对应的被监控对象, 是应用对应的 Service. 这跟本节一开始举例用到的 Pod 对应的 Custom Metrics URL 是不一样的. 当然**对于一个多实例应用来说, 通过 Service 来采集 Pod 的 Custom Metrics 其实才是合理的做法. **</p> <p>这时候, 可以通过一个名叫 hey 的测试工具来为应用增加一些访问压力, 具体做法如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token comment"># Install hey</span>
$ <span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token parameter variable">-v</span> /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey

$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">APP_ENDPOINT</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>kubectl get svc sample-metrics-app <span class="token parameter variable">-o</span> template <span class="token parameter variable">--template</span> <span class="token punctuation">{</span><span class="token punctuation">{</span>.spec.clusterIP<span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token variable">)</span></span><span class="token punctuation">;</span> <span class="token builtin class-name">echo</span> <span class="token variable">${APP_ENDPOINT}</span>
$ hey <span class="token parameter variable">-n</span> <span class="token number">50000</span> <span class="token parameter variable">-c</span> <span class="token number">1000</span> http://<span class="token variable">${APP_ENDPOINT}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>与此同时, 如果去访问应用 Service 的 Custom Metircs URL, 就会看到这个 URL 已经可以返回应用收到的 HTTP 请求数量了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-sSLk</span> https://<span class="token operator">&lt;</span>apiserver_ip<span class="token operator">&gt;</span>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;MetricValueList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;custom-metrics.metrics.k8s.io/v1beta1&quot;</span>,
  <span class="token string">&quot;metadata&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;selfLink&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests&quot;</span>
  <span class="token punctuation">}</span>,
  <span class="token string">&quot;items&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token string">&quot;describedObject&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Service&quot;</span>,
        <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;sample-metrics-app&quot;</span>,
        <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/__internal&quot;</span>
      <span class="token punctuation">}</span>,
      <span class="token string">&quot;metricName&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;http_requests&quot;</span>,
      <span class="token string">&quot;timestamp&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2018-11-30T20:56:34Z&quot;</span>,
      <span class="token string">&quot;value&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;501484m&quot;</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>**这里需要注意的, 是 Custom Metrics API 返回的 Value 的格式. **</p> <p>在为被监控应用编写 /metrics API 的返回值时, 其实比较容易计算的, 是该 Pod 收到的 HTTP request 的总数. 所以这个<a href="https://github.com/resouer/kubeadm-workshop/blob/master/images/autoscaling/server.js" target="_blank" rel="noopener noreferrer">应用的代码<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>其实是如下所示的样子:</p> <div class="language-go line-numbers-mode"><pre class="language-go"><code><span class="token keyword">if</span> <span class="token punctuation">(</span>request<span class="token punctuation">.</span>url <span class="token operator">==</span> <span class="token string">&quot;/metrics&quot;</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;# HELP http_requests_total The amount of requests served by the server in total\n# TYPE http_requests_total counter\nhttp_requests_total &quot;</span> <span class="token operator">+</span> totalrequests <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 应用在 /metrics 对应的 HTTP response 里返回的, 其实是 <strong>http_requests_total</strong> 的值. 这也就是 Prometheus 收集到的值.</p> <p>而 Custom Metrics APIServer 在收到对 http_requests 指标的访问请求之后, 它会从 Prometheus 里<strong>查询 http_requests_total 的值</strong>, 然后把它折算成一个以时间为单位的请求率, 最后把这个结果作为 http_requests 指标对应的值返回回去.</p> <p>所以说, 在对前面的 Custom Metircs URL 进行访问时, 会看到值是 501484m, 这里的格式, 其实就是 milli-requests, 相当于是在过去两分钟内, 每秒有 501 个请求. 这样应用的开发者就无需关心如何计算每秒的请求个数了. 而这样的 &quot;请求率&quot; 的格式, 是可以直接被 HPA 拿来使用的. 这时候, 如果同时查看 Pod 的个数的话, 就会看到 HPA 开始增加 Pod 的数目了.</p> <p>不过在这里可能会有一个疑问, Prometheus 项目, 又是<strong>如何知道采集哪些 Pod 的 /metrics API 作为监控指标的来源</strong>呢.</p> <p>实际上, 如果仔细观察一下前面创建应用的输出, 可以看到有一个类型是 <strong>ServiceMonitor</strong> 的对象也被创建了出来. 它的 YAML 文件如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> monitoring.coreos.com/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceMonitor
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> sample<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>app
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>
    <span class="token key atrule">service-monitor</span><span class="token punctuation">:</span> sample<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>app
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> sample<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>app
  <span class="token key atrule">endpoints</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> web
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>这个 ServiceMonitor 对象, 正是 <strong>Prometheus Operator 项目用来指定被监控 Pod 的一个配置文件</strong>. 可以看到, 其实是通过 <strong>Label Selector 为 Prometheus 来指定被监控应用</strong>的.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Kubernetes 里对自定义监控指标, 即 Custom Metrics 的设计与实现机制. 这套机制的可扩展性非常强, 也终于使得 Auto Scaling 在 Kubernetes 里面不再是一个 &quot;食之无味&quot; 的鸡肋功能了. 另外可以看到, Kubernetes 的 Aggregator APIServer, 是一个非常行之有效的 API 扩展机制. 而且 Kubernetes 社区已经为你提供了一套叫作 <a href="https://github.com/kubernetes-sigs/kubebuilder" target="_blank" rel="noopener noreferrer">KubeBuilder<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 的工具库, 帮助你生成一个 API Server 的完整代码框架, 你只需要在里面添加自定义 API, 以及对应的业务逻辑即可.</p> <h4 id="_50-让日志无处可逃-容器日志收集与管理"><a href="#_50-让日志无处可逃-容器日志收集与管理" class="header-anchor">#</a> 50 | 让日志无处可逃:容器日志收集与管理</h4> <p>前面讲解了 Kubernetes 的核心监控体系和自定义监控体系的设计与实现思路. 本节就来介绍一下 Kubernetes 里关于<strong>容器日志</strong>的处理方式.</p> <p>首先需要明确的是, Kubernetes 里面对容器日志的处理方式, 都叫作 <strong>cluster-level-logging</strong>, 即: <mark><strong>这个日志处理系统, 与容器, Pod 以及 Node 的生命周期都是完全无关的</strong></mark>. 这种设计当然是为了保证, 无论是容器挂了, Pod 被删除, 甚至节点宕机的时候, 应用的日志依然可以被正常获取到.</p> <p>而对于一个容器来说, 当应用把日志输出到 stdout 和 stderr 之后, 容器项目在默认情况下就会把这些日志输出到<strong>宿主机上的一个 JSON 文件</strong>里. 这样通过 kubectl logs 命令就可以看到这些容器的日志了. 上述机制, 就是今天要讲解的容器日志收集的基础假设. 而如果你的应用是把文件输出到其他地方, 比如直接输出到了容器里的某个文件里, 或者输出到了远程存储里, 那就属于特殊情况了. 当然本节也会对这些特殊情况的处理方法进行讲述.</p> <p>而 Kubernetes 本身, <strong>实际上是不会做容器日志收集工作的</strong>, 所以为了实现上述 cluster-level-logging, 需要在部署集群的时候, 提前对具体的日志方案进行规划. 而 Kubernetes 项目本身, 主要推荐了三种日志方案.</p> <p><mark><strong>第一种, 在 Node 上部署 logging agent, 将日志文件转发到后端存储里保存起来</strong></mark>​ **. ** 这个方案的架构图如下所示.</p> <p><img src="/img/75478cbe8db80e4e6565d72fd898f7a3-20230731162150-k21z4xp.png" alt=""></p> <p>不难看到, 这里的核心就在于 <strong>logging agent</strong>, 它一般都会<strong>以 DaemonSet 的方式运行在节点上, 然后将宿主机上的容器日志目录挂载进去, 最后由 logging-agent 把日志转发出去</strong>.</p> <p>举个例子, 可以通过 Fluentd 项目作为宿主机上的 logging-agent, 然后把日志转发到远端的 <strong>ElasticSearch</strong> 里保存起来供将来进行检索. 具体的操作过程, 可以通过阅读<a href="https://kubernetes.io/docs/user-guide/logging/elasticsearch" target="_blank" rel="noopener noreferrer">这篇文档<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>来了解. 另外在很多 Kubernetes 的部署里, 会自动为你启用 <a href="https://linux.die.net/man/8/logrotate" target="_blank" rel="noopener noreferrer">logrotate<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, 在日志文件超过 10MB 的时候自动对日志文件进行 rotate 操作.</p> <p>可以看到, 在 Node 上部署 logging agent 最大的优点, 在于<strong>一个节点只需要部署一个 agent, 并且不会对应用和 Pod 有任何侵入性</strong>. 所以这个方案在社区里是<strong>最常用</strong>的一种. 但是也不难看到, 这种方案的不足之处就在于, 它<strong>要求应用输出的日志, 都必须是直接输出到容器的 stdout 和 stderr 里</strong>.</p> <p>所以, **Kubernetes 容器日志方案的第二种, 就是对这种特殊情况的一个处理, 即: 当容器的日志只能输出到某些文件里的时候, 可以通过一个 sidecar 容器把这些日志文件重新输出到 sidecar 的 stdout 和 stderr 上, 这样就能够继续使用第一种方案了. ** 这个方案的具体工作原理, 如下所示.</p> <p><img src="/img/112f7358057e9ea10b0923068d6518c9-20230731162150-flc6i4c.png" alt="">
比如, 现在我的应用 Pod 只有一个容器, 它会把日志输出到容器里的  <strong>/var/log/1.log 和 2.log</strong> 这两个文件里. 这个 Pod 的 YAML 文件如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> counter
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> count
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> /bin/sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
    <span class="token punctuation">-</span> <span class="token punctuation">&gt;</span><span class="token scalar string">
      i=0;
      while true;
      do
        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;
        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>在这种情况下, 用 kubectl logs 命令是看不到应用的任何日志的. 而且前面讲解的, 最常用的方案一也是没办法使用的.</p> <p>那么这个时候, 就可以<strong>为这个 Pod 添加两个 sidecar 容器, 分别将上述两个日志文件里的内容重新以 stdout 和 stderr 的方式输出出来</strong>, 这个 YAML 文件的写法如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> counter
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> count
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> /bin/sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
    <span class="token punctuation">-</span> <span class="token punctuation">&gt;</span><span class="token scalar string">
      i=0;
      while true;
      do
        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;
        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> count<span class="token punctuation">-</span>log<span class="token punctuation">-</span><span class="token number">1</span>
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>/bin/sh<span class="token punctuation">,</span> <span class="token punctuation">-</span>c<span class="token punctuation">,</span> <span class="token string">'tail -n+1 -f /var/log/1.log'</span><span class="token punctuation">]</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> count<span class="token punctuation">-</span>log<span class="token punctuation">-</span><span class="token number">2</span>
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>/bin/sh<span class="token punctuation">,</span> <span class="token punctuation">-</span>c<span class="token punctuation">,</span> <span class="token string">'tail -n+1 -f /var/log/2.log'</span><span class="token punctuation">]</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br></div></div><p>这时候就可以通过 kubectl logs 命令查看这两个 sidecar 容器的日志, 间接看到应用的日志内容了, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs counter count-log-1
<span class="token number">0</span>: Mon Jan <span class="token number">1</span> 00:00:00 UTC <span class="token number">2001</span>
<span class="token number">1</span>: Mon Jan <span class="token number">1</span> 00:00:01 UTC <span class="token number">2001</span>
<span class="token number">2</span>: Mon Jan <span class="token number">1</span> 00:00:02 UTC <span class="token number">2001</span>
<span class="token punctuation">..</span>.
$ kubectl logs counter count-log-2
Mon Jan <span class="token number">1</span> 00:00:00 UTC <span class="token number">2001</span> INFO <span class="token number">0</span>
Mon Jan <span class="token number">1</span> 00:00:01 UTC <span class="token number">2001</span> INFO <span class="token number">1</span>
Mon Jan <span class="token number">1</span> 00:00:02 UTC <span class="token number">2001</span> INFO <span class="token number">2</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>由于 sidecar 跟主容器之间是<strong>共享 Volume 的</strong>, 所以这里的 sidecar 方案的<strong>额外性能损耗并不高</strong>, 也就是多占用一点 CPU 和内存罢了.</p> <p>但需要注意的是, 这时候宿主机上实际上会存在两份相同的日志文件: <strong>一份是应用自己写入的; 另一份则是 sidecar 的 stdout 和 stderr 对应的 JSON 文件</strong>. 这对磁盘是很大的浪费. 所以除非万不得已或者应用容器完全不可能被修改, 否则还是建议直接使用方案一, 或者直接使用下面的第三种方案.</p> <p>**第三种方案, 就是通过一个 sidecar 容器, 直接把应用的日志文件发送到远程存储里面去. **也就是相当于把方案一里的 logging agent, 放在了应用 Pod 里. 这种方案的架构如下所示:</p> <p><img src="/img/589a58d6ce8c9e74135fa2d709430b19-20230731162150-e70mf1f.png" alt="">​</p> <p>在这种方案里, 应用还可以直接把日志输出到固定的文件里而不是 stdout, 你的 logging-agent 还可以使用 fluentd, 后端存储还可以是 ElasticSearch. 只不过 fluentd 的输入源, 变成了应用的日志文件. 一般来说, 会把 fluentd 的输入源配置保存在一个 ConfigMap 里, 如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>config
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">fluentd.conf</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">
    &lt;source&gt;
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag count.format1
    &lt;/source&gt;</span>
  
    &lt;source<span class="token punctuation">&gt;</span>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag count.format2
    &lt;/source<span class="token punctuation">&gt;</span>
  
    &lt;match <span class="token important">**&gt;</span>
      type google_cloud
    &lt;/match<span class="token punctuation">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>然后在应用 Pod 的定义里, 就可以声明一个 Fluentd 容器作为 sidecar, 专门负责将应用生成的 1.log 和 2.log 转发到 ElasticSearch 当中. 这个配置如下所示:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> counter
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> count
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">args</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> /bin/sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
    <span class="token punctuation">-</span> <span class="token punctuation">&gt;</span><span class="token scalar string">
      i=0;
      while true;
      do
        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;
        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> count<span class="token punctuation">-</span>agent
    <span class="token key atrule">image</span><span class="token punctuation">:</span> k8s.gcr.io/fluentd<span class="token punctuation">-</span>gcp<span class="token punctuation">:</span><span class="token number">1.30</span>
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FLUENTD_ARGS
      <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token punctuation">-</span>c /etc/fluentd<span class="token punctuation">-</span>config/fluentd.conf
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>volume
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/fluentd<span class="token punctuation">-</span>config
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>volume
    <span class="token key atrule">configMap</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>config
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br></div></div><p>可以看到, 这个 Fluentd 容器使用的输入源, 就是通过引用前面编写的 ConfigMap 来指定的. 这里用到了 Projected Volume 来把 ConfigMap 挂载到 Pod 里. 如果对这个用法不熟悉的话, 可以再回顾下前面<a href="https://time.geekbang.org/column/article/40466" target="_blank" rel="noopener noreferrer">《 深入解析 Pod 对象(二): 使用进阶》<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>中的相关内容.</p> <p>需要注意的是, 这种方案虽然部署简单, 并且对宿主机非常友好, 但是这个 sidecar 容器很可能会消耗较多的资源, 甚至拖垮应用容器. 并且由于日志还是没有输出到 stdout 上, 所以通过 kubectl logs 是看不到任何日志输出的.</p> <p>以上 就是 Kubernetes 项目对容器应用日志进行管理最常用的三种手段了.</p> <blockquote><p>总结</p></blockquote> <p>本节讲解了 Kubernetes 项目对容器应用日志的收集方式. 综合对比以上三种方案, <mark><strong>比较建议将应用日志输出到 stdout 和 stderr, 然后通过在宿主机上部署 logging-agent 的方式来集中处理日志</strong></mark>. 这种方案不仅管理简单, kubectl logs 也可以用, 而且可靠性高, 并且宿主机本身, 很可能就自带了 rsyslogd 等非常成熟的日志收集组件来使用.</p> <p>除此之外, 还有一种方式就是在<strong>编写应用</strong>的时候, 就直接指定好日志的存储后端, 如下所示:</p> <p><img src="/img/06712e6113a31a4ad67438367b2094c0-20230731162150-vkmwdnc.png" alt="">​</p> <p>在这种方案下, Kubernetes 就完全不必操心容器日志的收集了, 这对于本身已经有完善的日志处理系统的公司来说, 是一个非常好的选择.</p> <p>最后需要指出的是, 无论是哪种方案, 都必须要<strong>及时将这些日志文件从宿主机上清理掉, 或者给日志目录专门挂载一些容量巨大的远程盘</strong>. 否则一旦主磁盘分区被打满, 整个系统就可能会陷入奔溃状态, 这是非常麻烦的.</p> <h3 id="结束篇"><a href="#结束篇" class="header-anchor">#</a> 结束篇</h3> <h4 id="_51-答疑-在问题中解决问题-在思考中产生思考"><a href="#_51-答疑-在问题中解决问题-在思考中产生思考" class="header-anchor">#</a> 51 | 答疑:在问题中解决问题,在思考中产生思考</h4> <blockquote><p>问题 1: 你是否知道如何修复容器中的 top 指令以及 /proc 文件系统中的信息呢?</p></blockquote> <p>其实这个问题的答案在提示里其实已经给出了, 即 lxcfs 方案. 通过 lxcfs, 可以把宿主机的 /var/lib/lxcfs/proc 文件系统挂载到 Docker 容器的 /proc 目录下. 使得容器中进程读取相应文件内容时, 实际上会从容器对应的 Cgroups 中读取正确的资源限制.  从而得到正确的 top 命令的返回值.</p> <blockquote><p>问题 2: 既然容器的 rootfs(比如, Ubuntu 镜像), 是以只读方式挂载的, 那么又如何在容器里修改 Ubuntu 镜像的内容呢?</p></blockquote> <p>这个问题的答案也同样出现在了提示里. 简单地说, 修改一个镜像里的文件的时候, 联合文件系统首先会从上到下在各个层中查找有没有目标文件. 如果找到, 就把这个文件复制到可读写层进行修改. 这个修改的结果会屏蔽掉下层的文件, 这种方式就被称为 copy-on-write.</p> <blockquote><p>问题 3: 在查看 Docker 容器的 Namespace 时, 是否注意到有一个叫 cgroup 的 Namespace? 它是 Linux 4.6 之后新增加的一个 Namespace, 你知道它的作用吗?</p></blockquote> <p>Linux 内核从 4.6 开始, 支持了一个新的 Namespace 叫作: Cgroup Namespace. 正常情况下, 在一个容器里查看 /proc/$PID/cgroup, 是会看到整个宿主机的 cgroup 信息的. 而有了 Cgroup Namespace 后, 每个容器里的进程都会有自己 Cgroup Namespace, 从而获得一个属于自己的 Cgroups 文件目录视图. 也就是说, Cgroups 文件系统也可以被 Namespace 隔离起来了.</p> <blockquote><p>问题 4: 你能否说出, Kubernetes 使用的这个&quot;控制器模式&quot;, 跟我们平常所说的&quot;事件驱动&quot;, 有什么区别和联系吗?</p></blockquote> <p>这里 &quot;控制器模式&quot; 和 &quot;事件驱动&quot; 最关键的区别在于:</p> <ul><li><strong>对于控制器来说, 被监听对象的变化是一个持续的信号</strong>, 比如变成 ADD 状态. 只要这个状态没变化, 那么此后无论任何时候控制器再去查询对象的状态, 都应该是 ADD.</li> <li><strong>而对于事件驱动来说, 它只会在 ADD 事件发生的时候发出一个事件</strong>. 如果控制器错过了这个事件, 那么它就有可能再也没办法知道 ADD 这个事件的发生了.</li></ul> <blockquote><p>问题 5: 在实际场景中, 有一些分布式应用的集群是这么工作的: 当一个新节点加入到集群时, 或者老节点被迁移后重建时, 这个节点可以从主节点或者其他从节点那里同步到自己所需要的数据.</p></blockquote> <p>在这种情况下, 你认为是否还有必要将这个节点 Pod 与它的 PV 进行一对一绑定呢? (提示: 这个问题的答案根据不同的项目是不同的. 关键在于, 重建后的节点进行数据恢复和同步的时候, 是不是一定需要原先它写在本地磁盘里的数据)</p> <p>这个问题的答案是不需要. 像这种不依赖于 PV 保持存储状态或者不依赖于 DNS 名字保持拓扑状态的 &quot;非典型&quot; 应用的管理, 都应该使用 Operator 来实现.</p> <blockquote><p>问题 6: 在 Kubernetes v1.11 之前, DaemonSet 所管理的 Pod 的调度过程, 实际上都是由 DaemonSet Controller 自己而不是由调度器完成的. 你能说出这其中有哪些原因吗?</p></blockquote> <p>这里的原因在于, 默认调度器之前的功能不是很完善, 比如缺乏优先级和抢占机制. 所以它没办法保证 DaemonSet, 尤其是部署时候的系统级的, 高优先级的 DaemonSet 一定会调度成功. 这种情况下, 就会影响到集群的部署了.</p> <blockquote><p>问题 7: 在 Operator 的实现过程中, 再一次用到了 CRD. 可是你一定要明白, CRD 并不是万能的, 它有很多场景不适用, 还有性能瓶颈. 你能列举出一些不适用 CRD 的场景么? 你知道造成 CRD 性能瓶颈的原因主要在哪里么?</p></blockquote> <p>CRD 目前不支持 protobuf, 当 API Object 数量 &gt;1K, 或者单个对象 &gt;1KB, 或者高频请求时, CRD 的响应都会有问题. 所以 <strong>CRD 千万不能也不应该被当作数据库使用</strong>.</p> <p>其实像 Kubernetes, 或者说 Etcd 本身, 最佳的使用场景就是作为配置管理的依赖. 此外, 如果业务需求不能用 CRD 进行建模的时候, 比如需要等待 API 最终返回, 或者需要检查 API 的返回值, 也是不能用 CRD 的. 同时, 当需要完整的 APIServer 而不是只关心 API 对象的时候, 请使用 API Aggregator.</p> <blockquote><p>问题 8: 正是由于需要使用&quot;延迟绑定&quot;这个特性, Local Persistent Volume 目前还不能支持 Dynamic Provisioning. 你是否能说出, 为什么&quot;延迟绑定&quot;会跟 Dynamic Provisioning 有冲突呢?</p></blockquote> <p>延迟绑定将 Volume Bind 的时机, 推迟到了第一个使用该 Volume 的 Pod 到达调度器的时候. 可是对于 Dynamic Provisioning 来说, 它是要在管理 Volume 的控制循环里就为 PVC 创建 PV 然后绑定起来的, 这个时间点跟 Pod 被调度的时间点是不相关的.</p> <blockquote><p>问题 9: 请你根据编写 FlexVolume 和 CSI 插件的流程, 分析一下什么时候该使用 FlexVolume, 什么时候应该使用 CSI?</p></blockquote> <p>CSI 与 FlexVolume 的最大区别, 在于 CSI 可以实现 Provision 阶段. 所以对于不需要 Provision 的情况, 比如你的远程存储服务总是事先准备好或者准备起来非常简单的情况下, 就可以考虑使用 FlexVolume. <strong>但在生产环境下都会优先推荐 CSI 的方案</strong>.</p> <blockquote><p>问题 10: Flannel 通过&quot;隧道&quot;机制, 实现了容器之间三层网络(IP 地址)的连通性. 但是根据这个机制的工作原理, 你认为 Flannel 能保证容器二层网络(MAC 地址)的连通性吗? 为什么呢?</p></blockquote> <p>不能保证, 因为 &quot;隧道&quot; 机制只能保证被封装的 IP 包可以到达目的地. 而只要网络插件能满足 Kubernetes 网络的三个假设, Kubernetes 并不关心网络插件的实现方式是把容器二层连通的, 还是三层连通的.</p> <blockquote><p>问题 11: 能否总结一下三层网络方案和&quot;隧道模式&quot;的异同, 以及各自的优缺点?</p></blockquote> <p>隧道模式最大的特点, 在于需要通过某种方式比如 UDP 或者 VXLAN 来对原始的容器间通信的网络包进行封装, 然后伪装成宿主机间的网络通信来完成容器跨主通信. 这个过程中就不可避免的需要<strong>封包和解封包</strong>. 这两个操作的性能损耗都是非常明显的. 而三层网络方案则避免了这个过程, 所以性能会得到很大的提升.</p> <p>不过隧道模式的优点在于, 它依赖的底层原理非常直白, 内核里的实现也非常成熟和稳定. 而三层网络方案, 相对来说维护成本会比较高, 容易碰到路由规则分发和设置出现问题的情况, 并且当容器数量很多时, 宿主机上的路由规则会非常复杂, 难以 Debug.</p> <p>所以最终选择选择哪种方案, 还是要看自己的具体需求.</p> <blockquote><p>问题 12: 为什么宿主机进入 MemoryPressure 或者 DiskPressure 状态后, 新的 Pod 就不会被调度到这台宿主机上呢?</p></blockquote> <p>在 Kubernetes 里, 实际上有一种叫作 Taint Nodes by Condition 的机制, 即当 Node 本身进入异常状态的时候, 比如 Condition 变成了 DiskPressure. 那么, Kubernetes 会通过 Controller 自动给 Node 加上对应的 Taint, 从而阻止新的 Pod 调度到这台宿主机上.</p> <blockquote><p>问题 13: Kubernetes 默认调度器与 Mesos 的&quot;两级&quot;调度器, 有什么异同呢?</p></blockquote> <p>Mesos 的两级调度器的设计, 是 Mesos 自己充当 0 层调度器(Layer 0), 负责统一管理整个集群的资源情况, 把可用资源以 Resource Offer 的方式暴露出去; 而上层的大数据框架(比如 Spark)则充当 1 层调度器(Layer 1), 它会负责根据 Layer 0 发来的 Resource Offer 来决定把任务调度到某个具体的节点上. 这样做的好处是:</p> <ul><li>第一, 上层大数据框架本身往往自己已经实现了调度逻辑, 这样它就可以很方便地接入到 Mesos 里面;</li> <li>第二, 这样的设计, 使得 Mesos 本身能够统一地对上层所有框架进行资源分配, 资源利用率和调度效率就可以得到很好的保证了.</li></ul> <p>相比之下, Kubernetes 的默认调度器实际上无论从功能还是性能上都要简单得多. 这也是为什么把 Spark 这样本身就具有调度能力的框架接入到 Kubernetes 里还是比较困难的.</p> <blockquote><p>问题 14: 当整个集群发生可能会影响调度结果的变化(比如添加或者更新 Node, 添加和更新 PV, Service 等)时, 调度器会执行一个被称为 MoveAllToActiveQueue 的操作, 把所调度失败的 Pod 从 unscheduelableQ 移动到 activeQ 里面. 请问这是为什么?</p></blockquote> <p>一个相似的问题是, 当一个已经调度成功的 Pod 被更新时, 调度器则会将 unschedulableQ 里所有跟这个 Pod 有 Affinity/Anti-affinity 关系的 Pod, 移动到 activeQ 里面. 请问这又是为什么呢?</p> <p>其实, 这两个问题的答案是一样的.</p> <p>在正常情况下, 默认调度器在调度失败后, 就会把该 Pod 放到 unschedulableQ 里. unschedulableQ 里的 Pod 是不会出现在下个调度周期里的. 但是, 当集群本身发生变化时, 这个 Pod 就有可能再次变成可调度的了, 所以这时候调度器要把它们移动到 activeQ 里面, 这样它们就获得了下一次调度的机会.</p> <p>类似地, 当原本已经调度成功的 Pod 被更新后, 也有可能触发 unschedulableQ 里与它有 Affinity 或者 Anti-Affinity 关系的 Pod 变成可调度的, 所以它也需要获得&quot;重新做人&quot;的机会.</p> <blockquote><p>问题 15: Device Plugin 为容器分配的 GPU 信息, 是通过 CRI 的哪个接口传递给 dockershim, 最后交给 Docker API 的呢?</p></blockquote> <p>既然 GPU 是 Devices 信息, 那当然是通过 CRI 的 CreateContainerRequest 接口. 这个接口的参数 ContainerConfig 里就有容器 Devices 的描述.</p> <blockquote><p>问题 16: 安全容器的意义, 绝不仅仅止于安全. 可以想象一下这样一个场景: 比如宿主机的 Linux 内核版本是 3.6, 但是应用却必须要求 Linux 内核版本是 4.0. 这时候就可以把这个应用运行在一个 KataContainers 里. 请问使用 gVisor 是否也能提供这种能力呢? 原因是什么呢?</p></blockquote> <p>答案是不能. gVisor 的实现里并没有一个真正的 Linux Guest Kernel 在运行. 所以它不能像 KataContainers 或者虚拟机那样, 实现容器和宿主机不同 Kernel 甚至不同操作系统的需求.</p> <p>但还是要强调一下, 以 gVisor 为代表的用户态 Kernel 方案是安全容器的未来, 只是现在还不够完善.</p> <blockquote><p>问题 17: 将日志直接输出到 stdout 和 stderr, 有没有什么其他的隐患或者问题呢? 如何进行处理呢?</p></blockquote> <p>这样做有一个问题, 就是日志都需要经过 Docker Daemon 的处理才会写到宿主机磁盘上, 所以宿主机没办法以容器为单位进行日志文件的 Rotate. 这时候还是要考虑通过宿主机的 Agent 来对容器日志进行处理和收集的方案.</p> <h4 id="特别放送-2019年-容器技术生态会发生些什么"><a href="#特别放送-2019年-容器技术生态会发生些什么" class="header-anchor">#</a> 特别放送 | 2019年,容器技术生态会发生些什么?</h4> <h5 id="_1-kubernetes项目被采纳度将持续增长"><a href="#_1-kubernetes项目被采纳度将持续增长" class="header-anchor">#</a> 1.Kubernetes项目被采纳度将持续增长</h5> <p>作为&quot;云原生&quot;(Cloud Native)理念落地的核心, Kubernetes 项目已经成为了构建容器化平台体系的默认选择. 但是不同于一个只能生产资源的集群管理工具, <mark><strong>Kubernetes 项目最大的价值, 乃在于它从一开始就提倡的声明式 API 和以此为基础&quot;控制器&quot;模式</strong></mark>​ **. **</p> <p>在这个体系的指导下, Kubernetes 项目保证了在自身突飞猛进的发展过程中 API 层的相对稳定和一定的向后兼容能力, 这是作为一个平台级项目被用户广泛接受和认可的重要前提.</p> <p><mark><strong>更重要的是, Kubernetes 项目为使用者提供了宝贵的 API 可扩展能力和良好的 API 编程范式, 催生出了一个完全基于 Kubernetes API 构建出来的上层应用服务生态. 可以说正是这个生态的逐步完善与日趋成熟, 才确立了 Kubernetes 项目如今在云平台领域牢不可破的领导地位</strong></mark>, 也间接宣告了其他竞品方案的边缘化.</p> <p>与此同时, 上述事实标准的确立, 也使得 &quot;正确和合理的使用了 Kubernetes 的能力&quot;, 在某种意义上成为了评判上层应用服务框架(比如 PaaS 和 Serverless )的一个重要依据: 这不仅包括了对框架本身复杂性和易用性的考量, 也包括了对框架可扩展性和演进趋势的预期与判断.</p> <p>不过相比于国外公有云上以 Kubernetes 为基础的容器化作业的高占比, 国内公有云市场对容器的采纳程度目前仍然处于比较初步的水平, 直接贩卖虚拟机及其关联 IaaS 层能力依然是国内绝大多数公有云提供商的主要业务形态. 所以, 不同于国外市场容器技术增长逐步趋于稳定, Kubernetes 公有云服务已经开始支撑头部互联网客户的情况, **Kubernetes 以及容器技术在国内云计算市场里依然具有巨大的增长空间和强劲的发展势头. **</p> <p>不难预测, Kubernetes 项目在国内公有云上的逐渐铺开, 会逐渐成为接下来几年国内公有云市场上的一个重要趋势. 而无论是国内外, 大量 Kubernetes 项目相关岗位的涌现, 正是验证这个趋势与变化的一个最直接的征兆.</p> <h5 id="_2-serverless化-与-多样性-将成为上层应用服务生态的两大关键词"><a href="#_2-serverless化-与-多样性-将成为上层应用服务生态的两大关键词" class="header-anchor">#</a> 2.&quot;Serverless化&quot;与&quot;多样性&quot;将成为上层应用服务生态的两大关键词</h5> <p>当云上的平台层被 Kubernetes 项目逐步统一之后, 过去长期纠结在应用编排, 调度与资源管理上裹足不前的 PaaS 项目得到了生产力的全面释放, 进而在云平台层之上催生出了一个日趋多样化的<strong>应用服务生态</strong>. 事实上, 这个生态的本质与 2014 年之前的 PaaS 生态没有太大不同. 只不过, 当原本 PaaS 项目的平台层功能(编排, 调度, 资源管理等)被剥离了出来之后, PaaS 终于可以专注于应用服务和发布流程管理这两个最核心的功能, 开始向更轻, 更薄, 更以应用为中心的方向进行演进. 而在这个过程中, Serverless 自然开始成为了主流话题.</p> <p>这里需要指出的是, Serverless 从 2014 年 AWS 发布 Lambda 时专门用来指代函数计算(或者说 FaaS)发展到今天, 已经被扩展成了包括大多数 PaaS 功能在内的一个泛指术语, 即: <strong>Serverless = FaaS + BaaS</strong>.</p> <p>而究其本质,  <strong>&quot;高可扩展性&quot;, &quot;工作流驱动&quot;和&quot;按使用计费&quot;</strong> , 可以认为是 Serverless 最主要的三个特征. 这也是为什么会说今天大家所谈论的 Serverless, 其实是经典 PaaS 演进到今天的一种 &quot;极端&quot; 形态.</p> <p>伴随着 Serverless 概念本身的 &quot;横向发展&quot;, 不难预料到, 2019 年之后云端的应用服务生态, 一定会趋于多样化, 进而覆盖到更多场景下的应用服务管理需求. <strong>并且无论是 Function, 传统应用, 容器, 存储服务, 网络服务, 都会开始尝试以不同的方式和形态嵌入到 &quot;高可扩展性&quot;, &quot;工作流驱动&quot;和&quot;按使用计费&quot; 这三个特征当中</strong>.</p> <p>当然这种变化趋势的原因也不言而喻: Serverless 三个特征背后所体现的, **乃是云端应用开发过程向 &quot;用户友好&quot; 和 &quot;低心智负担&quot; 方向演进的最直接途径. 而这种 &quot;简单, 经济, 可信赖&quot; 的朴实诉求, 正是云计算诞生的最初期许和永恒的发展方向. **</p> <p>而在这种上层应用服务能力向 Serverless 迁移的演进过程中, 不断被优化的 Auto-scaling 能力和细粒度的资源隔离技术, 将会成为确保 Serverless 能为用户带来价值的最有力保障.</p> <h5 id="_3-看得见-摸得着-能落地的-云原生"><a href="#_3-看得见-摸得着-能落地的-云原生" class="header-anchor">#</a> 3.看得见,摸得着,能落地的&quot;云原生&quot;</h5> <p>自从 CNCF 社区迅速崛起以来, &quot;云原生&quot; 三个字就成了各大云厂商竞相角逐的一个关键词. 不过相比于 Kubernetes 项目和容器技术实实在在的发展和落地过程, 云原生(Cloud Native)的概念却长期以来 &quot;曲高和寡&quot;, 让人很难说出个所以然来.</p> <p>其实 &quot;云原生&quot;的本质, 不是简单对 Kubernetes 生态体系的一个指代. <mark>&quot;云原生&quot; </mark>​<mark><strong>刻画出的, 是一个使用户能低心智负担的, 敏捷的, 以可扩展, 可复制的方式, 最大化利用&quot;云&quot;的能力, 发挥&quot;云&quot;的价值的一条最佳路径</strong></mark>.</p> <p>而这其中,  **&quot;**​<mark><strong>不可变基础设施</strong></mark>​ <strong>&quot;</strong>  是 &quot;云原生&quot; 的实践基础(这也是容器技术的核心价值); 而 Kubernetes, Prometheus, Envoy 等 <strong>CNCF 核心项目, 则可以认为是这个路径落地的最佳实践</strong>. 这套理论体系的发展过程, 与 CNCF 基金会创立的初衷和云原生生态的发展历程是完全一致的.</p> <p>也正是伴随着这样的发展过程, 云原生对于它的使用者的意义, 在 2019 年之后已经变得非常清晰: <strong>是否采用云原生技术体系, 实际上已经成为了一个关系到是不是要最大化 &quot;云&quot; 的价值, 是不是要在 &quot;云&quot; 上赢取最广泛用户群体的一个关键取舍</strong>. 这涉及到的是关系到整个组织的发展, 招聘, 产品形态等一系列核心问题, 而绝非一个单纯的技术决定.</p> <p>明白了这一层道理, 在 2019 年, 已经不难看到, 国内最顶尖的技术公司们, 都已经开始在云原生技术框架下发起了实实在在的技术体系升级与落地的 &quot;战役&quot;. 显然大家都已经注意到, <strong>相比于纠结于 &quot;云原生到底是什么&quot; 这样意识形态话题, 抓紧时间和机遇将 Kubernetes 及其周边核心技术生态在组织中生长起来, 并借此机会完成自身基础技术体系的转型与升级, 才是这些体量庞大的技术巨人赶上这次云计算浪潮的不二法宝</strong>.</p> <p>在这个背景下, 所谓 &quot;云原生&quot; 体系在这些公司的落地, 只是这个激动人心的技术革命背后的一个附加值而已.</p> <p>而在 &quot;云原生&quot; 这个关键词的含义不断清晰的过程中, 一定要再次强调: <strong>云原生不等于 CNCF, 更不等于 Kubernetes</strong>. 云原生固然源自于 Kubernetes 技术生态和理念, <strong>但也必然是一个超越 CNCF 和 Kubernetes 存在的一个全集</strong>. 它被创立的目的和始终在坚持探索的方向, **是使用户能够最大化利用 &quot;云&quot; 的能力, 发挥 &quot;云&quot; 的价值, 而不是在此过程中构建一个又一个不可复制, 不可扩展的 &quot;巨型烟囱&quot;. **</p> <p>所以说, 云原生这个词语的准确定义, 是围绕着 Kubernetes 技术生态为核心的, 但也一定是一个伴随着 CNCF 社区和 Kubernetes 项目不断演进而日趋完善的一个动态过程. 而更为重要的是, **在这次以 &quot;云&quot; 为关键词的技术革命当中, 每一个人都有可能成为 &quot;云原生&quot; 的一个重要的定义者. **</p> <h4 id="结束语-kubernetes-赢开发者赢天下"><a href="#结束语-kubernetes-赢开发者赢天下" class="header-anchor">#</a> 结束语 | Kubernetes:赢开发者赢天下</h4> <p>前面探讨了这样一个话题: Kubernetes 为什么会赢? 而在当时的讨论中, 有这样一个结论: <strong>Kubernetes 项目之所以能赢, 最重要的原因在于它争取到了云计算生态里的绝大多数开发者</strong>. 不过相信在那个时候, 你可能会对这个结论有所疑惑: 大家不都说 Kubernetes 是一个运维工具么? 怎么就和开发者搭上了关系呢?</p> <p>事实上, Kubernetes 项目发展到今天, 已经成为了云计算领域中平台层当仁不让的事实标准. 但这样的生态地位, 并不是一个运维工具或者 Devops 项目所能达成的. 这里的原因也很容易理解: Kubernetes 项目的成功, 是成千上万云计算平台上的开发者用脚投票的结果. 到现在相信也应该能够明白, 云计算平台上的开发者们所关心的, 并不是调度, 也不是资源管理, 更不是网络或者存储, 他们关心的只有一件事, 那就是 <strong>Kubernetes 的 API</strong>.</p> <p>这也是为什么, 在 Kubernetes 这个项目里, 只要是跟 API 相关的事情, 那就都是大事儿; 只要是想要在这个社区构建影响力的人或者组织, 就一定会在 API 层面展开角逐. 这一层 &quot;API 为王&quot; 的思路, 早已经深入到了 Kubernetes 里每一个 API 对象的每一个字段的设计过程当中.</p> <p>所以说, <mark><strong>Kubernetes 项目的本质其实只有一个, 那就是 &quot;控制器模式&quot;</strong></mark> . 这个思想, 不仅仅是 Kubernetes 项目里每一个组件的 &quot;设计模板&quot;, 也是 Kubernetes 项目能够将开发者们紧紧团结到自己身边的重要原因. 作为一个云计算平台的用户, 能够用一个 YAML 文件表达开发的应用的最终运行状态, 并且自动地对应用进行运维和管理. 这种信赖关系, 就是连接 Kubernetes 项目和开发者们最重要的纽带. 更重要的是, 当这个 API 趋向于足够稳定和完善的时候, 越来越多的开发者会自动汇集到这个 API 上来, 依托它所提供的能力构建出一个全新的生态.</p> <p>事实上, 在云计算发展的历史上, 像这样一个围绕一个 API 创建出一个 &quot;新世界&quot; 的例子, 已经出现过了一次, 这正是 AWS 和它庞大的开发者生态的故事. 而这一次 Kubernetes 项目的巨大成功, 其实就是 AWS 故事的另一个版本而已. 只不过相比于 AWS 作为基础设施层提供运维和资源抽象标准的故事, Kubernetes 生态终于把触角触碰到了<strong>应用开发者的边界</strong>, 使得应用的开发者可以有能力去关心自己开发的应用的运行状态和运维方法, 实现了经典 PaaS 项目很多年前就已经提出, 但却始终没能达成的美好愿景.</p> <p>这也是为什么一再强调, <mark><strong>Kubernetes 项目里最重要的, 是它的 &quot;容器设计模式&quot;, 是它的 API 对象, 是它的 API 编程范式</strong></mark>. 这些都是未来云计算时代的每一个开发者需要融会贯通, 融化到自己开发基因里的关键所在. 也只有这样, 作为一个开发者, 才能够开发和构建出符合未来云计算形态的应用. 而更重要的是, 也只有这样, 才能够借助云计算的力量, 让自己的应用真正产生价值. **不妨想象一下 Kubernetes 就是未来的 Linux 操作系统. ** 在这个云计算以前所未有的速度迅速普及的世界里, Kubernetes 项目很快就会像操作系统一样, 成为每一个技术从业者必备的基础知识.</p> <p>‍</p> <p>​​</p></div></div>  <div class="page-edit"><div class="edit-link"><a href="https://github.com/xugaoyi/vuepress-theme-vdoing/edit/main/docs/30.系统/3100.容器/10.容器/210.深入剖析Kubernetes(极客时间)🌸.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <!----></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/c6a42c/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Kubernetes实战🌸</div></a> <a href="/pages/caa314/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Istio</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/c6a42c/" class="prev">Kubernetes实战🌸</a></span> <span class="next"><a href="/pages/caa314/">Istio</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="mailto:1174520425@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/nanodaemony" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2019-2025
    <span>达尔文的猹 | MIT License</span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.3f3e0e10.js" defer></script><script src="/assets/js/2.e9fcb30c.js" defer></script><script src="/assets/js/3.1998f389.js" defer></script><script src="/assets/js/194.8cce60a9.js" defer></script>
  </body>
</html>
