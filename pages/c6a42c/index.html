<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Kubernetes实战🌸 | Pangolin Note</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="大道至简">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.56073ad3.css" as="style"><link rel="preload" href="/assets/js/app.3f3e0e10.js" as="script"><link rel="preload" href="/assets/js/2.e9fcb30c.js" as="script"><link rel="preload" href="/assets/js/3.1998f389.js" as="script"><link rel="preload" href="/assets/js/193.4a06bc12.js" as="script"><link rel="prefetch" href="/assets/js/10.0b55747f.js"><link rel="prefetch" href="/assets/js/100.b8912a99.js"><link rel="prefetch" href="/assets/js/101.a6740c8a.js"><link rel="prefetch" href="/assets/js/102.e31e89b2.js"><link rel="prefetch" href="/assets/js/103.2c5dbdae.js"><link rel="prefetch" href="/assets/js/104.0e9c02f6.js"><link rel="prefetch" href="/assets/js/105.8288396c.js"><link rel="prefetch" href="/assets/js/106.62f43c11.js"><link rel="prefetch" href="/assets/js/107.70c90211.js"><link rel="prefetch" href="/assets/js/108.b488ce56.js"><link rel="prefetch" href="/assets/js/109.12942650.js"><link rel="prefetch" href="/assets/js/11.ac3fd1ca.js"><link rel="prefetch" href="/assets/js/110.1e57ecba.js"><link rel="prefetch" href="/assets/js/111.6e33b4ea.js"><link rel="prefetch" href="/assets/js/112.bd52831b.js"><link rel="prefetch" href="/assets/js/113.868c1ac9.js"><link rel="prefetch" href="/assets/js/114.090549d1.js"><link rel="prefetch" href="/assets/js/115.d97f6c2b.js"><link rel="prefetch" href="/assets/js/116.097c4b4e.js"><link rel="prefetch" href="/assets/js/117.4024cfe2.js"><link rel="prefetch" href="/assets/js/118.e3057cba.js"><link rel="prefetch" href="/assets/js/119.4ef1fa3b.js"><link rel="prefetch" href="/assets/js/12.863eaabe.js"><link rel="prefetch" href="/assets/js/120.78f9df50.js"><link rel="prefetch" href="/assets/js/121.8e16df87.js"><link rel="prefetch" href="/assets/js/122.f21c0107.js"><link rel="prefetch" href="/assets/js/123.ea1cb375.js"><link rel="prefetch" href="/assets/js/124.01c04c6e.js"><link rel="prefetch" href="/assets/js/125.d383e301.js"><link rel="prefetch" href="/assets/js/126.3162cb47.js"><link rel="prefetch" href="/assets/js/127.c6bebae6.js"><link rel="prefetch" href="/assets/js/128.d659db60.js"><link rel="prefetch" href="/assets/js/129.2afdcf48.js"><link rel="prefetch" href="/assets/js/13.4f59ce35.js"><link rel="prefetch" href="/assets/js/130.beb9ed9d.js"><link rel="prefetch" href="/assets/js/131.35b05f8a.js"><link rel="prefetch" href="/assets/js/132.d77f4f93.js"><link rel="prefetch" href="/assets/js/133.4ceda6e5.js"><link rel="prefetch" href="/assets/js/134.11390c5f.js"><link rel="prefetch" href="/assets/js/135.32f9e38f.js"><link rel="prefetch" href="/assets/js/136.6d8bb0f2.js"><link rel="prefetch" href="/assets/js/137.9c0ffeef.js"><link rel="prefetch" href="/assets/js/138.88d86e5f.js"><link rel="prefetch" href="/assets/js/139.8c2c3d6c.js"><link rel="prefetch" href="/assets/js/14.11c82d35.js"><link rel="prefetch" href="/assets/js/140.c5c375d3.js"><link rel="prefetch" href="/assets/js/141.d703f30b.js"><link rel="prefetch" href="/assets/js/142.6a005a44.js"><link rel="prefetch" href="/assets/js/143.9b2edf6f.js"><link rel="prefetch" href="/assets/js/144.3fd013c9.js"><link rel="prefetch" href="/assets/js/145.b05079c5.js"><link rel="prefetch" href="/assets/js/146.ed5360a6.js"><link rel="prefetch" href="/assets/js/147.7408d86f.js"><link rel="prefetch" href="/assets/js/148.f390b370.js"><link rel="prefetch" href="/assets/js/149.95017f0a.js"><link rel="prefetch" href="/assets/js/15.bd143bd5.js"><link rel="prefetch" href="/assets/js/150.f0ede764.js"><link rel="prefetch" href="/assets/js/151.b7093623.js"><link rel="prefetch" href="/assets/js/152.a82a6c83.js"><link rel="prefetch" href="/assets/js/153.965e3fb2.js"><link rel="prefetch" href="/assets/js/154.9e49311e.js"><link rel="prefetch" href="/assets/js/155.cef33ba5.js"><link rel="prefetch" href="/assets/js/156.bcc397ae.js"><link rel="prefetch" href="/assets/js/157.90824739.js"><link rel="prefetch" href="/assets/js/158.efd429b9.js"><link rel="prefetch" href="/assets/js/159.122ff5b7.js"><link rel="prefetch" href="/assets/js/16.2449162b.js"><link rel="prefetch" href="/assets/js/160.0b806535.js"><link rel="prefetch" href="/assets/js/161.835052c6.js"><link rel="prefetch" href="/assets/js/162.ca34e2d2.js"><link rel="prefetch" href="/assets/js/163.08000d04.js"><link rel="prefetch" href="/assets/js/164.a1cc0109.js"><link rel="prefetch" href="/assets/js/165.65444686.js"><link rel="prefetch" href="/assets/js/166.7d73c84f.js"><link rel="prefetch" href="/assets/js/167.009d47a3.js"><link rel="prefetch" href="/assets/js/168.0afeae2a.js"><link rel="prefetch" href="/assets/js/169.7654cb31.js"><link rel="prefetch" href="/assets/js/17.eb7f9def.js"><link rel="prefetch" href="/assets/js/170.58569530.js"><link rel="prefetch" href="/assets/js/171.7db9ed40.js"><link rel="prefetch" href="/assets/js/172.3cb50ed4.js"><link rel="prefetch" href="/assets/js/173.0846425f.js"><link rel="prefetch" href="/assets/js/174.9e95f111.js"><link rel="prefetch" href="/assets/js/175.ef2098f2.js"><link rel="prefetch" href="/assets/js/176.c2635fe9.js"><link rel="prefetch" href="/assets/js/177.4fa38e8e.js"><link rel="prefetch" href="/assets/js/178.2be7037f.js"><link rel="prefetch" href="/assets/js/179.bf3bba28.js"><link rel="prefetch" href="/assets/js/18.0455f4d1.js"><link rel="prefetch" href="/assets/js/180.72c5f597.js"><link rel="prefetch" href="/assets/js/181.42287bcc.js"><link rel="prefetch" href="/assets/js/182.6cd4bf1a.js"><link rel="prefetch" href="/assets/js/183.05dbbfd9.js"><link rel="prefetch" href="/assets/js/184.03de7e00.js"><link rel="prefetch" href="/assets/js/185.42dd210e.js"><link rel="prefetch" href="/assets/js/186.28ee0f8a.js"><link rel="prefetch" href="/assets/js/187.bb58697b.js"><link rel="prefetch" href="/assets/js/188.1bc8be96.js"><link rel="prefetch" href="/assets/js/189.fac747e3.js"><link rel="prefetch" href="/assets/js/19.ae9e35d6.js"><link rel="prefetch" href="/assets/js/190.ea533ac7.js"><link rel="prefetch" href="/assets/js/191.6dc6eb13.js"><link rel="prefetch" href="/assets/js/192.184d249f.js"><link rel="prefetch" href="/assets/js/194.8cce60a9.js"><link rel="prefetch" href="/assets/js/195.93231a13.js"><link rel="prefetch" href="/assets/js/196.4a596bde.js"><link rel="prefetch" href="/assets/js/197.96c113cf.js"><link rel="prefetch" href="/assets/js/198.73ba3f67.js"><link rel="prefetch" href="/assets/js/199.74bab595.js"><link rel="prefetch" href="/assets/js/20.b542d0e7.js"><link rel="prefetch" href="/assets/js/200.69a6928a.js"><link rel="prefetch" href="/assets/js/201.9ffa7c5a.js"><link rel="prefetch" href="/assets/js/202.41edb652.js"><link rel="prefetch" href="/assets/js/203.7fabcef3.js"><link rel="prefetch" href="/assets/js/204.b0ae2f62.js"><link rel="prefetch" href="/assets/js/205.add8738b.js"><link rel="prefetch" href="/assets/js/206.f3a712ec.js"><link rel="prefetch" href="/assets/js/207.cd7dd729.js"><link rel="prefetch" href="/assets/js/208.78b33afa.js"><link rel="prefetch" href="/assets/js/209.9d3329ff.js"><link rel="prefetch" href="/assets/js/21.5a050318.js"><link rel="prefetch" href="/assets/js/210.d285d5ac.js"><link rel="prefetch" href="/assets/js/211.8791bb3f.js"><link rel="prefetch" href="/assets/js/212.84ed81a8.js"><link rel="prefetch" href="/assets/js/213.7b990580.js"><link rel="prefetch" href="/assets/js/214.da31f20c.js"><link rel="prefetch" href="/assets/js/215.9eeed659.js"><link rel="prefetch" href="/assets/js/216.9539f0ec.js"><link rel="prefetch" href="/assets/js/217.11b575be.js"><link rel="prefetch" href="/assets/js/218.a67f12f1.js"><link rel="prefetch" href="/assets/js/219.bfbb817a.js"><link rel="prefetch" href="/assets/js/22.2bc6f7e3.js"><link rel="prefetch" href="/assets/js/220.8b01342f.js"><link rel="prefetch" href="/assets/js/221.b450decd.js"><link rel="prefetch" href="/assets/js/222.97468507.js"><link rel="prefetch" href="/assets/js/223.b6dafd73.js"><link rel="prefetch" href="/assets/js/224.c86c18c6.js"><link rel="prefetch" href="/assets/js/225.b0dcf86e.js"><link rel="prefetch" href="/assets/js/226.02cc2999.js"><link rel="prefetch" href="/assets/js/227.8474ef5a.js"><link rel="prefetch" href="/assets/js/228.0298e421.js"><link rel="prefetch" href="/assets/js/229.6aeaf595.js"><link rel="prefetch" href="/assets/js/23.9995fce4.js"><link rel="prefetch" href="/assets/js/230.a5785286.js"><link rel="prefetch" href="/assets/js/231.d791daa4.js"><link rel="prefetch" href="/assets/js/232.97aeeb00.js"><link rel="prefetch" href="/assets/js/233.46e51e28.js"><link rel="prefetch" href="/assets/js/234.2cffe82f.js"><link rel="prefetch" href="/assets/js/235.14965da6.js"><link rel="prefetch" href="/assets/js/236.00a5afc0.js"><link rel="prefetch" href="/assets/js/237.3b73d52f.js"><link rel="prefetch" href="/assets/js/238.6e1db765.js"><link rel="prefetch" href="/assets/js/239.75866c5e.js"><link rel="prefetch" href="/assets/js/24.9141eeb2.js"><link rel="prefetch" href="/assets/js/240.af9c2cc9.js"><link rel="prefetch" href="/assets/js/241.388acab2.js"><link rel="prefetch" href="/assets/js/242.c60f2b48.js"><link rel="prefetch" href="/assets/js/243.d8e81b13.js"><link rel="prefetch" href="/assets/js/244.58b0b21d.js"><link rel="prefetch" href="/assets/js/245.c3768497.js"><link rel="prefetch" href="/assets/js/246.ac8bbe7a.js"><link rel="prefetch" href="/assets/js/247.d095f70a.js"><link rel="prefetch" href="/assets/js/248.e9f210b5.js"><link rel="prefetch" href="/assets/js/249.a9fad023.js"><link rel="prefetch" href="/assets/js/25.98e8593e.js"><link rel="prefetch" href="/assets/js/250.ae7f0ebb.js"><link rel="prefetch" href="/assets/js/251.9a617d55.js"><link rel="prefetch" href="/assets/js/252.ce082446.js"><link rel="prefetch" href="/assets/js/253.4575302d.js"><link rel="prefetch" href="/assets/js/254.5899a61b.js"><link rel="prefetch" href="/assets/js/255.66c69f31.js"><link rel="prefetch" href="/assets/js/256.8fd6c706.js"><link rel="prefetch" href="/assets/js/257.31b77e5e.js"><link rel="prefetch" href="/assets/js/258.eba48891.js"><link rel="prefetch" href="/assets/js/259.edf74b59.js"><link rel="prefetch" href="/assets/js/26.e69924ea.js"><link rel="prefetch" href="/assets/js/260.cf2ed36d.js"><link rel="prefetch" href="/assets/js/261.9e7792d8.js"><link rel="prefetch" href="/assets/js/262.e7b9433e.js"><link rel="prefetch" href="/assets/js/263.1185b25c.js"><link rel="prefetch" href="/assets/js/264.5e0f89cb.js"><link rel="prefetch" href="/assets/js/265.a38d3b94.js"><link rel="prefetch" href="/assets/js/266.2ef170b3.js"><link rel="prefetch" href="/assets/js/267.a7d14651.js"><link rel="prefetch" href="/assets/js/268.05b18748.js"><link rel="prefetch" href="/assets/js/269.dad48d6f.js"><link rel="prefetch" href="/assets/js/27.13612515.js"><link rel="prefetch" href="/assets/js/270.4f5a6b8d.js"><link rel="prefetch" href="/assets/js/271.7ebf6682.js"><link rel="prefetch" href="/assets/js/272.7c2fbfd1.js"><link rel="prefetch" href="/assets/js/273.8ee20732.js"><link rel="prefetch" href="/assets/js/274.d525242a.js"><link rel="prefetch" href="/assets/js/275.a8a19acb.js"><link rel="prefetch" href="/assets/js/276.df3a4eb4.js"><link rel="prefetch" href="/assets/js/277.336debda.js"><link rel="prefetch" href="/assets/js/278.c470625f.js"><link rel="prefetch" href="/assets/js/279.a91e1a64.js"><link rel="prefetch" href="/assets/js/28.ab7ae1df.js"><link rel="prefetch" href="/assets/js/280.87e25c9a.js"><link rel="prefetch" href="/assets/js/281.87c1ba25.js"><link rel="prefetch" href="/assets/js/282.dcd4dce0.js"><link rel="prefetch" href="/assets/js/283.abac2e00.js"><link rel="prefetch" href="/assets/js/284.f6079659.js"><link rel="prefetch" href="/assets/js/285.f1b39879.js"><link rel="prefetch" href="/assets/js/286.f6a79242.js"><link rel="prefetch" href="/assets/js/287.06bebe07.js"><link rel="prefetch" href="/assets/js/288.89e325df.js"><link rel="prefetch" href="/assets/js/289.3aa1bedd.js"><link rel="prefetch" href="/assets/js/29.ebe50f76.js"><link rel="prefetch" href="/assets/js/290.ee059c92.js"><link rel="prefetch" href="/assets/js/291.fa9a921a.js"><link rel="prefetch" href="/assets/js/292.2a8811cd.js"><link rel="prefetch" href="/assets/js/293.eec09cdf.js"><link rel="prefetch" href="/assets/js/294.dfac20dc.js"><link rel="prefetch" href="/assets/js/295.825d2070.js"><link rel="prefetch" href="/assets/js/296.f645861e.js"><link rel="prefetch" href="/assets/js/297.424fdb17.js"><link rel="prefetch" href="/assets/js/298.ebf87cdc.js"><link rel="prefetch" href="/assets/js/299.b8f19cbb.js"><link rel="prefetch" href="/assets/js/30.75237511.js"><link rel="prefetch" href="/assets/js/300.10fb6d4f.js"><link rel="prefetch" href="/assets/js/301.ef77c612.js"><link rel="prefetch" href="/assets/js/302.1b839763.js"><link rel="prefetch" href="/assets/js/303.609f7d98.js"><link rel="prefetch" href="/assets/js/304.1d255f19.js"><link rel="prefetch" href="/assets/js/305.b0234f2c.js"><link rel="prefetch" href="/assets/js/306.48677f64.js"><link rel="prefetch" href="/assets/js/307.14390d4b.js"><link rel="prefetch" href="/assets/js/308.fa730b28.js"><link rel="prefetch" href="/assets/js/309.0496b9e0.js"><link rel="prefetch" href="/assets/js/31.cf3f471a.js"><link rel="prefetch" href="/assets/js/310.a31676dc.js"><link rel="prefetch" href="/assets/js/311.ed53adc5.js"><link rel="prefetch" href="/assets/js/312.1b0ff2f1.js"><link rel="prefetch" href="/assets/js/313.bf123f32.js"><link rel="prefetch" href="/assets/js/314.4e6ce06b.js"><link rel="prefetch" href="/assets/js/315.8eb18560.js"><link rel="prefetch" href="/assets/js/32.74fef842.js"><link rel="prefetch" href="/assets/js/33.bc2d190b.js"><link rel="prefetch" href="/assets/js/34.d23438fc.js"><link rel="prefetch" href="/assets/js/35.7675c4d0.js"><link rel="prefetch" href="/assets/js/36.96a4161b.js"><link rel="prefetch" href="/assets/js/37.d1824f2f.js"><link rel="prefetch" href="/assets/js/38.4effff9e.js"><link rel="prefetch" href="/assets/js/39.6509914b.js"><link rel="prefetch" href="/assets/js/4.4d01750f.js"><link rel="prefetch" href="/assets/js/40.1509d08b.js"><link rel="prefetch" href="/assets/js/41.f2c1124f.js"><link rel="prefetch" href="/assets/js/42.e541d077.js"><link rel="prefetch" href="/assets/js/43.369de999.js"><link rel="prefetch" href="/assets/js/44.43959db0.js"><link rel="prefetch" href="/assets/js/45.282fbd31.js"><link rel="prefetch" href="/assets/js/46.1b83cfe9.js"><link rel="prefetch" href="/assets/js/47.c3a88e41.js"><link rel="prefetch" href="/assets/js/48.bc7c0a1b.js"><link rel="prefetch" href="/assets/js/49.92a4e5ba.js"><link rel="prefetch" href="/assets/js/5.c3991e24.js"><link rel="prefetch" href="/assets/js/50.9c488c6c.js"><link rel="prefetch" href="/assets/js/51.546ea632.js"><link rel="prefetch" href="/assets/js/52.0d2ccee1.js"><link rel="prefetch" href="/assets/js/53.6f52b5b1.js"><link rel="prefetch" href="/assets/js/54.c838b295.js"><link rel="prefetch" href="/assets/js/55.13af99ee.js"><link rel="prefetch" href="/assets/js/56.6be6d1ed.js"><link rel="prefetch" href="/assets/js/57.67c98b39.js"><link rel="prefetch" href="/assets/js/58.107e82ec.js"><link rel="prefetch" href="/assets/js/59.b3e5edc7.js"><link rel="prefetch" href="/assets/js/6.36a535f9.js"><link rel="prefetch" href="/assets/js/60.3b0b4ba5.js"><link rel="prefetch" href="/assets/js/61.3df843df.js"><link rel="prefetch" href="/assets/js/62.1213823d.js"><link rel="prefetch" href="/assets/js/63.e8f3f926.js"><link rel="prefetch" href="/assets/js/64.e1219050.js"><link rel="prefetch" href="/assets/js/65.5bf5bb0b.js"><link rel="prefetch" href="/assets/js/66.a2502afb.js"><link rel="prefetch" href="/assets/js/67.690dd59b.js"><link rel="prefetch" href="/assets/js/68.de7b0915.js"><link rel="prefetch" href="/assets/js/69.ac61dd61.js"><link rel="prefetch" href="/assets/js/7.5d254238.js"><link rel="prefetch" href="/assets/js/70.3edd41b1.js"><link rel="prefetch" href="/assets/js/71.d23407e9.js"><link rel="prefetch" href="/assets/js/72.a5bd01bc.js"><link rel="prefetch" href="/assets/js/73.c9f4dd57.js"><link rel="prefetch" href="/assets/js/74.66323fc1.js"><link rel="prefetch" href="/assets/js/75.e1a72e00.js"><link rel="prefetch" href="/assets/js/76.801268b4.js"><link rel="prefetch" href="/assets/js/77.3a5fda67.js"><link rel="prefetch" href="/assets/js/78.54d84cd4.js"><link rel="prefetch" href="/assets/js/79.fa10f4e3.js"><link rel="prefetch" href="/assets/js/8.e060e47d.js"><link rel="prefetch" href="/assets/js/80.d5b24fea.js"><link rel="prefetch" href="/assets/js/81.0e9da714.js"><link rel="prefetch" href="/assets/js/82.e96ff6d7.js"><link rel="prefetch" href="/assets/js/83.771cced1.js"><link rel="prefetch" href="/assets/js/84.220b69e7.js"><link rel="prefetch" href="/assets/js/85.7dcc5659.js"><link rel="prefetch" href="/assets/js/86.44ba2100.js"><link rel="prefetch" href="/assets/js/87.281da75d.js"><link rel="prefetch" href="/assets/js/88.12d88449.js"><link rel="prefetch" href="/assets/js/89.f28c86d3.js"><link rel="prefetch" href="/assets/js/9.8f9fef32.js"><link rel="prefetch" href="/assets/js/90.715cabb2.js"><link rel="prefetch" href="/assets/js/91.6ced7c82.js"><link rel="prefetch" href="/assets/js/92.c05b33ca.js"><link rel="prefetch" href="/assets/js/93.6bf5fb66.js"><link rel="prefetch" href="/assets/js/94.3561200e.js"><link rel="prefetch" href="/assets/js/95.0f2cf716.js"><link rel="prefetch" href="/assets/js/96.ac8487ea.js"><link rel="prefetch" href="/assets/js/97.48042b5a.js"><link rel="prefetch" href="/assets/js/98.441bb7f8.js"><link rel="prefetch" href="/assets/js/99.4b214d92.js">
    <link rel="stylesheet" href="/assets/css/0.styles.56073ad3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/magic.png" alt="Pangolin Note" class="logo"> <span class="site-name can-hide">Pangolin Note</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">🏡首页</a></div><div class="nav-item"><a href="/basic/" class="nav-link">基础</a></div><div class="nav-item"><a href="/develop/" class="nav-link">开发</a></div><div class="nav-item"><a href="/middleware/" class="nav-link">系统</a></div><div class="nav-item"><a href="/algorithm/" class="nav-link">算法</a></div><div class="nav-item"><a href="/books/" class="nav-link">🍀读书笔记</a></div><div class="nav-item"><a href="/work/" class="nav-link">工作</a></div><div class="nav-item"><a href="/pangolin/" class="nav-link">🌸达尔文的猹</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/avatarnew.png"> <div class="blogger-info"><h3>达尔文的猹</h3> <span>大道至简 悟在天成</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">🏡首页</a></div><div class="nav-item"><a href="/basic/" class="nav-link">基础</a></div><div class="nav-item"><a href="/develop/" class="nav-link">开发</a></div><div class="nav-item"><a href="/middleware/" class="nav-link">系统</a></div><div class="nav-item"><a href="/algorithm/" class="nav-link">算法</a></div><div class="nav-item"><a href="/books/" class="nav-link">🍀读书笔记</a></div><div class="nav-item"><a href="/work/" class="nav-link">工作</a></div><div class="nav-item"><a href="/pangolin/" class="nav-link">🌸达尔文的猹</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>系统</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>分布式系统理论</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/129626/" class="sidebar-link">分布式系统基础</a></li><li><a href="/pages/fb5d35/" class="sidebar-link">分布式共识算法</a></li><li><a href="/pages/12ac37/" class="sidebar-link">分布式系统组件</a></li><li><a href="/pages/d03ebf/" class="sidebar-link">分布式技术原理与算法解析(极客时间)🌸</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>系统接入层</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/f1b6c4/" class="sidebar-link">Nginx基础</a></li><li><a href="/pages/d4123d/" class="sidebar-link">深入拆解Tomcat与Jetty(极客时间)🌸</a></li><li><a href="/pages/baee2f/" class="sidebar-link">Netty</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>服务治理-注册发现与RPC</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/05077d/" class="sidebar-link">基础</a></li><li><a href="/pages/51b6aa/" class="sidebar-link">RPC实战与核心原理(极客时间)🌸</a></li><li><a href="/pages/0966ee/" class="sidebar-link">Zookeeper</a></li><li><a href="/pages/3b7e05/" class="sidebar-link">Nacos</a></li><li><a href="/pages/7f31f8/" class="sidebar-link">Dubbo</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>服务治理-流量控制</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/44bfa8/" class="sidebar-link">负载均衡</a></li><li><a href="/pages/4d5a6c/" class="sidebar-link">限流</a></li><li><a href="/pages/e0c561/" class="sidebar-link">熔断</a></li><li><a href="/pages/12ae40/" class="sidebar-link">网关路由</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>服务治理-系统监控与安全</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/8c9210/" class="sidebar-link">系统安全性</a></li><li><a href="/pages/c9bf40/" class="sidebar-link">系统监控组件</a></li><li><a href="/pages/3f3cf7/" class="sidebar-link">运维监控系统实战(极客时间)🌸</a></li><li><a href="/pages/e20e02/" class="sidebar-link">OAuth2.0实战课(极客时间)🌟</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>中间件-消息队列</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/96d94c/" class="sidebar-link">消息队列基础</a></li><li><a href="/pages/abf16c/" class="sidebar-link">RabbitMQ</a></li><li><a href="/pages/4fc3f1/" class="sidebar-link">Kafka</a></li><li><a href="/pages/013cfe/" class="sidebar-link">RocketMQ</a></li><li><a href="/pages/ed8d92/" class="sidebar-link">Disruptor</a></li><li><a href="/pages/249149/" class="sidebar-link">消息队列高手课(极客时间)🌟</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>中间件-缓存</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/946847/" class="sidebar-link">缓存基础</a></li><li><a href="/pages/e46d56/" class="sidebar-link">本地缓存</a></li><li><a href="/pages/0abfb9/" class="sidebar-link">Redis基础</a></li><li><a href="/pages/09236a/" class="sidebar-link">Redis持久化</a></li><li><a href="/pages/867f9b/" class="sidebar-link">Redis主从复制</a></li><li><a href="/pages/50cae1/" class="sidebar-link">Redis哨兵</a></li><li><a href="/pages/43b45c/" class="sidebar-link">Redis集群</a></li><li><a href="/pages/a32379/" class="sidebar-link">Redis内存管理与运维</a></li><li><a href="/pages/386037/" class="sidebar-link">Redis核心技术与实战(极客时间)🌸</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>中间件-其他</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/23044d/" class="sidebar-link">定时任务-XXLJob</a></li><li><a href="/pages/459117/" class="sidebar-link">ES与检索</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>系统设计与优化</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/41a845/" class="sidebar-link">凤凰架构</a></li><li><a href="/pages/68cc0b/" class="sidebar-link">左耳听风(极客时间)🌟</a></li><li><a href="/pages/e3e99c/" class="sidebar-link">从0开始学微服务(极客时间)🌸</a></li><li><a href="/pages/1e5368/" class="sidebar-link">高并发系统设计40问(极客时间)🌸</a></li><li><a href="/pages/33599f/" class="sidebar-link">系统性能调优必知必会(极客时间)🌸</a></li><li><a href="/pages/c83472/" class="sidebar-link">后端技术面试38讲(极客时间)</a></li><li><a href="/pages/4404b6/" class="sidebar-link">架构实战案例解析(极客时间)🌸</a></li><li><a href="/pages/8f1c1d/" class="sidebar-link">如何设计一个秒杀系统(极客时间)</a></li></ul></section></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>容器</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>容器</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/582acf/" class="sidebar-link">部署Minikube</a></li><li><a href="/pages/98e5e4/" class="sidebar-link">容器实战高手课(极客时间)🌸</a></li><li><a href="/pages/c6a42c/" aria-current="page" class="active sidebar-link">Kubernetes实战🌸</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/pages/f35c72/" class="sidebar-link">深入剖析Kubernetes(极客时间)🌸</a></li><li><a href="/pages/caa314/" class="sidebar-link">Istio</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>自动化运维</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/beb97f/" class="sidebar-link">持续集成(CICD)</a></li><li><a href="/pages/a0df2d/" class="sidebar-link">DevOps</a></li><li><a href="/pages/765815/" class="sidebar-link">SRE实战手册(极客时间)</a></li></ul></section></li></ul></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06970110><div class="articleInfo" data-v-06970110><ul class="breadcrumbs" data-v-06970110><li data-v-06970110><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06970110></a></li> <li data-v-06970110><a href="/middleware/#系统" data-v-06970110>系统</a></li><li data-v-06970110><a href="/middleware/#容器" data-v-06970110>容器</a></li><li data-v-06970110><a href="/middleware/#容器" data-v-06970110>容器</a></li></ul> <div class="info" data-v-06970110><div title="作者" class="author iconfont icon-touxiang" data-v-06970110><a href="https://github.com/nanodaemony" target="_blank" title="作者" class="beLink" data-v-06970110>NanoDaemony</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06970110><a href="javascript:;" data-v-06970110>2025-02-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">本文目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Kubernetes实战🌸<!----></h1>  <div class="theme-vdoing-content content__default"><h1 id="_200-kubernetes实战🌸"><a href="#_200-kubernetes实战🌸" class="header-anchor">#</a> 200.Kubernetes实战🌸</h1> <blockquote><p>本书《Kubernetes In Action》通过 Epub + PDF 的方式完成阅读整理.</p> <p>其中图片都在英文版的PDF获取, 质量非常高. 代码在英文版的 epub 文件获取.</p></blockquote> <h4 id="前言"><a href="#前言" class="header-anchor">#</a> 前言</h4> <p>本书分为三个部分, 涵盖 18 个章节.</p> <p>第一部分简要地介绍 Docker 和 Kubernetes, 如何设置 Kubernetes 集群, 以及如何在集群中运行一个简单的应用. 它包括两章:</p> <ul><li>第 1 章解释了什么是 Kubernetes, Kubernetes 的起源, 以及它如何帮助解决当今大规模应用管理的问题.</li> <li>第 2 章是关于如何构建容器镜像并在 Kubernetes 集群中运行的实践教程. 还解释了如何运行本地单节点 Kubernetes 集群, 以及在云上运行适当的多节点集群.</li></ul> <p>第二部分介绍了在 Kubernetes 中运行应用必须理解的关键概念. 本章内容如下:</p> <ul><li>第 3 章介绍了 Kubernetes 的基本构建模块--pod, 并解释了如何通过标签组织 pod 和其他 Kubernetes 对象.</li> <li>第 4 章将向你介绍 Kubernetes 如何通过自动重启容器来保持应用程序的健康. 还展示了如何正确地运行托管的 pod, 水平伸缩它们, 使它们能够抵抗集群节点的故障, 并在未来或定期运行它们.</li> <li>第 5 章介绍了 pod 如何向运行在集群内外的客户端暴露它们提供的服务, 还展示了运行在集群中的 pod 是如何发现和访问集群内外的服务的.</li> <li>第 6 章解释了在同一个 pod 中运行的多个容器如何共享文件, 以及如何管理持久化存储并使得 pod 可以访问.</li> <li>第 7 章介绍了如何将配置数据和敏感信息(如凭据)传递给运行在 pod 中的应用.</li> <li>第 8 章描述了应用如何获得正在运行的 Kubernetes 环境的信息, 以及如何通过与 Kubernetes 通信来更改集群的状态.</li> <li>第 9 章介绍了 Deployment 的概念, 并解释了在 Kubernetes 环境中运行和更新应用的正确方法.</li> <li>第 10 章介绍了一种运行需要稳定的标识和状态的有状态应用的专门方法.</li></ul> <p>第三部分深入研究了 Kubernetes 集群的内部, 介绍了一些额外的概念, 并从更高的角度回顾了在前两部分中所学到的所有内容. 这是最后一组章节:</p> <ul><li>第 11 章深入 Kubernetes 的底层, 解释了组成 Kubernetes 集群的所有组件, 以及每个组件的作用. 它还解释了 pod 如何通过网络进行通信, 以及服务如何跨多个 pod 形成负载平衡.</li> <li>第 12 章解释了如何保护 Kubernetes API 服务器, 以及通过扩展集群使用身份验证和授权.</li> <li>第 13 章介绍了 pod 如何访问节点的资源, 以及集群管理员如何防止 pod 访问节点的资源.</li> <li>第 14 章深入了解限制每个应用程序允许使用的计算资源, 配置应用的 QoS(Quality of Service)保证, 以及监控各个应用的资源使用情况. 还会介绍如何防止用户消耗太多资源.</li> <li>第 15 章讨论了如何通过配置 Kubernetes 来自动伸缩应用运行的副本数, 以及在当前集群节点数量不能接受任何新增应用时, 如何对集群进行扩容.</li> <li>第 16 章介绍了如何确保 pod 只被调度到特定的节点, 或者如何防止它们被调度到其他节点. 还介绍了如何确保 pod 被调度在一起, 或者如何防止它们调度在一起.</li> <li>第 17 章介绍了如何开发应用程序并部署在集群中. 还介绍了如何配置开发和测试工作流来提高开发效率.</li></ul> <p>随着章节的深入, 不仅可以了解单个构建 Kubernetes 的模块, 还可以逐步增加对使用 kubectl 命令行工具的理解.</p> <h4 id="_1-kubernetes介绍"><a href="#_1-kubernetes介绍" class="header-anchor">#</a> 1.Kubernetes介绍</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>应用的开发和部署方式在近几年的发展趋势</li> <li>容器如何保障应用间的隔离性, 以及减少应用对部署环境的依赖性</li> <li>Docker 容器如何在 Kubernetes 系统中应用</li> <li>Kubernetes 如何提高开发人员和系统管理员的工作效率</li></ul> <p>在过去, 多数的应用都是大型单体应用, 以单个进程或几个进程的方式, 运行于几台服务器之上. 这些应用的发布周期长, 而且迭代也不频繁. 每个发布周期结束前, 开发者会把应用程序打包后交付给运维团队, 运维人员再处理部署, 监控事宜, 并且在硬件发生故障时手动迁移应用.</p> <p>今天, 大型单体应用正被逐渐分解成小的, 可独立运行的组件, 我们称之为微服务. 微服务彼此之间解耦, 所以它们可以被独立开发, 部署, 升级, 伸缩. 这使得我们可以对每一个微服务实现快速迭代, 并且迭代的速度可以和市场需求变化的速度保持一致.</p> <p>但是, <strong>随着部署组件的增多和数据中心的增长, 配置, 管理并保持系统的正常运行变得越来越困难</strong>. 如果想要获得足够高的资源利用率并降低硬件成本, 把组件部署在什么地方变得越来越难以决策. 手动做所有的事情, 显然不太可行. <strong>需要一些自动化的措施, 包括自动调度, 配置, 监管和故障处理. 这正是 Kubernetes 的用武之地</strong>.</p> <p>Kubernetes 使开发者可以自主部署应用, 并且控制部署的频率, 完全脱离运维团队的帮助. Kubernetes 同时能让运维团队监控整个系统, 并且在硬件故障时重新调度应用. 系统管理员的工作重心, 从监管应用转移到了监管 Kubernetes, 以及剩余的系统资源, 因为 Kubernetes 会帮助监管所有的应用.</p> <p><strong>Kubernetes 抽象了数据中心的硬件基础设施, 使得对外暴露的只是一个巨大的资源池. 它让我们在部署和运行组件时, 不用关注底层的服务器. 使用 Kubernetes 部署多组件应用时, 它会为每个组件都选择一个合适的服务器, 部署之后它能够保证每个组件可以轻易地发现其他组件, 并彼此之间实现通信</strong>.</p> <p>所以说应用 Kubernetes 可以给大多数场景下的数据中心带来增益, 这不仅包括内部部署(on-premises)的数据中心, 如果是类似云厂商提供的那种超大型数据中心, 这种增益是更加明显的. 通过 Kubernetes, 云厂商提供给开发者的是一个可部署且可运行任何类型应用的简易化云平台, 云厂商的系统管理员可以不用关注这些海量应用到底是什么.</p> <p>随着越来越多的大公司把 Kubernetes 作为它们运行应用的最佳平台, Kubernetes 帮助企业标准化了无论是云端部署还是内部部署的应用交付方式.</p> <h5 id="_1-1-kubernetes系统的需求"><a href="#_1-1-kubernetes系统的需求" class="header-anchor">#</a> 1.1 Kubernetes系统的需求</h5> <p>在开始了解 Kubernetes 的细节之前, 来快速看一下近年来应用程序的开发部署是如何变化的. 变化是由两方面导致的, <strong>一方面是大型单体应用被拆解为更多的小型微服务, 另一方面是应用运行所依赖的基础架构的变化</strong>. 理解这些变化, 能帮助大家更好地看待使用 Kubernetes 和容器技术带来的好处.</p> <h6 id="_1-1-1-从单体应用到微服务"><a href="#_1-1-1-从单体应用到微服务" class="header-anchor">#</a> 1.1.1 从单体应用到微服务</h6> <p>单体应用由很多个组件组成, 这些组件紧密地耦合在一起, 由于它们在同一个操作系统进程中运行, 所以在开发, 部署, 管理的时候必须以同一个实体进行. 对单体应用来说, 即使是某个组件中一个小的修改, 都需要重新部署整个应用. 组件间缺乏严格的边界定义, 相互依赖, 日积月累导致系统复杂度提升, 整体质量也急剧恶化.</p> <p>运行一个单体应用, 通常需要一台能为整个应用提供足够资源的高性能服务器. 为了应对不断增长的系统负荷, 需要通过增加 CPU, 内存或其他系统资源的方式来对服务器做垂直扩展, 或者增加更多的跑这些应用程序的服务器的做水平扩展. 垂直扩展不需要应用程序做任何变化, 但是成本很快会越来越高, 并且通常会有瓶颈. 如果是水平扩展, 就可能需要应用程序代码做比较大的改动, 有时候甚至是不可行的, 比如系统的一些组件非常难于甚至不太可能去做水平扩展(像关系型数据库). 如果单体应用的任何一个部分不能扩展, 整个应用就不能扩展, 除非我们想办法把它拆分开.</p> <blockquote><p>将应用拆解为多个微服务</p></blockquote> <p>这些问题迫使大家将复杂的大型单体应用, <strong>拆分为小的可独立部署的微服务组件</strong>. 每个微服务以独立的进程(见图 1.1)运行, 并通过简单且定义良好的接口(API)与其他的微服务通信.</p> <p>服务之间可以通过类似 HTTP 这样的同步协议通信, 或者通过像 AMQP 这样的异步协议通信. 这些协议能够被大多数开发者所理解, 并且并不局限于某种编程语言. 这意味着任何一个微服务, 都可以用最适合的开发语言来实现.</p> <p><img src="/img/image-20240227212806-jpsytxq.png" alt="image" title="图1.1 单体应用中的组件与独立的微服务"></p> <p>因为每个微服务都是<strong>独立的进程</strong>, 提供相对静态的 API, 所以独立开发和部署单个微服务成了可能. 只要 API 不变或者向前兼容, 改动一个微服务, 并不会要求对其他微服务进行改动或者重新部署.</p> <blockquote><p>微服务的扩容</p></blockquote> <p>面向单体系统, 扩容针对的是整个系统, 而面向微服务架构, 扩容却只需要针对单个服务, 这意味着可以选择仅扩容那些需要更多资源的服务而保持其他的服务仍然维持在原来的规模. 如图 1.2 所示, 三种组件都被复制了多个, 并以多进程的方式部署在不同的服务器上, 而另外的组件只能以单体进程应用运行. <strong>当单体应用因为其中一部分无法扩容而整体被限制扩容时, 可以把应用拆分成多个微服务, 将那些能进行扩容的组件进行水平扩展, 不能进行扩容的组件进行垂直扩展</strong>.</p> <p><img src="/img/image-20240227212844-2qs6ekt.png" alt="image" title="图1.2 每个微服务能被单独扩容"></p> <blockquote><p>部署微服务</p></blockquote> <p>像大多数情况一样, 微服务也有缺点. 若你的系统仅包含少许可部署的组件, 管理那些组件是简单的. 决定每个组件部署在哪儿是不重要的, 因为没有那么多选择. <strong>当组件数量增加时, 部署相关的决定就变得越来越困难. 因为不仅组件部署的组合数在增加, 而且组件间依赖的组合数也在以更大的因素增加</strong>.</p> <p>微服务以团队形式完成工作, 所以需要找到彼此进行交流. 部署微服务时, 部署者需要正确地配置所有服务来使其作为一个单一系统能正确工作, 随着微服务的数量不断增加, 配置工作变得冗杂且易错, 特别是当你思考服务器宕机时运维团队需要做什么的时候.</p> <p>微服务还带来其他问题, 比如因为跨了多个进程和机器, 使得调试代码和定位异常调用变得困难. 幸运的是, 这些问题现在已经被诸如 Zipkin 这样的分布式定位系统解决.</p> <blockquote><p>环境需求的差异</p></blockquote> <p>正如已经提到的, 一个微服务架构中的组件不仅被独立部署, 也被独立开发. 因为它们的独立性, 出现不同的团队开发不同的组件是很正常的事实, 每个团队都有可能使用不同的库并在需求升级时替换它们. 如图 1.3 所示, 因为组件之间依赖的差异性, 应用程序需要同一个库的不同版本是不可避免的.</p> <p><img src="/img/image-20240227212932-5jb6rbv.png" alt="image" title="图1.3 多个应用在同一个主机上运行可能会有依赖冲突"></p> <p>部署动态链接的应用需要不同版本的共享库, 或者需要其他特殊环境, 在生产服务器部署并管理这种应用很快会成为运维团队的噩梦. 需要在同一个主机上部署的组件数量越大, 满足这些组件的所有需求就越难.</p> <h6 id="_1-1-2-为应用程序提供一个一致的环境"><a href="#_1-1-2-为应用程序提供一个一致的环境" class="header-anchor">#</a> 1.1.2 为应用程序提供一个一致的环境</h6> <p>不管你同时开发和部署多少个独立组件, 开发和运维团队总是需要解决的一个最大的问题是<mark><strong>程序运行环境的差异性</strong></mark>, 这种巨大差异不仅存在于开发环境与生产环境之间, 甚至存在于各个生产机器之间. 另外一个无法避免的事实是生产机器的环境会随着时间的推移而变化.</p> <p>这些差异性存在于从硬件到操作系统再到每台机器的可用库上. 生产环境是由运维团队管理的, 而开发者常常比较关心他们自己的开发环境. 这两组人对系统管理的理解程度是不同的, 这个理解偏差导致两个环境的系统有较大的差异, 系统管理员更重视保持系统更新最近的安全补丁, 而大多数开发者则并不太关心.</p> <p>生产系统可能要运行多个开发者或者开发团队的应用, 而对于开发者的电脑来说就不是这个情况了. 一个生产系统必须给所有它需要承载的应用提供合适的环境, 尽管这些应用可能需要不同的, 甚至带有冲突的版本库.</p> <p>为了减少仅会在生产环境才暴露的问题, <mark><strong>最理想的做法是让应用在开发和生产阶段可以运行在完全一样的环境下, 它们有完全一样的操作系统, 库, 系统配置, 网络环境和其他所有的条件</strong></mark>. 你也不想让这个环境随着时间推移而改变. 如果可能, 你想要确保在一台服务器上部署新的应用时, 不会影响到机器上已有的应用.</p> <h6 id="_1-1-3-迈向持续交付-devops和无运维"><a href="#_1-1-3-迈向持续交付-devops和无运维" class="header-anchor">#</a> 1.1.3 迈向持续交付:DevOps和无运维</h6> <p>在最近几年中, 大家看到了应用在开发流程和生产运维流程中的变化. 在过去, 开发团队的任务是创建应用并交付给运维团队, 然后运维团队部署应用并使它运行. 但是现在, 公司都意识到, 让同一个团队参与应用的开发, 部署, 运维的整个生命周期更好. 这意味着<mark><strong>开发者, QA 和运维团队彼此之间的合作需要贯穿整个流程. 这种实践被称为 DevOps</strong></mark>.</p> <blockquote><p>带来的优点</p></blockquote> <p>让开发者更多地在生产环境中运行应用, 能够使他们对用户的需求和问题, 以及运维团队维护应用所面临的困难, 有一个更好的理解. 应用程序开发者现在更趋向于将应用尽快地发布上线, 通过收集用户的反馈对应用做进一步开发.</p> <p>为了频繁地发布应用, 就需要简化部署流程. <strong>理想的状态是开发人员能够自己部署应用上线, 而不需要交付给运维人员操作</strong>. 但是, 部署应用往往需要具备对数据中心底层设备和硬件架构的理解. 开发人员却通常不知道或者不想知道这些细节.</p> <blockquote><p>让开发者和系统管理员做他们最擅长的</p></blockquote> <p>成功运行一个应用并服务于客户, 这是开发者和系统管理员共同的目标, 但他们也有着不同的个人目标和驱动因素. 开发者热衷于创造新的功能和提升用户体验, 他们通常不想成为确保底层操作系统已经更新所有安全补丁的那些人, 他们更喜欢把那些事留给系统管理员.</p> <p>运维团队负责管理生产部署流程及应用所在的硬件设备. 他们关心系统安全, 使用率, 以及其他对于开发者来说优先级不高的东西. 但运维人员不想处理所有应用组件之间暗含的内部依赖, 也不想考虑底层操作系统或者基础设施的改变会怎样影响到应用程序, 但是他们却不得不关注这些事情.</p> <p>理想情况是, <mark><strong>开发者是部署程序本身, 不需要知道硬件基础设施的任何情况, 也不需要和运维团队交涉, 这被叫作 NoOps</strong></mark>. 很明显, 你仍然需要有一些人来关心硬件基础设施, 但这些人不需要再处理应用程序的独特性.</p> <p>正如所看到的, Kubernetes 能让大家实现所有这些想法. <strong>通过对实际硬件做抽象, 然后将自身暴露成一个平台, 用于部署和运行应用程序. 它允许开发者自己配置和部署应用程序, 而不需要系统管理员的任何帮助, 让系统管理员聚焦于保持底层基础设施运转正常的同时, 不需要关注实际运行在平台上的应用程序</strong>.</p> <h5 id="_1-2-介绍容器技术"><a href="#_1-2-介绍容器技术" class="header-anchor">#</a> 1.2 介绍容器技术</h5> <p>在 1.1 节中, 罗列了一个不全面的开发和运维团队如今所面临的问题列表, 尽管有很多解决这些问题的方式, 但本书将关注如何用 Kubernetes 解决.</p> <p>Kubernetes 使用 Linux 容器技术来提供应用的隔离, 所以在钻研 Kubernetes 之前, 需要通过熟悉容器的基本知识来更加深入地理解 Kubernetes, 包括认识到存在的容器技术分支, 诸如 Docker 或者 rkt.</p> <h6 id="_1-2-1-什么是容器"><a href="#_1-2-1-什么是容器" class="header-anchor">#</a> 1.2.1 什么是容器</h6> <p>在 1.1.1 节中, 可以看到在同一台机器上运行的不同组件需要不同的, 可能存在冲突的依赖库版本, 或者是其他的不同环境需求.</p> <p>当一个应用程序仅由较少数量的大组件构成时, 完全可以接受给每个组件分配专用的虚拟机, 以及通过给每个组件提供自己的操作系统实例来隔离它们的环境. 但是当这些组件开始变小且数量开始增长时, 如果你不想浪费硬件资源, 又想持续压低硬件成本, 那就不能给每个组件配置一个虚拟机了. 但是这还不仅仅是浪费硬件资源, 因为每个虚拟机都需要被单独配置和管理, 所以增加虚拟机的数量也就导致了人力资源的浪费, 因为这增加了系统管理员的工作负担.</p> <blockquote><p>用 Linux 容器技术隔离组件</p></blockquote> <p>开发者不是使用虚拟机来隔离每个微服务环境(或者通常说的软件进程), 而是正在转向 Linux 容器技术. 容器允许你在同一台机器上运行多个服务, 不仅提供不同的环境给每个服务, 而且将它们互相隔离. 容器类似虚拟机, 但开销小很多.</p> <p><strong>一个容器里运行的进程实际上运行在宿主机的操作系统上, 就像所有其他进程一样(不像虚拟机, 进程是运行在不同的操作系统上的). 但在容器里的进程仍然是和其他进程隔离的. 对于容器内进程本身而言, 就好像是在机器和操作系统上运行的唯一一个进程</strong>.</p> <blockquote><p>比较虚拟机和容器</p></blockquote> <p><strong>和虚拟机比较, 容器更加轻量级, 它允许在相同的硬件上运行更多数量的组件. 主要是因为每个虚拟机需要运行自己的一组系统进程, 这就产生了除组件进程消耗以外的额外计算资源损耗. 从另一方面说, 一个容器仅仅是运行在宿主机上被隔离的单个进程, 仅消耗应用容器消耗的资源, 不会有其他进程的开销</strong>.</p> <p>因为虚拟机的额外开销, 导致没有足够的资源给每个应用开一个专用的虚拟机, 最终会将多个应用程序分组塞进每个虚拟机. 当使用容器时, 正如图 1.4 所示, 能够(也应该)让每个应用有一个容器. 最终结果就是可以在同一台裸机上运行更多的应用程序.</p> <p><img src="/img/image-20240227213001-aufw3q1.png" alt="image" title="图1.4 使用虚拟机来隔离一组应用程序与使用容器隔离单个应用程序"></p> <p>当在一台主机上运行三个虚拟机的时候, 就拥有了三个<strong>完全分离</strong>的操作系统, 它们运行并共享一台裸机. 在那些虚拟机之下是宿主机的操作系统与一个管理程序, 它将物理硬件资源分成较小部分的虚拟硬件资源, 从而被每个虚拟机里的操作系统使用. 运行在那些虚拟机里的应用程序会执行虚拟机操作系统的系统调用, 然后虚拟机内核会通过管理程序在宿主机上的物理来 CPU 执行 x86 指令.</p> <p>注意 存在两种类型的管理程序. 第一种类型的管理程序不会使用宿主机 OS, 而第二种类型的会.</p> <p><strong>多个容器则会完全执行运行在宿主机上的同一个内核的系统调用, 此内核是唯一一个在宿主机操作系统上执行 x86 指令的内核. CPU 也不需要做任何对虚拟机能做那样的虚拟化</strong>(如图 1.5 所示).</p> <p><img src="/img/image-20240227213033-35q1qpk.png" alt="image" title="图1.5 虚拟机和容器中的应用程序对 CPU 的不同使用方式"></p> <p>虚拟机的主要好处是它们提供完全隔离的环境, 因为每个虚拟机运行在它自己的 Linux 内核上, 而容器都是调用同一个内核, 这自然会有安全隐患. 如果硬件资源有限, 那当有少量进程需要隔离的时候, 虚拟机就可以成为一个选项. 为了在同一台机器上运行大量被隔离的进程, 容器因它的低消耗而成为一个更好的选择. 记住, <strong>每个虚拟机运行它自己的一组系统服务, 而容器则不会, 因为它们都运行在同一个操作系统上</strong>. 那也就意味着运行一个容器不用像虚拟机那样要开机, 它的进程可以很快被启动.</p> <blockquote><p>容器实现隔离机制介绍</p></blockquote> <p>你可能会好奇, 如果多个进程运行在同一个操作系统上, 那<strong>容器到底是怎样隔离它们</strong>的. 有两个机制可用: <mark><strong>第一个是 Linux 命名空间(namespace), 它使每个进程只看到它自己的系统视图(文件, 进程, 网络接口, 主机名等); 第二个是 Linux 控制组(cgroups), 它限制了进程能使用的资源量(CPU, 内存, 网络带宽等)</strong></mark> .</p> <blockquote><p>用 Linux 命名空间隔离进程</p></blockquote> <p><strong>默认情况下, 每个 Linux 系统最初仅有一个命名空间. 所有系统资源(诸如文件系统, 用户 ID, 网络接口等)属于这一个命名空间</strong>. 但是你能创建额外的命名空间, 以及在它们之间组织资源. 对于一个进程, 可以在其中一个命名空间中运行它. <strong>进程将只能看到同一个命名空间下的资源</strong>. 当然, 会存在多种类型的多个命名空间, 所以一个进程不单单只属于某一个命名空间, 而属于每个类型的一个命名空间.</p> <p>存在以下类型的命名空间:</p> <ul><li><strong>Mount(mnt)</strong></li> <li><strong>Process ID(pid)</strong></li> <li><strong>Network(net)</strong></li> <li><strong>Inter-process communicaion(ipd)</strong></li> <li><strong>UTS</strong></li> <li>User ID(user)</li></ul> <p><mark><strong>每种命名空间被用来隔离一组特定的资源</strong></mark>. 例如, UTS 命名空间决定了运行在命名空间里的进程能看见哪些主机名和域名. 通过分派两个不同的 UTS 命名空间给一对进程, 能使它们看见不同的本地主机名. 换句话说, 这两个进程就好像正在两个不同的机器上运行一样(至少就主机名而言是这样的).</p> <p>同样地, 一个进程属于什么 Network 命名空间决定了运行在进程里的应用程序能看见什么网络接口. <strong>每个网络接口属于一个命名空间, 但是可以从一个命名空间转移到另一个. 每个容器都使用它自己的网络命名空间, 因此每个容器仅能看见它自己的一组网络接口</strong>.</p> <p>现在你应该已经了解命名空间是如何隔离容器中运行的应用的.</p> <blockquote><p>限制进程的可用资源</p></blockquote> <p><mark><strong>另外的隔离性就是限制容器能使用的系统资源. 这通过 cgroups 来实现. cgroups 是一个 Linux 内核功能, 它被用来限制一个进程或者一组进程的资源使用. 一个进程的资源(CPU, 内存, 网络带宽等)使用量不能超出被分配的量</strong></mark>. 这种方式下, 进程不能过分使用为其他进程保留的资源, 这和进程运行在不同的机器上是类似的.</p> <h6 id="_1-2-2-docker容器平台介绍"><a href="#_1-2-2-docker容器平台介绍" class="header-anchor">#</a> 1.2.2 Docker容器平台介绍</h6> <p>尽管容器技术已经出现很久, 却是随着 Docker 容器平台的出现而变得广为人知. Docker 是第一个使容器能在不同机器之间移植的系统. 它不仅简化了打包应用的流程, 也简化了打包应用的库和依赖, 甚至整个操作系统的文件系统能被打包成一个简单的可移植的包, 这个包可以被用来在任何其他运行 Docker 的机器上使用.</p> <p>当用 Docker 运行一个被打包的应用程序时, 它能看见你捆绑的文件系统的内容, 不管运行在开发机器还是生产机器上, 它都能看见相同的文件, 即使生产机器运行的是完全不同的操作系统. 应用程序不会关心它所在服务器上的任何东西, 所以生产服务器上是否安装了和你开发机完全相同的一组库是不需要关心的.</p> <p>例如, 如果用整个红帽企业版 Linux(RHEL)的文件打包了应用程序, 不管在装有 Fedora 的开发机上运行它, 还是在装有 Debian 或者其他 Linux 发行版的服务器上运行它, 应用程序都认为它运行在 RHEL 中. <strong>只是内核可能不同</strong>.</p> <p>与在虚拟机中安装操作系统得到一个虚拟机镜像, 再将应用程序打包到镜像里, 通过分发整个虚拟机镜像到主机, 使应用程序能够运行起来类似, Docker 也能够达到相同的效果, 但不是使用虚拟机来实现应用隔离, 而是使用之前几节中提到的 Linux 容器技术来达到和虚拟机相同级别的隔离. 容器也不使用庞大的单个虚拟机镜像, 它使用较小的容器镜像.</p> <p><strong>基于 Docker 容器的镜像和虚拟机镜像的一个很大的不同是容器镜像是由多层构成, 它能在多个镜像之间共享和征用. 如果某个已经被下载的容器镜像已经包含了后面下载镜像的某些层, 那么后面下载的镜像就无须再下载这些层</strong>.</p> <blockquote><p>Docker的概念</p></blockquote> <p><strong>Docker 是一个打包, 分发和运行应用程序的平台</strong>. 它允许<mark><strong>将应用程序和应用程序所依赖的整个环境打包在一起</strong></mark>. 这既可以是一些应用程序需要的库, 也可以是一个被安装的操作系统所有可用的文件. Docker 使得传输这个包到一个中央仓库成为可能, 然后这个包就能被分发到任何运行 Docker 的机器上, 在那儿被执行(大部分情况是这样的, 但并不尽然, 后面将做出解释).</p> <p>三个主要概念组成了这种情形:</p> <ul><li><strong>镜像</strong>. Docker <strong>镜像里包含了打包的应用程序及其所依赖的环境. 它包含应用程序可用的文件系统和其他元数据, 如镜像运行时的可执行文件路径</strong>.</li> <li><strong>镜像仓库</strong>. Docker 镜像仓库<strong>用于存放 Docker 镜像</strong>, 以及促进不同人和不同电脑之间共享这些镜像. 当你编译你的镜像时, 要么可以在编译它的电脑上运行, 要么可以先上传镜像到一个镜像仓库, 然后下载到另外一台电脑上并运行它. 某些仓库是公开的, 允许所有人从中拉取镜像, 同时也有一些是私有的, 仅部分人和机器可接入.</li> <li><strong>容器</strong>. Docker <strong>容器通常是一个 Linux 容器, 它基于 Docker 镜像被创建</strong>. 一个运行中的容器是一个运行在 Docker 主机上的进程, 但它和主机, **以及所有运行在主机上的其他进程都是隔离的. 这个进程也是资源受限的, 意味着它只能访问和使用分配给它的资源(CPU, 内存等). **</li></ul> <blockquote><p>构建,分发和运行Dcoker镜像</p></blockquote> <p>图 1.6 显示了这三个概念以及它们之间的关系. 开发人员首先构建一个镜像, 然后把镜像推到镜像仓库中. 因此, 任何可以访问镜像仓库的人都可以使用该镜像. 然后, 他们可以将镜像拉取到任何运行着 Docker 的机器上并运行镜像. <strong>Docker 会基于镜像创建一个独立的容器, 并运行二进制可执行文件指定其作为镜像的一部分</strong>.</p> <p><img src="/img/image-20240227213117-8hn1e0z.png" alt="image" title="图1.6 Docker 镜像, 镜像仓库和容器"></p> <blockquote><p>对比虚拟机与 Docker 容器</p></blockquote> <p>由上文可知, Linux 容器和虚拟机的确有相像之处, 但容器更轻量级. 现在看一下 Docker 容器和虚拟机的具体比较(以及 Docker 镜像和虚拟机镜像的比较). 如图例 1.7 所示, 相同的 6 个应用程序分别运行在虚拟机上和用 Docker 容器运行.</p> <p><img src="/img/image-20240227213146-7k6i5ho.png" alt="image" title="图1.7 在3个虚拟机上运行6个应用及用 Docker 容器运行它们"></p> <p>你会注意到应用 A 和应用 B 无论是运行在虚拟机上还是作为两个分离容器运行时都可以访问相同的二进制和库. 在虚拟机里, 这是显然的, 因为两个应用都看到相同的文件系统. <mark><strong>但是每个容器有它自己隔离的文件系统, 那应用 A 和应用 B 如何共享同样的文件</strong></mark>?</p> <blockquote><p>镜像层</p></blockquote> <p>前面已经说过 Docker 镜像由多层构成. <strong>不同镜像可能包含完全相同的层, 因为这些 Docker 镜像都是基于另一个镜像之上构建的, 不同的镜像都能使用相同的父镜像作为它们的基础镜像</strong>. 这提升了镜像在网络上的分发效率, 当传输某个镜像时, 因为相同的层已被之前的镜像传输, 那么这些层就不需要再被传输.</p> <p>层不仅使分发更高效, 也有助于减少镜像的存储空间. 每一层仅被存一次, <mark><strong>当基于相同基础层的镜像被创建成两个容器时, 它们就能够读相同的文件. 但是如果其中一个容器写入某些文件, 另外一个是无法看见文件变更的. 因此, 即使它们共享文件, 仍然彼此隔离. 这是因为容器镜像层是只读的. 容器运行时, 一个新的可写层在镜像层之上被创建. 容器中进程写入位于底层的一个文件时, 此文件的一个拷贝在顶层被创建, 进程写的是此拷贝</strong></mark>.</p> <blockquote><p>容器镜像可移植性的限制</p></blockquote> <p>理论上, 一个容器镜像能运行在任何一个运行 Docker 的机器上. 但有一个小警告--一个关于运行在一台机器上的所有容器共享主机 Linux 内核的警告. 如果一个容器化的应用需要一个特定的内核版本, 那它可能不能在每台机器上都工作. 如果一台机器上运行了一个不匹配的 Linux 内核版本, 或者没有相同内核模块可用, 那么此应用就不能在其上运行.</p> <p>虽然容器相比虚拟机轻量许多, 但也给运行于其中的应用带来了一些局限性. 虚拟机没有这些局限性, 因为每个虚拟机都运行自己的内核.</p> <p>还不仅是内核的问题. 一个在特定硬件架构之上编译的容器化应用, 只能在有相同硬件架构的机器上运行. 不能将一个 x86架构编译的应用容器化后, 又期望它能运行在 ARM 架构的机器上. 你仍然需要一台虚拟机来做这件事情.</p> <h6 id="_1-2-3-rkt-一个docker的替代方案"><a href="#_1-2-3-rkt-一个docker的替代方案" class="header-anchor">#</a> 1.2.3 rkt-一个Docker的替代方案</h6> <p>Docker 是第一个使容器成为主流的容器平台. <mark><strong>Docker 本身并不提供进程隔离, 实际上容器隔离是在 Linux 内核之上使用诸如 Linux 命名空间和 cgroups 之类的内核特性完成的, Docker 仅简化了这些特性的使用</strong></mark>.</p> <p>在 Docker 成功后, 开放容器计划(OCI)就开始围绕容器格式和运行时创建了开放工业标准. Docker 是计划的一部分, rkt(发音为&quot;rock-it&quot;)则是另外一个 Linux 容器引擎.</p> <p>和 Docker 一样, rkt 也是一个运行容器的平台, 它强调安全性, 可构建性并遵从开放标准. 它使用 OCI 容器镜像, 甚至可以运行常规的 Docker 容器镜像.</p> <p>这本书只集中于使用 Docker 作为 Kubernetes 的容器, 因为它是 Kubernetes 最初唯一支持的容器类型. 最近 Kubernetes 也开始支持 rkt 及其他的容器类型.</p> <p>在这里提到 rkt 的原因是, 不应该错误地认为 Kubernetes 是一个专为 Docker 容器设计的容器编排系统. 实际上, 在阅读这本书的过程中, 你将会认识到 Kubernetes 的核心远不止是编排容器. 容器恰好是在不同集群节点上运行应用的最佳方式. 有了这些意识, 终于可以深入探讨本书所讲的核心内容--Kubernetes 了.</p> <h5 id="_1-3-kubernetes介绍"><a href="#_1-3-kubernetes介绍" class="header-anchor">#</a> 1.3 Kubernetes介绍</h5> <p>前面已经展示了, <strong>随着系统可部署组件的数量增长, 把它们都管理起来会变得越来越困难. 需要一个更好的方式来部署和管理这些组件, 并支持基础设施的全球性伸缩</strong>, 谷歌可能是第一个意识到这一点的公司. 谷歌等全球少数几个公司运行着成千上万的服务器, 而且在如此海量规模下, 不得不处理部署管理的问题. 这推动着他们找出解决方案使成千上万组件的管理变得有效且成本低廉.</p> <h6 id="_1-3-1-初衷"><a href="#_1-3-1-初衷" class="header-anchor">#</a> 1.3.1 初衷</h6> <p>这些年来, 谷歌开发出了一个叫 Borg 的内部系统(后来还有一个新系统叫 Omega), 应用开发者和系统管理员管理那些数以千计的应用程序和服务都受益于它的帮助. 除了简化开发和管理, 它也帮助他们获得了更高的基础设施利用率, 在你的组织如此庞大时, 这很重要. 当你运行成千上万台机器时, 哪怕一丁点的利用率提升也意味着节约了数百万美元, 所以, 开发这个系统的动机是显而易见的.</p> <p>在保守 Borg 和 Omega 秘密数十年之后, 2014年, 谷歌开放了 Kubernetes, 一个基于 Borg, Omega 及其他谷歌内部系统实践的开源系统.</p> <h6 id="_1-3-2-深入浅出地了解-kubernetes"><a href="#_1-3-2-深入浅出地了解-kubernetes" class="header-anchor">#</a> 1.3.2 深入浅出地了解 Kubernetes</h6> <p><strong>Kubernetes 是一个软件系统, 它允许你在其上很容易地部署和管理容器化的应用</strong>. 它依赖于 Linux 容器的特性来运行异构应用, 而无须知道这些应用的内部详情, 也不需要手动将这些应用部署到每台机器. 因为这些应用运行在容器里, 它们不会影响运行在同一台服务器上的其他应用, 当你是为完全不同的组织机构运行应用时, 这就很关键了. 这对于云供应商来说是至关重要的, 因为它们在追求高硬件可用率的同时也必须保障所承载应用的完全隔离.</p> <p>Kubernetes 使你在数以千计的电脑节点上运行软件时就像所有这些节点是单个大节点一样. 它将底层基础设施抽象, 这样做同时简化了应用的开发, 部署, 以及对开发和运维团队的管理.</p> <p>通过 Kubernetes 部署应用程序时, 你的集群包含多少节点都是一样的. 集群规模不会造成什么差异性, 额外的集群节点只是代表一些额外的可用来部署应用的资源</p> <blockquote><p>Kubernetes的核心功能</p></blockquote> <p>图 1.8 展示了一幅最简单的 Kubernetes 系统图. <strong>整个系统由一个主节点和若干个工作节点组成. 开发者把一个应用列表提交到主节点, Kubernetes 会将它们部署到集群的工作节点</strong>. 组件被部署在哪个节点对于开发者和系统管理员来说都不用关心.</p> <p><img src="/img/image-20240227230222-toi9zp3.png" alt="image" title="图1.8 Kubernetes 暴露整个数据中心作为单个开发平台"></p> <p><strong>开发者能指定一些应用必须一起运行, Kubernetes 将会在一个工作节点上部署它们. 其他的将被分散部署到集群中, 但是不管部署在哪儿, 它们都能以相同的方式互相通信</strong>.</p> <blockquote><p>帮助开发者聚焦核心应用功能</p></blockquote> <p>Kubernetes 可以被当作<mark><strong>集群的一个操作系统</strong></mark>来看待. 它降低了开发者不得不在他们的应用里实现一些和基础设施相关服务的心智负担. 他们现在依赖于 Kubernetes 来提供这些服务, 包括<strong>服务发现, 扩容, 负载均衡, 自恢复, 甚至领导者的选举</strong>. 应用程序开发者因此能集中精力实现应用本身的功能而不用浪费时间思索怎样集成应用与基础设施.</p> <blockquote><p>帮助运维团队获取更高的资源利用率</p></blockquote> <p>Kubernetes 将容器化应用运行在集群的某个地方, 并提供信息给应用组件来发现彼此并保证它们的运行. 因为应用程序不关心它运行在哪个节点上, Kubernetes 能在任何时间迁移应用并通过混合和匹配应用来获得比手动调度高很多的<strong>资源利用率</strong>.</p> <h6 id="_1-3-3-kubernetes集群架构"><a href="#_1-3-3-kubernetes集群架构" class="header-anchor">#</a> 1.3.3 Kubernetes集群架构</h6> <p>上面从上帝视角看到了 Kubernetes 的架构, 现在近距离看一下 <strong>Kubernetes 集群由什么组成</strong>. 在硬件级别, <strong>一个 Kubernetes 集群由很多节点组成</strong>, 这些节点被分成以下两种类型:</p> <ul><li><strong>主节点, 它承载着 Kubernetes 控制和管理整个集群系统的控制面板</strong>.</li> <li><strong>工作节点, 它们运行用户实际部署的应用</strong>.</li></ul> <p>图 1.9 展示了运行在这两组节点上的组件, 接下来进一步解释.</p> <p><img src="/img/image-20240227230250-gqo6iq3.png" alt="image" title="图1.9 组成一个 Kubernetes 集群的组件"></p> <blockquote><p>控制面板</p></blockquote> <p><strong>控制面板用于控制集群并使它工作</strong>. 它包含多个组件, 组件可以运行在单个主节点上或者通过副本分别部署在多个主节点以确保高可用性. 这些组件是:</p> <ul><li><strong>Kubernetes API 服务器</strong>, 你和其他控制面板组件都要和它通信.</li> <li><strong>Scheculer</strong>, 它<strong>调度</strong>你的应用(为应用的每个可部署组件分配一个工作节点).</li> <li><strong>Controller Manager</strong>, 它执行<strong>集群级别的功能</strong>, 如复制组件, 持续跟踪工作节点, 处理节点失败等.</li> <li><strong>etcd</strong>, 一个可靠的分布式数据存储, 它能<strong>持久化存储集群配置</strong>.</li></ul> <p>控制面板的组件持有并控制集群状态, 但是它们<strong>不运行应用程序</strong>. 这是由工作节点完成的.</p> <blockquote><p>工作节点</p></blockquote> <p><strong>工作节点是运行容器化应用的机器</strong>. 运行, 监控和管理应用服务的任务是由以下组件完成的:</p> <ul><li><strong>Docker, rtk 或其他的容器类型</strong>.</li> <li><strong>Kubelet</strong>, 它与 API 服务器通信, 并<strong>管理它所在节点的容器</strong>.</li> <li><strong>Kubernetes Service Proxy(kube-proxy)</strong> , 它<strong>负责组件之间的负载均衡网络流量</strong>.</li></ul> <p>后面将在第 11 章中详细解释所有这些组件. 笔者不喜欢先解释事物是如何工作的, 然后再解释它的功能并教人们如何使用它. 就像学习开车, 你不想知道引擎盖下是什么, 你首先想要学习怎样从 A 点开到 B 点. 只有在学会了如何做到这一点后, 你才会对汽车如何使这成为可能产生兴趣. 毕竟, 知道引擎盖下面是什么, 可能在有一天它抛锚后你被困在路边时, 会帮助你让车再次移动.</p> <h6 id="_1-3-4-在kubernetes中运行应用"><a href="#_1-3-4-在kubernetes中运行应用" class="header-anchor">#</a> 1.3.4 在Kubernetes中运行应用</h6> <p>为了在 Kubernetes 中运行应用, 首先<strong>需要将应用打包进一个或多个容器镜像, 再将那些镜像推送到镜像仓库, 然后将应用的描述发布到 Kubernetes API 服务器</strong>.</p> <p>该描述包括诸如容器镜像或者包含应用程序组件的容器镜像, 这些组件如何相互关联, 以及哪些组件需要同时运行在同一个节点上和哪些组件不需要同时运行等信息. 此外, 该描述还包括哪些组件为内部或外部客户提供服务且应该通过单个 IP 地址暴露, 并使其他组件可以发现.</p> <blockquote><p>描述信息怎样成为一个运行的容器</p></blockquote> <p><mark><strong>当 API 服务器处理应用的描述时, 调度器调度指定组的容器到可用的工作节点上, 调度是基于每组所需的计算资源, 以及调度时每个节点未分配的资源. 然后, 那些节点上的 Kubelet 指示容器运行时(例如 Docker)拉取所需的镜像并运行容器</strong></mark>.</p> <p>仔细看图 1.10 以更好地理解如何在 Kubernetes 中部署应用程序. 应用描述符列出了四个容器, 并将它们分为三组(这些集合被称为 <strong>pod</strong>, 后面将在第 3 章中解释它们是什么). 前两个 pod 只包含一个容器, 而最后一个包含两个. 这意味着<strong>两个容器都需要协作运行, 不应该相互隔离</strong>. 在每个 pod 旁边, 还可以看到一个数字, 表示需要并行运行的每个 pod 的副本数量. <mark><strong>在向 Kubernetes 提交描述符之后, 它将把每个 pod 的指定副本数量调度到可用的工作节点上. 节点上的 Kubelets 将告知 Docker 从镜像仓库中拉取容器镜像并运行容器</strong></mark>.</p> <blockquote><p>保持容器运行</p></blockquote> <p>一旦应用程序运行起来, <strong>Kubernetes 就会不断地确认应用程序的部署状态始终与你提供的描述相匹配</strong>. 例如, 如果你指出你需要运行五个 web 服务器实例, 那么 Kubernetes 总是保持正好运行五个实例. 如果实例之一停止了正常工作, 比如当进程崩溃或停止响应时, Kubernetes 将<strong>自动重启它</strong>.</p> <p>同理, <strong>如果整个工作节点死亡或无法访问, Kubernetes 将为在故障节点上运行的所有容器选择新节点, 并在新选择的节点上运行它们</strong>.</p> <p><img src="/img/image-20240227230331-ksrmeik.png" alt="image" title="图1.10 Kubernetes 体系结构的基本概述和在它之上运行的应用程序"></p> <blockquote><p>扩展副本数量</p></blockquote> <p>当应用程序运行时, 可以决定要<strong>增加或减少副本量</strong>, 而 Kubernetes 将分别增加附加的或停止多余的副本. 甚至可以把决定最佳副本数目的工作交给 Kubernetes. 它可以根据实时指标(如 CPU 负载, 内存消耗, 每秒查询或应用程序公开的任何其他指标)<strong>自动调整副本数</strong>.</p> <blockquote><p>命中移动目标</p></blockquote> <p>前面已经说过, Kubernetes 可能需要在集群中迁移你的容器. 当它们运行的节点失败时, 或者为了给其他容器腾出地方而从节点移除时, 就会发生这种情况. 如果容器向运行在集群中的其他容器或者外部客户端提供服务, 那么当容器在集群内频繁调度时, 它们该如何正确使用这个容器? 当这些容器被复制并分布在整个集群中时, 客户端如何连接到提供服务的容器呢?</p> <p>为了让客户能够轻松地找到提供特定服务的容器, 可以告诉 Kubernetes 哪些容器提供相同的服务, 而 Kubernetes 将通过一个静态 IP 地址暴露所有容器, 并将该地址暴露给集群中运行的所有应用程序. <strong>这是通过环境变量完成的, 但是客户端也可以通过良好的 DNS 查找服务 IP. kube-proxy 将确保到服务的连接可跨提供服务的容器实现负载均衡. 服务的 IP 地址保持不变, 因此客户端始终可以连接到它的容器, 即使它们在集群中移动</strong>.</p> <h6 id="_1-3-5-使用kubernetes的好处"><a href="#_1-3-5-使用kubernetes的好处" class="header-anchor">#</a> 1.3.5 使用Kubernetes的好处</h6> <p>如果在所有服务器上部署了 Kubernetes, 那么运维团队就不需要再部署应用程序. 因为容器化的应用程序已经包含了运行所需的所有内容, 系统管理员不需要安装任何东西来部署和运行应用程序. 在任何部署 Kubernetes 的节点上, Kubernetes 可以在不需要系统管理员任何帮助的情况下立即运行应用程序.</p> <blockquote><p>简化应用程序部署</p></blockquote> <p>由于 Kubernetes 将其所有工作节点公开为一个部署平台, 因此<strong>应用程序开发人员可以自己开始部署应用程序</strong>, 不需要了解组成集群的服务器.</p> <p>实际上, 现在所有节点都是一组等待应用程序使用它们的计算资源. 开发人员通常不关心应用程序运行在哪个服务器上, 只要服务器能够为应用程序提供足够的系统资源即可.</p> <p>在某些情况下, 开发人员确实关心应用程序应该运行在哪种硬件上. 如果节点是异构的, 那么你将会发现你希望某些应用程序在具有特定功能的节点上运行, 并在其他的节点上运行其他应用程序. 例如, 你的一个应用程序可能需要在使用 ssd 而不是 HDDs 的系统上运行, 而其他应用程序在 HDDs 上运行良好. 在这种情况下, 你显然希望确保特定的应用程序总是被调度到有 SSD 的节点上.</p> <p>在不使用 Kubernetes 的情况下, 系统管理员将选择一个具有 SSD 的特定节点, 并在那里部署应用程序. 但是当使用 Kubernetes 时, 与其选择应用程序应该运行在某一特定节点上, 不如告诉 Kubernetes 只在具有 SSD 的节点中进行选择. 后面在第 3 章会学到如何做到这一点.</p> <blockquote><p>更好地利用硬件</p></blockquote> <p>通过在服务器上装配 Kubernetes, 并使用它运行应用程序而不是手动运行它们, 你已经将应用程序与基础设施分离开来. 当你告诉 Kubernetes 运行你的应用程序时, 你在<strong>让它根据应用程序的资源需求描述和每个节点上的可用资源选择最合适的节点来运行你的应用程序</strong>.</p> <p><strong>通过使用容器, 不再用把这个应用绑定到一个特定的集群节点, 而允许应用程序在任何时候都在集群中自由迁移, 所以在集群上运行的不同应用程序组件可以被混合和匹配来紧密打包到集群节点. 这将确保节点的硬件资源得到尽可能好的利用</strong>.</p> <p>可以随时在集群中移动应用程序的能力, 使得 Kubernetes 可以比人工更好地利用基础设施. 人类不擅长寻找最优的组合, 尤其是当所有选项的数量都很大的时候, 比如当你有许多应用程序组件和许多服务器节点时, 所有的组件可以部署在所有的节点上. 显然, 计算机可以比人类更好, 更快地完成这项工作.</p> <blockquote><p>健康检查和自修复</p></blockquote> <p>在服务器发生故障时, 拥有一个允许在任何时候跨集群迁移应用程序的系统也很有价值. 随着集群大小的增加, 你将更频繁地处理出现故障的计算机组件.</p> <p><strong>Kubernetes 监控你的应用程序组件和它们运行的节点, 并在节点出现故障时自动将它们重新调度到其他节点</strong>. 这使运维团队不必手动迁移应用程序组件, 并允许团队立即专注于修复节点本身, 并将其修好送回到可用的硬件资源池中, 而不是将重点放在重新定位应用程序上.</p> <p>如果基础设施有足够的备用资源来允许正常的系统运行, 即使故障节点没有恢复, 运维团队甚至不需要立即对故障做出反应, 比如在凌晨3点. 他们可以睡得很香, 在正常的工作时间再处理失败的节点.</p> <blockquote><p>自动扩容</p></blockquote> <p>使用 Kubernetes 来管理部署的应用程序, 也意味着运维团队不需要不断地监控单个应用程序的负载, 以对突发负载峰值做出反应. 如前所述, 可以告诉 Kubernetes 监视每个应用程序使用的资源, 并不断调整每个应用程序的运行实例数量.</p> <p>如果 Kubernetes 运行在云基础设施上, 在这些基础设施中, 添加额外的节点就像通过云供应商的 API 请求它们一样简单, 那么 <strong>Kubernetes 甚至可以根据部署的应用程序的需要自动地将整个集群规模放大或缩小</strong>.</p> <blockquote><p>简化应用部署</p></blockquote> <p>前一节中描述的特性主要对运维团队有利. 但是开发人员呢? Kubernetes 是否也给他们带来什么好处? 这毋庸置疑.</p> <p>如果回过头来看看, 应用程序开发和生产流程中都运行在同一个环境中, 这对发现 bug 有很大的影响. 我们都同意越早发现一个 bug, 修复它就越容易, 修复它需要的工作量也就越少. 由于是在开发阶段就修复 bug, 所以这意味着他们的工作量减少了.</p> <p>还有一个事实是, 开发人员不需要实现他们通常会实现的特性. 这包括在集群应用中发现服务和对端. 这是由 Kubernetes 来完成的而不是应用. 通常, 应用程序只需要查找某些环境变量或执行 DNS 查询. 如果这还不够, 应用程序可以直接查询 Kubernetes API 服务器以获取该信息和其他信息. 像这样查询 Kubernetes API 服务器, 甚至可以使开发人员不必实现诸如复杂的集群 leader 选举机制.</p> <p>作为最后一个关于 Kubernetes 带来什么的例子, 还需要考虑到开发者们的信心增加. 当他们知道, 新版本的应用将会被推出时 Kubernetes 可以自动检测一个应用的新版本是否有问题, 如果是则立即停止其滚动更新, 这种信心的增强通常会加速应用程序的持续交付, 这对整个组织都有好处.</p> <h5 id="_1-4-本章小结"><a href="#_1-4-本章小结" class="header-anchor">#</a> 1.4 本章小结</h5> <p>在这个介绍性章节中, 你已经看到了近年来应用程序的变化, 以及它们现在如何变得更难部署和管理. 我们已经介绍了 Kubernetes, 并展示了它如何与 Docker 或其他容器平台一起帮助部署和管理应用程序及其运行的基础设施. 你已经学到了:</p> <ul><li>单体应用程序更容易部署, 但随着时间的推移更难维护, 并且有时难以扩展.</li> <li>基于微服务的应用程序体系结构使每个组件的开发更容易, 但是很难配置和部署它们作为单个系统工作.</li> <li>Linux 容器提供的好处与虚拟机差不多, 但它们轻量许多, 并且允许更好地利用硬件.</li> <li>通过允许更简单快捷地将容器化应用和其操作系统环境一起管理, Docker 改进了现有的 Linux 容器技术.</li> <li>Kubernetes 将整个数据中心暴露为用于运行应用程序的单个计算资源.</li> <li>开发人员可以通过 Kubernetes 部署应用程序, 而无须系统管理员的帮助.</li> <li>通过让 Kubernetes 自动地处理故障节点, 系统管理员可以睡得更好.</li></ul> <p>下一章将通过构建一个应用程序并在 Docker 和 Kubernetes 中运行它, 来上手实践.</p> <h4 id="_2-开始使用kubernetes和docker"><a href="#_2-开始使用kubernetes和docker" class="header-anchor">#</a> 2.开始使用Kubernetes和Docker</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>使用 Docker 创建, 运行及共享容器镜像</li> <li>在本地部署单节点的 Kubernetes 集群</li> <li>在 Google Kubernetes Engine 上部署 Kubernetes 集群</li> <li>配置和使用命令行客户端--kubectl</li> <li>在 Kubernetes 上部署应用并进行水平伸缩</li></ul> <p>在深入学习 Kubernetes 的概念之前, 先来看看<strong>如何创建一个简单的应用, 把它打包成容器镜像并在远端的 Kubernetes 集群(如托管在 Google Kubernetes Engine 中)或本地单节点集群中运行</strong>. 这会对整个 Kubernetes 体系有较好的了解, 并且会让接下来几个章节对 Kubernetes 基本概念的学习变得简单.</p> <h5 id="_2-1-创建-运行及共享容器镜像"><a href="#_2-1-创建-运行及共享容器镜像" class="header-anchor">#</a> 2.1 创建,运行及共享容器镜像</h5> <p>正如在之前章节所介绍的, 在 Kubernetes 中运行应用需要打包好的容器镜像. 本节将会对 Docker 的使用做简单的介绍. 接下来的几节中将会介绍:</p> <ol><li>安装 Docker 并运行第一个 &quot;Hello world&quot; 容器</li> <li>创建一个简单的 Node.js 应用并部署在 Kubernetes 中</li> <li>把应用打包成可以独立运行的容器镜像</li> <li>基于镜像运行容器</li> <li>把镜像推送到 Docker Hub, 这样任何人在任何地方都可以使用</li></ol> <h6 id="_2-1-1-安装docker并运行hello-world容器"><a href="#_2-1-1-安装docker并运行hello-world容器" class="header-anchor">#</a> 2.1.1 安装Docker并运行Hello World容器</h6> <p>首先, 需要在 Linux 主机上<strong>安装 Docker</strong>. 如果使用的不是 Linux 操作系统, 就需要启动 Linux 虚拟机(VM)并在虚拟机中运行 Docker. 如果使用的是 Mac 或 Windows 系统, Docker 将会自己启动一个虚拟机并在虚拟机中运行 Docker 守护进程. Docker 客户端可执行文件可以在宿主操作系统中使用, 并可以与虚拟机中的守护进程通信.</p> <p>根据操作系统的不同, 按照 http://docs.docker.com/engine/installation/ 上的指南安装 Docker. 安装完成后, 可以通过运行 Docker 客户端可执行文件来执行各种 Docker 命令. 例如, 可以试着从 Docker Hub 的公共镜像仓库拉取, 运行镜像, Docker hub 中有许多随时可用的常见镜像, 其中就包括 busybox, 可以用来运行简单的 echo&quot;Hello world&quot; 命令.</p> <blockquote><p>运行Hello World容器</p></blockquote> <p>busybox 是一个单一可执行文件, 包含多种标准 UNIX 命令行工具, 如: echo, ls, gzip 等. 除了包含 echo 命令的 busybox 命令, 也可以使用如 Fedora, Ubuntu 等功能完备的镜像.</p> <p>如何才能运行 busybox 镜像呢? 无须下载或者安装任何东西. 使用 docker run 命令然后指定需要运行的镜像的名字, 以及需要执行的命令(可选), 如下面这段代码.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker run busybox echo &quot;Hello world&quot;</span>
Unable to <span class="token function">find</span> image <span class="token string">'busybox:latest'</span> locally
latest: Pulling from library/busybox
9ad63333ebc9: Pull complete 
Digest: sha256:6d9ac9237a84afe1516540f40a0fafdc86859b2141954b4d643af7066d598b74
Status: Downloaded newer image <span class="token keyword">for</span> busybox:latest
Hello world
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这或许看起来并不那么令人印象深刻, 但非常棒的是仅仅使用一个简单的命令就下载, 运行一个完整的 &quot;应用&quot;, 而不用安装应用或是做其他的事情. 目前的应用是单一可执行文件(busybox), 但也可以是一个有许多依赖的复杂应用. 整个配置运行应用的过程是完全一致的. 同样重要的是应用是在容器内部被执行的, 完全独立于其他所有主机上运行的进程.</p> <blockquote><p>背后的原理</p></blockquote> <p>图 2.1 展示了执行 docker run 命令之后发生的事情. 首先, Docker 会检查 busybox:latest 镜像<strong>是否已经存在于本机</strong>. 如果没有, Docker 会从 http://docker.io 的 Docker 镜像中心拉取镜像. 镜像下载到本机之后, <strong>Docker 基于这个镜像创建一个容器并在容器中运行命令. echo 命令打印文字到标准输出流, 然后进程终止, 容器停止运行</strong>.</p> <p><img src="/img/image-20240227230420-2tdkbzw.png" alt="image" title="图2.1 在一个基于 busybox 镜像的容器中运行 echo &quot;Hello world&quot;"></p> <blockquote><p>运行其他镜像</p></blockquote> <p>运行其他的容器镜像和运行 busybox 镜像是一样的, 甚至可能更简单, 因为可以不需要指定执行命令. 就像例子中的 echo &quot;Hello world&quot;, 被执行的命令通常都会被包含在镜像中, 但也可以根据需要进行覆盖. 在浏览器中搜索 http://hub.docker.com 或其他公开的镜像中心的可用镜像之后, 可以像这样在 Docker 中运行镜像:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run <span class="token operator">&lt;</span>image<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>容器镜像的版本管理</p></blockquote> <p>当然, 所有的软件包都会更新, 所以通常每个包都不止一个版本. Docker 支持同一镜像的多个版本. 每一个版本必须有唯一的 tag 名. 当引用镜像没有显式地指定 tag 时, Docker 会默认指定 tag 为 latest. 如果想要运行别的版本的镜像, 需要像这样指定镜像的版本:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run <span class="token operator">&lt;</span>image<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>tag<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h6 id="_2-1-2-创建一个简单的node-js应用"><a href="#_2-1-2-创建一个简单的node-js应用" class="header-anchor">#</a> 2.1.2 创建一个简单的Node.js应用</h6> <p>现在有了一个可以工作的 Docker 环境来创建应用. 接下来会构建一个简单的 Node.js Web 应用, 并把它打包到容器镜像中. 这个应用会接收 HTTP 请求并响应应用运行的主机名. 这样, 应用运行在容器中, 看到的是自己的主机名而不是宿主机名, 即使它也像其他进程一样运行在宿主机上. 这在后面会非常有用, 当应用部署在 Kubernetes 上并进行伸缩时(水平伸缩, 复制应用到多个节点), 你会发现 HTTP 请求切换到了应用的不同实例上.</p> <p>应用包含一个名为 app.js 的文件, 详见下面的代码清单.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token builtin class-name">let</span> http <span class="token operator">=</span> require<span class="token punctuation">(</span><span class="token string">&quot;http&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

http.createServer<span class="token punctuation">(</span> <span class="token punctuation">(</span>request, response<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span>
   response.writeHead<span class="token punctuation">(</span><span class="token number">200</span>, <span class="token punctuation">{</span><span class="token string">'Content-Type'</span><span class="token builtin class-name">:</span> <span class="token string">'text/plain'</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
   response.end<span class="token punctuation">(</span><span class="token string">'Hello World\n'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>.listen<span class="token punctuation">(</span><span class="token number">8081</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
console.log<span class="token punctuation">(</span><span class="token string">'Server running at http://127.0.0.1:8081/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>代码清晰地说明了实现的功能. 这里在 8080 端口启动了一个 HTTP 服务器. 服务器会以状态码 200 OK 和文字 &quot;<code>You've hit &lt;hostname&gt;</code>​&quot; 来响应每个请求. 请求 handler 会把客户端的 IP 打印到标准输出, 以便日后查看.</p> <p>注意: 返回的主机名是服务器真实的主机名, 不是客户端发出的 HTTP 请求中头的 Host 字段.</p> <p>现在可以直接下载安装 Node.js 来测试代码了, 但是这不是必需的, 因为可以直接用 Docker 把应用打包成镜像, 这样在需要运行的主机上就无须下载和安装其他的东西(当然不包括安装 Docker 来运行镜像).</p> <h6 id="_2-1-3-为镜像创建dockerfile"><a href="#_2-1-3-为镜像创建dockerfile" class="header-anchor">#</a> 2.1.3 为镜像创建Dockerfile</h6> <p>为了把应用打包成镜像, 首先需要<mark><strong>创建一个叫 Dockerfile 的文件, 它包含了一系列构建镜像时会执行的指令. Dockerfile 文件需要和 app.js 文件在同一目录, 并包含下面代码清单中的命令</strong></mark>.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FROM node:7
ADD app.js /app.js
ENTRYPOINT <span class="token punctuation">[</span><span class="token string">&quot;node&quot;</span>, <span class="token string">&quot;app.js&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><mark><strong>From 行定义了镜像的起始内容(构建所基于的基础镜像)</strong></mark> . 这个例子中使用的是 node 镜像的 tag 7 版本. 第二行中把 app.js 文件从本地文件夹添加到镜像的根目录, 保持 app.js 这个文件名. <mark><strong>最后一行定义了当镜像被运行时需要被执行的命令</strong></mark>, 这个例子中, 命令是 node app.js.</p> <blockquote><p>选择基础镜像</p></blockquote> <p>你或许在想, 为什么要选择这个镜像作为基础镜像. 因为这个应用是 Node.js 应用, <strong>镜像需要包含可执行的 node 二进制文件来运行应用</strong>. 你也可以使用任何包含这个二进制文件的镜像, 或者甚至可以使用 Linux 发行版的基础镜像, 如 fedora 或 ubuntu, 然后在镜像构建的时候安装 Node.js. 但是由于 node 镜像是专门用来运行 Node.js 应用的, 并且包含了运行应用所需的一切, 所以把它当作基础镜像.</p> <h6 id="_2-1-4-构建容器镜像"><a href="#_2-1-4-构建容器镜像" class="header-anchor">#</a> 2.1.4 构建容器镜像</h6> <p>现在有了 Dockerfile 和 app.js 文件, 这是用来<strong>构建镜像的所有文件</strong>. 运行下面的 Docker 命令来构建镜像:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> build <span class="token parameter variable">-t</span> kubia <span class="token builtin class-name">.</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>下面是执行的过程:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker build -t kubia .</span>
<span class="token punctuation">[</span>+<span class="token punctuation">]</span> Building <span class="token number">61</span>.8s <span class="token punctuation">(</span><span class="token number">7</span>/7<span class="token punctuation">)</span> FINISHED                                                                                   docker:default
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span>internal<span class="token punctuation">]</span> load build definition from Dockerfile                                                                          <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> transferring dockerfile: 155B                                                                                          <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span>internal<span class="token punctuation">]</span> load metadata <span class="token keyword">for</span> docker.io/library/node:latest                                                                <span class="token number">5</span>.1s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span>internal<span class="token punctuation">]</span> load .dockerignore                                                                                             <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> transferring context: 2B                                                                                               <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span>internal<span class="token punctuation">]</span> load build context                                                                                             <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> transferring context: 339B                                                                                             <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token number">1</span>/2<span class="token punctuation">]</span> FROM docker.io/library/node:latest@sha256:abc4a25c8b5a2b460f3144aabfc8941ecd7e4fb721e0b14b635e70394c1899fb         <span class="token number">56</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> resolve docker.io/library/node:latest@sha256:abc4a25c8b5a2b460f3144aabfc8941ecd7e4fb721e0b14b635e70394c1899fb          <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:1b13d4e1a46e5e969702ec92b7c787c1b6891bff7c21ad378ff6dbc9e751d5d4 <span class="token number">49</span>.56MB / <span class="token number">49</span>.56MB                             <span class="token number">24</span>.4s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:30d85599795460b2d9d24c6b87c53ec60555b601705cc83bea31632240500980 <span class="token number">64</span>.14MB / <span class="token number">64</span>.14MB                             <span class="token number">32</span>.5s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:abc4a25c8b5a2b460f3144aabfc8941ecd7e4fb721e0b14b635e70394c1899fb <span class="token number">1</span>.21kB / <span class="token number">1</span>.21kB                                <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:993a29391cfd6233155551d2b292ac085e244dff5a5c8eb17d6f2bc6c4b4d5a1 <span class="token number">2</span>.00kB / <span class="token number">2</span>.00kB                                <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:80eddb4b8564965167c1fe7ba2d42f03215dda04e2dfff4f780ec0959fdb152c <span class="token number">7</span>.34kB / <span class="token number">7</span>.34kB                                <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:1c74526957fc2157e8b0989072dc99b9582b398c12d1dcd40270fd76231bab0c <span class="token number">24</span>.05MB / <span class="token number">24</span>.05MB                              <span class="token number">3</span>.9s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:ad5739181616b815fae7edc6bba689496674acbcf44e48a57fc7cc13a379b3a2 <span class="token number">211</span>.10MB / <span class="token number">211</span>.10MB                           <span class="token number">35</span>.7s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:b2aed33d4664181472f1d65045888f7184526fd927cdd9db1eff9c641a35cde4 <span class="token number">3</span>.37kB / <span class="token number">3</span>.37kB                               <span class="token number">25</span>.8s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:1b13d4e1a46e5e969702ec92b7c787c1b6891bff7c21ad378ff6dbc9e751d5d4                                     <span class="token number">4</span>.3s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:1688fca40fa1daff56a3ec6543b757e5b0f2f36bda7d337083b00106391e30c0 <span class="token number">49</span>.30MB / <span class="token number">49</span>.30MB                             <span class="token number">37</span>.5s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:1c74526957fc2157e8b0989072dc99b9582b398c12d1dcd40270fd76231bab0c                                     <span class="token number">1</span>.3s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:30d85599795460b2d9d24c6b87c53ec60555b601705cc83bea31632240500980                                     <span class="token number">5</span>.4s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:e028f1255c746a5d63eedbb75d5f62e0ce7f42401bbf8a7749a26ec9c6331fac <span class="token number">2</span>.23MB / <span class="token number">2</span>.23MB                               <span class="token number">35</span>.8s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> sha256:49b4ed55fa733d211f4854387b40a9e2d5b1c4e140d3d685a0c2b6a1bd8e3d80 451B / 451B                                   <span class="token number">36</span>.7s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:ad5739181616b815fae7edc6bba689496674acbcf44e48a57fc7cc13a379b3a2                                    <span class="token number">13</span>.6s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:b2aed33d4664181472f1d65045888f7184526fd927cdd9db1eff9c641a35cde4                                     <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:1688fca40fa1daff56a3ec6543b757e5b0f2f36bda7d337083b00106391e30c0                                     <span class="token number">4</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:e028f1255c746a5d63eedbb75d5f62e0ce7f42401bbf8a7749a26ec9c6331fac                                     <span class="token number">0</span>.2s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> extracting sha256:49b4ed55fa733d211f4854387b40a9e2d5b1c4e140d3d685a0c2b6a1bd8e3d80                                     <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token number">2</span>/2<span class="token punctuation">]</span> ADD index.js /index.js                                                                                              <span class="token number">0</span>.6s
 <span class="token operator">=</span><span class="token operator">&gt;</span> exporting to image                                                                                                        <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> exporting layers                                                                                                       <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> writing image sha256:59e732e36856b72bfbbb5fa19a6212a1692444573077ed723f89ae00a43cb2d7                                  <span class="token number">0</span>.0s
 <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token operator">=</span><span class="token operator">&gt;</span> naming to docker.io/library/kubia 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><p>图 2.2 展示了镜像构建的过程. 用户告诉 Docker 需要基于当前目录(注意命令结尾的点)构建一个叫 kubia 的镜像, <strong>Docker 会在目录中寻找 Dockerfile, 然后基于其中的指令构建镜像</strong>.</p> <p><img src="/img/image-20240227230504-jb7tsdc.png" alt="image" title="图2.2 基于 Dockerfile 构建一个新的容器镜像"></p> <blockquote><p>镜像是如何构建的</p></blockquote> <p>构建过程不是由 Docker 客户端进行的, 而是将整个目录的文件上传到 Docker 守护进程并在那里进行的. Docker 客户端和守护进程不要求在同一台机器上. 如果在一台非 Linux 操作系统中使用 Docker, 客户端就运行在宿主操作系统上, 但是守护进程运行在一个虚拟机内. 由于构建目录中的文件都被上传到了守护进程中, 如果包含了大量的大文件而且守护进程不在本地运行, 上传过程会花费更多的时间.</p> <p>提示: <strong>不要在构建目录中包含任何不需要的文件</strong>, 这样会减慢构建的速度, 尤其当 Docker 守护进程运行在一个远端机器的时候.</p> <p>在构建过程中, Docker 首次会从公开的镜像仓库(Docker Hub)拉取基础镜像(node:7), 除非已经拉取过镜像并存储在本机上了.</p> <blockquote><p>镜像分层</p></blockquote> <p>镜像不是一个大的二进制块, 而是由<strong>多层</strong>组成的, 在运行 busybox 例子时你可能已经注意到(每一层有一行 Pull complete), <strong>不同镜像可能会共享分层, 这会让存储和传输变得更加高效</strong>. 比如, 如果创建了多个基于相同基础镜像(比如例子中的 node:7)的镜像, 所有组成基础镜像的分层<strong>只会被存储一次</strong>. 拉取镜像的时候, <strong>Docker 会独立下载每一层. 一些分层可能已经存储在机器上了, 所以 Docker 只会下载未被存储的分层</strong>.</p> <p>你或许会认为每个 Dockerfile 只创建一个新层, 但是并不是这样的. <mark><strong>构建镜像时, Dockerfile 中每一条单独的指令都会创建一个新层</strong></mark>. 镜像构建的过程中, 拉取基础镜像所有分层之后, Docker 在它们上面创建一个新层并且添加 app.js. 然后会创建另一层来指定镜像被运行时所执行的命令. 最后一层会被标记为 kubia:latest. 图 2.3 展示了这个过程, 同时也展示另外一个叫 other:latest 的镜像如何与我们构建的镜像<strong>共享同一层</strong> Node.js 镜像.</p> <p><img src="/img/image-20240227230528-wche0vq.png" alt="image" title="图2.3 容器镜像是由多层组成的, 每一层可以被不同镜像复用"></p> <p>构建完成时, <strong>新的镜像会存储在本地</strong>. 下面的代码展示了如何通过 Docker 列出本地存储的镜像:</p> <p><strong>代码清单-2.4 列出本地存储的镜像</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker images</span>
REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
kubia        latest    c376744dd01f   <span class="token number">20</span> seconds ago   <span class="token number">1</span>.1GB
busybox      latest    3f57d9401f8d   <span class="token number">13</span> days ago      <span class="token number">4</span>.26MB
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><blockquote><p>比较使用Dockerfile和手动构建镜像</p></blockquote> <p><strong>Dockerfile 是使用 Docker 构建容器镜像的常用方式</strong>, 但也可以通过运行已有镜像容器来手动构建镜像, 在容器中运行命令, 退出容器, 然后把最终状态作为新镜像. 用 Dockerfile 构建镜像是与此相同的, 但是是自动化且可重复的, 随时可以通过修改 Dockerfile 重新构建镜像而无须手动重新输入命令.</p> <h6 id="_2-1-5-运行容器镜像"><a href="#_2-1-5-运行容器镜像" class="header-anchor">#</a> 2.1.5 运行容器镜像</h6> <p>以下的命令可以用来运行镜像:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker run --name kubia-container -p 8081:8081 -d kubia</span>
4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这条命令告知 Docker <strong>基于 kubia 镜像创建一个叫 kubia-container 的新容器</strong>. 这个容器与命令行分离(-d 标志), 这意味着在<strong>后台运行</strong>. 本机上的 <strong>8081 端口会被映射到容器内的 8081 端口(-p 8081:8081 选项)</strong> , 所以可以通过 http://localhost:8081 访问这个应用.</p> <p>如果没有在本机上运行 Docker 守护进程(比如使用的是 Mac 或 Windows 系统, 守护进程会运行在 VM 中), 需要使用 VM 的主机名或 IP 来代替 localhost 运行守护进程. 可以通过 DOCKER_HOST 这个环境变量查看主机名.</p> <blockquote><p>访问应用</p></blockquote> <p>现在试着通过 http://localhost:8081 访问应用(确保使用 Docker 主机名或 IP 替换 localhost):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># curl localhost:8081</span>
Hello World
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这是应用的响应. 现在应用运行在容器中, 与其他东西隔离.</p> <blockquote><p>列出所有运行中的容器</p></blockquote> <p>下面的代码清单列出了所有的运行中的容器, 可以查看列表(为了更好的可读性, 列表被分成了两行显示).</p> <p><strong>代码清单-2.5 列出运行中的容器</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker ps</span>
CONTAINER ID   IMAGE     COMMAND                   CREATED          STATUS          PORTS                                       NAMES
4f0fce9ec4ea   kubia     <span class="token string">&quot;docker-entrypoint.s…&quot;</span>   <span class="token number">56</span> seconds ago   Up <span class="token number">55</span> seconds   <span class="token number">0.0</span>.0.0:8081-<span class="token operator">&gt;</span><span class="token number">8081</span>/tcp, :::8081-<span class="token operator">&gt;</span><span class="token number">8081</span>/tcp   kubia-container
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>有一个容器在运行. Docker 会打印出<strong>每一个容器的 ID 和名称, 容器运行所使用的镜像, 以及容器中执行的命令</strong>.</p> <blockquote><p>获取更多的容器信息</p></blockquote> <p>docker ps 只会展示容器的大部分基础信息. 可以使用 <strong>docker inspect</strong> 查看更多的信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker inspect kubia-container</span>
<span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
        <span class="token string">&quot;Id&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2&quot;</span>,
        <span class="token string">&quot;Created&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-01-31T01:36:30.40632305Z&quot;</span>,
        <span class="token string">&quot;Path&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;docker-entrypoint.sh&quot;</span>,
        <span class="token string">&quot;Args&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token string">&quot;/bin/sh&quot;</span>,
            <span class="token string">&quot;-c&quot;</span>,
            <span class="token string">&quot;node app.js&quot;</span>
        <span class="token punctuation">]</span>,
        <span class="token string">&quot;State&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;Status&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;running&quot;</span>,
            <span class="token string">&quot;Running&quot;</span><span class="token builtin class-name">:</span> true,
            <span class="token string">&quot;Paused&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;Restarting&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;OOMKilled&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;Dead&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;Pid&quot;</span><span class="token builtin class-name">:</span> <span class="token number">2479</span>,
            <span class="token string">&quot;ExitCode&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;Error&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;StartedAt&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-01-31T01:36:30.846930428Z&quot;</span>,
            <span class="token string">&quot;FinishedAt&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;0001-01-01T00:00:00Z&quot;</span>
        <span class="token punctuation">}</span>,
        <span class="token string">&quot;Image&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;sha256:c376744dd01f61a3f6331a1f1f64f7138481a2cf70db4151dafeffdcc5df8acd&quot;</span>,
        <span class="token string">&quot;ResolvConfPath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/containers/4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2/resolv.conf&quot;</span>,
        <span class="token string">&quot;HostnamePath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/containers/4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2/hostname&quot;</span>,
        <span class="token string">&quot;HostsPath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/containers/4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2/hosts&quot;</span>,
        <span class="token string">&quot;LogPath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/containers/4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2/4f0fce9ec4eada2c4a4759ef051abdb6f24dbab59f80060a859d1bc1a26cfed2-json.log&quot;</span>,
        <span class="token string">&quot;Name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/kubia-container&quot;</span>,
        <span class="token string">&quot;RestartCount&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
        <span class="token string">&quot;Driver&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;overlay2&quot;</span>,
        <span class="token string">&quot;Platform&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;linux&quot;</span>,
        <span class="token string">&quot;MountLabel&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
        <span class="token string">&quot;ProcessLabel&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
        <span class="token string">&quot;AppArmorProfile&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
        <span class="token string">&quot;ExecIDs&quot;</span><span class="token builtin class-name">:</span> null,
        <span class="token string">&quot;HostConfig&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;Binds&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;ContainerIDFile&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;LogConfig&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;Type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;json-file&quot;</span>,
                <span class="token string">&quot;Config&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            <span class="token punctuation">}</span>,
            <span class="token string">&quot;NetworkMode&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default&quot;</span>,
            <span class="token string">&quot;PortBindings&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;8081/tcp&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                    <span class="token punctuation">{</span>
                        <span class="token string">&quot;HostIp&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
                        <span class="token string">&quot;HostPort&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;8081&quot;</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">]</span>
            <span class="token punctuation">}</span>,
            <span class="token string">&quot;RestartPolicy&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;Name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;no&quot;</span>,
                <span class="token string">&quot;MaximumRetryCount&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>
            <span class="token punctuation">}</span>,
            <span class="token string">&quot;AutoRemove&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;VolumeDriver&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;VolumesFrom&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;ConsoleSize&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                <span class="token number">33</span>,
                <span class="token number">131</span>
            <span class="token punctuation">]</span>,
            <span class="token string">&quot;CapAdd&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;CapDrop&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;CgroupnsMode&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;host&quot;</span>,
            <span class="token string">&quot;Dns&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;DnsOptions&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;DnsSearch&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;ExtraHosts&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;GroupAdd&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;IpcMode&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;private&quot;</span>,
            <span class="token string">&quot;Cgroup&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;Links&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;OomScoreAdj&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;PidMode&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;Privileged&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;PublishAllPorts&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;ReadonlyRootfs&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;SecurityOpt&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;UTSMode&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;UsernsMode&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;ShmSize&quot;</span><span class="token builtin class-name">:</span> <span class="token number">67108864</span>,
            <span class="token string">&quot;Runtime&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;runc&quot;</span>,
            <span class="token string">&quot;Isolation&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;CpuShares&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;Memory&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;NanoCpus&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;CgroupParent&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;BlkioWeight&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;BlkioWeightDevice&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;BlkioDeviceReadBps&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;BlkioDeviceWriteBps&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;BlkioDeviceReadIOps&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;BlkioDeviceWriteIOps&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;CpuPeriod&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;CpuQuota&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;CpuRealtimePeriod&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;CpuRealtimeRuntime&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;CpusetCpus&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;CpusetMems&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;Devices&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;DeviceCgroupRules&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;DeviceRequests&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;MemoryReservation&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;MemorySwap&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;MemorySwappiness&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;OomKillDisable&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;PidsLimit&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;Ulimits&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
            <span class="token string">&quot;CpuCount&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;CpuPercent&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;IOMaximumIOps&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;IOMaximumBandwidth&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;MaskedPaths&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                <span class="token string">&quot;/proc/asound&quot;</span>,
                <span class="token string">&quot;/proc/acpi&quot;</span>,
                <span class="token string">&quot;/proc/kcore&quot;</span>,
                <span class="token string">&quot;/proc/keys&quot;</span>,
                <span class="token string">&quot;/proc/latency_stats&quot;</span>,
                <span class="token string">&quot;/proc/timer_list&quot;</span>,
                <span class="token string">&quot;/proc/timer_stats&quot;</span>,
                <span class="token string">&quot;/proc/sched_debug&quot;</span>,
                <span class="token string">&quot;/proc/scsi&quot;</span>,
                <span class="token string">&quot;/sys/firmware&quot;</span>,
                <span class="token string">&quot;/sys/devices/virtual/powercap&quot;</span>
            <span class="token punctuation">]</span>,
            <span class="token string">&quot;ReadonlyPaths&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                <span class="token string">&quot;/proc/bus&quot;</span>,
                <span class="token string">&quot;/proc/fs&quot;</span>,
                <span class="token string">&quot;/proc/irq&quot;</span>,
                <span class="token string">&quot;/proc/sys&quot;</span>,
                <span class="token string">&quot;/proc/sysrq-trigger&quot;</span>
            <span class="token punctuation">]</span>
        <span class="token punctuation">}</span>,
        <span class="token string">&quot;GraphDriver&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;Data&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;LowerDir&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/overlay2/1fc6defcb50017c4cebaacd67f3ceb118209c6063a3c2801d0145ace6b5dae3b-init/diff:/var/lib/docker/overlay2/uxhfsfshjhi6qkmc0oome2b9t/diff:/var/lib/docker/overlay2/2c5c76db7d1135ec2809292a39a25bdf2182eca42b0542b7a5e31846788d0a71/diff:/var/lib/docker/overlay2/542725b014213a84e64c7e2859176f83d616ed09ecbc3bb49640323b98812484/diff:/var/lib/docker/overlay2/6473c82f459fdcb5adbd7d0e5a317d88ce234c608a52fe123d56098a0dd4393c/diff:/var/lib/docker/overlay2/98c3eb626f5ecccb2bf595b1a303a6895bff932263661876907a22c6a6c33f99/diff:/var/lib/docker/overlay2/29af9845f91f8f065be72e86b97d2fe88fb5e4747c3e2e5906c366e59c4e14da/diff:/var/lib/docker/overlay2/ccb76bcea58267d7959847928e6ca4978ac8d1a4f0c734f755d82f4a6577a7b0/diff:/var/lib/docker/overlay2/fc5d14d023523285251aec4833a137d10da876e0d5ce9a99dc3540b5e4fd8d40/diff:/var/lib/docker/overlay2/3612f94227991ed89d3fda9915dde519f5122073196c7f96121daa43d9f822dc/diff&quot;</span>,
                <span class="token string">&quot;MergedDir&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/overlay2/1fc6defcb50017c4cebaacd67f3ceb118209c6063a3c2801d0145ace6b5dae3b/merged&quot;</span>,
                <span class="token string">&quot;UpperDir&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/overlay2/1fc6defcb50017c4cebaacd67f3ceb118209c6063a3c2801d0145ace6b5dae3b/diff&quot;</span>,
                <span class="token string">&quot;WorkDir&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/lib/docker/overlay2/1fc6defcb50017c4cebaacd67f3ceb118209c6063a3c2801d0145ace6b5dae3b/work&quot;</span>
            <span class="token punctuation">}</span>,
            <span class="token string">&quot;Name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;overlay2&quot;</span>
        <span class="token punctuation">}</span>,
        <span class="token string">&quot;Mounts&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,
        <span class="token string">&quot;Config&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;Hostname&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;4f0fce9ec4ea&quot;</span>,
            <span class="token string">&quot;Domainname&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;User&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;AttachStdin&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;AttachStdout&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;AttachStderr&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;ExposedPorts&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;8081/tcp&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            <span class="token punctuation">}</span>,
            <span class="token string">&quot;Tty&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;OpenStdin&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;StdinOnce&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;Env&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                <span class="token string">&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span>,
                <span class="token string">&quot;NODE_VERSION=21.6.1&quot;</span>,
                <span class="token string">&quot;YARN_VERSION=1.22.19&quot;</span>
            <span class="token punctuation">]</span>,
            <span class="token string">&quot;Cmd&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                <span class="token string">&quot;/bin/sh&quot;</span>,
                <span class="token string">&quot;-c&quot;</span>,
                <span class="token string">&quot;node app.js&quot;</span>
            <span class="token punctuation">]</span>,
            <span class="token string">&quot;Image&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kubia&quot;</span>,
            <span class="token string">&quot;Volumes&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;WorkingDir&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;Entrypoint&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                <span class="token string">&quot;docker-entrypoint.sh&quot;</span>
            <span class="token punctuation">]</span>,
            <span class="token string">&quot;OnBuild&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;Labels&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        <span class="token punctuation">}</span>,
        <span class="token string">&quot;NetworkSettings&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;Bridge&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;SandboxID&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;6296082bdce1eeefc0fdf32f87f406a9e050b482d43840d7720e7fb7cf6248fb&quot;</span>,
            <span class="token string">&quot;SandboxKey&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/run/docker/netns/6296082bdce1&quot;</span>,
            <span class="token string">&quot;Ports&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;8081/tcp&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                    <span class="token punctuation">{</span>
                        <span class="token string">&quot;HostIp&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;0.0.0.0&quot;</span>,
                        <span class="token string">&quot;HostPort&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;8081&quot;</span>
                    <span class="token punctuation">}</span>,
                    <span class="token punctuation">{</span>
                        <span class="token string">&quot;HostIp&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;::&quot;</span>,
                        <span class="token string">&quot;HostPort&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;8081&quot;</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">]</span>
            <span class="token punctuation">}</span>,
            <span class="token string">&quot;HairpinMode&quot;</span><span class="token builtin class-name">:</span> false,
            <span class="token string">&quot;LinkLocalIPv6Address&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;LinkLocalIPv6PrefixLen&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;SecondaryIPAddresses&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;SecondaryIPv6Addresses&quot;</span><span class="token builtin class-name">:</span> null,
            <span class="token string">&quot;EndpointID&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;a03f2fe76deff57c126b058d57c147c0a3fd48ee2215772fd35d0684636766a6&quot;</span>,
            <span class="token string">&quot;Gateway&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;172.17.0.1&quot;</span>,
            <span class="token string">&quot;GlobalIPv6Address&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;GlobalIPv6PrefixLen&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
            <span class="token string">&quot;IPAddress&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;172.17.0.2&quot;</span>,
            <span class="token string">&quot;IPPrefixLen&quot;</span><span class="token builtin class-name">:</span> <span class="token number">16</span>,
            <span class="token string">&quot;IPv6Gateway&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
            <span class="token string">&quot;MacAddress&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;02:42:ac:11:00:02&quot;</span>,
            <span class="token string">&quot;Networks&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;bridge&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                    <span class="token string">&quot;IPAMConfig&quot;</span><span class="token builtin class-name">:</span> null,
                    <span class="token string">&quot;Links&quot;</span><span class="token builtin class-name">:</span> null,
                    <span class="token string">&quot;Aliases&quot;</span><span class="token builtin class-name">:</span> null,
                    <span class="token string">&quot;MacAddress&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;02:42:ac:11:00:02&quot;</span>,
                    <span class="token string">&quot;NetworkID&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;acb529e8c4ff4dc4e0c4a5883acc0f574b00f6e01917bc9f880eaf62cf3e47e8&quot;</span>,
                    <span class="token string">&quot;EndpointID&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;a03f2fe76deff57c126b058d57c147c0a3fd48ee2215772fd35d0684636766a6&quot;</span>,
                    <span class="token string">&quot;Gateway&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;172.17.0.1&quot;</span>,
                    <span class="token string">&quot;IPAddress&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;172.17.0.2&quot;</span>,
                    <span class="token string">&quot;IPPrefixLen&quot;</span><span class="token builtin class-name">:</span> <span class="token number">16</span>,
                    <span class="token string">&quot;IPv6Gateway&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
                    <span class="token string">&quot;GlobalIPv6Address&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
                    <span class="token string">&quot;GlobalIPv6PrefixLen&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
                    <span class="token string">&quot;DriverOpts&quot;</span><span class="token builtin class-name">:</span> null,
                    <span class="token string">&quot;DNSNames&quot;</span><span class="token builtin class-name">:</span> null
                <span class="token punctuation">}</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br><span class="line-number">118</span><br><span class="line-number">119</span><br><span class="line-number">120</span><br><span class="line-number">121</span><br><span class="line-number">122</span><br><span class="line-number">123</span><br><span class="line-number">124</span><br><span class="line-number">125</span><br><span class="line-number">126</span><br><span class="line-number">127</span><br><span class="line-number">128</span><br><span class="line-number">129</span><br><span class="line-number">130</span><br><span class="line-number">131</span><br><span class="line-number">132</span><br><span class="line-number">133</span><br><span class="line-number">134</span><br><span class="line-number">135</span><br><span class="line-number">136</span><br><span class="line-number">137</span><br><span class="line-number">138</span><br><span class="line-number">139</span><br><span class="line-number">140</span><br><span class="line-number">141</span><br><span class="line-number">142</span><br><span class="line-number">143</span><br><span class="line-number">144</span><br><span class="line-number">145</span><br><span class="line-number">146</span><br><span class="line-number">147</span><br><span class="line-number">148</span><br><span class="line-number">149</span><br><span class="line-number">150</span><br><span class="line-number">151</span><br><span class="line-number">152</span><br><span class="line-number">153</span><br><span class="line-number">154</span><br><span class="line-number">155</span><br><span class="line-number">156</span><br><span class="line-number">157</span><br><span class="line-number">158</span><br><span class="line-number">159</span><br><span class="line-number">160</span><br><span class="line-number">161</span><br><span class="line-number">162</span><br><span class="line-number">163</span><br><span class="line-number">164</span><br><span class="line-number">165</span><br><span class="line-number">166</span><br><span class="line-number">167</span><br><span class="line-number">168</span><br><span class="line-number">169</span><br><span class="line-number">170</span><br><span class="line-number">171</span><br><span class="line-number">172</span><br><span class="line-number">173</span><br><span class="line-number">174</span><br><span class="line-number">175</span><br><span class="line-number">176</span><br><span class="line-number">177</span><br><span class="line-number">178</span><br><span class="line-number">179</span><br><span class="line-number">180</span><br><span class="line-number">181</span><br><span class="line-number">182</span><br><span class="line-number">183</span><br><span class="line-number">184</span><br><span class="line-number">185</span><br><span class="line-number">186</span><br><span class="line-number">187</span><br><span class="line-number">188</span><br><span class="line-number">189</span><br><span class="line-number">190</span><br><span class="line-number">191</span><br><span class="line-number">192</span><br><span class="line-number">193</span><br><span class="line-number">194</span><br><span class="line-number">195</span><br><span class="line-number">196</span><br><span class="line-number">197</span><br><span class="line-number">198</span><br><span class="line-number">199</span><br><span class="line-number">200</span><br><span class="line-number">201</span><br><span class="line-number">202</span><br><span class="line-number">203</span><br><span class="line-number">204</span><br><span class="line-number">205</span><br><span class="line-number">206</span><br><span class="line-number">207</span><br><span class="line-number">208</span><br><span class="line-number">209</span><br><span class="line-number">210</span><br><span class="line-number">211</span><br><span class="line-number">212</span><br><span class="line-number">213</span><br><span class="line-number">214</span><br><span class="line-number">215</span><br><span class="line-number">216</span><br><span class="line-number">217</span><br><span class="line-number">218</span><br><span class="line-number">219</span><br><span class="line-number">220</span><br><span class="line-number">221</span><br><span class="line-number">222</span><br><span class="line-number">223</span><br><span class="line-number">224</span><br><span class="line-number">225</span><br><span class="line-number">226</span><br><span class="line-number">227</span><br><span class="line-number">228</span><br></div></div><p>Docker 会打印出包含<strong>容器底层信息</strong>的长 JSON.</p> <h6 id="_2-1-6-探索运行容器的内部"><a href="#_2-1-6-探索运行容器的内部" class="header-anchor">#</a> 2.1.6 探索运行容器的内部</h6> <p>来看看容器内部的环境. 由于<strong>一个容器里可以运行多个进程, 所以总是可以运行新的进程去看看里面发生了什么</strong>. 如果镜像里有可用的 shell 二进制可执行文件, 也可以运行一个 shell.</p> <blockquote><p>在已有的容器内部运行 shell</p></blockquote> <p>镜像基于的 Node.js 镜像包含了 bash shell, 所以可以像这样<strong>在容器内运行 shell</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">[</span>root@nano1 docker<span class="token punctuation">]</span><span class="token comment"># docker exec -it kubia-container bash</span>
root@4f0fce9ec4ea:/<span class="token comment"># </span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这会<strong>在已有的 kubia-container 容器内部运行 bash. bash 进程会和主容器进程拥有相同的命名空间</strong>. 这样可以从内部探索容器, 查看 Node.js 和应用是如何在容器里运行的. -it 选项是下面两个选项的简写:</p> <ul><li>-i, 确保标准输入流保持开放. 需要在 shell 中输入命令.</li> <li>-t, 分配一个伪终端(TTY).</li></ul> <p>如果希望像平常一样使用 shell, 需要同时使用这两个选项(如果缺少第一个选项就无法输入任何命令. 如果缺少第二个选项, 那么命令提示符不会显示, 并且一些命令会提示 TERM 变量没有设置).</p> <blockquote><p>从内部探索容器</p></blockquote> <p>下面的代码展示了如何使用 shell 查看<strong>容器内运行的进程</strong>.</p> <p><strong>代码清单-2.6 从容器内列出进程</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@4f0fce9ec4ea:/<span class="token comment"># ps aux</span>
<span class="token environment constant">USER</span>        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root          <span class="token number">1</span>  <span class="token number">0.0</span>  <span class="token number">0.0</span>   <span class="token number">2560</span>   <span class="token number">472</span> ?        Ss   01:36   <span class="token number">0</span>:00 /bin/sh <span class="token parameter variable">-c</span> <span class="token function">node</span> app.js
root          <span class="token number">8</span>  <span class="token number">0.0</span>  <span class="token number">0.5</span> <span class="token number">992276</span> <span class="token number">20292</span> ?        Sl   01:36   <span class="token number">0</span>:00 <span class="token function">node</span> app.js
root         <span class="token number">15</span>  <span class="token number">0.0</span>  <span class="token number">0.0</span>   <span class="token number">4172</span>  <span class="token number">2056</span> pts/0    Ss   01:39   <span class="token number">0</span>:00 <span class="token function">bash</span>
root         <span class="token number">21</span>  <span class="token number">0.0</span>  <span class="token number">0.0</span>   <span class="token number">8084</span>  <span class="token number">2820</span> pts/0    R+   01:40   <span class="token number">0</span>:00 <span class="token function">ps</span> aux
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>只看到了这些进程, 宿主机上没有看到其他进程.</p> <blockquote><p>容器内的进程运行在主机操作系统上</p></blockquote> <p>如果现在打开另一个终端, 然后<strong>列出主机操作系统上的进程</strong>, 连同其他的主机进程依然会发现<strong>容器内的进程</strong>, 如代码清单 2.7 所示.</p> <p><strong>代码清单-2.7 运行在主机操作系统上的容器进程</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">[</span>root@nano1 ~<span class="token punctuation">]</span><span class="token comment"># ps aux | grep app.js</span>
root       <span class="token number">2479</span>  <span class="token number">0.0</span>  <span class="token number">0.0</span>   <span class="token number">2560</span>   <span class="token number">472</span> ?        Ss   09:36   <span class="token number">0</span>:00 /bin/sh <span class="token parameter variable">-c</span> <span class="token function">node</span> app.js
root       <span class="token number">2502</span>  <span class="token number">0.0</span>  <span class="token number">0.5</span> <span class="token number">992276</span> <span class="token number">20292</span> ?        Sl   09:36   <span class="token number">0</span>:00 <span class="token function">node</span> app.js
root       <span class="token number">2591</span>  <span class="token number">0.0</span>  <span class="token number">0.0</span> <span class="token number">112828</span>   <span class="token number">996</span> pts/1    S+   09:42   <span class="token number">0</span>:00 <span class="token function">grep</span> <span class="token parameter variable">--color</span><span class="token operator">=</span>auto app.js
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这证明了<mark><strong>运行在容器中的进程是运行在主机操作系统上的</strong></mark>. 如果你足够敏锐, 会发现<mark><strong>进程的 ID 在容器中与主机上不同. 容器使用独立的 PID Linux 命名空间并且有着独立的系列号, 完全独立于进程树</strong></mark>.</p> <blockquote><p>容器的文件系统也是独立的</p></blockquote> <p>正如拥有独立的进程树一样, <mark><strong>每个容器也拥有独立的文件系统</strong></mark>. 在容器内列出根目录的内容, <strong>只会展示容器内的文件, 包括镜像内的所有文件, 再加上容器运行时创建的任何文件(类似日志文件)</strong> , 如下面的代码清单所示.</p> <p><strong>代码清单-2.8 容器拥有完整的文件系统</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@4f0fce9ec4ea:/<span class="token comment"># ls /</span>
app.js	bin  boot  dev	etc  home  lib	lib32  lib64  libx32  media  mnt  opt  proc  root  run	sbin  srv  sys	tmp  usr  var
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>其中<strong>包含 app.js 文件和其他系统目录, 这些目录是正在使用的 node:7 基础镜像的一部分</strong>. 可以使用 exit 命令来退出容器返回宿主机(类似于登出 ssh session).</p> <p>提示: 进入容器对于调试容器内运行的应用来说是非常有用的. 出错时, 需要做的第一件事是查看应用运行的系统的真实状态. 需要记住的是, 应用不仅拥有独立的文件系统, 还有进程, 用户, 主机名和网络接口.</p> <h6 id="_2-1-7-停止和删除容器"><a href="#_2-1-7-停止和删除容器" class="header-anchor">#</a> 2.1.7 停止和删除容器</h6> <p>可以通过告知 Docker 停止 kubia-container 容器来停止应用:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker stop kubia-container</span>
kubia-container
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>因为没有其他的进程在容器内运行, 这会停止容器内运行的主进程. 容器本身仍然存在并且可以通过 <code>docker ps -a</code>​ 来查看. -a 选项打印出所有的容器, 包括运行中的和已经停止的. 想要真正地删除一个容器, 需要运行 docker rm :</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker rm kubia-container</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这会删除容器, 所有的内容会被删除并且无法再次启动.</p> <h6 id="_2-1-8-向镜像仓库推送镜像"><a href="#_2-1-8-向镜像仓库推送镜像" class="header-anchor">#</a> 2.1.8 向镜像仓库推送镜像</h6> <p>现在构建的镜像只可以在本机使用. 为了在任何机器上都可以使用, 可以把镜像推送到一个外部的镜像仓库. 为了简单起见, 不需要搭建一个私有的镜像仓库, 而是可以推送镜像到公开可用的 Docker Hub(http://hub.docker.com) 镜像中心.</p> <p>在推送之前, 需要重新根据 Docker Hub 的规则<strong>标注镜像</strong>. Docker Hub 允许向以你的 Docker Hub ID 开头的镜像仓库推送镜像. 可以在 http://hub.docker.com 上注册 Docker Hub ID. 下面的例子中会使用笔者自己的 ID(luksa), 请在每次出现时替换自己的 ID.</p> <blockquote><p>使用附加标签标注镜像</p></blockquote> <p>一旦知道了自己的 ID, 就可以重命名镜像, 现在镜像由 kubia 改为 luksa/kubia(用自己的 Docker Hub ID 代替 luksa):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker tag kubia nano/kubia</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这不会重命名标签, 而是<strong>给同一个镜像创建一个额外的标签</strong>. 可以通过 docker images 命令列出本机存储的镜像来加以确认, 如下面的代码清单所示.</p> <p><strong>代码清单-2.9 一个容器镜像可以有多个标签</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker images</span>
REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
kubia        latest    c376744dd01f   <span class="token number">11</span> hours ago   <span class="token number">1</span>.1GB
nano/kubia   latest    c376744dd01f   <span class="token number">11</span> hours ago   <span class="token number">1</span>.1GB
busybox      latest    3f57d9401f8d   <span class="token number">13</span> days ago    <span class="token number">4</span>.26MB
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>正如所看到的, <strong>kubia 和 luksa/kubia 指向同一个镜像 ID, 所以实际上是同一个镜像的两个标签</strong>.</p> <blockquote><p>向 Docker Hub 推送镜像</p></blockquote> <p>在向 Docker Hub 推送镜像之前, 先需要使用 docker login 命令和自己的用户 ID 登录, 然后就可以像这样向 Docker Hub 推送 yourid/kubia 镜像:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker push nano/kubia</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>在不同机器上运行镜像</p></blockquote> <p>在推送完成之后, 镜像便可以给任何人使用. 可以在任何机器上运行下面的命令来运行镜像:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># docker run -p 8081:8081 -d nano/kubia</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这非常简单. 最棒的是<strong>应用每次都运行在完全一致的环境中</strong>. 如果在你的机器上正常运行, 也会在所有的 Linux 机器上正常运行. 无须担心主机是否安装了 Node.js. 事实上, <strong>就算安装了, 应用也并不会使用, 因为它使用的是镜像内部安装的</strong>.</p> <h5 id="_2-2-配置kubernetes集群"><a href="#_2-2-配置kubernetes集群" class="header-anchor">#</a> 2.2 配置Kubernetes集群</h5> <p>现在, 应用被打包在一个容器镜像中, 并通过 Docker Hub 给大家使用, 可以将它部署到 Kubernetes 集群中, 而不是直接在 Docker 中运行. 但是需要先<strong>设置集群</strong>.</p> <p>设置一个完整的, 多节点的 Kubernetes 集群并不是一项简单的工作, 特别是如果你不精通 Linux 和网络管理的话. 一个适当的 Kubernetes 安装需要包含<strong>多个物理或虚拟机</strong>, 并需要正确地设置网络, 以便在 Kubernetes 集群内运行的所有容器都可以在相同的扁平网络环境内相互连通.</p> <p>安装 Kubernetes 集群的方法有许多. 这些方法在 <code>http://kubernetes.io</code>​ 的文档中有详细描述. 我们不会在这里列出所有, 因为内容在不断变化, 但 Kubernetes 可以在本地的开发机器, 自己组织的机器集群或是虚拟机提供商(Google Compute Engine, Amazon EC2, Microsoft Azure 等)上运行, 或者使用托管的 Kubernetes 集群, 如 Google Kubernetes Engine(以前称为 Google Container Engine).</p> <p>在这一章中, 将介绍用两种简单的方法构建可运行的 Kubernetes 集群, 你将会看到如何在本地机器上运行单节点 Kubernetes 集群, 以及如何访问运行在 Google Kubernetes Engine(GKE)上的托管集群.</p> <p>第三个选项是<strong>使用 kubeadm 工具安装一个集群</strong>, 这会在附录 B 中介绍, 这里的说明展示了如何使<strong>用虚拟机建立一个三节点的 Kubernetes 集群</strong>, 但是建议在阅读本书的前 11 章之后再尝试.</p> <p>另一个选择是在亚马逊的 AWS(Amazon Web Services)上安装 Kubernetes. 为此, 可以查看 kops 工具, 它是在前面一段提到的 kubeadm 基础之上构建的, 可以在 http://github.com/kubernetes/kops 中找到. 它帮助你在 AWS 上部署生产级, 高可用的 Kubernetes 集群, 并最终会支持其他平台(Google Kubernetes Engine, VMware, vSphere 等).</p> <h6 id="_2-2-1-用minikube运行一个本地单节点kubernetes集群"><a href="#_2-2-1-用minikube运行一个本地单节点kubernetes集群" class="header-anchor">#</a> 2.2.1 用Minikube运行一个本地单节点Kubernetes集群</h6> <p><strong>使用 Minikube 是运行 Kubernetes 集群最简单, 最快捷的途径. Minikube 是一个构建单节点集群的工具, 对于测试 Kubernetes 和本地开发应用都非常有用.</strong></p> <p>虽然不能展示与管理多节点应用相关的一些 Kubernetes 特性, 但是单节点集群足以探索本书中讨论的大多数主题.</p> <p>注意: <strong>安装 docker 之后, 一定要改一下 docker 的镜像源, 不然很容易影响后面的 minikube 的操作</strong>.</p> <p>可以通过修改 daemon 配置文件 <code>/etc/docker/daemon.json</code>​ 来使用加速器, 具体如下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /etc/docker
$ <span class="token function">sudo</span> <span class="token function">tee</span> /etc/docker/daemon.json <span class="token operator">&lt;&lt;-</span><span class="token string">'EOF'
{
  &quot;registry-mirrors&quot;: [&quot;https://wyq8e6ae.mirror.aliyuncs.com&quot;]
}
EOF</span>
$ <span class="token function">sudo</span> systemctl daemon-reload
$ <span class="token function">sudo</span> systemctl restart <span class="token function">docker</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><blockquote><p>安装Minikube</p></blockquote> <p>Minikube 是一个需要下载并放到路径中的二进制文件. 它适用于 OSX, Linux 和 Windows 系统. 最好访问 GitHub 上的 Minikube 代码仓库(http://github.com/kubernetes/minikube), 按照说明来安装它.</p> <p>minicute 的安装文档参考: <code>https://minikube.sigs.k8s.io/docs/start/</code>​</p> <p>安装完成后, 可以看看 minicube 的命令如下:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token punctuation">[</span>root@nano1 <span class="token operator">~</span><span class="token punctuation">]</span># minikube 
minikube 提供并管理针对开发工作流程优化的本地 Kubernetes 集群<span class="token punctuation">.</span> 

<span class="token literal-property property">基本命令</span><span class="token operator">:</span> 
  start            启动本地 Kubernetes 集群
  status           获取本地 Kubernetes 集群状态
  stop             停止正在运行的本地 Kubernetes 集群
  <span class="token keyword">delete</span>           删除本地的 Kubernetes 集群
  dashboard        访问在 minikube 集群中运行的 kubernetes dashboard
  pause            暂停 Kubernetes
  unpause          恢复 Kubernetes

镜像命令
  docker<span class="token operator">-</span>env       提供将终端的 docker<span class="token operator">-</span>cli 指向 minikube 内部 Docker Engine 的说明<span class="token punctuation">.</span> <span class="token punctuation">(</span>用于直接在
minikube 内构建 docker 镜像<span class="token punctuation">)</span>
  podman<span class="token operator">-</span>env       配置环境以使用 minikube's Podman service
  cache            管理 images 缓存
  image            管理 images

<span class="token literal-property property">配置和管理命令</span><span class="token operator">:</span> 
  addons           启用或禁用 minikube 插件
  config           修改持久配置值
  profile          <span class="token function">获取或列出当前配置文件</span><span class="token punctuation">(</span>集群<span class="token punctuation">)</span>
  update<span class="token operator">-</span>context   <span class="token constant">IP</span>或端口更改的情况下更新 kubeconfig 配置文件

<span class="token literal-property property">网络和连接命令</span><span class="token operator">:</span> 
  service          返回用于连接到 service 的 <span class="token constant">URL</span>
  tunnel           连接到 LoadBalancer 服务

<span class="token literal-property property">高级命令</span><span class="token operator">:</span> 
  mount            将指定的目录挂载到 minikube
  ssh              登录到 minikube <span class="token function">环境</span><span class="token punctuation">(</span>用于调试<span class="token punctuation">)</span>
  kubectl          运行与集群版本匹配的 kubectl 二进制文件
  node             添加<span class="token punctuation">,</span> 删除或者列出其他的节点
  cp               将指定的文件复制到 minikube

故障排除命令
  ssh<span class="token operator">-</span>key          检索指定节点的 ssh 密钥路径
  ssh<span class="token operator">-</span>host         检索指定节点的 ssh 主机密钥
  ip               检索指定节点的<span class="token constant">IP</span>地址
  logs             返回用于调试本地 Kubernetes 集群的日志
  update<span class="token operator">-</span>check     打印当前版本和最新版本
  version          打印 minikube 版本
  options          <span class="token function">显示全局命令行选项列表</span> <span class="token punctuation">(</span>应用于所有命令<span class="token punctuation">)</span><span class="token punctuation">.</span> 

Other Commands<span class="token operator">:</span>
  completion       生成命令补全的 shell 脚本
  license          将依赖项的 licenses 输出到一个目录
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br></div></div><blockquote><p>使用 Minikue 启动一个 Kubernetes 集群</p></blockquote> <p>当安装了 Minikube 之后, 可以立即使用下面的命令启动 Kubernetes 集群.</p> <p><strong>代码清单-2.10 启动一个 Minikube 虚拟机</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube start
* Centos <span class="token number">7.9</span>.2009 上的 minikube v1.32.0
* 自动选择 <span class="token function">docker</span> 驱动
* 使用具有 root 权限的 Docker 驱动程序
* 正在集群 minikube 中启动控制平面节点 minikube
* 正在拉取基础镜像 <span class="token punctuation">..</span>.
* 正在下载 Kubernetes v1.28.3 的预加载文件<span class="token punctuation">..</span>.
    <span class="token operator">&gt;</span> index.docker.io/kicbase/sta<span class="token punctuation">..</span>.:  <span class="token number">453.90</span> MiB / <span class="token number">453.90</span> MiB  <span class="token number">100.00</span>% <span class="token number">10.75</span> M
    <span class="token operator">&gt;</span> preloaded-images-k8s-v18-v1<span class="token punctuation">..</span>.:  <span class="token number">205.67</span> MiB / <span class="token number">403.35</span> MiB  <span class="token number">50.99</span>% <span class="token number">3.22</span> MiB<span class="token operator">!</span> minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.42, but successfully downloaded docker.io/kicbase/stable:v0.0.42 as a fallback image
    <span class="token operator">&gt;</span> preloaded-images-k8s-v18-v1<span class="token punctuation">..</span>.:  <span class="token number">403.35</span> MiB / <span class="token number">403.35</span> MiB  <span class="token number">100.00</span>% <span class="token number">2.53</span> Mi
* Creating <span class="token function">docker</span> container <span class="token punctuation">(</span>CPUs<span class="token operator">=</span><span class="token number">2</span>, <span class="token assign-left variable">Memory</span><span class="token operator">=</span>2200MB<span class="token punctuation">)</span> <span class="token punctuation">..</span>.
* 正在 Docker <span class="token number">24.0</span>.7 中准备 Kubernetes v1.28.3…
  - 正在生成证书和密钥<span class="token punctuation">..</span>.
  - 正在启动控制平面<span class="token punctuation">..</span>.
  - 配置 RBAC 规则 <span class="token punctuation">..</span>.
* 配置 bridge CNI <span class="token punctuation">(</span>Container Networking Interface<span class="token punctuation">)</span> <span class="token punctuation">..</span>.
  - 正在使用镜像 gcr.io/k8s-minikube/storage-provisioner:v5
* 正在验证 Kubernetes 组件<span class="token punctuation">..</span>.
* 启用插件:  storage-provisioner, default-storageclass
* kubectl not found. If you need it, try: <span class="token string">'minikube kubectl -- get pods -A'</span>
* 完成<span class="token operator">!</span> kubectl 现在已配置, 默认使用<span class="token string">&quot;minikube&quot;</span>集群和<span class="token string">&quot;default&quot;</span>命名空间
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>启动集群需要花费超过一分钟的时间, 所以在命令完成之前不要中断它.</p> <p>这里由于 docker 安装之后默认只能 root 操作, 所以可以在 root 状态下, 把普通用户加入 docker 安全组:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># usermod -aG docker nano</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样才能切换普通用户启动 minicube.</p> <blockquote><p>安装Kubernetes客户端(kubectl)</p></blockquote> <p>要与 Kubernetes 进行交互, 还需要 kubectl CLI 客户端. 同样, 需要做的就是<strong>下载它</strong>, 可以参考官方文档进行安装: <code>https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/</code>​.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-LO</span> <span class="token string">&quot;https://dl.k8s.io/release/<span class="token variable"><span class="token variable">$(</span><span class="token function">curl</span> <span class="token parameter variable">-L</span> <span class="token parameter variable">-s</span> https://dl.k8s.io/release/stable.txt<span class="token variable">)</span></span>/bin/linux/amd64/kubectl&quot;</span>
<span class="token comment"># 切换到root</span>
<span class="token comment"># install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</span>
<span class="token comment"># kubectl version --client</span>
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><blockquote><p>使用kubectl查看集群是否正常工作</p></blockquote> <p>要验证集群是否正常工作, 可以使用以下所示的 kubectl cluster-info 命令.</p> <p><strong>代码清单-2.11 展示集群信息</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use <span class="token string">'kubectl cluster-info dump'</span><span class="token builtin class-name">.</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这里显示集群已经启动. 它显示了各种 Kubernetes 组件的 URL, 包括 API 服务器和 Web 控制台.</p> <p>提示: 可以运行 minikube ssh 登录到 Minikube VM 并从内部探索它. 例如, 可以查看在节点上运行的进程.</p> <h6 id="_2-2-2-使用google-kubernetes-engine托管kubernetes集群"><a href="#_2-2-2-使用google-kubernetes-engine托管kubernetes集群" class="header-anchor">#</a> 2.2.2 使用Google Kubernetes Engine托管Kubernetes集群</h6> <p>如果你想探索一个<strong>完善的多节点 Kubernetes 集群</strong>, 可以使用托管的 Google Kubernetes Engine(GKE)集群. 这样无须手动设置所有的集群节点和网络, 因为这对于刚开始使用 Kubernetes 的人来说太复杂了. 使用例如 GKE 这样的托管解决方案可以确保不会出现配置错误, 不工作或部分工作的集群.</p> <blockquote><p>配置一个 Google Cloud 项目并且下载必需的客户端二进制</p></blockquote> <p>在设置新的 Kubernetes 集群之前, 需要设置 GKE 环境. 因为这个过程可能会改变, 所以不在这里列出具体的说明. 阅读 https://cloud.google.com/containerengine/docs/before-begin 中的说明后就可以开始了.</p> <p>整个过程大致包括:</p> <p>1.注册谷歌账户.</p> <p>2.在 Google Cloud Platform 控制台中创建一个项目.</p> <p>3.开启账单. 这会需要你的信用卡信息, 但是谷歌提供了为期12个月的免费试用. 而且在免费试用结束后不会自动续费.</p> <p>4.开启 Kubernetes Engine API.</p> <p>5.下载安装 Google Cloud SDK(这包含 gcloud 命令行工具, 需要创建一个 Kubernetes 集群).</p> <p>6.使用 gcloud components install kubectl 安装 kubectl 命令行工具.</p> <p>注意 某些操作(例如步骤2中的操作)可能需要几分钟才能完成, 所以在此期间可以喝杯咖啡放松一下.</p> <blockquote><p>创建一个三节点 Kubernetes 集群</p></blockquote> <p>完成安装后, 可以使用下面代码清单中的命令创建一个包含三个工作节点的 Kubernetes 集群.</p> <p>现在已经有一个正在运行的 Kubernetes 集群, 包含了三个工作节点, 如图 2.4 所示. 你在使用三个节点来更好地演示适用于多节点的特性, 如果需要的话可以使用较少数量的节点.</p> <p><img src="/img/image-20240227230814-85doi7i.png" alt="image" title="图2.4 三个工作节点的 Kubernetes 集群"></p> <blockquote><p>获取集群概览</p></blockquote> <p>图 2.4 能够让你对集群, 以及如何与集群交互有一个初步的认识. 每个节点运行着 Docker, Kubelet 和 kube-proxy. 可以通过 kubectl 命令行客户端向运行在主节点上的 Kubernetes API 服务器发出 REST 请求以与集群交互.</p> <blockquote><p>通过列出集群节点查看集群是否在运行</p></blockquote> <p>现在可以使用 kubectl 命令列出集群中的所有节点, 如下面的代码清单所示.</p> <p><strong>代码清单-2.13 使用 kubectl 列出集群节点</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   27m   v1.28.3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>kubectl get 命令可以列出各种 Kubernetes 对象</strong>. 你将会经常使用到它, 但它通常只会显示对象最基本的信息.</p> <p>提示: 可以使用 <code>gcloud compute ssh &lt;node-name&gt;</code>​ 登录到其中一个节点, 查看节点上运行了什么.</p> <blockquote><p>查看对象的更多信息</p></blockquote> <p>要查看关于对象的更详细的信息, 可以<strong>使用 kubectl describe 命令</strong>, 它显示了更多信息.</p> <p>这里省略了 describe 命令的实际输出, 因为内容非常多且在书中是完全不可读的. 输出显示了节点的<strong>状态, CPU 和内存数据, 系统信息, 运行容器的节点</strong>等.</p> <p>在前面的 kubectl describe 示例中, 显式地指定了节点的名称, 但也可以执行一个简单的 kubectl describe node 命令, 而无须指定节点名, 它将打印出所有节点的描述信息.</p> <p>提示: 当只有一个给定类型的对象存在时, 不指定对象名就运行 description 和 get 命令是很提倡的, 这样不会浪费时间输入或复制, 粘贴对象的名称.</p> <p>当讨论减少输入的时候, 开始在 Kubernetes 运行第一个应用程序之前, 先学习如何让 kubectl 命令的使用变得更容易.</p> <h6 id="_2-2-3-为kubectl配置别名和命令行补齐"><a href="#_2-2-3-为kubectl配置别名和命令行补齐" class="header-anchor">#</a> 2.2.3 为kubectl配置别名和命令行补齐</h6> <p>kubectl 会被经常使用. 很快你就会发现每次不得不打全命令是非常痛苦的. 在继续之前, 花一分钟为 kubectl 设置别名和 tab 命令补全可让使用变得简单.</p> <blockquote><p>创建别名</p></blockquote> <p>在整本书中, 一直会使用 kubectl 可执行文件的全名, 但是你可以添加一个较短的别名, 如 k, 这样就不用每次都输入 kubectl 了. 如果还没有设置别名, 这里会告诉你如何定义. 将下面的代码添加到 <code>～/.bashrc</code>​ 或类似的文件中:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token builtin class-name">alias</span> <span class="token assign-left variable">k</span><span class="token operator">=</span>kubectl
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>注意: 如果你已经在用 gcloud 配置集群, 就已经有可执行文件 k 了.</p> <p>现在终于可以在 Kubernetes 上运行第一个应用了.</p> <h5 id="_2-3-在kubernetes上运行第一个应用"><a href="#_2-3-在kubernetes上运行第一个应用" class="header-anchor">#</a> 2.3 在Kubernetes上运行第一个应用</h5> <p>因为这可能是第一次, 所以会使用最简单的方法在 Kubernetes 上运行应用程序. 通常, <strong>需要准备一个 JSON 或 YAML, 包含想要部署的所有组件描述的配置文件</strong>, 但是因为还没有介绍可以在 Kubernetes 中创建的组件类型, 所以这里将使用一个简单的单行命令来运行应用.</p> <h6 id="_2-3-1-部署node-js应用"><a href="#_2-3-1-部署node-js应用" class="header-anchor">#</a> 2.3.1 部署Node.js应用</h6> <p><strong>部署应用程序最简单的方式是使用 kubectl run 命令, 该命令可以创建所有必要的组件而无需 JSON 或 YAML 文件</strong>. 这样的话, 就不需要深入了解每个组件对象的结构. 试着运行之前创建, 推送到 Docker Hub 的镜像. 下面是在 Kubernetes 中运行的代码:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run kubia <span class="token parameter variable">--image</span><span class="token operator">=</span>luksa/kubia <span class="token parameter variable">--port</span><span class="token operator">=</span><span class="token number">8080</span> <span class="token parameter variable">--generator</span><span class="token operator">=</span>run/v1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>​<code>--image=luksa/kubia</code>​ 显示的是指定要运行的容器镜像, <code>--port=8080</code>​ 选项告诉 Kubernetes 应用正在监听 8080 端口. 最后一个标志(--generator)需要解释一下, 通常并不会使用到它, 它让 Kubernetes 创建一个 <strong>ReplicationController</strong>, 而不是 Deployment. 稍后你将在本章中了解到什么是 ReplicationController, 但是直到第 9 章才会介绍 Deployment, 所以不会在这里创建 Deployment.</p> <p>注意: 这里 --generator 参数已经被废弃了, 继续使用会报错的. 可以像下面这样执行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run kubia <span class="token parameter variable">--image</span><span class="token operator">=</span>luksa/kubia <span class="token parameter variable">--port</span><span class="token operator">=</span><span class="token number">8080</span>
pod/kubia created
$ kubectl expose pod kubia <span class="token parameter variable">--type</span><span class="token operator">=</span>NodePort 
service/kubia exposed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>正如前面命令的输出所示, 已经<strong>创建了一个名为 kubia 的 ReplicationController</strong>. 如前所述, 我们将在本章的后面看到. 从底层开始, 把注意力放在创建的容器上(可以假设已经创建了一个容器, 因为在 run 命令中指定了一个容器镜像).</p> <blockquote><p>介绍pod</p></blockquote> <p>你或许在想, 是否有一个列表显示所有正在运行的容器, 可以通过类似于 kuberctl get containers 的命令获取. 这并不是 Kubernetes 的工作, 它不直接处理单个容器. 相反, <strong>它使用多个共存容器的理念. 这组容器就叫作 pod</strong>.</p> <p><mark><strong>一个 pod 是一组紧密相关的容器, 它们总是一起运行在同一个工作节点上, 以及同一个 Linux 命名空间中. 每个 pod 就像一个独立的逻辑机器, 拥有自己的 IP, 主机名, 进程等, 运行一个独立的应用程序. 应用程序可以是单个进程, 运行在单个容器中, 也可以是一个主应用进程或者其他支持进程, 每个进程都在自己的容器中运行. 一个 pod 的所有容器都运行在同一个逻辑机器上, 而其他 pod 中的容器, 即使运行在同一个工作节点上, 也会出现在不同的节点上</strong></mark>.</p> <p>为了更好地理解容器, pod 和节点之间的关系, 请查看图 2.5. 每个 pod 都有自己的 IP, 并包含一个或多个容器, 每个容器都运行一个应用进程. pod 分布在不同的工作节点上.</p> <p><img src="/img/image-20240227230913-0sfwzbe.png" alt="image" title="图2.5 容器, pod 及物理工作节点之间的关系"></p> <blockquote><p>列出pod</p></blockquote> <p>不能列出单个容器, 因为它们不是独立的 Kubernetes 对象, 但是可以<strong>列出 pod</strong>. 来看看如何使用 kubectl 在下面的代码清单中列出 pod.</p> <p><strong>代码清单-2.14 列出 pod</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
kubia   <span class="token number">1</span>/1     pending   <span class="token number">0</span>          10s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>STATUS 列的 pending 表示 pod 仍然处于挂起状态, pod 的单个容器显示为还未就绪的状态(这是 READY 列中的 0/1的含义). pod 还没有运行的原因是: 该 pod 被分配到的工作节点正在下载容器镜像, 完成之后才可以运行. 下载完成后, 将创建 pod 的容器, 然后 pod 会变为运行状态, 如下面的代码清单所示. pod 状态会随着创建过程慢慢变化, 可能会持续一分钟. 注意这里需要保证机器的内存够大, 自测时, 内存太小会导致一直启动不起来. 下面的 running 表示已经启动起来了.</p> <p><strong>代码清单-2.15 再次列出 pod 查看 pod 的状态是否变化</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
kubia   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          3m41s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>要查看有关 pod 的更多信息, 还可以<strong>使用 kubectl describe pod 命令</strong>, 就像之前查看工作节点一样. 如果 pod 停留在挂起状态, 那么可能是 Kubernetes 无法从镜像中心拉取镜像. 如果你正在使用自己的镜像, 确保它在 Docker Hub 上是公开的. 为了确保能够成功地拉取镜像, 可以试着在另一台机器上使用 docker pull 命令手动拉取镜像.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pod kubia
Name:             kubia
Namespace:        default
Priority:         <span class="token number">0</span>
Service Account:  default
Node:             minikube/192.168.49.2
Start Time:       Wed, <span class="token number">31</span> Jan <span class="token number">2024</span> <span class="token number">22</span>:04:40 +0800
Labels:           <span class="token assign-left variable">run</span><span class="token operator">=</span>kubia
Annotations:      <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Status:           Running
IP:               <span class="token number">10.244</span>.0.15
IPs:
  IP:  <span class="token number">10.244</span>.0.15
Containers:
  kubia:
    Container ID:   docker://5f69a734108b5130f0c9408fb0bf249bd9752f25de1c9e3ae2e172663b8f6794
    Image:          luksa/kubia
    Image ID:       docker-pullable://luksa/kubia@sha256:3f28e304dc0f63dc30f273a4202096f0fa0d08510bd2ee7e1032ce600616de24
    Port:           <span class="token number">8080</span>/TCP
    Host Port:      <span class="token number">0</span>/TCP
    State:          Running
      Started:      Wed, <span class="token number">31</span> Jan <span class="token number">2024</span> <span class="token number">22</span>:04:44 +0800
    Ready:          True
    Restart Count:  <span class="token number">0</span>
    Environment:    <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxplt <span class="token punctuation">(</span>ro<span class="token punctuation">)</span>
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vxplt:
    Type:                    Projected <span class="token punctuation">(</span>a volume that contains injected data from multiple sources<span class="token punctuation">)</span>
    TokenExpirationSeconds:  <span class="token number">3607</span>
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <span class="token operator">&lt;</span>nil<span class="token operator">&gt;</span>
    DownwardAPI:             <span class="token boolean">true</span>
QoS Class:                   BestEffort
Node-Selectors:              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute <span class="token assign-left variable">op</span><span class="token operator">=</span>Exists <span class="token keyword">for</span> 300s
                             node.kubernetes.io/unreachable:NoExecute <span class="token assign-left variable">op</span><span class="token operator">=</span>Exists <span class="token keyword">for</span> 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  6m24s  default-scheduler  Successfully assigned default/kubia to minikube
  Normal  Pulling    6m24s  kubelet            Pulling image <span class="token string">&quot;luksa/kubia&quot;</span>
  Normal  Pulled     6m20s  kubelet            Successfully pulled image <span class="token string">&quot;luksa/kubia&quot;</span> <span class="token keyword">in</span> <span class="token number">3</span>.255s <span class="token punctuation">(</span><span class="token number">3</span>.255s including waiting<span class="token punctuation">)</span>
  Normal  Created    6m20s  kubelet            Created container kubia
  Normal  Started    6m20s  kubelet            Started container kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br></div></div><blockquote><p>幕后发生的事情</p></blockquote> <p>为了可视化所发生的事情, 请看图 2.6. 它显示了<strong>在 Kubernetes 中运行容器镜像所必需的两个步骤. 首先, 构建镜像并将其推送到 Docker Hub. 这是必要的, 因为在本地机器上构建的镜像只能在本地机器上可用, 但是需要使它可以访问运行在工作节点上的 Docker 守护进程</strong>.</p> <p>当运行 kubectl 命令时, 它通过向 Kubernetes API 服务器发送一个 REST HTTP 请求, 在集群中创建一个<strong>新的 ReplicationController 对象</strong>. 然后, ReplicationController 创建了一个新的 pod, 调度器将其调度到一个工作节点上. Kubelet 看到 pod 被调度到节点上, 就告知 Docker 从镜像中心中拉取指定的镜像, 因为本地没有该镜像. 下载镜像后, Docker 创建并运行容器.</p> <p>展示另外两个节点是为了显示上下文. 它们没有在这个过程中扮演任何角色, 因为 pod 没有调度到它们上面.</p> <p>定义: <mark><strong>术语调度(scheduling)的意思是将 pod 分配给一个节点. pod 会立即运行, 而不是将要运行</strong></mark>.</p> <p><img src="/img/image-20240227230953-8s9mhbc.png" alt="image" title="图2.6 在 Kubernetes 中运行 luksa/kubia 容器镜像"></p> <h6 id="_2-3-2-访问web应用"><a href="#_2-3-2-访问web应用" class="header-anchor">#</a> 2.3.2 访问Web应用</h6> <p>如何访问正在运行的 pod? 前面提到过<strong>每个 pod 都有自己的 IP 地址, 但是这个地址是集群内部的, 不能从集群外部访问. 要让 pod 能够从外部访问, 需要通过服务对象公开它, 要创建一个特殊的 LoadBalancer 类型的服务</strong>. 因为如果你创建一个常规服务(一个 ClusterIP 服务), 比如 pod, 它也只能从集群内部访问. 通过创建 LoadBalancer 类型的服务, 将创建一个<strong>外部的负载均衡</strong>, 可以通过负载均衡的公共 IP 访问 pod.</p> <blockquote><p>创建一个服务对象</p></blockquote> <p>要创建服务, 需要告知 Kubernetes 对外暴露之前创建的 <strong>ReplicationController</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl expose rc kubia <span class="token parameter variable">--type</span><span class="token operator">=</span>LoadBalancer <span class="token parameter variable">--name</span> kubia-http
service/kubia-http exposed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 这里 rc 是个缩写, 表示 **ReplicationController, ** 大多数资源类型都有这样的缩写, 所以不必输入全名(例如, pods 的缩写是 po,service 的缩写是 svc, 等等).</p> <blockquote><p>列出服务</p></blockquote> <p>expose 命令的输出中提到一个<strong>名为 kubian-http 的服务</strong>. 服务是类似于 pod 和 Node 的对象, 因此可以通过运行 kubectl get services 命令查看新创建的服务对象, 如下面的代码清单所示.</p> <p><strong>代码清单-2.16 列出服务</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">service</span>
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>          AGE
kubernetes   ClusterIP      <span class="token number">10.96</span>.0.1       <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">443</span>/TCP          97m
kubia-http   LoadBalancer   <span class="token number">10.105</span>.26.139   <span class="token operator">&lt;</span>pending<span class="token operator">&gt;</span>     <span class="token number">8080</span>:31109/TCP   5s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>该列表显示了两个服务. 暂时忽略 kubernetes 服务, 仔细查看创建的 kubian-http 服务. 它还<strong>没有外部 IP 地址(pending 状态)</strong> , 因为 Kubernetes 运行的<strong>云基础设施创建负载均衡需要一段时间</strong>. 负载均衡启动后, 应该会<strong>显示服务的外部 IP 地址</strong>. 过一会儿再看一下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">service</span>
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>          AGE
kubernetes   ClusterIP      <span class="token number">10.96</span>.0.1       <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">443</span>/TCP          113m
kubia-http   LoadBalancer   <span class="token number">10.105</span>.26.139   <span class="token number">192.168</span>.126.64     <span class="token number">8080</span>:31109/TCP   15m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>注意 Minikube 不支持 LoadBalancer 类型的服务, 因此服务不会有外部 IP. 但是可以通过外部端口访问服务. 在下一节的提示中将介绍这是如何做到的.</p> <blockquote><p>使用外部 IP 访问服务</p></blockquote> <p>现在可以通过服务的外部 IP 和端口向 pod 发送请求:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token number">192.168</span>.126.64:8080
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在, 应用程序在三个节点的 Kubernetes 集群(如果使用 Minikube, 则是一个单节点集群)上运行起来了. 如果你忘了建立整个集群所需的步骤, 那么只需两个简单的命令就可以让应用运行起来, 并且让全世界的用户都能访问它.</p> <p>提示: 使用 Minikube 的时候, 可以运行 <code>minikube service kubia-http</code>​ 获取可以访问服务的 IP 和端口.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">[</span>nano1@nano1 root<span class="token punctuation">]</span>$ minikube <span class="token function">service</span> kubia-http
<span class="token operator">|</span>-----------<span class="token operator">|</span>------------<span class="token operator">|</span>-------------<span class="token operator">|</span>---------------------------<span class="token operator">|</span>
<span class="token operator">|</span> NAMESPACE <span class="token operator">|</span>    NAME    <span class="token operator">|</span> TARGET PORT <span class="token operator">|</span>            URL            <span class="token operator">|</span>
<span class="token operator">|</span>-----------<span class="token operator">|</span>------------<span class="token operator">|</span>-------------<span class="token operator">|</span>---------------------------<span class="token operator">|</span>
<span class="token operator">|</span> default   <span class="token operator">|</span> kubia-http <span class="token operator">|</span>        <span class="token number">8080</span> <span class="token operator">|</span> http://192.168.49.2:31109 <span class="token operator">|</span>
<span class="token operator">|</span>-----------<span class="token operator">|</span>------------<span class="token operator">|</span>-------------<span class="token operator">|</span>---------------------------<span class="token operator">|</span>
* 正通过默认浏览器打开服务 default/kubia-http<span class="token punctuation">..</span>.
  http://192.168.49.2:31109
<span class="token punctuation">[</span>nano1@nano1 root<span class="token punctuation">]</span>$ <span class="token function">curl</span> http://192.168.49.2:31109
You've hit kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>如果仔细观察, 会发现应用<strong>将 pod 名称作为它的主机名</strong>. 如前所述, <strong>每个 pod 都像一个独立的机器, 具有自己的 IP 地址和主机名. 尽管应用程序运行在工作节点的操作系统中, 但对应用程序来说, 它似乎是在一个独立的机器上运行, 而这台机器本身就是应用程序的专用机器, 没有其他的进程一同运行</strong>.</p> <h6 id="_2-3-3-系统的逻辑部分"><a href="#_2-3-3-系统的逻辑部分" class="header-anchor">#</a> 2.3.3 系统的逻辑部分</h6> <p>到目前为止, 主要介绍了系统实际的<strong>物理组件</strong>. 三个工作节点是运行 Docker 和 Kubelet 的 VM, 还有一个控制整个系统的主节点. 实际上, 我们并不知道主节点是否管理着 Kubernetes 控制层的所有组件, 或者它们是否跨多个节点. 这并不重要, 因为只与单点访问的 API 服务器进行交互.</p> <p>除了这个系统的<strong>物理视图, 还有一个单独的, 逻辑的视图</strong>. 之前已经提到过 pod, ReplicationController 和服务. 所有这些都将在后面几章中介绍, 但是先快速地看看它们是如何组合在一起的, 以及它们在应用中扮演什么角色.</p> <blockquote><p>ReplicationController, pod 和服务是如何组合在一起的</p></blockquote> <p>正如前面解释过的, 没有直接创建和使用容器. 相反, <strong>Kubernetes 的基本构件是 pod</strong>. 但是, 你并没有真的创建出任何 pod, 至少不是直接创建. <strong>通过运行 kubectl run 命令, 创建了一个 ReplicationController, 它用于创建 pod 实例</strong>. 为了使该 pod 能够从集群外部访问, 需要让 Kubernetes 将该 ReplicationController 管理的所有 pod 由<strong>一个服务对外暴露</strong>. 图 2.7 给出了这三种元素组合的大致情况.</p> <p><img src="/img/image-20240227231022-59m493k.png" alt="image" title="图2.7 由 ReplicationController, pod 和服务组成的系统"></p> <blockquote><p>pod和它的容器</p></blockquote> <p>在系统中最重要的组件是 pod. 它只包含一个容器, 但是<strong>通常一个 pod 可以包含任意数量的容器</strong>. 容器内部是 Node.js 进程, 该进程绑定到 8080 端口, 等待 HTTP 请求. <strong>pod 有自己独立的私有 IP 地址和主机名</strong>.</p> <blockquote><p>ReplicationController 的角色</p></blockquote> <p>下一个组件是 kubia <strong>ReplicationController</strong>. 它确<strong>保始终存在一个运行中的 pod 实例</strong>. 通常, <strong>ReplicationController 用于复制 pod(即创建 pod 的多个副本)并让它们保持运行</strong>. 示例中没有指定需要多少 pod 副本, 所以 ReplicationController 创建了一个副本. 如果你的 pod 因为任何原因消失了, 那么 ReplicationController 将创建一个新的 pod 来替换消失的 pod.</p> <blockquote><p>为什么需要服务</p></blockquote> <p>系统的第三个组件是 kubian-http 服务. 要理解为什么需要服务, 需要学习有关 pod 的关键细节. <strong>pod 的存在是短暂的, 一个 pod 可能会在任何时候消失, 或许因为它所在节点发生故障, 或许因为有人删除了 pod, 或者因为 pod 被从一个健康的节点剔除了. 当其中任何一种情况发生时, 如前所述, 消失的 pod 将被 ReplicationController 替换为新的 pod. 新的 pod 与替换它的 pod 具有不同的 IP 地址.</strong> <mark><strong>这就是需要服务的地方--解决不断变化的 pod IP 地址的问题, 以及在一个固定的 IP 和端口对上对外暴露多个 pod</strong></mark>.</p> <p><strong>当一个服务被创建时, 它会得到一个静态的 IP, 在服务的生命周期中这个 IP 不会发生改变. 客户端应该通过固定 IP 地址连接到服务, 而不是直接连接 pod. 服务会确保其中一个 pod 接收连接, 而不关心 pod 当前运行在哪里(以及它的 IP 地址是什么)</strong> .</p> <p><strong>服务表示一组或多组提供相同服务的 pod 的静态地址. 到达服务 IP 和端口的请求将被转发到属于该服务的一个容器的 IP 和端口.</strong></p> <h6 id="_2-3-4-水平伸缩应用"><a href="#_2-3-4-水平伸缩应用" class="header-anchor">#</a> 2.3.4 水平伸缩应用</h6> <p>现在有了一个正在运行的应用, 由 ReplicationController 监控并保持运行, 并通过服务暴露访问. 现在来创造更多魔法.</p> <p>使用 Kubernetes 的一个主要好处是可以简单地<strong>扩展部署</strong>. 来看看扩容 pod 有多容易. 接下来要把运行实例的数量增加到三个.</p> <p>pod 由一个 ReplicationController 管理. 来查看 kubectl get 命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get replicationcontrollers
NAME        DESIRED        CURRENT    AGE
kubia       <span class="token number">1</span>              <span class="token number">1</span>          17m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><blockquote><p>使用 kubectl get 列出所有类型的资源</p></blockquote> <p>一直在使用相同的基本命令 kubectl get 来列出集群中的资源. 你已经使用此命令列出节点, pod, 服务和 ReplicationController 对象. <strong>不指定资源类型调用 kubectl get 可以列出所有可能类型的对象</strong>. 然后这些类型可以使用各种 kubectl 命令, 例如 get, describe 等. 列表还显示了前面提到的缩写.</p> <p>该列表显示了一个名为 kubia 的单个 ReplicationController. DESIRED 列显示了希望 ReplicationController 保持的 pod 副本数, 而 CURRENT 列显示当前运行的 pod 数. 在示例中, 希望 pod 副本为 1, 而现在就有一个副本正在运行.</p> <blockquote><p>增加期望的副本数</p></blockquote> <p>为了增加 pod 的副本数, 需要<strong>改变 ReplicationController 期望的副本数</strong>, 如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl scale rc kubia <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">3</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在已经告诉 Kubernetes <strong>需要确保 pod 始终有三个实例在运行</strong>. 注意, 你没有告诉 Kubernetes 需要采取什么行动, 也没有告诉 Kubernetes 增加两个 pod, <strong>只设置新的期望的实例数量并让 Kubernetes 决定需要采取哪些操作来实现期望的状态</strong>.</p> <p><mark><strong>这是 Kubernetes 最基本的原则之一. 不是告诉 Kubernetes 应该执行什么操作, 而是声明性地改变系统的期望状态, 并让 Kubernetes 检查当前的状态是否与期望的状态一致. 在整个 Kubernetes 世界中都是这样的</strong></mark>.</p> <blockquote><p>查看扩容的结果</p></blockquote> <p>前面增加了 pod 的副本数. 再次列出 ReplicationController 查看更新后的副本数:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> rc
<span class="token constant">NAME</span>        <span class="token constant">DESIRED</span>        <span class="token constant">CURRENT</span>    <span class="token constant">AGE</span>
kubia       <span class="token number">3</span>              <span class="token number">3</span>          27m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>由于 pod 的实际数量已经增加到三个(从 CURRENT 列中可以看出), 列出所有的 pod 时显示的应该是三个而不是一个:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod
NAME            READY   STATUS    RESTARTS   AGE
kubia-hczji     <span class="token number">1</span>/1     Running   <span class="token number">0</span>          3m41s
kubia-idsa8     <span class="token number">0</span>/1     Pending   <span class="token number">0</span>          7s
kubia-zb233     <span class="token number">1</span>/1     Running   <span class="token number">0</span>          8m42s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>正如你所看到的, 有三个 pod 而不是一个. 两个已经在运行, 一个仍在挂起中, 一旦容器镜像下载完毕并启动容器, 挂起的 pod 会马上运行.</p> <p>正如你所看到的, 给应用扩容是非常简单的. 一旦应用在生产中运行并且需要扩容, 可以使用一个命令添加额外的实例, 而不必手动安装和运行其他副本.</p> <p>记住, <strong>应用本身需要支持水平伸缩. Kubernetes 并不会让你的应用变得可扩展, 它只是让应用的扩容或缩容变得简单</strong>.</p> <blockquote><p>当切换到服务时请求切换到所有三个 pod 上</p></blockquote> <p>因为现在应用的多个实例在运行, 来看看如果再次请求服务的 URL 会发生什么. 会不会总是切换到应用的同一个实例呢?​​</p> <p><strong>现在请求随机地切换到不同的 pod. 当 pod 有多个实例时 Kubernetes 服务就会这样做. 服务作为负载均衡挡在多个 pod 前面</strong>. 当只有一个 pod 时, 服务为单个 pod 提供一个静态地址. 无论服务后面是单个 pod 还是一组 pod, 这些 pod 在集群内创建, 消失, 这意味着它们的 IP 地址会发生变化, 但服务的地址总是相同的. 这使得无论有多少 pod, 以及它们的地址如何变化, 客户端都可以很容易地连接到 pod.</p> <blockquote><p>可视化系统的新状态</p></blockquote> <p>下面来可视化一下现在的系统, 看看和以前相比发生了什么变化. 图 2.8 显示了系统的新状态. 仍然有一个服务和一个 ReplicationController, 但是现在<strong>有三个 pod 实例</strong>, 它们都是由 ReplicationController 管理的. 服务不再将所有请求发送到单个 pod, 而是将它们分散到所有三个 pod 中.</p> <p><img src="/img/image-20240227231112-iq88adn.png" alt="image" title="图2.8 由同一 ReplicationController 管理并通过服务 IP 和端口暴露的 pod 的三个实例"></p> <h6 id="_2-3-5-查看应用运行在哪个节点上"><a href="#_2-3-5-查看应用运行在哪个节点上" class="header-anchor">#</a> 2.3.5 查看应用运行在哪个节点上</h6> <p>你可能想知道 pod 被调度到哪个节点上. 在 Kubernetes 的世界中, <strong>pod 运行在哪个节点上并不重要, 只要它被调度到一个可以提供 pod 正常运行所需的 CPU 和内存的节点就可以了</strong>.</p> <p>不管调度到哪个节点, 容器中运行的<strong>所有应用都具有相同类型的操作系统</strong>. 每个 pod 都有自己的 IP, 并且可以与任何其他 pod 通信, 不论其他 pod 是运行在同一个节点上, 还是运行在另一个节点上. 每个 pod 都被分配到所需的计算资源, 因此这些资源是由一个节点提供还是由另一个节点提供, 并没有任何区别.</p> <blockquote><p>列出 pod 时显示 pod IP 和 pod 的节点</p></blockquote> <p>如果仔细观察, 可能已经注意到 kubectl get pods 命令甚至没有显示任何关于这些 pod 调度到的节点的信息. 这是因为它通常不是 pod 最重要的信息.</p> <p>但是可以使用 -o wide 选项请求<strong>显示其他列</strong>. 在列出 pod 时, 该选项显示 pod 的 IP 和所运行的节点:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod <span class="token parameter variable">-o</span> wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
kubia   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          87m   <span class="token number">10.244</span>.0.15   minikube   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>           <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><blockquote><p>使用 kubectl describe 查看 pod 的其他细节</p></blockquote> <p>还可以<strong>使用 kubectl describe 命令</strong>来查看节点, 该命令显示了 pod 的许多其他细节, 如下面的代码清单所示, 其实这个命令之前演示过.</p> <p><strong>代码清单-2.18 使用 kubectl describe 描述一个 pod</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pod kubia
Name:             kubia
Namespace:        default
Priority:         <span class="token number">0</span>
Service Account:  default
Node:             minikube/192.168.49.2
Start Time:       Wed, <span class="token number">31</span> Jan <span class="token number">2024</span> <span class="token number">22</span>:04:40 +0800
Labels:           <span class="token assign-left variable">run</span><span class="token operator">=</span>kubia
Annotations:      <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Status:           Running
IP:               <span class="token number">10.244</span>.0.15
IPs:
  IP:  <span class="token number">10.244</span>.0.15
Containers:
  kubia:
    Container ID:   docker://5f69a734108b5130f0c9408fb0bf249bd9752f25de1c9e3ae2e172663b8f6794
    Image:          luksa/kubia
    Image ID:       docker-pullable://luksa/kubia@sha256:3f28e304dc0f63dc30f273a4202096f0fa0d08510bd2ee7e1032ce600616de24
    Port:           <span class="token number">8080</span>/TCP
    Host Port:      <span class="token number">0</span>/TCP
    State:          Running
      Started:      Wed, <span class="token number">31</span> Jan <span class="token number">2024</span> <span class="token number">22</span>:04:44 +0800
    Ready:          True
    Restart Count:  <span class="token number">0</span>
    Environment:    <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxplt <span class="token punctuation">(</span>ro<span class="token punctuation">)</span>
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vxplt:
    Type:                    Projected <span class="token punctuation">(</span>a volume that contains injected data from multiple sources<span class="token punctuation">)</span>
    TokenExpirationSeconds:  <span class="token number">3607</span>
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <span class="token operator">&lt;</span>nil<span class="token operator">&gt;</span>
    DownwardAPI:             <span class="token boolean">true</span>
QoS Class:                   BestEffort
Node-Selectors:              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute <span class="token assign-left variable">op</span><span class="token operator">=</span>Exists <span class="token keyword">for</span> 300s
                             node.kubernetes.io/unreachable:NoExecute <span class="token assign-left variable">op</span><span class="token operator">=</span>Exists <span class="token keyword">for</span> 300s
Events:                      <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br></div></div><p>​​这展示 pod 的一些其他信息, <strong>pod 调度到的节点, 启动的时间, pod 使用的镜像</strong>, 以及其他有用的信息.</p> <h6 id="_2-3-6-介绍kubernetes-dashboard"><a href="#_2-3-6-介绍kubernetes-dashboard" class="header-anchor">#</a> 2.3.6 介绍Kubernetes dashboard</h6> <p>到目前为止, 只使用了 kubectl 命令行工具. 如果更喜欢图形化的 web 用户界面, 你会很高兴地听到 Kubernetes 也提供了一个不错的 web dashboard.</p> <p><strong>dashboard 可以列出部署在集群中的所有 pod, ReplicationController, 服务和其他部署在集群中的对象, 以及创建, 修改和删除它们</strong>, 如图 2.9 所示.</p> <p>尽管你不会在本书中使用 dashboard, 在 kubectl 创建或修改对象之后, 还是可以随时打开它, 快速查看集群中部署内容的图形化视图.</p> <blockquote><p>访问 GKE 集群的 dashboard</p></blockquote> <p>如果你正在使用 Google Kubernetes Engine, 可以通过 kubectl clusterinfo 命令找到 dashboard 的 URL.</p> <p><img src="/img/image-20240227231147-vpz4ip7.png" alt="image" title="图2.9 Kubernetes dashboard 的页面截图"></p> <blockquote><p>访问 Minikube 的 dashboard</p></blockquote> <p>要打开使用 Minikube 的 Kubernetes 集群的 dashboard, 请运行以下命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube dashboard
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>dashboard 将在默认浏览器中打开. 与 GKE 不同的是, 不需要输入任何凭证来访问它.</p> <h5 id="_2-4-本章小结"><a href="#_2-4-本章小结" class="header-anchor">#</a> 2.4 本章小结</h5> <p>希望这个初始实践章节已经向你展示了 Kubernetes 并不是很难上手的复杂平台, 希望你已经准备好深入学习关于它的所有知识. 读完这一章, 你应该知道如何:</p> <ul><li>拉取并且运行任何公开的镜像.</li> <li>把应用打包成容器镜像, 并且推送到远端的公开镜像仓库让大家都可以使用.</li> <li>进入运行中的容器并检查运行环境.</li> <li>在 GKE 上创建一个多节点的 K8s 集群.</li> <li>为 kubectl 命令行工具设置别名和 tab 补全.</li> <li>在 Kubernetes 集群中列出查看节点, pod, 服务和 ReplicationController.</li> <li>在 Kubernetes 中运行容器并可以在集群外访问.</li> <li>了解 pod, ReplicationController 和服务是关联的基础场景.</li> <li>通过改变 ReplicationController 的复本数对应用进行水平伸缩.</li> <li>在 Minikube 和 GKE 中访问基于 web 的 Kubernetes dashboard.</li></ul> <h4 id="_3-pod-运行于kubernetes中的容器"><a href="#_3-pod-运行于kubernetes中的容器" class="header-anchor">#</a> 3.pod:运行于Kubernetes中的容器</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>创建, 启动和停止 pod</li> <li>使用标签组织 pod 和其他资源</li> <li>使用特定标签对所有 pod 执行操作</li> <li>使用命名空间将多个 pod 分到不重叠的组中</li> <li>调度 pod 到指定类型的工作节点</li></ul> <p>上一章已经大致介绍了在 Kubernetes 中创建的基本组件, 包括它们的基本功能概述. 那么接下来将更加详细地<strong>介绍所有类型的 Kubernetes 对象(或资源)</strong> , 以便你理解在何时, 如何及为何要使用每一个对象. 其中 pod 是 Kubernetes 中最为重要的核心概念, 而其他对象仅仅是在管理, 暴露 pod 或被 pod 使用, 所以将首先介绍 pod 这一核心概念.</p> <h5 id="_3-1-介绍pod"><a href="#_3-1-介绍pod" class="header-anchor">#</a> 3.1 介绍pod</h5> <p><mark><strong>pod 是一组并置的容器, 代表了 Kubernetes 中的基本构建模块. 在实际应用中并不会单独部署容器, 更多的是针对一组 pod 的容器进行部署和操作. 然而这并不意味着一个 pod 总是要包含多个容器--实际上只包含一个单独容器的 pod 也是非常常见的. 值得注意的是, 当一个 pod 包含多个容器时, 这些容器总是运行于同一个工作节点上--一个 pod 绝不会跨越多个工作节点</strong></mark>, 如图 3.1 所示.</p> <p><img src="/img/image-20240227231213-inm395a.png" alt="image" title="图3.1 一个 pod 的所有容器都运行在同一个节点上; 一个 pod 绝不跨越两个节点"></p> <h6 id="_3-1-1-为何需要pod"><a href="#_3-1-1-为何需要pod" class="header-anchor">#</a> 3.1.1 为何需要pod</h6> <p>关于<strong>为何需要 pod 这种容器? 为何不直接使用容器? 为何甚至需要同时运行多个容器? 难道不能简单地把所有进程都放在一个单独的容器中吗</strong>? 接下来一一回答上述问题.</p> <blockquote><p>为何多个容器比单个容器中包含多个进程要好</p></blockquote> <p>想象一个由多个进程组成的应用程序, 无论是通过 ipc(进程间通信)还是本地存储文件进行通信, 都要求它们运行于<strong>同一台机器上</strong>. 在 Kubernetes 中, 经常在容器中运行进程, 由于每一个容器都非常像一台独立的机器, 此时你可能认为在单个容器中运行多个进程是合乎逻辑的, 然而在实践中这种做法并不合理.</p> <p><mark><strong>容器被设计为每个容器只运行一个进程(除非进程本身产生子进程)</strong></mark> . 如果在单个容器中运行多个不相关的进程, 那么保持所有进程运行, 管理它们的日志等将会是我们的责任. 例如, 需要包含一种在进程崩溃时能够自动重启的机制. 同时这些进程都将记录到相同的标准输出中, 而此时将很难确定每个进程分别记录了什么.</p> <p>综上所述, <strong>需要让每个进程运行于自己的容器中, 而这就是 Docker 和 Kubernetes 期望使用的方式</strong>.</p> <h6 id="_3-1-2-了解pod"><a href="#_3-1-2-了解pod" class="header-anchor">#</a> 3.1.2 了解pod</h6> <p><strong>由于不能将多个进程聚集在一个单独的容器中, 就需要另一种更高级的结构来将容器绑定在一起, 并将它们作为一个单元进行管理, 这就是 pod 背后的根本原理</strong>.</p> <p><strong>在包含容器的 pod 下, 可以同时运行一些密切相关的进程, 并为它们提供(几乎)相同的环境, 此时这些进程就好像全部运行于单个容器中一样, 同时又保持着一定的隔离</strong>. 这样一来, 便能全面地利用容器所提供的特性, 同时对这些进程来说它们就像运行在一起一样, 实现两全其美.</p> <blockquote><p>同一pod中容器之间的部分隔离</p></blockquote> <p>在上一章中, 已经了解到<strong>容器之间彼此是完全隔离</strong>的, 但此时我们期望的是<strong>隔离容器组</strong>, 而不是单个容器, 并<strong>让每个容器组内的容器共享一些资源, 而不是全部(换句话说, 没有完全隔离)</strong> .</p> <p><mark><strong>Kubernetes 通过配置 Docker 来让一个 pod 内的所有容器共享相同的 Linux 命名空间, 而不是每个容器都有自己的一组命名空间</strong></mark>.</p> <p>由于<strong>一个 pod 中的所有容器</strong>都在相同的 network 和 UTS 命名空间下运行(在这里讨论的是 Linux 命名空间), 所以它们<strong>都共享相同的主机名和网络接口</strong>. 同样地, 这些容器也都在相同的 IPC 命名空间下运行, 因此能够<strong>通过 IPC 进行通信</strong>. 在最新的 Kubernetes 和 Docker 版本中, 它们也能够共享相同的 PID 命名空间, 但是该特征默认是未激活的.</p> <p>注意: 当同一个 pod 中的容器使用单独的 PID 命名空间时, 在容器中执行 ps aux 就只会看到容器自己的进程.</p> <p>但当涉及文件系统时, 情况就有所不同. 由于大多数容器的文件系统来自容器镜像, 因此默认情况下, 每个容器的文件系统与其他容器完全隔离. 但可以使用名为 Volume 的 Kubernetes 资源来共享文件目录, 关于这一概念将在第 6 章进行讨论.</p> <blockquote><p>容器如何共享相同的IP和端口空间</p></blockquote> <p>这里需强调的一点是, <mark><strong>由于一个 pod 中的容器运行于相同的 Network 命名空间中, 因此它们共享相同的 IP 地址和端口空间</strong></mark>. 这意味着在同一 pod 中的容器运行的<strong>多个进程需要注意不能绑定到相同的端口号</strong>, 否则会导致端口冲突, 但这只涉及同一 pod 中的容器. 由于每个 pod 都有独立的端口空间, 对于不同 pod 中的容器来说则永远不会遇到端口冲突. 此外, <strong>一个 pod 中的所有容器也都具有相同的 loopback 网络接口, 因此容器可以通过 localhost 与同一 pod 中的其他容器进行通信</strong>.</p> <blockquote><p>介绍平坦 pod 间网络</p></blockquote> <p>Kubernetes 集群中的<strong>所有 pod 都在同一个共享网络地址空间中</strong>(如图 3.2 所示), 这意味着<strong>每个 pod 都可以通过其他 pod 的 IP 地址来实现相互访问</strong>. 换句话说, 这也表示它们之间没有 NAT(网络地址转换)网关. 当两个 pod 彼此之间发送网络数据包时, 它们都会将对方的实际 IP 地址看作数据包中的源 IP.</p> <p><img src="/img/image-20240227231239-p14slnd.png" alt="image" title="图3.2 每个 pod 获取可路由的 IP 地址, 其他 pod 都可以在该 IP 地址下看到该 pod"></p> <p>因此, pod 之间的通信其实是非常简单的. 不论是将两个 pod 安排在单一的还是不同的工作节点上, 同时不管实际节点间的网络拓扑结构如何, 这些 pod 内的容器都能够像<strong>在无 NAT 的平坦网络中一样相互通信, 就像局域网(LAN)上的计算机一样</strong>. 此时, 每个 pod 都有自己的 IP 地址, 并且可以通过这个专门的网络实现 pod 之间互相访问. 这个专门的网络通常是由额外的软件基于真实链路实现的.</p> <p>总结本节涵盖的内容: <mark><strong>pod 是逻辑主机, 其行为与非容器世界中的物理主机或虚拟机非常相似. 此外, 运行在同一个 pod 中的进程与运行在同一物理机或虚拟机上的进程相似, 只是每个进程都封装在一个容器之中</strong></mark>.</p> <h6 id="_3-1-3-通过pod合理管理容器"><a href="#_3-1-3-通过pod合理管理容器" class="header-anchor">#</a> 3.1.3 通过pod合理管理容器</h6> <p>将 pod 视为独立的机器, 其中每个机器只托管一个特定的应用. 过去大家习惯于将各种应用程序塞进同一台主机, 但是 pod 不是这么干的. 由于 pod 比较轻量, 所以<strong>可以在几乎不导致任何额外开销的前提下拥有尽可能多的 pod</strong>. 与将所有内容填充到一个 pod 中不同, <strong>应该将应用程序组织到多个 pod 中, 而每个 pod 只包含紧密相关的组件或进程</strong>.</p> <p>说到这里, 对于一个由前端应用服务器和后端数据库组成的<strong>多层应用程序</strong>, 你认为应该将其配置为单个 pod 还是两个 pod 呢? 下面将对该问题做进一步探讨.</p> <blockquote><p>将多层应用分散到多个pod中</p></blockquote> <p>虽然可以<strong>在单个 pod 中同时运行前端服务器和数据库这两个容器, 但这种方式并不值得推荐</strong>. 前面已经讨论过, 同一 pod 的所有容器总是运行在一起, 但对于 Web 服务器和数据库来说, 它们真的需要在同一台计算机上运行吗? 答案显然是否定的, 它们不应该被放到同一个 pod 中. 那假如你非要把它们放在一起, 有错吗? 某种程度上来说, 是的.</p> <p>如果前端和后端都在同一个容器中, 那么两者将始终在同一台计算机上运行. 如果你有一个双节点 Kubernetes 集群, 而只有一个单独的 pod, 那么你将始终只会用一个工作节点, 而不会充分利用第二个节点上的计算资源(CPU 和内存). 因此更合理的做法是将 pod 拆分到两个工作节点上, 允许 Kubernetes 将前端安排到一个节点, 将后端安排到另一个节点, 从而提高基础架构的利用率.</p> <blockquote><p>基于扩缩容考虑而分割到多个pod中</p></blockquote> <p><mark><strong>另一个不应该将应用程序都放到单一 pod 中的原因就是扩缩容</strong></mark>. pod 也是扩缩容的基本单位, 对于 Kubernetes 来说, 它不能横向扩缩单个容器, 只能扩缩整个 pod. 这意味着如果你的 pod 由一个前端和一个后端容器组成, 那么当你扩大 pod 的实例数量时, 比如扩大为两个, 最终会得到两个前端容器和两个后端容器.</p> <p>通常来说, 前端组件与后端组件具有完全不同的扩缩容需求, 所以一般倾向于分别独立地扩缩它们. 更不用说, 像数据库这样的后端服务器, 通常比无状态的前端 web 服务器更难扩展. 因此, <strong>如果需要单独扩缩容器, 那么这个容器很明确地应该被部署在单独的 pod 中</strong>.</p> <blockquote><p>何时在pod中使用多个容器</p></blockquote> <p><strong>将多个容器添加到单个 pod 的主要原因是应用可能由一个主进程和一个或多个辅助进程组成</strong>, 如图 3.3 所示.</p> <p><img src="/img/image-20240227231304-ijfafxg.png" alt="image" title="图3.3 pod 应该包含紧密耦合的容器组(通常是一个主容器和支持主容器的其他容器)"></p> <p>例如, pod 中的主容器可以是一个仅仅服务于某个目录中的文件的 Web 服务器, 而另一个容器(所谓的 sidecar 容器)则定期从外部源下载内容并将其存储在 Web 服务器目录中. 在第 6 章中, 将看到在这种情况下需要使用 Kubernetes Volume, 并将其挂载到两个容器中.</p> <p><strong>sidecar 容器的其他例子包括日志轮转器和收集器, 数据处理器, 通信适配器</strong>等.</p> <blockquote><p>决定何时在pod中使用多个容器</p></blockquote> <p>回顾一下容器应该如何分组到 pod 中: 当决定是将两个容器放入一个 pod 还是两个单独的 pod 时, 需要问自己以下问题:</p> <ul><li><p><strong>它们需要一起运行还是可以在不同的主机上运行?</strong></p></li> <li><p><strong>它们代表的是一个整体还是相互独立的组件?</strong></p></li> <li><p><strong>它们必须一起进行扩缩容还是可以分别进行?</strong></p> <p>‍</p></li></ul> <p>基本上, 一般总是应该<strong>倾向于在单独的 pod 中运行容器, 除非有特定的原因要求它们是同一 pod 的一部分</strong>. 图 3.4 将有助于记忆这一点.</p> <p><img src="/img/image-20240227231332-fxhmvo0.png" alt="image" title="图3.4 容器不应该包含多个进程, pod 也不应该包含多个并不需要运行在同一主机上的容器"></p> <p>尽管 pod 可以包含多个容器, 但为了保持现在的简单性, 本章将仅讨论单容器 pod 有关的问题. 稍后将在第 6 章看到如何在一个 pod 中使用多个容器.</p> <h5 id="_3-2-以yaml或json描述文件创建pod"><a href="#_3-2-以yaml或json描述文件创建pod" class="header-anchor">#</a> 3.2 以YAML或JSON描述文件创建pod</h5> <p><mark><strong>pod 和其他 Kubernetes 资源通常是通过向 Kubernetes REST API 提供 JSON 或 YAML 描述文件来创建的</strong></mark>. 此外还有其他更简单的创建资源的方法, 比如在前一章中使用的 kubectl run 命令, 但这些方法通常只允许配置一组有限的属性. 另外, 通过 YAML 文件定义所有的 Kubernetes 对象之后, 还可以将它们存储在<strong>版本控制系统</strong>中, 充分利用版本控制所带来的便利性.</p> <p>因此, 为了配置每种类型资源的各种属性, 需要了解并理解 <strong>Kubernetes API 对象定义</strong>. 通过本书学习各种资源类型时, 将会了解其中的大部分内容. 需要注意的是, 这里并不会解释每一个独立属性, 因此在创建对象时还应参考 http://kubernetes.io/docs/reference/ 中的 Kubernetes API 参考文档.</p> <h6 id="_3-2-1-检查现有pod的yaml描述文件"><a href="#_3-2-1-检查现有pod的yaml描述文件" class="header-anchor">#</a> 3.2.1 检查现有pod的YAML描述文件</h6> <p>假设已经在上一章中创建了一些 pod, 接下来就来看看这些 pod 的 YAML 文件是如何定义的. 将使用带有 -o yaml 选项的 kubectl get 命令来获取 pod 的整个 YAML 定义, 正如下面的代码清单所示.</p> <p><strong>代码清单-3.1 已部署 pod 的完整 YAML</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod kubia <span class="token parameter variable">-o</span> yaml
apiVersion: v1
<span class="token comment"># kubernetes对象类型</span>
kind: Pod
<span class="token comment"># pod元数据(名称, 标签和注解等)</span>
metadata:
  creationTimestamp: <span class="token string">&quot;2024-01-31T14:04:40Z&quot;</span>
  labels:
    run: kubia
  name: kubia
  namespace: default
  resourceVersion: <span class="token string">&quot;9855&quot;</span>
  uid: f6d512bb-e37a-439a-a83c-3390b8d5fb8a
<span class="token comment"># pod规格/内容(pod的容器列表, volume等)</span>
spec:
  containers:
  - image: luksa/kubia
    imagePullPolicy: Always
    name: kubia
    ports:
    - containerPort: <span class="token number">8080</span>
      protocol: TCP
    resources: <span class="token punctuation">{</span><span class="token punctuation">}</span>
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-vxplt
      readOnly: <span class="token boolean">true</span>
  dnsPolicy: ClusterFirst
  enableServiceLinks: <span class="token boolean">true</span>
  nodeName: minikube
  preemptionPolicy: PreemptLowerPriority
  priority: <span class="token number">0</span>
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: <span class="token punctuation">{</span><span class="token punctuation">}</span>
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: <span class="token number">30</span>
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: <span class="token number">300</span>
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: <span class="token number">300</span>
  volumes:
  - name: kube-api-access-vxplt
    projected:
      defaultMode: <span class="token number">420</span>
      sources:
      - serviceAccountToken:
          expirationSeconds: <span class="token number">3607</span>
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
<span class="token comment"># pod及其内部容器的详细状态</span>
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-01-31T14:04:40Z&quot;</span>
    status: <span class="token string">&quot;True&quot;</span>
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-02-01T13:31:18Z&quot;</span>
    message: <span class="token string">'containers with unready status: [kubia]'</span>
    reason: ContainersNotReady
    status: <span class="token string">&quot;False&quot;</span>
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-02-01T13:31:18Z&quot;</span>
    message: <span class="token string">'containers with unready status: [kubia]'</span>
    reason: ContainersNotReady
    status: <span class="token string">&quot;False&quot;</span>
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-01-31T14:04:40Z&quot;</span>
    status: <span class="token string">&quot;True&quot;</span>
    type: PodScheduled
  containerStatuses:
  - containerID: docker://5f69a734108b5130f0c9408fb0bf249bd9752f25de1c9e3ae2e172663b8f6794
    image: luksa/kubia:latest
    imageID: docker-pullable://luksa/kubia@sha256:3f28e304dc0f63dc30f273a4202096f0fa0d08510bd2ee7e1032ce600616de24
    lastState:
      terminated:
        containerID: docker://5f69a734108b5130f0c9408fb0bf249bd9752f25de1c9e3ae2e172663b8f6794
        exitCode: <span class="token number">137</span>
        finishedAt: <span class="token string">&quot;2024-02-01T13:31:10Z&quot;</span>
        reason: Error
        startedAt: <span class="token string">&quot;2024-01-31T14:04:44Z&quot;</span>
    name: kubia
    ready: <span class="token boolean">false</span>
    restartCount: <span class="token number">0</span>
    started: <span class="token boolean">false</span>
    state:
      waiting:
        message: Back-off pulling image <span class="token string">&quot;luksa/kubia&quot;</span>
        reason: ImagePullBackOff
  hostIP: <span class="token number">192.168</span>.49.2
  phase: Running
  podIP: <span class="token number">10.244</span>.0.18
  podIPs:
  - ip: <span class="token number">10.244</span>.0.18
  qosClass: BestEffort
  startTime: <span class="token string">&quot;2024-01-31T14:04:40Z&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br></div></div><p>上述代码清单的内容看上去较为复杂, 但一旦理解了基础知识并知道如何区分重要部分和细枝末节时, 它就变得非常简单. 此外, 稍后将看到, 当创建一个新的 pod 时, 需要写的 YAML 相对来说则要短得多.</p> <blockquote><p>介绍 pod 定义的主要部分</p></blockquote> <p>pod 定义由这么几个部分组成: <strong>首先是 YAML 中使用的 Kubernetes API 版本和 YAML 描述的资源类型</strong>; 其次是几乎在所有 Kubernetes 资源中都可以找到的<strong>三大重要部分</strong>:</p> <ul><li><mark><strong>metadata 包括名称, 命名空间, 标签和关于该容器的其他信息.</strong></mark></li> <li><mark><strong>spec 包含 pod 内容的实际说明, 例如 pod 的容器, 卷和其他数据.</strong></mark></li> <li><mark><strong>status 包含运行中的 pod 的当前信息, 例如 pod 所处的条件, 每个容器的描述和状态, 以及内部 IP 和其他基本信息.</strong></mark></li></ul> <p>代码清单 3.1 展示了一个正在运行的 pod 的完整描述, 其中包含了它的状态. status 部分包含只读的运行时数据, 该数据展示了给定时刻的资源状态. 而<mark><strong>在创建新的 pod 时, 永远不需要提供 status 部分</strong></mark>.</p> <p>上述三部分展示了 Kubernetes API 对象的典型结构. 其他对象也都具有相同的结构, 这使得理解新对象相对来说更加容易.</p> <p>对上述 YAML 中的每个属性进行深究的意义并不大, 因此接下来将关注如何创建 pod 的最基本的 YAML.</p> <h6 id="_3-2-2-为pod创建一个简单的yaml描述文件"><a href="#_3-2-2-为pod创建一个简单的yaml描述文件" class="header-anchor">#</a> 3.2.2 为pod创建一个简单的YAML描述文件</h6> <p>下面将创建一个名为 <strong>kubia-manual.yaml</strong> 的文件(可以在任意目录下创建该文件). 下面的清单展示了该文件的全部内容.</p> <p><strong>代码清单-3.2 一个基本的 pod manifest:kubia-manual.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token comment"># 表示描述的是一个pod</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token comment"># pod的名称</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>manual
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token comment"># 创建容器使用的镜像</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia    <span class="token comment"># 容器的名称</span>
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token comment"># 应用监听的端口</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>很明显, 该代码清单比代码清单 3.1 中的定义要简单得多. 接下来就对整个描述文件进行深入探讨, 该文件遵循 Kubernetes API 的 v1 版本. 这里描述的<strong>资源类型是 pod</strong>, 名称为 kubia-manual; 该 pod 由基于 luksa/kubia 镜像的单个容器组成. 此外还给该容器命名, 并表示它正在监听 8080 端口.</p> <blockquote><p>指定容器端口</p></blockquote> <p>在 pod 定义中指定端口纯粹是<strong>展示性</strong>的(informational). 忽略它们对于客户端是否可以通过端口连接到 pod 不会带来任何影响. 如果容器通过绑定到地址 0.0.0.0 的端口接收连接, 那么即使端口未明确列出在 pod spec 中, 其他 pod 也依旧能够连接到该端口. 但明确定义端口仍是有意义的, 在端口定义下, 每个使用集群的人都可以快速查看每个 pod 对外暴露的端口. 此外可以在本书的后续内容中看到, 明确定义端口还允许你为每个端口指定一个名称, 这样一来更加方便使用.</p> <blockquote><p>使用 kubectl explain 来发现可能的 API 对象字段</p></blockquote> <p>在准备 manifest 时, 可以转到 http://kubernetes.io/docs/api 上的 Kubernetes 参考文档查看每个 API 对象支持哪些属性, 也可以<strong>使用 kubectl explain 命令</strong>.</p> <p>例如, 当从头创建一个 pod manifest 时, 可以从请求 kubectl 来解释 pod 开始:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl explain pods
<span class="token key atrule">KIND</span><span class="token punctuation">:</span>       Pod
<span class="token key atrule">VERSION</span><span class="token punctuation">:</span>    v1

<span class="token key atrule">DESCRIPTION</span><span class="token punctuation">:</span>
    Pod is a collection of containers that can run on a host. This resource is
    created by clients and scheduled onto hosts.
  
<span class="token key atrule">FIELDS</span><span class="token punctuation">:</span>
  apiVersion	&lt;string<span class="token punctuation">&gt;</span>
    APIVersion defines the versioned schema of this representation of an object.
    Servers should convert recognized schemas to the latest internal value<span class="token punctuation">,</span> and
    <span class="token key atrule">may reject unrecognized values. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/community/contributors/devel/sig<span class="token punctuation">-</span>architecture/api<span class="token punctuation">-</span>conventions.md<span class="token comment">#resources</span>

  kind	&lt;string<span class="token punctuation">&gt;</span>
    Kind is a string value representing the REST resource this object
    represents. Servers may infer this from the endpoint the client submits
    <span class="token key atrule">requests to. Cannot be updated. In CamelCase. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/community/contributors/devel/sig<span class="token punctuation">-</span>architecture/api<span class="token punctuation">-</span>conventions.md<span class="token comment">#types-kinds</span>

  metadata	&lt;ObjectMeta<span class="token punctuation">&gt;</span>
    <span class="token key atrule">Standard object's metadata. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/community/contributors/devel/sig<span class="token punctuation">-</span>architecture/api<span class="token punctuation">-</span>conventions.md<span class="token comment">#metadata</span>

  spec	&lt;PodSpec<span class="token punctuation">&gt;</span>
    <span class="token key atrule">Specification of the desired behavior of the pod. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/community/contributors/devel/sig<span class="token punctuation">-</span>architecture/api<span class="token punctuation">-</span>conventions.md<span class="token comment">#spec-and-status</span>

  status	&lt;PodStatus<span class="token punctuation">&gt;</span>
    Most recently observed status of the pod. This data may not be up to date.
    <span class="token key atrule">Populated by the system. Read-only. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/community/contributors/devel/sig<span class="token punctuation">-</span>architecture/api<span class="token punctuation">-</span>conventions.md<span class="token comment">#spec-and-status</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br></div></div><p>Kubectl 打印出对象的解释并列出对象可以包含的属性, 接下来就可以深入了解各个属性的更多信息. 例如, 可以这样查看 spec 属性:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl explain pod.spec
<span class="token key atrule">KIND</span><span class="token punctuation">:</span>       Pod
<span class="token key atrule">VERSION</span><span class="token punctuation">:</span>    v1

<span class="token key atrule">FIELD</span><span class="token punctuation">:</span> spec &lt;PodSpec<span class="token punctuation">&gt;</span>

<span class="token key atrule">DESCRIPTION</span><span class="token punctuation">:</span>
    <span class="token key atrule">Specification of the desired behavior of the pod. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/community/contributors/devel/sig<span class="token punctuation">-</span>architecture/api<span class="token punctuation">-</span>conventions.md<span class="token comment">#spec-and-status</span>
    PodSpec is a description of a pod.
  
<span class="token key atrule">FIELDS</span><span class="token punctuation">:</span>
  activeDeadlineSeconds	&lt;integer<span class="token punctuation">&gt;</span>
    Optional duration in seconds the pod may be active on the node relative to
    StartTime before the system will actively try to mark it failed and kill
    associated containers. Value must be a positive integer.

  affinity	&lt;Affinity<span class="token punctuation">&gt;</span>
    If specified<span class="token punctuation">,</span> the pod's scheduling constraints

  automountServiceAccountToken	&lt;boolean<span class="token punctuation">&gt;</span>
    AutomountServiceAccountToken indicates whether a service account token
    should be automatically mounted.

  containers	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>Container<span class="token punctuation">&gt;</span> <span class="token punctuation">-</span>required<span class="token punctuation">-</span>
    List of containers belonging to the pod. Containers cannot currently be
    added or removed. There must be at least one container in a Pod. Cannot be
    updated.

  dnsConfig	&lt;PodDNSConfig<span class="token punctuation">&gt;</span>
    Specifies the DNS parameters of a pod. Parameters specified here will be
    merged to the generated DNS configuration based on DNSPolicy.

  dnsPolicy	&lt;string<span class="token punctuation">&gt;</span>
    Set DNS policy for the pod. Defaults to &quot;ClusterFirst&quot;. Valid values are
    'ClusterFirstWithHostNet'<span class="token punctuation">,</span> <span class="token string">'ClusterFirst'</span><span class="token punctuation">,</span> 'Default' or 'None'. DNS
    parameters given in DNSConfig will be merged with the policy selected with
    DNSPolicy. To have DNS options set along with hostNetwork<span class="token punctuation">,</span> you have to
    specify DNS policy explicitly to 'ClusterFirstWithHostNet'.
  
    <span class="token key atrule">Possible enum values</span><span class="token punctuation">:</span>
     <span class="token punctuation">-</span> `&quot;ClusterFirst&quot;` indicates that the pod should use cluster DNS first
    unless hostNetwork is true<span class="token punctuation">,</span> if it is available<span class="token punctuation">,</span> then fall back on the
    default (as determined by kubelet) DNS settings.
     <span class="token punctuation">-</span> `&quot;ClusterFirstWithHostNet&quot;` indicates that the pod should use cluster DNS
    first<span class="token punctuation">,</span> if it is available<span class="token punctuation">,</span> then fall back on the default (as determined by
    kubelet) DNS settings.
     <span class="token punctuation">-</span> `&quot;Default&quot;` indicates that the pod should use the default (as determined
    by kubelet) DNS settings.
     <span class="token punctuation">-</span> `&quot;None&quot;` indicates that the pod should use empty DNS settings. DNS
    parameters such as nameservers and search paths should be defined via
    DNSConfig.

  enableServiceLinks	&lt;boolean<span class="token punctuation">&gt;</span>
    EnableServiceLinks indicates whether information about services should be
    injected into pod's environment variables<span class="token punctuation">,</span> matching the syntax of Docker
    <span class="token key atrule">links. Optional</span><span class="token punctuation">:</span> Defaults to true.

  ephemeralContainers	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>EphemeralContainer<span class="token punctuation">&gt;</span>
    List of ephemeral containers run in this pod. Ephemeral containers may be
    run in an existing pod to perform user<span class="token punctuation">-</span>initiated actions such as debugging.
    This list cannot be specified when creating a pod<span class="token punctuation">,</span> and it cannot be modified
    by updating the pod spec. In order to add an ephemeral container to an
    existing pod<span class="token punctuation">,</span> use the pod's ephemeralcontainers subresource.

  hostAliases	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>HostAlias<span class="token punctuation">&gt;</span>
    HostAliases is an optional list of hosts and IPs that will be injected into
    the pod's hosts file if specified. This is only valid for non<span class="token punctuation">-</span>hostNetwork
    pods.

  hostIPC	&lt;boolean<span class="token punctuation">&gt;</span>
    <span class="token key atrule">Use the host's ipc namespace. Optional</span><span class="token punctuation">:</span> Default to false.

  hostNetwork	&lt;boolean<span class="token punctuation">&gt;</span>
    Host networking requested for this pod. Use the host's network namespace. If
    this option is set<span class="token punctuation">,</span> the ports that will be used must be specified. Default
    to false.

  hostPID	&lt;boolean<span class="token punctuation">&gt;</span>
    <span class="token key atrule">Use the host's pid namespace. Optional</span><span class="token punctuation">:</span> Default to false.

  hostUsers	&lt;boolean<span class="token punctuation">&gt;</span>
    <span class="token key atrule">Use the host's user namespace. Optional</span><span class="token punctuation">:</span> Default to true. If set to true or
    not present<span class="token punctuation">,</span> the pod will be run in the host user namespace<span class="token punctuation">,</span> useful for when
    the pod needs a feature only available to the host user namespace<span class="token punctuation">,</span> such as
    loading a kernel module with CAP_SYS_MODULE. When set to false<span class="token punctuation">,</span> a new userns
    is created for the pod. Setting false is useful for mitigating container
    breakout vulnerabilities even allowing users to run their containers as root
    without actually having root privileges on the host. This field is
    alpha<span class="token punctuation">-</span>level and is only honored by servers that enable the
    UserNamespacesSupport feature.

  hostname	&lt;string<span class="token punctuation">&gt;</span>
    Specifies the hostname of the Pod If not specified<span class="token punctuation">,</span> the pod's hostname will
    be set to a system<span class="token punctuation">-</span>defined value.

  imagePullSecrets	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>LocalObjectReference<span class="token punctuation">&gt;</span>
    ImagePullSecrets is an optional list of references to secrets in the same
    namespace to use for pulling any of the images used by this PodSpec. If
    specified<span class="token punctuation">,</span> these secrets will be passed to individual puller implementations
    <span class="token key atrule">for them to use. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//kubernetes.io/docs/concepts/containers/images<span class="token comment">#specifying-imagepullsecrets-on-a-pod</span>

  initContainers	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>Container<span class="token punctuation">&gt;</span>
    List of initialization containers belonging to the pod. Init containers are
    executed in order prior to containers being started. If any init container
    fails<span class="token punctuation">,</span> the pod is considered to have failed and is handled according to its
    restartPolicy. The name for an init container or normal container must be
    unique among all containers. Init containers may not have Lifecycle actions<span class="token punctuation">,</span>
    Readiness probes<span class="token punctuation">,</span> Liveness probes<span class="token punctuation">,</span> or Startup probes. The
    resourceRequirements of an init container are taken into account during
    scheduling by finding the highest request/limit for each resource type<span class="token punctuation">,</span> and
    then using the max of of that value or the sum of the normal containers.
    Limits are applied to init containers in a similar fashion. Init containers
    <span class="token key atrule">cannot currently be added or removed. Cannot be updated. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//kubernetes.io/docs/concepts/workloads/pods/init<span class="token punctuation">-</span>containers/

  nodeName	&lt;string<span class="token punctuation">&gt;</span>
    NodeName is a request to schedule this pod onto a specific node. If it is
    non<span class="token punctuation">-</span>empty<span class="token punctuation">,</span> the scheduler simply schedules this pod onto that node<span class="token punctuation">,</span> assuming
    that it fits resource requirements.

  nodeSelector	&lt;map<span class="token punctuation">[</span>string<span class="token punctuation">]</span>string<span class="token punctuation">&gt;</span>
    NodeSelector is a selector which must be true for the pod to fit on a node.
    Selector which must match a node's labels for the pod to be scheduled on
    <span class="token key atrule">that node. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//kubernetes.io/docs/concepts/configuration/assign<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>node/

  os	&lt;PodOS<span class="token punctuation">&gt;</span>
    Specifies the OS of the containers in the pod. Some pod and container fields
    are restricted if this is set.
  
    If the OS field is set to linux<span class="token punctuation">,</span> <span class="token key atrule">the following fields must be unset</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span>securityContext.windowsOptions
  
    If the OS field is set to windows<span class="token punctuation">,</span> <span class="token key atrule">following fields must be unset</span><span class="token punctuation">:</span> <span class="token punctuation">-</span>
    spec.hostPID <span class="token punctuation">-</span> spec.hostIPC <span class="token punctuation">-</span> spec.hostUsers <span class="token punctuation">-</span>
    spec.securityContext.seLinuxOptions <span class="token punctuation">-</span> spec.securityContext.seccompProfile <span class="token punctuation">-</span>
    spec.securityContext.fsGroup <span class="token punctuation">-</span> spec.securityContext.fsGroupChangePolicy <span class="token punctuation">-</span>
    spec.securityContext.sysctls <span class="token punctuation">-</span> spec.shareProcessNamespace <span class="token punctuation">-</span>
    spec.securityContext.runAsUser <span class="token punctuation">-</span> spec.securityContext.runAsGroup <span class="token punctuation">-</span>
    spec.securityContext.supplementalGroups <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.seLinuxOptions <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.seccompProfile <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.capabilities <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.readOnlyRootFilesystem <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.privileged <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.allowPrivilegeEscalation <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.procMount <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.runAsUser <span class="token punctuation">-</span>
    spec.containers<span class="token punctuation">[</span>*<span class="token punctuation">]</span>.securityContext.runAsGroup

  overhead	&lt;map<span class="token punctuation">[</span>string<span class="token punctuation">]</span>Quantity<span class="token punctuation">&gt;</span>
    Overhead represents the resource overhead associated with running a pod for
    a given RuntimeClass. This field will be autopopulated at admission time by
    the RuntimeClass admission controller. If the RuntimeClass admission
    controller is enabled<span class="token punctuation">,</span> overhead must not be set in Pod create requests. The
    RuntimeClass admission controller will reject Pod create requests which have
    the overhead already set. If RuntimeClass is configured and selected in the
    PodSpec<span class="token punctuation">,</span> Overhead will be set to the value defined in the corresponding
    RuntimeClass<span class="token punctuation">,</span> <span class="token key atrule">otherwise it will remain unset and treated as zero. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/enhancements/keps/sig<span class="token punctuation">-</span>node/688<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>overhead/README.md

  preemptionPolicy	&lt;string<span class="token punctuation">&gt;</span>
    PreemptionPolicy is the Policy for preempting pods with lower priority. One
    of Never<span class="token punctuation">,</span> PreemptLowerPriority. Defaults to PreemptLowerPriority if unset.
  
    <span class="token key atrule">Possible enum values</span><span class="token punctuation">:</span>
     <span class="token punctuation">-</span> `&quot;Never&quot;` means that pod never preempts other pods with lower priority.
     <span class="token punctuation">-</span> `&quot;PreemptLowerPriority&quot;` means that pod can preempt other pods with lower
    priority.

  priority	&lt;integer<span class="token punctuation">&gt;</span>
    The priority value. Various system components use this field to find the
    priority of the pod. When Priority Admission Controller is enabled<span class="token punctuation">,</span> it
    prevents users from setting this field. The admission controller populates
    this field from PriorityClassName. The higher the value<span class="token punctuation">,</span> the higher the
    priority.

  priorityClassName	&lt;string<span class="token punctuation">&gt;</span>
    If specified<span class="token punctuation">,</span> indicates the pod's priority. &quot;system<span class="token punctuation">-</span>node<span class="token punctuation">-</span>critical&quot; and
    &quot;system<span class="token punctuation">-</span>cluster<span class="token punctuation">-</span>critical&quot; are two special keywords which indicate the
    highest priorities with the former being the highest priority. Any other
    name must be defined by creating a PriorityClass object with that name. If
    not specified<span class="token punctuation">,</span> the pod priority will be default or zero if there is no
    default.

  readinessGates	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>PodReadinessGate<span class="token punctuation">&gt;</span>
    If specified<span class="token punctuation">,</span> all readiness gates will be evaluated for pod readiness. A pod
    is ready when all its containers are ready AND all conditions specified in
    <span class="token key atrule">the readiness gates have status equal to &quot;True&quot; More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/enhancements/keps/sig<span class="token punctuation">-</span>network/580<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>readiness<span class="token punctuation">-</span>gates

  resourceClaims	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>PodResourceClaim<span class="token punctuation">&gt;</span>
    ResourceClaims defines which ResourceClaims must be allocated and reserved
    before the Pod is allowed to start. The resources will be made available to
    those containers which consume them by name.
  
    This is an alpha field and requires enabling the DynamicResourceAllocation
    feature gate.
  
    This field is immutable.

  restartPolicy	&lt;string<span class="token punctuation">&gt;</span>
    Restart policy for all containers within the pod. One of Always<span class="token punctuation">,</span> OnFailure<span class="token punctuation">,</span>
    Never. In some contexts<span class="token punctuation">,</span> only a subset of those values may be permitted.
    <span class="token key atrule">Default to Always. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//kubernetes.io/docs/concepts/workloads/pods/pod<span class="token punctuation">-</span>lifecycle/<span class="token comment">#restart-policy</span>
  
    <span class="token key atrule">Possible enum values</span><span class="token punctuation">:</span>
     <span class="token punctuation">-</span> `&quot;Always&quot;`
     <span class="token punctuation">-</span> `&quot;Never&quot;`
     <span class="token punctuation">-</span> `&quot;OnFailure&quot;`

  runtimeClassName	&lt;string<span class="token punctuation">&gt;</span>
    RuntimeClassName refers to a RuntimeClass object in the node.k8s.io group<span class="token punctuation">,</span>
    which should be used to run this pod.  If no RuntimeClass resource matches
    the named class<span class="token punctuation">,</span> the pod will not be run. If unset or empty<span class="token punctuation">,</span> the &quot;legacy&quot;
    RuntimeClass will be used<span class="token punctuation">,</span> which is an implicit class with an empty
    <span class="token key atrule">definition that uses the default runtime handler. More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//git.k8s.io/enhancements/keps/sig<span class="token punctuation">-</span>node/585<span class="token punctuation">-</span>runtime<span class="token punctuation">-</span>class

  schedulerName	&lt;string<span class="token punctuation">&gt;</span>
    If specified<span class="token punctuation">,</span> the pod will be dispatched by specified scheduler. If not
    specified<span class="token punctuation">,</span> the pod will be dispatched by default scheduler.

  schedulingGates	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>PodSchedulingGate<span class="token punctuation">&gt;</span>
    SchedulingGates is an opaque list of values that if specified will block
    scheduling the pod. If schedulingGates is not empty<span class="token punctuation">,</span> the pod will stay in
    the SchedulingGated state and the scheduler will not attempt to schedule the
    pod.
  
    SchedulingGates can only be set at pod creation time<span class="token punctuation">,</span> and be removed only
    afterwards.
  
    This is a beta feature enabled by the PodSchedulingReadiness feature gate.

  securityContext	&lt;PodSecurityContext<span class="token punctuation">&gt;</span>
    SecurityContext holds pod<span class="token punctuation">-</span>level security attributes and common container
    <span class="token key atrule">settings. Optional</span><span class="token punctuation">:</span> Defaults to empty.  See type description for default
    values of each field.

  serviceAccount	&lt;string<span class="token punctuation">&gt;</span>
    DeprecatedServiceAccount is a depreciated alias for ServiceAccountName.
    <span class="token key atrule">Deprecated</span><span class="token punctuation">:</span> Use serviceAccountName instead.

  serviceAccountName	&lt;string<span class="token punctuation">&gt;</span>
    ServiceAccountName is the name of the ServiceAccount to use to run this pod.
    <span class="token key atrule">More info</span><span class="token punctuation">:</span>
    https<span class="token punctuation">:</span>//kubernetes.io/docs/tasks/configure<span class="token punctuation">-</span>pod<span class="token punctuation">-</span>container/configure<span class="token punctuation">-</span>service<span class="token punctuation">-</span>account/

  setHostnameAsFQDN	&lt;boolean<span class="token punctuation">&gt;</span>
    If true the pod's hostname will be configured as the pod's FQDN<span class="token punctuation">,</span> rather than
    the leaf name (the default). In Linux containers<span class="token punctuation">,</span> this means setting the
    FQDN in the hostname field of the kernel (the nodename field of struct
    utsname). In Windows containers<span class="token punctuation">,</span> this means setting the registry value of
    hostname for the registry key
    HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Tcpip\Parameters to
    FQDN. If a pod does not have FQDN<span class="token punctuation">,</span> this has no effect. Default to false.

  shareProcessNamespace	&lt;boolean<span class="token punctuation">&gt;</span>
    Share a single process namespace between all of the containers in a pod.
    When this is set containers will be able to view and signal processes from
    other containers in the same pod<span class="token punctuation">,</span> and the first process in each container
    will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be
    <span class="token key atrule">set. Optional</span><span class="token punctuation">:</span> Default to false.

  subdomain	&lt;string<span class="token punctuation">&gt;</span>
    If specified<span class="token punctuation">,</span> the fully qualified Pod hostname will be
    &quot;&lt;hostname<span class="token punctuation">&gt;</span>.&lt;subdomain<span class="token punctuation">&gt;</span>.&lt;pod namespace<span class="token punctuation">&gt;</span>.svc.&lt;cluster domain<span class="token punctuation">&gt;</span>&quot;. If not
    specified<span class="token punctuation">,</span> the pod will not have a domainname at all.

  terminationGracePeriodSeconds	&lt;integer<span class="token punctuation">&gt;</span>
    Optional duration in seconds the pod needs to terminate gracefully. May be
    decreased in delete request. Value must be non<span class="token punctuation">-</span>negative integer. The value
    zero indicates stop immediately via the kill signal (no opportunity to shut
    down). If this value is nil<span class="token punctuation">,</span> the default grace period will be used instead.
    The grace period is the duration in seconds after the processes running in
    the pod are sent a termination signal and the time when the processes are
    forcibly halted with a kill signal. Set this value longer than the expected
    cleanup time for your process. Defaults to 30 seconds.

  tolerations	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>Toleration<span class="token punctuation">&gt;</span>
    If specified<span class="token punctuation">,</span> the pod's tolerations.

  topologySpreadConstraints	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>TopologySpreadConstraint<span class="token punctuation">&gt;</span>
    TopologySpreadConstraints describes how a group of pods ought to spread
    across topology domains. Scheduler will schedule pods in a way which abides
    by the constraints. All topologySpreadConstraints are ANDed.

  volumes	&lt;<span class="token punctuation">[</span><span class="token punctuation">]</span>Volume<span class="token punctuation">&gt;</span>
    List of volumes that can be mounted by containers belonging to the pod. More
    <span class="token key atrule">info</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//kubernetes.io/docs/concepts/storage/volumes
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br><span class="line-number">118</span><br><span class="line-number">119</span><br><span class="line-number">120</span><br><span class="line-number">121</span><br><span class="line-number">122</span><br><span class="line-number">123</span><br><span class="line-number">124</span><br><span class="line-number">125</span><br><span class="line-number">126</span><br><span class="line-number">127</span><br><span class="line-number">128</span><br><span class="line-number">129</span><br><span class="line-number">130</span><br><span class="line-number">131</span><br><span class="line-number">132</span><br><span class="line-number">133</span><br><span class="line-number">134</span><br><span class="line-number">135</span><br><span class="line-number">136</span><br><span class="line-number">137</span><br><span class="line-number">138</span><br><span class="line-number">139</span><br><span class="line-number">140</span><br><span class="line-number">141</span><br><span class="line-number">142</span><br><span class="line-number">143</span><br><span class="line-number">144</span><br><span class="line-number">145</span><br><span class="line-number">146</span><br><span class="line-number">147</span><br><span class="line-number">148</span><br><span class="line-number">149</span><br><span class="line-number">150</span><br><span class="line-number">151</span><br><span class="line-number">152</span><br><span class="line-number">153</span><br><span class="line-number">154</span><br><span class="line-number">155</span><br><span class="line-number">156</span><br><span class="line-number">157</span><br><span class="line-number">158</span><br><span class="line-number">159</span><br><span class="line-number">160</span><br><span class="line-number">161</span><br><span class="line-number">162</span><br><span class="line-number">163</span><br><span class="line-number">164</span><br><span class="line-number">165</span><br><span class="line-number">166</span><br><span class="line-number">167</span><br><span class="line-number">168</span><br><span class="line-number">169</span><br><span class="line-number">170</span><br><span class="line-number">171</span><br><span class="line-number">172</span><br><span class="line-number">173</span><br><span class="line-number">174</span><br><span class="line-number">175</span><br><span class="line-number">176</span><br><span class="line-number">177</span><br><span class="line-number">178</span><br><span class="line-number">179</span><br><span class="line-number">180</span><br><span class="line-number">181</span><br><span class="line-number">182</span><br><span class="line-number">183</span><br><span class="line-number">184</span><br><span class="line-number">185</span><br><span class="line-number">186</span><br><span class="line-number">187</span><br><span class="line-number">188</span><br><span class="line-number">189</span><br><span class="line-number">190</span><br><span class="line-number">191</span><br><span class="line-number">192</span><br><span class="line-number">193</span><br><span class="line-number">194</span><br><span class="line-number">195</span><br><span class="line-number">196</span><br><span class="line-number">197</span><br><span class="line-number">198</span><br><span class="line-number">199</span><br><span class="line-number">200</span><br><span class="line-number">201</span><br><span class="line-number">202</span><br><span class="line-number">203</span><br><span class="line-number">204</span><br><span class="line-number">205</span><br><span class="line-number">206</span><br><span class="line-number">207</span><br><span class="line-number">208</span><br><span class="line-number">209</span><br><span class="line-number">210</span><br><span class="line-number">211</span><br><span class="line-number">212</span><br><span class="line-number">213</span><br><span class="line-number">214</span><br><span class="line-number">215</span><br><span class="line-number">216</span><br><span class="line-number">217</span><br><span class="line-number">218</span><br><span class="line-number">219</span><br><span class="line-number">220</span><br><span class="line-number">221</span><br><span class="line-number">222</span><br><span class="line-number">223</span><br><span class="line-number">224</span><br><span class="line-number">225</span><br><span class="line-number">226</span><br><span class="line-number">227</span><br><span class="line-number">228</span><br><span class="line-number">229</span><br><span class="line-number">230</span><br><span class="line-number">231</span><br><span class="line-number">232</span><br><span class="line-number">233</span><br><span class="line-number">234</span><br><span class="line-number">235</span><br><span class="line-number">236</span><br><span class="line-number">237</span><br><span class="line-number">238</span><br><span class="line-number">239</span><br><span class="line-number">240</span><br><span class="line-number">241</span><br><span class="line-number">242</span><br><span class="line-number">243</span><br><span class="line-number">244</span><br><span class="line-number">245</span><br><span class="line-number">246</span><br><span class="line-number">247</span><br><span class="line-number">248</span><br><span class="line-number">249</span><br><span class="line-number">250</span><br><span class="line-number">251</span><br><span class="line-number">252</span><br><span class="line-number">253</span><br><span class="line-number">254</span><br><span class="line-number">255</span><br><span class="line-number">256</span><br><span class="line-number">257</span><br><span class="line-number">258</span><br><span class="line-number">259</span><br><span class="line-number">260</span><br><span class="line-number">261</span><br><span class="line-number">262</span><br><span class="line-number">263</span><br><span class="line-number">264</span><br><span class="line-number">265</span><br><span class="line-number">266</span><br><span class="line-number">267</span><br><span class="line-number">268</span><br><span class="line-number">269</span><br><span class="line-number">270</span><br><span class="line-number">271</span><br><span class="line-number">272</span><br><span class="line-number">273</span><br><span class="line-number">274</span><br><span class="line-number">275</span><br><span class="line-number">276</span><br><span class="line-number">277</span><br><span class="line-number">278</span><br><span class="line-number">279</span><br><span class="line-number">280</span><br><span class="line-number">281</span><br><span class="line-number">282</span><br><span class="line-number">283</span><br><span class="line-number">284</span><br><span class="line-number">285</span><br><span class="line-number">286</span><br><span class="line-number">287</span><br><span class="line-number">288</span><br><span class="line-number">289</span><br><span class="line-number">290</span><br><span class="line-number">291</span><br><span class="line-number">292</span><br><span class="line-number">293</span><br></div></div><h6 id="_3-2-3-使用kubectl-create来创建pod"><a href="#_3-2-3-使用kubectl-create来创建pod" class="header-anchor">#</a> 3.2.3 使用kubectl create来创建pod</h6> <p>使用 kubectl create 命令从 YAML 文件创建 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-manual.yml 
pod/kubia-manual created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>​<code>kubectl create -f</code>​ 命令用于从 YAML 或 JSON 文件创建任何资源(不只是 pod).</p> <blockquote><p>得到运行中 pod 的完整定义</p></blockquote> <p>pod 创建完成后, 可以<strong>请求 Kubernetes 来获得完整的 YAML</strong>, 可以看到它与之前看到的 YAML 文件非常相似. 在下一节中将了解返回定义中出现的其他字段, 接下来就直接使用以下命令来查看该 pod 的完整描述文件:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po kubia-manual <span class="token parameter variable">-o</span> yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
  name: kubia-manual
  namespace: default
  resourceVersion: <span class="token string">&quot;10872&quot;</span>
  uid: 9ed0f723-bb1c-47fe-acf8-c28a5f4c7a54
spec:
  containers:
  - image: luksa/kubia
    imagePullPolicy: Always
    name: kubia
    ports:
    - containerPort: <span class="token number">8080</span>
      protocol: TCP
    resources: <span class="token punctuation">{</span><span class="token punctuation">}</span>
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-b96j5
      readOnly: <span class="token boolean">true</span>
  dnsPolicy: ClusterFirst
  enableServiceLinks: <span class="token boolean">true</span>
  nodeName: minikube
  preemptionPolicy: PreemptLowerPriority
  priority: <span class="token number">0</span>
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: <span class="token punctuation">{</span><span class="token punctuation">}</span>
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: <span class="token number">30</span>
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: <span class="token number">300</span>
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: <span class="token number">300</span>
  volumes:
  - name: kube-api-access-b96j5
    projected:
      defaultMode: <span class="token number">420</span>
      sources:
      - serviceAccountToken:
          expirationSeconds: <span class="token number">3607</span>
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
    status: <span class="token string">&quot;True&quot;</span>
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
    message: <span class="token string">'containers with unready status: [kubia]'</span>
    reason: ContainersNotReady
    status: <span class="token string">&quot;False&quot;</span>
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
    message: <span class="token string">'containers with unready status: [kubia]'</span>
    reason: ContainersNotReady
    status: <span class="token string">&quot;False&quot;</span>
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
    status: <span class="token string">&quot;True&quot;</span>
    type: PodScheduled
  containerStatuses:
  - image: luksa/kubia
    imageID: <span class="token string">&quot;&quot;</span>
    lastState: <span class="token punctuation">{</span><span class="token punctuation">}</span>
    name: kubia
    ready: <span class="token boolean">false</span>
    restartCount: <span class="token number">0</span>
    started: <span class="token boolean">false</span>
    state:
      waiting:
        message: Back-off pulling image <span class="token string">&quot;luksa/kubia&quot;</span>
        reason: ImagePullBackOff
  hostIP: <span class="token number">192.168</span>.49.2
  phase: Pending
  podIP: <span class="token number">10.244</span>.0.20
  podIPs:
  - ip: <span class="token number">10.244</span>.0.20
  qosClass: BestEffort
  startTime: <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br></div></div><p>也可以让 kubectl 返回 JSON 格式而不是 YAML 格式(显然, 即使使用 YAML 创建 pod, 同样也可以获取 JSON 格式的描述文件):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po kubia-manual <span class="token parameter variable">-o</span> json
<span class="token punctuation">{</span>
    <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
    <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Pod&quot;</span>,
    <span class="token string">&quot;metadata&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;creationTimestamp&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>,
        <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kubia-manual&quot;</span>,
        <span class="token string">&quot;namespace&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default&quot;</span>,
        <span class="token string">&quot;resourceVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;10872&quot;</span>,
        <span class="token string">&quot;uid&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;9ed0f723-bb1c-47fe-acf8-c28a5f4c7a54&quot;</span>
    <span class="token punctuation">}</span>,
    <span class="token string">&quot;spec&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;containers&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token string">&quot;image&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;luksa/kubia&quot;</span>,
                <span class="token string">&quot;imagePullPolicy&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Always&quot;</span>,
                <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kubia&quot;</span>,
                <span class="token string">&quot;ports&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                    <span class="token punctuation">{</span>
                        <span class="token string">&quot;containerPort&quot;</span><span class="token builtin class-name">:</span> <span class="token number">8080</span>,
                        <span class="token string">&quot;protocol&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;TCP&quot;</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">]</span>,
                <span class="token string">&quot;resources&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>,
                <span class="token string">&quot;terminationMessagePath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/dev/termination-log&quot;</span>,
                <span class="token string">&quot;terminationMessagePolicy&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;File&quot;</span>,
                <span class="token string">&quot;volumeMounts&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                    <span class="token punctuation">{</span>
                        <span class="token string">&quot;mountPath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;</span>,
                        <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kube-api-access-b96j5&quot;</span>,
                        <span class="token string">&quot;readOnly&quot;</span><span class="token builtin class-name">:</span> <span class="token boolean">true</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">]</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span>,
        <span class="token string">&quot;dnsPolicy&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ClusterFirst&quot;</span>,
        <span class="token string">&quot;enableServiceLinks&quot;</span><span class="token builtin class-name">:</span> true,
        <span class="token string">&quot;nodeName&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;minikube&quot;</span>,
        <span class="token string">&quot;preemptionPolicy&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PreemptLowerPriority&quot;</span>,
        <span class="token string">&quot;priority&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
        <span class="token string">&quot;restartPolicy&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Always&quot;</span>,
        <span class="token string">&quot;schedulerName&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default-scheduler&quot;</span>,
        <span class="token string">&quot;securityContext&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>,
        <span class="token string">&quot;serviceAccount&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default&quot;</span>,
        <span class="token string">&quot;serviceAccountName&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default&quot;</span>,
        <span class="token string">&quot;terminationGracePeriodSeconds&quot;</span><span class="token builtin class-name">:</span> <span class="token number">30</span>,
        <span class="token string">&quot;tolerations&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token string">&quot;effect&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;NoExecute&quot;</span>,
                <span class="token string">&quot;key&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;node.kubernetes.io/not-ready&quot;</span>,
                <span class="token string">&quot;operator&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Exists&quot;</span>,
                <span class="token string">&quot;tolerationSeconds&quot;</span><span class="token builtin class-name">:</span> <span class="token number">300</span>
            <span class="token punctuation">}</span>,
            <span class="token punctuation">{</span>
                <span class="token string">&quot;effect&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;NoExecute&quot;</span>,
                <span class="token string">&quot;key&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;node.kubernetes.io/unreachable&quot;</span>,
                <span class="token string">&quot;operator&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Exists&quot;</span>,
                <span class="token string">&quot;tolerationSeconds&quot;</span><span class="token builtin class-name">:</span> <span class="token number">300</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span>,
        <span class="token string">&quot;volumes&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kube-api-access-b96j5&quot;</span>,
                <span class="token string">&quot;projected&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                    <span class="token string">&quot;defaultMode&quot;</span><span class="token builtin class-name">:</span> <span class="token number">420</span>,
                    <span class="token string">&quot;sources&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                        <span class="token punctuation">{</span>
                            <span class="token string">&quot;serviceAccountToken&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                                <span class="token string">&quot;expirationSeconds&quot;</span><span class="token builtin class-name">:</span> <span class="token number">3607</span>,
                                <span class="token string">&quot;path&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;token&quot;</span>
                            <span class="token punctuation">}</span>
                        <span class="token punctuation">}</span>,
                        <span class="token punctuation">{</span>
                            <span class="token string">&quot;configMap&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                                <span class="token string">&quot;items&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                                    <span class="token punctuation">{</span>
                                        <span class="token string">&quot;key&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ca.crt&quot;</span>,
                                        <span class="token string">&quot;path&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ca.crt&quot;</span>
                                    <span class="token punctuation">}</span>
                                <span class="token punctuation">]</span>,
                                <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kube-root-ca.crt&quot;</span>
                            <span class="token punctuation">}</span>
                        <span class="token punctuation">}</span>,
                        <span class="token punctuation">{</span>
                            <span class="token string">&quot;downwardAPI&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                                <span class="token string">&quot;items&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
                                    <span class="token punctuation">{</span>
                                        <span class="token string">&quot;fieldRef&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                                            <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
                                            <span class="token string">&quot;fieldPath&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;metadata.namespace&quot;</span>
                                        <span class="token punctuation">}</span>,
                                        <span class="token string">&quot;path&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;namespace&quot;</span>
                                    <span class="token punctuation">}</span>
                                <span class="token punctuation">]</span>
                            <span class="token punctuation">}</span>
                        <span class="token punctuation">}</span>
                    <span class="token punctuation">]</span>
                <span class="token punctuation">}</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span>
    <span class="token punctuation">}</span>,
    <span class="token string">&quot;status&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;conditions&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token string">&quot;lastProbeTime&quot;</span><span class="token builtin class-name">:</span> null,
                <span class="token string">&quot;lastTransitionTime&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>,
                <span class="token string">&quot;status&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;True&quot;</span>,
                <span class="token string">&quot;type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Initialized&quot;</span>
            <span class="token punctuation">}</span>,
            <span class="token punctuation">{</span>
                <span class="token string">&quot;lastProbeTime&quot;</span><span class="token builtin class-name">:</span> null,
                <span class="token string">&quot;lastTransitionTime&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>,
                <span class="token string">&quot;message&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;containers with unready status: [kubia]&quot;</span>,
                <span class="token string">&quot;reason&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ContainersNotReady&quot;</span>,
                <span class="token string">&quot;status&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;False&quot;</span>,
                <span class="token string">&quot;type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Ready&quot;</span>
            <span class="token punctuation">}</span>,
            <span class="token punctuation">{</span>
                <span class="token string">&quot;lastProbeTime&quot;</span><span class="token builtin class-name">:</span> null,
                <span class="token string">&quot;lastTransitionTime&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>,
                <span class="token string">&quot;message&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;containers with unready status: [kubia]&quot;</span>,
                <span class="token string">&quot;reason&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ContainersNotReady&quot;</span>,
                <span class="token string">&quot;status&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;False&quot;</span>,
                <span class="token string">&quot;type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ContainersReady&quot;</span>
            <span class="token punctuation">}</span>,
            <span class="token punctuation">{</span>
                <span class="token string">&quot;lastProbeTime&quot;</span><span class="token builtin class-name">:</span> null,
                <span class="token string">&quot;lastTransitionTime&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>,
                <span class="token string">&quot;status&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;True&quot;</span>,
                <span class="token string">&quot;type&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PodScheduled&quot;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span>,
        <span class="token string">&quot;containerStatuses&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token string">&quot;image&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;luksa/kubia&quot;</span>,
                <span class="token string">&quot;imageID&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;&quot;</span>,
                <span class="token string">&quot;lastState&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>,
                <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;kubia&quot;</span>,
                <span class="token string">&quot;ready&quot;</span><span class="token builtin class-name">:</span> false,
                <span class="token string">&quot;restartCount&quot;</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
                <span class="token string">&quot;started&quot;</span><span class="token builtin class-name">:</span> false,
                <span class="token string">&quot;state&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                    <span class="token string">&quot;waiting&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
                        <span class="token string">&quot;message&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Back-off pulling image <span class="token entity" title="\&quot;">\&quot;</span>luksa/kubia<span class="token entity" title="\&quot;">\&quot;</span>&quot;</span>,
                        <span class="token string">&quot;reason&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ImagePullBackOff&quot;</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">}</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span>,
        <span class="token string">&quot;hostIP&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;192.168.49.2&quot;</span>,
        <span class="token string">&quot;phase&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Pending&quot;</span>,
        <span class="token string">&quot;podIP&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;10.244.0.20&quot;</span>,
        <span class="token string">&quot;podIPs&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span>
                <span class="token string">&quot;ip&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;10.244.0.20&quot;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">]</span>,
        <span class="token string">&quot;qosClass&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;BestEffort&quot;</span>,
        <span class="token string">&quot;startTime&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;2024-02-01T13:52:40Z&quot;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br><span class="line-number">118</span><br><span class="line-number">119</span><br><span class="line-number">120</span><br><span class="line-number">121</span><br><span class="line-number">122</span><br><span class="line-number">123</span><br><span class="line-number">124</span><br><span class="line-number">125</span><br><span class="line-number">126</span><br><span class="line-number">127</span><br><span class="line-number">128</span><br><span class="line-number">129</span><br><span class="line-number">130</span><br><span class="line-number">131</span><br><span class="line-number">132</span><br><span class="line-number">133</span><br><span class="line-number">134</span><br><span class="line-number">135</span><br><span class="line-number">136</span><br><span class="line-number">137</span><br><span class="line-number">138</span><br><span class="line-number">139</span><br><span class="line-number">140</span><br><span class="line-number">141</span><br><span class="line-number">142</span><br><span class="line-number">143</span><br><span class="line-number">144</span><br><span class="line-number">145</span><br><span class="line-number">146</span><br><span class="line-number">147</span><br><span class="line-number">148</span><br><span class="line-number">149</span><br><span class="line-number">150</span><br><span class="line-number">151</span><br><span class="line-number">152</span><br><span class="line-number">153</span><br><span class="line-number">154</span><br><span class="line-number">155</span><br><span class="line-number">156</span><br><span class="line-number">157</span><br><span class="line-number">158</span><br><span class="line-number">159</span><br><span class="line-number">160</span><br><span class="line-number">161</span><br></div></div><blockquote><p>在 pod 列表中查看新创建的 pod</p></blockquote> <p>创建好 pod 之后, 如何知道它是否正在运行? 此时可以列出 pod 来查看它们的状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME           READY   STATUS             RESTARTS   AGE
kubia          <span class="token number">0</span>/1     ImagePullBackOff   <span class="token number">0</span>          23h
kubia-manual   <span class="token number">0</span>/1     Running            <span class="token number">0</span>          103s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这里可以看到 kubia-manual 这个 pod, 状态显示它正在运行. 有可能你像笔者一样想要通过与 pod 的实际通信来确认其正在运行, 但该方法将在之后进行讨论. 现在先查看应用的日志来检查是否存在错误.</p> <h6 id="_3-2-4-查看应用程序日志"><a href="#_3-2-4-查看应用程序日志" class="header-anchor">#</a> 3.2.4 查看应用程序日志</h6> <p>小型 Node.js 应用将日志记录到进程的标准输出. <strong>容器化的应用程序通常会将日志记录到标准输出和标准错误流, 而不是将其写入文件, 这就允许用户可以通过简单, 标准的方式查看不同应用程序的日志</strong>.</p> <p>容器运行时(在例子中为 Docker)将这些流重定向到文件, 并允许运行以下命令来获取容器的日志:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> logs <span class="token operator">&lt;</span>container id<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>使用 ssh 命令登录到 pod 正在运行的节点, 并使用 docker logs 命令查看其日志, 但 Kubernetes 提供了一种更为简单的方法.</p> <blockquote><p>使用 kubectl logs 命令获取 pod 日志</p></blockquote> <p>为了查看 pod 的日志(更准确地说是容器的日志), 只需要在<strong>本地机器上</strong>运行以下命令(不需要 ssh 到任何地方):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs kubia-manual
Kubia server starting<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在向 Node.js 应用程序发送任何 Web 请求之前, 日志只显示一条关于服务器启动的语句. 正如我们所见, 如果该 pod 只包含一个容器, 那么查看这种在 Kubernetes 中运行的应用程序的日志则非常简单.</p> <p>注意: 每天或者每次日志文件达到 10MB 大小时, 容器日志都会<strong>自动轮替</strong>. kubectl logs 命令仅显示最后一次轮替后的日志条目.</p> <blockquote><p>获取多容器 pod 的日志时指定容器名称</p></blockquote> <p>如果 pod 包含多个容器, 在运行 kubectl logs 命令时则必须通过包含 <code>-c &lt;容器名称&gt;</code>​ 选项来显式<strong>指定容器名称</strong>. 在 kubia-manual pod 中, 将容器的名称设置为 kubia, 所以如果该 pod 中有其他容器, 可以通过如下命令获取其日志:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs kubia-manual <span class="token parameter variable">-c</span> kubia
Kubia server starting<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这里需要注意的是, 这里只能获取仍然存在的 pod 的日志. <strong>当一个 pod 被删除时, 它的日志也会被删除</strong>. 如果希望在 pod 删除之后仍然可以获取其日志, 就需要设置中心化的, 集群范围的日志系统, 将所有日志存储到中心存储中. 在第 17 章中将会解释如何设置集中的日志系统.</p> <h6 id="_3-2-5-向pod发送请求"><a href="#_3-2-5-向pod发送请求" class="header-anchor">#</a> 3.2.5 向pod发送请求</h6> <p>kubectl get 命令和应用日志显示该 pod 正在运行, 但如何在实际操作中看到该状态呢? 在前一章中, 使用 kubectl expose 命令创建了一个 service, 以便在外部访问该 pod. 由于有一整章专门介绍 service, 因此本章并不打算使用该方法. 此外, 还有其他连接到 pod 以进行测试和调试的方法, 其中之一便是<strong>通过端口转</strong>发.</p> <blockquote><p>将本地网络端口转发到pod中的端口</p></blockquote> <p>如果想要在不通过 service 的情况下与某个特定的 pod 进行通信(出于调试或其他原因), Kubernetes 将允许<strong>配置端口转发到该 pod</strong>. 可以通过 <code>kubectl port-forward</code>​ 命令完成上述操作. 例如以下命令会将机器的本地端口 8888 转发到的 kubia-manual pod 的端口 8080:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl port-forward kubia-manual <span class="token number">8888</span>:8080
Forwarding from <span class="token number">127.0</span>.0.1:8888 -<span class="token operator">&gt;</span> <span class="token number">8080</span>
Forwarding from <span class="token punctuation">[</span>::1<span class="token punctuation">]</span>:8888 -<span class="token operator">&gt;</span> <span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>此时端口转发正在运行, 可以<strong>通过本地端口连接到 pod</strong>.</p> <blockquote><p>通过端口转发连接到pod</p></blockquote> <p>在另一个终端中, 通过运行在 localhost:8888 上的 kubectl portforward 代理, 可以使用 curl 命令向 pod 发送一个 HTTP 请求:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># curl localhost:8888</span>
You've hit kubia-manual
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>图 3.5 展示了发送请求时的简化视图. 实际上, kubectl 进程和 pod 之间还有一些额外的组件, 但现在暂时不关注它们.</p> <p><img src="/img/image-20240227231433-0niyp6w.png" alt="image" title="图3.5 描述使用 kubectl port-forward 和 curl 时的简单视图"></p> <p>像这样使用端口转发是一种测试特定 pod 的有效方法, 后面也将在这本书中学习其他类似的方法.</p> <h5 id="_3-3-使用标签组织pod"><a href="#_3-3-使用标签组织pod" class="header-anchor">#</a> 3.3 使用标签组织pod</h5> <p>现在集群中只有两个正在运行的 pod. 但部署实际应用程序时, 大多数用户最终将运行更多的 pod. 随着 pod 数量的增加, 将它们<strong>分类到子集</strong>的需求也就变得越来越明显了.</p> <p>例如, 对于微服务架构, 部署的微服务数量可以轻松超过 20 个甚至更多. 这些组件可能是副本(部署同一组件的多个副本)和多个不同的发布版本(stable, beta, canary 等)同时运行. 这样一来可能会导致系统中拥有数百个 pod, 如果没有可以有效组织这些组件的机制, 将会导致产生巨大的混乱, 如图 3.6 所示. 该图展示了多个微服务的 pod, 包括一些运行多副本集, 以及其他运行于同一微服务中的不同版本.</p> <p><img src="/img/image-20240227231457-lkogcn5.png" alt="image" title="图3.6 微服务架构中未分类的 pod"></p> <p>很明显, 需要一种能够<strong>基于任意标准将上述 pod 组织成更小群体的方式, 这样一来处理系统的每个开发人员和系统管理员都可以轻松地看到哪个 pod 是什么</strong>. 此外, 我们希望通过一次操作对属于某个组的所有 pod 进行操作, 而不必单独为每个 pod 执行操作.</p> <p>我们可以<strong>通过标签来组织 pod 和所有其他 Kubernetes 对象.</strong></p> <h6 id="_3-3-1-介绍标签"><a href="#_3-3-1-介绍标签" class="header-anchor">#</a> 3.3.1 介绍标签</h6> <p>标签是一种简单却功能强大的 Kubernetes 特性, <strong>不仅可以组织 pod, 也可以组织所有其他的 Kubernetes 资源</strong>. 详细来讲, <strong>标签是可以附加到资源的任意键值对, 用以选择具有该确切标签的资源(这是通过标签选择器完成的). 只要标签的 key 在资源内是唯一的, 一个资源便可以拥有多个标签</strong>. 通常在创建资源时就会将标签附加到资源上, 但之后也可以再添加其他标签, 或者修改现有标签的值, 而无须重新创建资源.</p> <p>接下来回到图 3.6 中的微服务示例. 通过给这些 pod 添加标签, 可以得到一个更组织化的系统, 以便于理解. 此时每个 pod 都标有两个标签:</p> <ul><li><strong>app</strong>, 它指定 pod 属于哪个应用, 组件或微服务.</li> <li><strong>rel</strong>, 它显示在 pod 中运行的应用程序版本是 stable, beta 还是 canary.</li></ul> <p>定义: 金丝雀发布是指在部署新版本时, 先只让一小部分用户体验新版本以观察新版本的表现, 然后再向所有用户进行推广, 这样可以防止暴露有问题的版本给过多的用户.</p> <p>如图 3.7 所示, 通过<strong>添加这两个标签</strong>基本上可以将 pod 组织为<strong>两个维度</strong>(基于应用的横向维度和基于版本的纵向维度).</p> <p><img src="/img/image-20240227231534-fs43mhq.png" alt="image" title="图3.7 使用 pod 标签组织微服务架构中的 pod"></p> <p>每个可以访问集群的开发或运维人员都可以通过查看 pod 标签轻松看到系统的结构, 以及每个 pod 的角色.</p> <h6 id="_3-3-2-创建pod时指定标签"><a href="#_3-3-2-创建pod时指定标签" class="header-anchor">#</a> 3.3.2 创建pod时指定标签</h6> <p>现在, 可以通过创建一个带有两个标签的新 pod 来查看标签的实际应用. 使用以下代码清单中的内容创建一个名为 kubia-manual-with-labels.yaml 的新文件.</p> <p><strong>代码清单-3.3 带标签的 pod: kubia-manual-with-labels.yaml</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: v1
<span class="token comment"># 表示描述的是一个pod</span>
kind: Pod
<span class="token comment"># pod的名称</span>
metadata:
  name: kubia-manual-v2
  <span class="token comment"># 两个标签被附加到pod上</span>
  labels:
    creation_method: manual
    env: prod
spec:
  containers:
  <span class="token comment"># 创建容器使用的镜像</span>
  - image: luksa/kubia
    name: kubia    <span class="token comment"># 容器的名称</span>
    ports:
    - containerPort: <span class="token number">8080</span>    <span class="token comment"># 应用监听的端口</span>
      protocol: TCP
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>​<code>metadata.labels</code>​ 部分已经包含了 creation_method=manual 和 env=prod 标签. 现在来创建该 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-manual-with-labels.yaml 
pod/kubia-manual-v2 created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>kubectl get pods 命令<strong>默认不会列出任何标签</strong>, 但可以<strong>使用 --showlabels 选项</strong>来查看:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod --show-labels
NAME              READY   STATUS    RESTARTS   AGE   LABELS
kubia-manual      <span class="token number">1</span>/1     Running   <span class="token number">0</span>          16m   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
kubia-manual-v2   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          28s   <span class="token assign-left variable">creation_method</span><span class="token operator">=</span>manual,env<span class="token operator">=</span>prod
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>如果你只对<strong>某些标签</strong>感兴趣, 可以使用 -L 选项指定它们并将它们分别显示在自己的列中, 而不是列出所有标签. 接下来再次列出所有 pod, 并将附加到 pod kubia-manual-v2 上的<strong>两个标签的列</strong>展示如下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-L</span> creation_method,env
NAME              READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV
kubia-manual      <span class="token number">1</span>/1     Running   <span class="token number">0</span>          17m                   
kubia-manual-v2   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          93s   manual            prod
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h6 id="_3-3-3-修改现有pod的标签"><a href="#_3-3-3-修改现有pod的标签" class="header-anchor">#</a> 3.3.3 修改现有pod的标签</h6> <p>标签也可以<strong>在现有 pod 上进行添加和修改</strong>. 由于 pod kubia-manual 也是手动创建的, 所以为其添加 creation_method=manual 标签:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl label po kubia-manual <span class="token assign-left variable">creation_method</span><span class="token operator">=</span>manual
pod/kubia-manual labeled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在, 将 kubia-manual-v2 pod 上的 env=prod 标签更改为 env=debug, 以演示现有标签也可以被更改.</p> <p>注意: 在更改现有标签时, 需要使用 --overwrite 选项.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl label po kubia-manual-v2 <span class="token assign-left variable">env</span><span class="token operator">=</span>debug <span class="token parameter variable">--overwrite</span>
pod/kubia-manual-v2 labeled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>再次列出 pod 以查看更新后的标签:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-L</span> creation_method,env
NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
kubia-manual      <span class="token number">1</span>/1     Running   <span class="token number">0</span>          18m     manual          
kubia-manual-v2   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          3m12s   manual            debug
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 目前将标签附加到资源上看起来并没有什么价值, 在现有资源上更改标签也是如此. 但在下一章中将证实, 这会是一项令人难以置信的强大功能. 而首先需要看看这些标签除了在列出 pod 时用以简单显示外, 还可以用来做什么.</p> <h5 id="_3-4-通过标签选择器列出pod子集"><a href="#_3-4-通过标签选择器列出pod子集" class="header-anchor">#</a> 3.4 通过标签选择器列出pod子集</h5> <p>在上一节中将标签附加到资源上, 以便在列出资源时可以看到每个资源旁边的标签, 这看起来并没有什么有趣的地方. 但值得注意的是, <strong>标签要与标签选择器结合在一起. 标签选择器允许我们选择标记有特定标签的 pod 子集, 并对这些 pod 执行操作. 可以说标签选择器是一种能够根据是否包含具有特定值的特定标签来过滤资源的准则</strong>.</p> <p>标签选择器根据资源的以下条件来选择资源:</p> <ul><li>包含(或不包含)使用特定键的标签</li> <li>包含具有特定键和值的标签</li> <li>包含具有特定键的标签, 但其值与指定的不同</li></ul> <h6 id="_3-4-1-使用标签选择器列出pod"><a href="#_3-4-1-使用标签选择器列出pod" class="header-anchor">#</a> 3.4.1 使用标签选择器列出pod</h6> <p>接下来使用标签选择器在之前创建的 pod 上进行操作, 以观察手动创建的所有 pod(用 creation_method=manual 标记了它们), 并执行以下操作:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-l</span> <span class="token assign-left variable">creation_method</span><span class="token operator">=</span>manual
NAME             READY         STATUS         RESTARTS         AGE
kubia-manual     <span class="token number">1</span>/1           Running        <span class="token number">0</span>                51m
kubia-manual-v2  <span class="token number">1</span>/1           Running        <span class="token number">0</span>                37m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>列出<strong>包含 env 标签</strong>的所有 pod, 无论其值如何:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-l</span> <span class="token function">env</span>
NAME              READY   STATUS    RESTARTS   AGE
kubia-manual-v2   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          84s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>同样列出没有 env 标签的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-l</span> <span class="token string">'!env'</span>
NAME           READY   STATUS    RESTARTS   AGE
kubia-manual   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          2m13s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: 确保使用单引号来圈引 <code>!env</code>​, 这样 bash shell 才不会解释感叹号(感叹号在 bash 中有特殊含义, 表示事件指示器).</p> <p>同理, 也可以将 pod 与以下标签选择器进行匹配:</p> <ul><li>​<code>creation_method!=manual</code>​ 选择带有 creation_method 标签, 并且值不等于 manual 的 pod.</li> <li>​<code>env in(prod,devel)</code>​ 选择带有 env 标签且值为 prod 或 devel 的 pod.</li> <li>​<code>env notin(prod,devel)</code>​ 选择带有 env 标签, 但其值不是 prod 或 devel 的 pod.</li></ul> <p>接下来回到面向微服务的架构示例中的 pod, 可以使用标签选择器 app=pc(如图 3.8 所示)选择属于 product catalog 微服务的所有 pod.</p> <p><img src="/img/image-20240202232357-4k8ynpu.png" alt="image" title="图3.8 使用标签选择器&quot;app=pc&quot;选择 product catalog 微服务的 pod"></p> <h6 id="_3-4-2-在标签选择器中使用多个条件"><a href="#_3-4-2-在标签选择器中使用多个条件" class="header-anchor">#</a> 3.4.2 在标签选择器中使用多个条件</h6> <p>在包含<strong>多个逗号分隔</strong>的情况下, 可以在标签选择器中<strong>同时使用多个条件</strong>, 此时资源需要<strong>全部匹配</strong>才算成功匹配了选择器. 例如, 如果只想选择 product catalog 微服务的 beta 版本 pod, 可以使用以下选择器: <code>app=pc,rel=beta</code>​(如图 3.9 所示).</p> <p><img src="/img/image-20240202232526-18rfbch.png" alt="image" title="图3.9 通过多个标签选择器选择 pod"></p> <p><strong>标签选择器不仅可以列出 pod, 在对一个子集中的所有 pod 都执行操作时也具有重要意义</strong>. 例如, 在本章的后面将看到如何使用标签选择器来实现一次删除多个 pod. 此外标签选择器不只是被 kubectl 使用, 在后续内容中也将看到它们在内部也被使用过.</p> <h5 id="_3-5-使用标签和选择器来约束pod调度"><a href="#_3-5-使用标签和选择器来约束pod调度" class="header-anchor">#</a> 3.5 使用标签和选择器来约束pod调度</h5> <p>迄今为止创建的所有 pod 都是<strong>近乎随机地调度到工作节点上</strong>的. 正如前一章所提到的, 这恰恰是在 Kubernetes 集群中工作的正确方式. 由于 Kubernetes 将集群中的所有节点抽象为一个整体的大型部署平台, 因此对于 pod 实际调度到哪个节点而言是无关紧要的. <strong>对于每个 pod 而言, 它获得所请求的确切数量的计算资源(CPU, 内存等)及其从其他 pod 的可访问性, 完全不受该 pod 所调度到的节点的影响, 所以通常来说没有任何需要指定 Kubernetes 把 pod 调度到哪里的需求</strong>.</p> <p>当然, 某些情况下, 我们希望对<strong>将 pod 调度到何处持一定发言权</strong>, 你的<strong>硬件基础设施并不是同质</strong>便是一个很好的例子. 如果某些工作节点使用机械硬盘, 而其他节点使用固态硬盘, 那么你可能想将一些 pod 调度到一组节点, 同时将其他 pod 调度到另一组节点. 另外, 当需要将执行 GPU 密集型运算的 pod 调度到实际提供 GPU 加速的节点上时, 也需要 pod 调度.</p> <p>这里不会特别说明 pod 应该调度到哪个节点上, 因为这将会使应用程序与基础架构强耦合, 从而违背了 Kubernetes 对运行在其上的应用程序隐藏实际的基础架构的整个构想. 但如果想对一个 pod 应该调度到哪里拥有发言权, 那就不应该直接指定一个确切的节点, 而应该<strong>用某种方式描述对节点的需求, 使 Kubernetes 选择一个符合这些需求的节点</strong>. 这恰恰可以通过节点标签和节点标签选择器完成.</p> <h6 id="_3-5-1-使用标签分类工作节点"><a href="#_3-5-1-使用标签分类工作节点" class="header-anchor">#</a> 3.5.1 使用标签分类工作节点</h6> <p>如前所述, pod 并不是唯一可以附加标签的 Kubernetes 资源. <mark><strong>标签可以附加到任何 Kubernetes 对象上, 包括节点</strong></mark>. 通常来说, 当运维团队<strong>向集群添加新节点时, 他们将通过附加标签来对节点进行分类, 这些标签指定节点提供的硬件类型, 或者任何在调度 pod 时能提供便利的其他信息</strong>.</p> <p>假设集群中的一个节点刚添加完成, 它包含一个用于通用 GPU 计算的 GPU. 我们希望向节点添加标签来展示这个功能特性, 可以通过将标签 <code>gpu=true</code>​ 添加到其中一个节点来实现(只需从 kubectl get nodes 返回的列表中选择一个):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl label <span class="token function">node</span> gke-kubia-85f6-node-0rrx <span class="token assign-left variable">gpu</span><span class="token operator">=</span>true
<span class="token function">node</span> <span class="token string">&quot;gke-kubia-85f6-node-0rrx&quot;</span> labeled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在可以在列出节点时使用标签选择器, 就像之前操作 pod 一样, 列出只包含标签 gpu=true 的节点:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get nodes <span class="token parameter variable">-l</span> <span class="token assign-left variable">gpu</span><span class="token operator">=</span>true
NAME                         STATUS     AGE
gke-kubia-85f6-node-0rrx     Ready      1d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>与预期相符, 此时只有一个节点具有此标签. 当然还可以尝试列出所有节点, 并告知 kubectl 展示一个显示每个节点的 gpu 标签值附加列(<code>kubectl get nodes -L gpu</code>​).</p> <h6 id="_3-5-2-将pod调度到特定节点"><a href="#_3-5-2-将pod调度到特定节点" class="header-anchor">#</a> 3.5.2 将pod调度到特定节点</h6> <p>现在, 假设想部署一个需要 GPU 来执行其工作的新 pod. 为了<strong>让调度器只在提供适当 GPU 的节点中进行选择</strong>, 需要在 pod 的 YAML 文件中<strong>添加一个节点选择器</strong>. 使用以下代码清单中的内容创建一个名为 <code>kubia-gpu.yaml</code>​ 的文件, 然后使用 kubectl create-f kubia-gpu.yaml 命令创建该 pod.</p> <p><strong>代码清单-3.4 使用标签选择器将 pod 调度到特定节点: kubia-gpu.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>gpu
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token comment"># 节点选择器要求Kubernetes只将pod部署到包含标签gpu=true的节点上</span>
  <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>
    <span class="token key atrule">gpu</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>这里只是在 spec 部分添加了一个 nodeSelector 字段. <strong>当创建该 pod 时, 调度器将只在包含标签 gpu=true 的节点中选择</strong>(在这个例子中, 只有一个这样的节点).</p> <h6 id="_3-5-3-调度到一个特定节点"><a href="#_3-5-3-调度到一个特定节点" class="header-anchor">#</a> 3.5.3 调度到一个特定节点</h6> <p>同样地, 也可以将 pod 调度到某个确定的节点, 由于每个节点都有一个唯一标签, 其中键为 <code>kubernetes.io/hostname</code>​, 值为该节点的实际主机名, 因此也可以将 pod 调度到某个确定的节点. 但如果节点处于离线状态, 通过 hostname 标签将 nodeSelector 设置为特定节点可能会导致 pod 不可调度. 一般绝不应该考虑单个节点, 而是应该通过标签选择器考虑符合特定标准的逻辑节点组.</p> <p>这是一个关于标签和标签选择器是如何工作, 以及如何使用它们影响 Kubernetes 操作的快速演示. 当在接下来的两章中讨论 Replication-Controllers 和 Service 时, 标签选择器的重要性和实用性也将变得更加明显.</p> <p>注意: 在第 16 章将介绍其他影响 pod 调度到哪个节点的方式.</p> <h5 id="_3-6-注解pod"><a href="#_3-6-注解pod" class="header-anchor">#</a> 3.6 注解pod</h5> <p>除标签外, <strong>pod 和其他对象还可以包含注解</strong>. 注解也是键值对, 所以它们本质上与标签非常相似. 但与标签不同, <strong>注解并不是为了保存标识信息而存在的, 它们不能像标签一样用于对对象进行分组. 当可以通过标签选择器选择对象时, 就不存在注解选择器这样的东西</strong>.</p> <p>另一方面, 注解可以容纳更多的信息, 并且<strong>主要用于工具使用</strong>. Kubernetes 也会将一些注解自动添加到对象, 但其他的注解则需要由用户手动添加.</p> <p>向 Kubernetes 引入新特性时, 通常也会使用注解. 一般来说, 新功能的 alpha 和 beta 版本不会向 API 对象引入任何新字段, 因此使用的是注解而不是字段, 一旦所需的 API 更改变得清晰并得到所有相关人员的认可, 就会引入新的字段并废弃相关注解.</p> <p>大量使用注解可以为每个 pod 或其他 API 对象添加说明, 以便每个使用该集群的人都可以快速查找有关每个单独对象的信息. 例如, 指定创建对象的人员姓名的注解可以使在集群中工作的人员之间的协作更加便利.</p> <h6 id="_3-6-1-查找对象的注解"><a href="#_3-6-1-查找对象的注解" class="header-anchor">#</a> 3.6.1 查找对象的注解</h6> <p>看一个 Kubernetes <strong>自动添加注解</strong>到前一章中创建的 pod 的注解示例. 为了查看注解, 需要获取 pod 的完整 YAML 文件或使用 kubectl describe 命令. 在下述代码清单中使用第一个方法.</p> <p><strong>代码清单-3.5 pod 的注解</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po kubia-zxzij <span class="token parameter variable">-o</span> yaml
apiVersion: v1
kind: pod
metadata:
  annotations:
    kubernetes.io/created-by: <span class="token operator">|</span>
      <span class="token punctuation">{</span><span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span><span class="token string">&quot;SerializedReference&quot;</span>, <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span><span class="token string">&quot;v1&quot;</span>,
         <span class="token string">&quot;reference&quot;</span>:<span class="token punctuation">{</span><span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span><span class="token string">&quot;ReplicationController&quot;</span>, <span class="token string">&quot;namespace&quot;</span><span class="token builtin class-name">:</span><span class="token string">&quot;default&quot;</span>, <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>正如你所见, <code>kubernetes.io/created-by</code>​ 注解保存了创建该 pod 的对象的一些 JSON 数据, 而没有涉及太多细节, 因此注解并不会是我们想要放入标签的东西. 相对而言, 标签应该简短一些, 而注解则可以包含相对更多的数据(总共不超过 256KB).</p> <p>注意: kubernetes.io/created-by 注解在版本 1.8 中已经<strong>废弃</strong>, 将会在版本 1.9 中删除, 所以在 YAML 文件中不会再看到该注解.</p> <h6 id="_3-6-2-添加和修改注解"><a href="#_3-6-2-添加和修改注解" class="header-anchor">#</a> 3.6.2 添加和修改注解</h6> <p>显然, 和标签一样, 注解可以在创建时就添加到 pod 中, 也可以在之后再对现有的 pod 进行添加或修改. 其中将注解添加到现有对象的最简单的方法是通过 kubectl annotate 命令.</p> <p>可以尝试添加注解到 kubia-manual pod 中:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl annotate pod kubia-manual mycompany.com/someannotation<span class="token operator">=</span><span class="token string">&quot;foo bar&quot;</span>
pod <span class="token string">&quot;kubia-manual&quot;</span> annotated
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>我们已将注解 <code>mycompany.com/someannotation</code>​ 添加为值 foo bar. 使用这种格式的注解键来避免键冲突是一个好方法. 当不同的工具或库向对象添加注解时, 如果它们不像刚刚那样使用唯一的前缀, 可能会意外地覆盖对方的注解.</p> <p>使用 kubectl describe 命令查看刚刚添加的注解:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe pod kubia-manual
<span class="token punctuation">..</span>.
Annotations: mycompany.com/someannotation<span class="token operator">=</span>foo bar
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h5 id="_3-7-使用命名空间对资源进行分组"><a href="#_3-7-使用命名空间对资源进行分组" class="header-anchor">#</a> 3.7 使用命名空间对资源进行分组</h5> <p>首先回到标签的概念, 前面已经看到标签是如何将 pod 和其他对象组织成组的. 由于<strong>每个对象都可以有多个标签, 因此这些对象组可以重叠</strong>. 另外, 当在集群中工作(例如通过 kubectl)时, 如果没有明确指定标签选择器, 就总能看到所有对象.</p> <p>但是, 当你想<strong>将对象分割成完全独立且不重叠的组时, 又该如何呢</strong>? 可能你每次只想在一个小组内进行操作, <strong>因此 Kubernetes 也能将对象分组到命名空间中</strong>. 这和第 2 章中讨论的用于相互隔离进程的 Linux 命名空间不一样, <strong>Kubernetes 命名空间简单地为对象名称提供了一个作用域</strong>. 此时并不会将所有资源都放在同一个命名空间中, 而是将它们<strong>组织到多个命名空间</strong>中, 这样可以允许我们多次使用相同的资源名称(跨不同的命名空间).</p> <h6 id="_3-7-1-了解对命名空间的需求"><a href="#_3-7-1-了解对命名空间的需求" class="header-anchor">#</a> 3.7.1 了解对命名空间的需求</h6> <p><strong>在使用多个 namespace 的前提下, 可以将包含大量组件的复杂系统拆分为更小的不同组, 这些不同组也可以用于在多租户环境中分配资源, 将资源分配为生产, 开发和 QA 环境</strong>, 或者以其他任何你需要的方式分配资源. <strong>资源名称只需在命名空间内保持唯一即可, 因此两个不同的命名空间可以包含同名的资源</strong>. 虽然大多数类型的资源都与命名空间相关, 但仍有一些与它无关, 其中之一便是全局且未被约束于单一命名空间的节点资源. 在后续章节还将接触到其他一些集群级别的资源.</p> <p>现在来看看如何使用命名空间.</p> <h6 id="_3-7-2-发现其他命名空间及其pod"><a href="#_3-7-2-发现其他命名空间及其pod" class="header-anchor">#</a> 3.7.2 发现其他命名空间及其pod</h6> <p>首先列出集群中的所有命名空间:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get ns
NAME              STATUS   AGE
default           Active   25h
kube-node-lease   Active   25h
kube-public       Active   25h
kube-system       Active   25h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>到目前为止, 前面只在 <strong>default</strong> 命名空间中进行操作. 当使用 kubectl get 命令列出资源时, 从未明确指定命名空间, 因此 kubectl 总是<strong>默认为 default 命名空间</strong>, 只显示该命名空间下的对象. 但从列表中可以看到还存在 kube-public 和 kube-system 命名空间. 接下来可以使用 kubectl 命令<strong>指定命名空间来列出只属于该命名空间的 pod</strong>, 如下所示为属于 kube-system 命名空间的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">--namespace</span> kube-system
NAME                               READY   STATUS    RESTARTS      AGE
coredns-5dd5756b68-96p4k           <span class="token number">1</span>/1     Running   <span class="token number">3</span> <span class="token punctuation">(</span>62m ago<span class="token punctuation">)</span>   25h
etcd-minikube                      <span class="token number">1</span>/1     Running   <span class="token number">3</span> <span class="token punctuation">(</span>62m ago<span class="token punctuation">)</span>   25h
kube-apiserver-minikube            <span class="token number">1</span>/1     Running   <span class="token number">3</span> <span class="token punctuation">(</span>62m ago<span class="token punctuation">)</span>   25h
kube-controller-manager-minikube   <span class="token number">1</span>/1     Running   <span class="token number">3</span> <span class="token punctuation">(</span>62m ago<span class="token punctuation">)</span>   25h
kube-proxy-24zm6                   <span class="token number">1</span>/1     Running   <span class="token number">3</span> <span class="token punctuation">(</span>62m ago<span class="token punctuation">)</span>   25h
kube-scheduler-minikube            <span class="token number">1</span>/1     Running   <span class="token number">3</span> <span class="token punctuation">(</span>62m ago<span class="token punctuation">)</span>   25h
storage-provisioner                <span class="token number">1</span>/1     Running   <span class="token number">6</span> <span class="token punctuation">(</span>41m ago<span class="token punctuation">)</span>   25h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>提示: 也可以使用 -n 来代替 --namespace</p> <p>本书后面会继续了解这些 pod(如果此处显示的 pod 与你系统上的 pod 不匹配, 请不用担心). 从命名空间的名称可以清楚地看到, 这些资源与 Kubernetes 系统本身是密切相关的. <strong>通过将它们放在单独的命名空间中, 可以保持一切组织良好</strong>. 如果它们都在默认的命名空间中, 同时与我们自己创建的资源混合在一起, 那么就很难区分这些资源属于哪里, 并且也可能会无意中删除一些系统资源.</p> <p>namespace 使用户能够将不属于一组的资源分到不重叠的组中. <strong>如果有多个用户或用户组正在使用同一个 Kubernetes 集群, 并且它们都各自管理自己独特的资源集合, 那么它们就应该分别使用各自的命名空间. 这样一来, 它们就不用特别担心无意中修改或删除其他用户的资源, 也无须关心名称冲突. 如前所述, 命名空间为资源名称提供了一个作用域</strong>.</p> <p>除了隔离资源, 命名空间还可用于仅允许某些用户访问某些特定资源, 甚至限制单个用户可用的计算资源数量. 关于这些内容将在第 12～14 章进行具体介绍.</p> <h6 id="_3-7-3-创建一个命名空间"><a href="#_3-7-3-创建一个命名空间" class="header-anchor">#</a> 3.7.3 创建一个命名空间</h6> <p><mark><strong>命名空间是一种和其他资源一样的 Kubernetes 资源, 因此可以通过将 YAML 文件提交到 Kubernetes API 服务器来创建该资源</strong></mark>. 现在就具体实践操作一下吧.</p> <blockquote><p>从YAML文件创建命名空间</p></blockquote> <p>首先, 创建一个包含以下代码清单内容的 <strong>custom-namespace.yaml</strong> 文件(可以在本书的代码归档中找到它).</p> <p><strong>代码清单-3.6 namespace 的 YAML 定义: custom-namespace.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token comment"># 表示定义的是一个命名空间</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Namespace
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token comment"># 命名空间的名称</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> custom<span class="token punctuation">-</span>namespace
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>现在, 使用 kubectl 将文件提交到 Kubernetes API 服务器:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> custom-namespace.yml 
namespace/custom-namespace created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>使用 kubectl create namespace 命令创建命名空间</p></blockquote> <p>虽然写出上面这样的文件并不困难, 但这仍然是一件麻烦事. 幸运的是, 还可以使用专用的 <code>kubectl create namespace 命令</code>​创建命名空间, 这比编写 YAML 文件快得多. 而之所以选择使用 YAML 文件, 只是为了<mark><strong>强化 Kubernetes 中的所有内容都是一个 API 对象</strong></mark>这一概念. 可以通过向 API 服务器提交 YAML manifest 来实现创建, 读取, 更新和删除.</p> <p>可以像这样创建命名空间:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create namespace custom-namespace
namespace <span class="token string">&quot;custom-namespace&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 尽管大多数对象的名称必须符合 RFC 1035(域名)中规定的命名规范, 这意味着它们可能只包含字母, 数字, 横杠(-)和点号, 但命名空间(和另外几个)不允许包含点号.</p> <h6 id="_3-7-4-管理其他命名空间中的对象"><a href="#_3-7-4-管理其他命名空间中的对象" class="header-anchor">#</a> 3.7.4 管理其他命名空间中的对象</h6> <p>如果想要在刚创建的命名空间中<strong>创建资源</strong>, 可以<mark><strong>选择在 metadata 字段中添加一个 namespace: custom-namespace 属性, 也可以在使用 kubectl create 命令创建资源时指定命名空间</strong></mark>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-manual.yml <span class="token parameter variable">-n</span> custom-namespace
pod/kubia-manual created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>此时就有两个<strong>同名</strong>的 pod(kubia-manual). 一个在 default 命名空间中, 另一个在 custom-namespace 中.</p> <p><strong>在列出, 描述, 修改或删除其他命名空间中的对象时, 需要给 kubectl 命令传递--namespace(或-n)选项. 如果不指定命名空间, kubectl 将在当前上下文中配置的默认命名空间中执行操作</strong>. 而当前上下文的命名空间和当前上下文本身都可以通过 kubectl config 命令进行更改. 要了解有关管理 kubectl 上下文的更多信息, 请参阅附录 A.</p> <p>提示: 要想快速切换到不同的命名空间, 可以设置以下别名: <code>alias kcd='kubectl config set-context $(kubectl config currentcontext)--namespace'</code>​. 然后, 可以使用 kcd some-namespace 在命名空间之间进行切换.</p> <h6 id="_3-7-5-命名空间提供的隔离"><a href="#_3-7-5-命名空间提供的隔离" class="header-anchor">#</a> 3.7.5 命名空间提供的隔离</h6> <p>在结束命名空间这一部分之前, 需要解释一下<strong>命名空间不提供什么</strong>--至少不是开箱即用的. 尽管命名空间将对象分隔到不同的组, 只允许你对属于特定命名空间的对象进行操作, 但<mark><strong>实际上命名空间之间并不提供对正在运行的对象的任何隔离</strong></mark>.</p> <p>例如, 你可能会认为当不同的用户在不同的命名空间中部署 pod 时, 这些 pod 应该彼此隔离, 并且无法通信, 但事实却并非如此. <strong>命名空间之间是否提供网络隔离取决于 Kubernetes 所使用的网络解决方案</strong>. 当该解决方案不提供命名空间间的网络隔离时, 如果命名空间 foo 中的某个 pod 知道命名空间 bar 中 pod 的 IP 地址, 那它就可以将流量(例如 HTTP 请求)发送到另一个 pod.</p> <h5 id="_3-8-停止和移除pod"><a href="#_3-8-停止和移除pod" class="header-anchor">#</a> 3.8 停止和移除pod</h5> <p>到目前为止, 已经创建了一些应该仍在运行的 pod. 其中有四个 pod 在 default 命名空间中运行, 一个 pod 在 custom-namespace 中运行. 由于已经不需要这些 pod 了, 所以此时考虑停止它们.</p> <h6 id="_3-8-1-按名称删除pod"><a href="#_3-8-1-按名称删除pod" class="header-anchor">#</a> 3.8.1 按名称删除pod</h6> <p>首先, 将按名称删除 kubia-gpu pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po kubia-gpu
pod <span class="token string">&quot;kubia-gpu&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在删除 pod 的过程中, 实际上在<strong>指示 Kubernetes 终止该 pod 中的所有容器. Kubernetes 向进程发送一个 SIGTERM 信号并等待一定的秒数(默认为30), 使其正常关闭. 如果它没有及时关闭, 则通过 SIGKILL 终止该进程. 因此, 为了确保你的进程总是正常关闭, 进程需要正确处理 SIGTERM 信号</strong>.</p> <p>提示: 还可以通过指定多个空格分隔的名称来删除多个 pod(例如: kubectl delete po pod1 pod2).</p> <h6 id="_3-8-2-使用标签选择器删除pod"><a href="#_3-8-2-使用标签选择器删除pod" class="header-anchor">#</a> 3.8.2 使用标签选择器删除pod</h6> <p>与根据名称指定 pod 进行删除不同, 此时将使用标签选择器的知识来停止 kubia-manual 和 kubia-manual-v2 pod. 这两个 pod 都<strong>包含标签 creation_method=manual</strong>, 因此可以通过使用一个标签选择器来删除它们:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po <span class="token parameter variable">-l</span> <span class="token assign-left variable">creation_method</span><span class="token operator">=</span>manual
pod <span class="token string">&quot;kubia-manual&quot;</span> deleted
pod <span class="token string">&quot;kubia-manual-v2&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在之前的微服务示例中, 有几十个(或可能有几百个)pod. 例如, 通过<strong>指定 rel=canary 标签</strong>选择器(如图 3.10 所示), 可以一次删除所有金丝雀 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po <span class="token parameter variable">-l</span> <span class="token assign-left variable">rel</span><span class="token operator">=</span>canary
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><img src="/img/image-20240203001911-iz7kwhe.png" alt="image" title="图3.10 通过 rel=canary 标签选择器选择并删除所有金丝雀 pod"></p> <h6 id="_3-8-3-通过删除整个命名空间来删除pod"><a href="#_3-8-3-通过删除整个命名空间来删除pod" class="header-anchor">#</a> 3.8.3 通过删除整个命名空间来删除pod</h6> <p>再回过头看看 custom-namespace 中的 pod. 此时不再需要该命名空间中的 pod, 也不需要命名空间本身. 这意味着, 可以简单地<strong>删除整个命名空间</strong>(pod 将会伴随命名空间自动删除). 现在使用以下命令删除 custom-namespace:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete ns custom-namespace
namespace <span class="token string">&quot;custom-namespace&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h6 id="_3-8-4-删除命名空间中的所有pod-但保留命名空间"><a href="#_3-8-4-删除命名空间中的所有pod-但保留命名空间" class="header-anchor">#</a> 3.8.4 删除命名空间中的所有pod,但保留命名空间</h6> <p>此时已经清理了几乎所有的东西, 但在第 2 章中用 kubectl run 命令创建的 pod 怎么样了呢? 该 pod 目前仍然在运行.</p> <p>这一次不再删除一个特定 pod, 而是通过<strong>使用 --all 选项告诉 Kubernetes 删除当前命名空间中的所有 pod</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po <span class="token parameter variable">--all</span>
pod <span class="token string">&quot;kubia-zxzij&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在再次检查有没有遗留依然在运行的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME         READY     STATUS      RESTARTS      AGE
kubia-09as0  <span class="token number">1</span>/1       Running     <span class="token number">0</span>             1d
kubia-zxzij  <span class="token number">1</span>/1       Terminating <span class="token number">0</span>             1d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>什么? ! 可以看到, 在 kubia-zxzij pod 正在终止时, 却出现一个之前并没有出现过的叫作  kubia-09as0 的新 pod. 无论进行了多少遍的全部删除 pod, 都会冒出一个名为 kubia-something 的新 pod.</p> <p>你可能还记得我们使用 kubectl run 命令创建了第一个 pod. 在第 2 章中提过<strong>这不会直接创建 pod, 而是创建一个 ReplicationCcontroller, 然后再由 ReplicationCcontroller 创建 pod. 因此只要删除由该 ReplicationCcontroller 创建的 pod, 它便会立即创建一个新的 pod. 如果想要删除该 pod, 还需要删除这个 ReplicationCcontroller</strong>.</p> <h6 id="_3-8-5-删除命名空间中的-几乎-所有资源"><a href="#_3-8-5-删除命名空间中的-几乎-所有资源" class="header-anchor">#</a> 3.8.5 删除命名空间中的(几乎)所有资源</h6> <p>通过使用单个命令删除当前命名空间中的所有资源, 可以删除 ReplicationCcontroller 和 pod, 以及创建的所有 service:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete all <span class="token parameter variable">--all</span>
pod <span class="token string">&quot;kubia-09as0&quot;</span> deleted
replicationcontroller <span class="token string">&quot;kubia&quot;</span> deleted
<span class="token function">service</span> <span class="token string">&quot;kubernetes&quot;</span> deleted
<span class="token function">service</span> <span class="token string">&quot;kubia-http&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>命令中的<strong>第一个 all 指定正在删除所有资源类型, 而 --all 选项指定将删除所有资源实例, 而不是按名称指定它们</strong>(在运行前一个删除命令时已经使用过此选项).</p> <p>注意: 使用 all 关键字删除所有内容并不是真的完全删除所有内容. 一些资源(比如将在第 7 章中介绍的 Secret)会被保留下来, 并且需要被明确指定删除.</p> <p>删除资源时, kubectl 将打印它删除的每个资源的名称. 在列表中, 可以看到在第 2 章中创建的名为 kubia 的 ReplicationController 和名为 kubia-http 的 Service.</p> <p>注意: kubectl delete all --all 命令也会删除名为 kubernetes 的 Service, 但它应该会在几分钟后自动重新创建.</p> <h5 id="_3-9-本章小结"><a href="#_3-9-本章小结" class="header-anchor">#</a> 3.9 本章小结</h5> <p>阅读本章之后, 你应该对 Kubernetes 的核心模块有了系统的了解. 在接下来的几章中学到的概念也都与 pod 有着直接关联.</p> <p>在本章中, 你应该已经掌握:</p> <ul><li>如何决定是否应将某些容器组合在一个 pod 中.</li> <li>pod 可以运行多个进程, 这和非容器世界中的物理主机类似.</li> <li>可以编写 YAML 或 JSON 描述文件用于创建 pod, 然后查看 pod 的规格及其当前状态.</li> <li>使用标签来组织 pod, 并且一次在多个 pod 上执行操作.</li> <li>可以使用节点标签将 pod 只调度到提供某些指定特性的节点上.</li> <li>注解允许人们, 工具或库将更大的数据块附加到 pod.</li> <li>命名空间可用于允许不同团队使用同一集群, 就像它们使用单独的 Kubernetes 集群一样.</li> <li>使用 kubectl explain 命令快速查看任何 Kubernetes 资源的信息.</li></ul> <p>在下一章, 将会了解到 <strong>ReplicationController</strong> 和其他管理 pod 的资源.</p> <h4 id="_4-副本机制和其他控制器-部署托管的pod"><a href="#_4-副本机制和其他控制器-部署托管的pod" class="header-anchor">#</a> 4.副本机制和其他控制器:部署托管的pod</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>保持 pod 的健康</li> <li>运行同—个 pod 的多个实例</li> <li>在节点异常之后自动重新调度 pod</li> <li>水平缩放 pod</li> <li>在集群节点上运行系统级的 pod</li> <li>运行批量任务</li> <li>调度任务定时执行或者在未来执行—次</li></ul> <p>正如前面所学到的, pod 代表了 Kubernetes 中的基本部署单元, 而且你已知道如何手动创建, 监督和管理它们. 但是在实际的用例里, 你希望你的部署能<strong>自动保持运行, 并且保持健康, 无须任何手动干预</strong>. 要做到这一点, 你几乎不会直接创建 pod, <mark><strong>而是创建 ReplicationController 或 Deployment 这样的资源, 接着由它们来创建并管理实际的 pod</strong></mark>.</p> <p>当你创建未托管的 pod(就像在前一章中创建的那些)时, 会选择一个集群节点来运行 pod, 然后在该节点上运行容器. 在本章中将了解到, Kubernetes 接下来会监控这些容器, 并且在它们失败的时候自动重新启动它们. 但是如果整个节点失败, 那么节点上的 pod 会丢失, 并且不会被新节点替换, 除非这些 pod 由前面提到的 ReplicationController 或类似资源来管理. 在本章中, 你将了解 Kubernetes 如何检查容器是否仍然存在, 如果不存在则重新启动容器. 还将学到如何运行托管的 pod-既可以无限期运行, 也可以执行单个任务, 然后终止运行.</p> <h5 id="_4-1-保持pod健康"><a href="#_4-1-保持pod健康" class="header-anchor">#</a> 4.1 保持pod健康</h5> <p>使用 Kubernetes 的一个主要好处是, 可以给 Kubernetes 一个容器列表来由其保持容器在集群中的运行. 可以通过让 Kubernetes 创建 pod 资源, 为其选择一个工作节点并在该节点上运行该 pod 的容器来完成此操作. 但是, <strong>如果其中一个容器终止, 或一个 pod 的所有容器都终止, 怎么办</strong>?</p> <p><mark><strong>只要将 pod 调度到某个节点, 该节点上的 Kubelet 就会运行 pod 的容器, 从此只要该 pod 存在, 就会保持运行. 如果容器的主进程崩溃, Kubelet 将重启容器. 如果应用程序中有一个导致它每隔一段时间就会崩溃的 bug, Kubernetes 会自动重启应用程序, 所以即使应用程序本身没有做任何特殊的事, 在 Kubernetes 中运行也能自动获得自我修复的能力</strong></mark>.</p> <p>即使进程没有崩溃, 有时应用程序也会停止正常工作. 例如, 具有内存泄漏的 Java 应用程序将开始抛出 OutOfMemoryErrors, 但 JVM 进程会一直运行. 如果有一种方法, 能让应用程序向 Kubernetes 发出信号, 告诉 Kubernetes 它<strong>运行异常并让 Kubernetes 重新启动</strong>, 那就很棒了.</p> <p>前面已经说过, 一个崩溃的容器会自动重启, 所以也许你会想到, 可以在<strong>应用中捕获这类错误, 并在错误发生时退出该进程</strong>. 当然可以这样做, 但这仍然不能解决所有的问题.</p> <p>例如, 你的应用因为无限循环或死锁而停止响应. 为确保应用程序在这种情况下可以重新启动, 必须从外部检查应用程序的运行状况, 而不是依赖于应用的内部检测.</p> <h6 id="_4-1-1-介绍存活探针"><a href="#_4-1-1-介绍存活探针" class="header-anchor">#</a> 4.1.1 介绍存活探针</h6> <p>Kubernetes 可以通过<mark><strong>存活探针(liveness probe)检查容器是否还在运行</strong></mark>. 可以<strong>为 pod 中的每个容器单独指定存活探针</strong>. 如果探测失败, Kubernetes 将定期执行探针并重新启动容器.</p> <p>注意: 将在下一章中学习到 Kubernetes 还支持<strong>就绪探针</strong>(readiness probe), 一定不要混淆两者. 它们适用于两种不同的场景.</p> <p>Kubernetes 有以下三种探测容器的机制:</p> <ul><li><strong>HTTP GET 探针</strong>对容器的 IP 地址(你指定的端口和路径)执行 HTTP GET 请求. 如果探测器收到响应, 并且响应状态码不代表错误(换句话说, 如果 HTTP 响应状态码是2xx 或3xx), 则认为探测成功. 如果服务器返回错误响应状态码或者根本没有响应, 那么探测就被认为是失败的, 容器将被重新启动.</li> <li><strong>TCP 套接字探针</strong>尝试与容器指定端口建立 TCP 连接. 如果连接成功建立, 则探测成功. 否则, 容器重新启动.</li> <li><strong>Exec 探针</strong>在容器内执行任意命令, 并检查命令的退出状态码. 如果状态码是 0, 则探测成功. 所有其他状态码都被认为失败.</li></ul> <h6 id="_4-1-2-创建基于http的存活探针"><a href="#_4-1-2-创建基于http的存活探针" class="header-anchor">#</a> 4.1.2 创建基于HTTP的存活探针</h6> <p>来看看如何为你的 Node.js 应用添加一个存活探针. 因为它是一个 Web 应用程序, 所以添加一个存活探针来检查其 Web 服务器是否提供请求是有意义的. 但是因为这个 Node.js 应用程序太简单了, 所以不得不人为地让它失败.</p> <p>要正确演示存活探针, 需要稍微修改应用程序. 在第五个请求之后, 给每个请求返回 HTTP 状态码500(Internal Server Error)--应用程序将正确处理前五个客户端请求, 之后每个请求都会返回错误. 多亏了存活探针, 应用在这个时候会重启, 使其能够再次正确处理客户端请求.</p> <p>可以在本书的代码档案中找到新应用程序的代码(在 Chapter04/kubia-unhealthy 文件夹中). 笔者已经将容器镜像推送到 Docker Hub, 因此你不需要自己构建它了.</p> <p>下面将创建一个包含 HTTP GET 存活探针的新 pod, 下面的代码清单显示了 pod 的 yaml.</p> <p><strong>代码清单-4.1 将存活探针添加到 pod:kubia-liveness-probe.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>liveness
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token comment"># 指定镜像</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">-</span>unhealthy
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
    <span class="token comment"># 配置存活探针</span>
    <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>
      <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>
        <span class="token key atrule">path</span><span class="token punctuation">:</span> /
        <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>该 pod 的描述文件<strong>定义了一个 httpGet 存活探针</strong>, 该探针告诉 Kubernetes 定期在端口 8080 路径上执行 HTTP GET 请求, 以确定该容器是否健康. 这些请求在容器运行后立即开始.</p> <p>经过五次这样的请求(或实际的客户端请求)后, 应用程序开始返回 HTTP 状态码 500, Kubernetes 会认为探测失败并重启容器.</p> <h6 id="_4-1-3-使用存活探针"><a href="#_4-1-3-使用存活探针" class="header-anchor">#</a> 4.1.3 使用存活探针</h6> <p>要查看存活探针是如何工作的, 请尝试立即创建该 pod. 大约一分半钟后, 容器将重启. 可以通过运行 kubectl get 看到:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po kubia-liveness
NAME             READY     STATUS     RESTARTS     AGE
kubia-liveness   <span class="token number">1</span>/1       Running    <span class="token number">1</span>            2m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>RESTARTS 列显示 pod 的容器已被重启一次(如果你再等一分半钟, 它会再次重启, 然后无限循环下去).</p> <blockquote><p>获取崩溃容器的应用日志</p></blockquote> <p>在前一章学习了如何使用 kubectl logs 打印应用程序的日志. 如果容器重启, kubectl logs 命令将显示<strong>当前容器的日志</strong>. 当你想知道为什么前一个容器终止时, 你想看到的是<strong>前一个容器的日志</strong>, 而不是当前容器的. 可以通过<strong>添加 --previous 选项</strong>来完成:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs mypod <span class="token parameter variable">--previous</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以通过查看 kubectl describe 的内容来了解为什么必须重启容器, 如下面的代码清单所示.</p> <p><strong>代码清单-4.2 重启容器后的 pod 描述</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe po kubia-liveness
Name: kubia-liveness
<span class="token punctuation">..</span>.
Containers:
  kubia:
    Container ID: docker://480986f8
    Image: luksa/kubia-unhealthy
    Image ID: docker://sha256:2b208508
    Port:
    <span class="token comment"># 当前状态是running</span>
    State: Running
      Started: Sun, <span class="token number">14</span> May <span class="token number">2017</span> <span class="token number">11</span>:41:40 +0200
    <span class="token comment"># 前一个容器被终止并且错误码为137退出</span>
    Last State: Terminated
      Reason: Error
      Exit Code: <span class="token number">137</span>
      Started: Mon, 01 Jan 0001 00:00:00 +0000
      Finished: Sun, <span class="token number">14</span> May <span class="token number">2017</span> <span class="token number">11</span>:41:38 +0200
    Ready: True
    <span class="token comment"># 容器已经被重启1次</span>
    Restart Count: <span class="token number">1</span>
    Liveness: http-get http://:8080/ <span class="token assign-left variable">delay</span><span class="token operator">=</span>0s <span class="token assign-left variable">timeout</span><span class="token operator">=</span>1s
    <span class="token assign-left variable">period</span><span class="token operator">=</span>10s <span class="token comment">#success=1 #failure=3</span>
<span class="token punctuation">..</span>.
Events:
<span class="token punctuation">..</span>. Killing container with <span class="token function">id</span> docker://95246981:pod <span class="token string">&quot;kubia-liveness ...&quot;</span>
container <span class="token string">&quot;kubia&quot;</span> is unhealthy, it will be killed and re-created.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p>可以看到容器现在正在运行, 但之前由于错误而终止. 退出代码为 137, 这有特殊的含义--表示<strong>该进程由外部信号终止</strong>. 数字 137 是两个数字的总和: 128+x, 其中 x 是终止进程的信号编号. 在这个例子中, x 等于 9, 这是 SIGKILL 的信号编号, 意味着这个进程被强行终止.</p> <p>在底部列出的事件显示了容器为什么终止--<strong>Kubernetes 发现容器不健康, 所以终止并重新创建</strong>.</p> <p>注意: <strong>当容器被强行终止时, 会创建一个全新的容器, 而不是重启原来的容器</strong>.</p> <h6 id="_4-1-4-配置存活探针的附加属性"><a href="#_4-1-4-配置存活探针的附加属性" class="header-anchor">#</a> 4.1.4 配置存活探针的附加属性</h6> <p>你可能已经注意到, kubectl describe 还显示关于存活探针的附加信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>Liveness: http-get http://:8080/ <span class="token assign-left variable">delay</span><span class="token operator">=</span>0s <span class="token assign-left variable">timeout</span><span class="token operator">=</span>1s <span class="token assign-left variable">period</span><span class="token operator">=</span>10s <span class="token comment">#success=1#failure=3</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>除了明确指定的存活探针选项, 还可以看到其他属性, 例如 delay(延迟), timeout(超时), period(周期)等. delay=0s 部分显示在容器启动后立即开始探测. timeout 仅设置为 1 秒, 因此容器必须在 1 秒内进行响应, 不然这次探测记作失败. 每 10 秒探测一次容器(period=10s), 并在探测连续三次失败(#failure=3)后<strong>重启容器</strong>.</p> <p><strong>定义探针时可以自定义这些附加参数</strong>. 例如, 要设置初始延迟, 请将 initialDelaySeconds 属性添加到存活探针的配置中, 如下面的代码清单所示.</p> <p><strong>代码清单-4.3 具有初始延迟的存活探针: kubia-liveness-probe-initial-delay.yaml</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  <span class="token comment"># 指定镜像</span>
  - image: luksa/kubia-unhealthy
    name: kubia
    <span class="token comment"># 配置存活探针</span>
    livenessProbe:
      httpGet:
        path: /
        port: <span class="token number">8080</span>
      <span class="token comment"># 15秒之后才开始探活</span>
      initialDelaySeconds: <span class="token number">15</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>如果没有设置初始延迟, <strong>探针将在启动时立即开始探测容器</strong>, 这通常会导致探测失败, 因为应用程序还没准备好开始接收请求. 如果失败次数超过阈值, 在应用程序能正确响应请求之前, 容器就会重启.</p> <p>提示: <strong>务必记得设置一个初始延迟来说明应用程序的启动时间</strong>.</p> <p>很多场合都会看到这种情况, 用户很困惑为什么他们的容器正在重启. 但是如果使用 kubectl describe, 他们会看到容器以退出码 137 或 143 结束, 并告诉他们该 pod 是被迫终止的. 此外, pod 事件的列表将显示容器因 liveness 探测失败而被终止. 如果你在 pod 启动时看到这种情况, 那是因为<strong>未能适当设置 initialDelaySeconds</strong>.</p> <p>注意: 退出代码 137 表示进程被外部信号终止, 退出代码为 128+9(SIGKILL). 同样, 退出代码 143 对应于 128+15(SIGTERM).</p> <h6 id="_4-1-5-创建有效的存活探针"><a href="#_4-1-5-创建有效的存活探针" class="header-anchor">#</a> 4.1.5 创建有效的存活探针</h6> <p><mark><strong>对于在生产中运行的 pod, 一定要定义一个存活探针</strong></mark>. 没有探针的话, Kubernetes 无法知道应用是否还活着. 只要进程还在运行, Kubernetes 会认为容器是健康的.</p> <blockquote><p>存活探针应该检查什么</p></blockquote> <p><strong>简易的存活探针仅仅检查了服务器是否响应</strong>. 虽然这看起来可能过于简单, 但即使是这样的存活探针也可以创造奇迹, 因为如果容器内运行的 web 服务器停止响应 HTTP 请求, 它将重启容器. 与没有存活探针相比, 这是一项重大改进, 而且在大多数情况下可能已足够.</p> <p><mark><strong>但为了更好地进行存活检查, 需要将探针配置为请求特定的 URL 路径(例如 /health), 并让应用从内部对内部运行的所有重要组件执行状态检查, 以确保它们都没有终止或停止响应</strong></mark>.</p> <p>提示: 请确保 /health HTTP 端点<strong>不需要认证</strong>, 否则探测会一直失败, 导致你的容器无限重启.</p> <p>一定要检查应用程序的内部, 而没有任何外部因素的影响. 例如, 当服务器无法连接到后端数据库时, 前端 Web 服务器的存活探针不应该返回失败. 如果问题的底层原因在数据库中, 重启 Web 服务器容器不会解决问题. 由于存活探测将再次失败, 你将反复重启容器直到数据库恢复.</p> <blockquote><p>保持探针轻量</p></blockquote> <p><strong>存活探针不应消耗太多的计算资源, 并且运行不应该花太长时间</strong>. 默认情况下, 探测器执行的频率相对较高, 必须在一秒之内执行完毕. 一个过重的探针会大大减慢你的容器运行. 在本书的后面, 还将学习如何限制容器可用的 CPU 时间. 探针的 CPU 时间计入容器的 CPU 时间配额, 因此使用重量级的存活探针将减少主应用程序进程可用的 CPU 时间.</p> <p>如果<strong>在容器中运行 Java 应用程序, 请确保使用 HTTP GET 存活探针</strong>, 而不是启动全新 JVM 以获取存活信息的 Exec 探针. 任何基于 JVM 或类似的应用程序也是如此, 它们的启动过程需要大量的计算资源.</p> <blockquote><p>无须在探针中实现重试循环</p></blockquote> <p>你已经看到, 探针的失败阈值是可配置的, 并且通常在容器被终止之前探针必须失败多次. 但即使你将失败阈值设置为 1, Kubernetes 为了确认一次探测的失败, 会尝试若干次. 因此在探针中自己实现重试循环是浪费精力.</p> <blockquote><p>存活探针小结</p></blockquote> <p>你现在知道 Kubernetes 会在你的容器崩溃或其存活探针失败时, 通过重启容器来保持运行. 这项任务由承载 pod 的节点上的 Kubelet 执行-在主服务器上运行的 Kubernetes Control Plane 组件不会参与此过程.</p> <p>但<strong>如果节点本身崩溃, 那么 Control Plane 必须为所有随节点停止运行的 pod 创建替代品</strong>. 它不会直接创建的 pod 执行此操作. 这些 pod 只被 Kubelet 管理, 但由于 Kubelet 本身运行在节点上, 所以如果节点异常终止, 它将无法执行任何操作.</p> <p>为了确保应用程序在另一个节点上重新启动, 需要使用 ReplicationController 或类似机制管理 pod, 下面将在本章其余部分讨论该机制.</p> <h5 id="_4-2-了解replicationcontroller"><a href="#_4-2-了解replicationcontroller" class="header-anchor">#</a> 4.2 了解ReplicationController</h5> <p><mark><strong>ReplicationController 是一种 Kubernetes 资源, 可确保它的 pod 始终保持运行状态. 如果 pod 因任何原因消失(例如节点从集群中消失或由于该 pod 已从节点中逐出), 则 ReplicationController 会注意到缺少了 pod 并创建替代 pod</strong></mark>.</p> <p>图 4.1 显示了当一个节点下线且带有两个 pod 时会发生什么. pod A 是被直接创建的, 因此是非托管的 pod, 而 pod B 由 ReplicationController 管理. 节点异常退出后, ReplicationController 会创建一个新的 pod(pod B2)来替换缺少的 pod B, 而 pod A 完全丢失, 因为没有东西负责重建它.</p> <p>图中的 ReplicationController 只管理一个 pod, <strong>但一般而言, ReplicationController 旨在创建和管理一个 pod 的多个副本(replicas)</strong> . 这就是 ReplicationController 名字的由来.</p> <p><img src="/img/image-20240227231656-vlgmv28.png" alt="image" title="图4.1 节点故障时, 只有 ReplicationController 管理的 pod 被重新创建"></p> <h6 id="_4-2-1-replicationcontroller的操作"><a href="#_4-2-1-replicationcontroller的操作" class="header-anchor">#</a> 4.2.1 ReplicationController的操作</h6> <p><strong>ReplicationController 会持续监控正在运行的 pod 列表, 并保证相应 &quot;类型&quot; 的 pod 的数目与期望相符. 如正在运行的 pod 太少, 它会根据 pod 模板创建新的副本. 如正在运行的 pod 太多, 它将删除多余的副本</strong>. 你可能会对有多余的副本感到奇怪. 这可能有几个原因:</p> <ul><li>有人会手动创建相同类型的 pod.</li> <li>有人更改现有的 pod 的&quot;类型&quot;.</li> <li>有人减少了所需的 pod 的数量, 等等.</li></ul> <p>笔者已经使用过几次 pod &quot;类型&quot; 这种说法, 但这是不存在的. ReplicationController 不是根据 pod 类型来执行操作的, 而是<strong>根据 pod 是否匹配某个标签选择器</strong>(前一章中了解了它们).</p> <blockquote><p>介绍控制器的协调流程</p></blockquote> <p><mark><strong>ReplicationController 的工作是确保 pod 的数量始终与其标签选择器匹配. 如果不匹配, 则 ReplicationController 将根据所需, 采取适当的操作来协调 pod 的数量</strong></mark>. 图 4.2 显示了 ReplicationController 的操作.</p> <p><img src="/img/image-20240227231717-6wdqi7v.png" alt="image" title="图 4.2 一个 ReplicationController 的协调流程"></p> <blockquote><p>了解 ReplicationController 的三部分</p></blockquote> <p>一个 ReplicationController 有三个主要部分(如图 4.3 所示):</p> <ul><li><strong>label selector(标签选择器), 用于确定 ReplicationController 作用域中有哪些 pod</strong></li> <li><strong>replica count(副本个数), 指定应运行的 pod 数量</strong></li> <li><strong>pod template(pod 模板), 用于创建新的 pod 副本</strong></li></ul> <p><img src="/img/image-20240203103519-r74szc4.png" alt="image" title="图4.3 ReplicationController 的三个关键部分(pod 选择器, 副本个数和 pod 模板)"></p> <p><strong>ReplicationController 的副本个数, 标签选择器, 甚至是 pod 模板都可以随时修改, 但只有副本数目的变更会影响现有的 pod</strong>.</p> <blockquote><p>更改控制器的标签选择器或 pod 模板的效果</p></blockquote> <p>更改标签选择器和 pod 模板对现有 pod 没有影响. 更改标签选择器会使现有的 pod 脱离 ReplicationController 的范围, 因此控制器会<strong>停止关注它们</strong>. 在创建 pod 后, ReplicationController 也不关心其 pod 的实际 &quot;内容&quot;(容器镜像, 环境变量及其他). 因此, 该模板仅影响由此 ReplicationController 创建的新 pod. 可以将其视为创建新 pod 的曲奇切模(cookie cutter).</p> <blockquote><p>使用 ReplicationController 的好处</p></blockquote> <p>像 Kubernetes 中的许多事物一样, ReplicationController 尽管是一个令人难以置信的简单概念, 却提供或启用了以下强大功能:</p> <ul><li><strong>确保一个 pod(或多个 pod 副本)持续运行</strong>, 方法是在现有 pod 丢失时启动一个新 pod.</li> <li>集群节点发生故障时, 它<strong>将为故障节点上运行的所有 pod(即受 ReplicationController 控制的节点上的那些 pod)创建替代副本</strong>.</li> <li>它能轻松<strong>实现 pod 的水平伸缩</strong>-手动和自动都可以(参见第 15 章中的 pod 的水平自动伸缩).</li></ul> <p>注意: pod 实例永远不会重新安置到另一个节点. 相反, ReplicationController 会<strong>创建一个全新的 pod 实例, 它与正在替换的实例无关</strong>.</p> <h6 id="_4-2-2-创建一个replicationcontroller"><a href="#_4-2-2-创建一个replicationcontroller" class="header-anchor">#</a> 4.2.2 创建一个ReplicationController</h6> <p>下面了解一下如何创建一个 ReplicationController, 然后看看它如何让你的 pod 运行. 就像 pod 和其他 Kubernetes 资源, 可以通过<strong>上传 JSON 或 YAML 描述文件到 Kubernetes API 服务器来创建 ReplicationController</strong>.</p> <p>你将为你的 ReplicationController 创建名为 <strong>kubia-rc.yaml</strong> 的 YAML 文件, 如下面的代码清单所示.</p> <p><strong>代码清单-4.4 ReplicationController 的 YAML 定义: kubia-rc.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ReplicationController     <span class="token comment"># 这里资源类型定义ReplicationController(RC)</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia     <span class="token comment"># ReplicationController的名字</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>     <span class="token comment"># pod实例的目标数目</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>       <span class="token comment"># pod选择器决定了RC的操作对象</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
  <span class="token key atrule">template</span><span class="token punctuation">:</span>       <span class="token comment"># 下面都是创建pod所用的pod模版</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
        <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>上传文件到 API 服务器时, Kubernetes 会创建一个<strong>名为 kubia 的新 ReplicationController, 它确保符合标签选择器 app=kubia 的 pod 实例始终是三个</strong>. 当没有足够的 pod 时, 根据提供的 pod 模板创建新的 pod. 模板的内容与前一章中创建的 pod 定义几乎相同.</p> <p>模板中的 <strong>pod 标签显然必须和 ReplicationController 的标签选择器匹配, 否则控制器将无休止地创建新的容器</strong>. 因为启动新 pod 不会使实际的副本数量接近期望的副本数量. 为了防止出现这种情况, API 服务会校验 ReplicationController 的定义, 不会接收错误配置.</p> <p>根本不指定选择器也是一种选择. 在这种情况下, 它会自动根据 pod 模板中的标签自动配置.</p> <p>提示: <strong>定义 ReplicationController 时不要指定 pod 选择器, 让 Kubernetes 从 pod 模板中提取它. 这样 YAML 更简短</strong>.</p> <p>要创建 ReplicationController, 请使用已知的 kubectl create 命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-rc.yml
replicationcontroller/kubia created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>一旦创建了 ReplicationController, 它就开始工作. 来看看它都会做什么.</p> <h6 id="_4-2-3-使用replicationcontroller"><a href="#_4-2-3-使用replicationcontroller" class="header-anchor">#</a> 4.2.3 使用ReplicationController</h6> <p>由于<strong>没有任何 pod 有 app=kubia 标签, ReplicationController 会根据 pod 模板启动三个新的 pod</strong>. 列出 pod 以查看 ReplicationController 是否完成了它应该做的事情:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-hw7qj   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          84s
kubia-lbjxr   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          84s
kubia-mknn7   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          84s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>它确实创建了三个 pod. 现在 ReplicationController 正在管理这三个 pod. 接下来, 你将通过稍稍破坏它们来观察 ReplicationController 如何响应.</p> <blockquote><p>查看 ReplicationController 对已删除的 pod 的响应</p></blockquote> <p>首先, 手动删除其中一个 pod, 以查看 ReplicationController 如何立即启动新容器, 从而将匹配容器的数量恢复为三:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete pod kubia-hw7qj
pod <span class="token string">&quot;kubia-hw7qj&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以发现已经自动创建一个新的 pod(AGE 最小的那个就是):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-lbjxr   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          2m20s
kubia-lgfcw   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          33s
kubia-mknn7   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          2m20s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>ReplicationController 再次完成了它的工作. 这是非常有用的.</p> <blockquote><p>获取有关 ReplicationController 的信息</p></blockquote> <p>通过 kubectl get 命令显示的关于 ReplicationController 的信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   <span class="token number">3</span>         <span class="token number">3</span>         <span class="token number">3</span>       3m9s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: <strong>使用 rc 作为 replicationcontroller 的简写</strong>.</p> <p>可以看到三列显示了<strong>所需的 pod 数量, 实际的 pod 数量, 以及其中有多少 pod 已准备就绪</strong>(当我们在下一章谈论准备就绪探针时, 将了解这些含义). 可以通过 kubectl describe 命令看到 ReplicationController 的附加信息.</p> <p><strong>代码清单-4.5 显示使用 kubectl describe 的 ReplicationController 的详细信息</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe rc kubia
Name:         kubia
Namespace:    default
Selector:     <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
Labels:       <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
Annotations:  <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token comment"># pod实例的实际数量与目标数量</span>
Replicas:     <span class="token number">3</span> current / <span class="token number">3</span> desired
<span class="token comment"># 每种状态下的pod数量</span>
Pods Status:  <span class="token number">3</span> Running / <span class="token number">0</span> Waiting / <span class="token number">0</span> Succeeded / <span class="token number">0</span> Failed
Pod Template:
  Labels:  <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
  Containers:
   kubia:
    Image:        luksa/kubia
    Port:         <span class="token number">8080</span>/TCP
    Host Port:    <span class="token number">0</span>/TCP
    Environment:  <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
    Mounts:       <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
  Volumes:        <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token comment"># 与这个ReplicationController有关的事件, 下面列出总共创建了四个容器</span>
Events:
  Type    Reason            Age    From                    Message
  ----    ------            ----   ----                    -------
  Normal  SuccessfulCreate  3m59s  replication-controller  Created pod: kubia-hw7qj
  Normal  SuccessfulCreate  3m59s  replication-controller  Created pod: kubia-lbjxr
  Normal  SuccessfulCreate  3m59s  replication-controller  Created pod: kubia-mknn7
  Normal  SuccessfulCreate  2m12s  replication-controller  Created pod: kubia-lgfcw
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><p>当前的副本数与所需的数量相符, 因为控制器已经创建了一个新的 pod. 它显示了四个正在运行的 pod, 因为被终止的 pod 仍在运行中, 尽管它并未计入当前的副本个数中. 底部的事件列表显示了 ReplicationController 的行为--它到目前为止创建了四个 pod.</p> <blockquote><p>控制器如何创建新的pod</p></blockquote> <p><mark><strong>控制器通过创建一个新的替代 pod 来响应 pod 的删除操作</strong></mark>(见图 4.4). 从技术上讲, 它并没有对删除本身做出反应, 而是<mark><strong>针对由此产生的状态--pod 数量不足</strong></mark>.</p> <p>虽然 ReplicationController 会立即收到删除 pod 的通知(API 服务器允许客户端监听资源和资源列表的更改), 但这不是它创建替代 pod 的原因. 该通知会<strong>触发控制器检查实际的 pod 数量并采取适当的措施</strong>.</p> <p><img src="/img/image-20240227231756-d5wmw5b.png" alt="image" title="图4.4 如果一个 pod 消失, ReplicationController 将发现 pod 数目更少并创建一个新的替代 pod(图中实例名称可能不一样因为和本地跑的名称不一致)"></p> <blockquote><p>应对节点故障</p></blockquote> <p>看着 ReplicationController 对手动删除 pod 做出响应没什么意思, 所以来看一个更好的示例. 如果使用 Google Kubernetes Engine 来运行这些示例, 那么已经有一个三节点 Kubernetes 集群. 你将<strong>从网络中断开其中一个节点来模拟节点故障</strong>.</p> <p>注意: 如果使用 Minikube, 则无法做这个练习, 因为<strong>只有一个节点同时充当主节点和工作节点</strong>.</p> <p>如果节点在没有 Kubernetes 的场景中发生故障, 运维人员需要手动将节点上运行的应用程序迁移到其他机器. 而现在, Kubernetes 会自动执行此操作. 在 ReplicationController 检测到它的 pod 已关闭后不久, 它将启动新的 pod 以替换它们.</p> <p>下面在实践中看看这个行为. 需要使用 gcloud compute ssh 命令 ssh 进入其中一个节点, 然后使用 sudo ifconfig eth0 down <strong>关闭其网络接口</strong>, 如下面的代码清单所示.</p> <p>注意: 通过使用 -o wide 选项列出 pod, 选择至少运行一个 pod 的节点.</p> <p><strong>代码清单-4.6 通过关闭网络接口来模拟节点故障</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud compute <span class="token function">ssh</span> gke-kubia-default-pool-b46381f1-zwko
Enter passphrase <span class="token keyword">for</span> key <span class="token string">'/home/luksa/.ssh/google_compute_engine'</span><span class="token builtin class-name">:</span>
Welcome to Kubernetes v1.6.4<span class="token operator">!</span>
<span class="token punctuation">..</span>.
luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ <span class="token function">sudo</span> <span class="token function">ifconfig</span> eth0 down
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>当关闭网络接口时, ssh 会话将停止响应, 所以需要打开另一个终端或强行退出 ssh 会话. 在新终端中, 可以列出节点以查看 Kubernetes 是否检测到节点下线. 这需要一分钟左右的时间. 然后, 该节点的状态显示为 NotReady:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">node</span>
NAME                                     STATUS     AGE
gke-kubia-default-pool-b46381f1-opc5     Ready      5h
gke-kubia-default-pool-b46381f1-s8gj     Ready      5h
gke-kubia-default-pool-b46381f1-zwko     NotReady   5h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>如果现在列出 pod, 那么你仍然会看到三个与之前相同的 pod, 因为 Kubernetes 在重新调度 pod 之前会等待一段时间(如果节点因临时网络故障或 Kubelet 重新启动而无法访问). 如果节点在几分钟内无法访问, 则调度到该节点的 pod 的状态将变为 <strong>Unknown</strong>. 此时, ReplicationController 将立即启动一个新的 pod. 可以通过再次列出 pod 来看到这一点:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods
NAME             READY         STATUS     RESTARTS     AGE
kubia-oini2      <span class="token number">1</span>/1           Running    <span class="token number">0</span>            10m
kubia-k0xz6      <span class="token number">1</span>/1           Running    <span class="token number">0</span>            10m
kubia-q3vkg      <span class="token number">1</span>/1           Unknown    <span class="token number">0</span>            10m
kubia-dmdck      <span class="token number">1</span>/1           Running    <span class="token number">0</span>            5s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>注意 pod 的存活时间, 你会发现 kubia-dmdck pod 是新的. 你再次拥有三个运行的 pod 实例, 这意味着 ReplicationController 再次开始它的工作, 将系统的实际状态置于所需状态.</p> <p>如果一个节点不可用(发生故障或无法访问), 会发生同样的情况. 立即进行人为干预就没有必要了. 系统会自我修复. 要恢复节点, 需要使用以下命令重置它:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>当节点再次启动时, 其状态应该返回到 Ready, 并且状态为 Unknown 的 pod 将被删除.</p> <h6 id="_4-2-4-将pod移入或移出replicationcontroller的作用域"><a href="#_4-2-4-将pod移入或移出replicationcontroller的作用域" class="header-anchor">#</a> 4.2.4 将pod移入或移出ReplicationController的作用域</h6> <p><mark><strong>由 ReplicationController 创建的 pod 并不是绑定到 ReplicationController. 在任何时刻, ReplicationController 管理与标签选择器匹配的 pod. 通过更改 pod 的标签, 可以将它从 ReplicationController 的作用域中添加或删除. 它甚至可以从一个 ReplicationController 移动到另一个</strong></mark>.</p> <p>提示: 尽管一个 pod 没有绑定到一个 ReplicationController, 但该 pod 在 <strong>metadata.ownerReferences</strong> 字段中引用它, 可以轻松使用它来找到一个 pod 属于哪个 ReplicationController.</p> <p><mark><strong>如果更改了一个 pod 的标签, 使它不再与 ReplicationController 的标签选择器相匹配, 那么该 pod 就变得和其他手动创建的 pod 一样了. 它不再被任何东西管理. 如果运行该节点的 pod 异常终止, 它显然不会被重新调度. 但请记住, 当更改 pod 的标签时, ReplicationController 发现一个 pod 丢失了, 并启动一个新的 pod 替换它</strong></mark>.</p> <p>让我们通过你的 pod 试试看. 由于你的 ReplicationController 管理具有 app=kubia 标签的 pod, 因此需要删除这个标签或修改其值以将该 pod 移出 ReplicationController 的管理范围. 添加另一个标签并没有用, 因为 ReplicationController 不关心该 pod 是否有任何附加标签, <strong>它只关心该 pod 是否具有标签选择器中引用的所有标签</strong>.</p> <blockquote><p>给 ReplicationController 管理的 pod 加标签</p></blockquote> <p>需要确认的是, 如果向 ReplicationController 管理的 pod <strong>添加其他标签, 它并不关心</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 添加其他标签</span>
$ kubectl label pod kubia-mknn7 <span class="token assign-left variable">type</span><span class="token operator">=</span>special
pod/kubia-mknn7 labeled
<span class="token comment"># 查看pod</span>
$ kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-lbjxr   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          17m   <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
kubia-lgfcw   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          15m   <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
kubia-mknn7   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          17m   <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia,type<span class="token operator">=</span>special
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>给其中一个 pod 添加了 type=special 标签, 再次列出所有 pod 会显示和以前一样的三个 pod. 因为从 ReplicationController 角度而言, <strong>没发生任何更改</strong>.</p> <blockquote><p>更改已托管的 pod 的标签</p></blockquote> <p>现在, 更改 app=kubia 标签. <strong>这将使该 pod 不再与 ReplicationController 的标签选择器相匹配, 只剩下两个匹配的 pod. 因此, ReplicationController 会启动一个新的 pod</strong>, 将数目恢复为三:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl label pod kubia-mknn7 <span class="token assign-left variable">app</span><span class="token operator">=</span>foo <span class="token parameter variable">--overwrite</span>
pod/kubia-mknn7 labeled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>​<code>--overwrite</code>​ 参数是必要的, 否则 kubectl 将只打印出警告, 并不会更改标签. 这样是为了防止你想要添加新标签时无意中更改现有标签的值. 再次列出所有 pod 时会显示四个 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods <span class="token parameter variable">-L</span> app
NAME          READY   STATUS    RESTARTS   AGE     APP
kubia-bttsd   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          4m31s   kubia    <span class="token comment"># 新创建的pod</span>
kubia-lbjxr   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          29m     kubia
kubia-lgfcw   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          27m     kubia
kubia-mknn7   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          29m     foo     <span class="token comment"># 这个pod标签变了, 不再由rc管理</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>注意: 使用 -L app 选项在列中显示 app 标签.</p> <p>现在有四个 pod: 一个不是由你的 ReplicationController 管理的, 其他三个是. 其中包括新建的 pod.</p> <p>图 4.5 说明了<strong>当更改 pod 的标签, 使得它们不再与 ReplicationController 的 pod 选择器匹配时, 发生的事情</strong>. 可以看到三个 pod 和 ReplicationController. 在将 pod 的标签从 app=kubia 更改为 app=foo 之后, ReplicationController 就不管这个 pod 了. 由于控制器的副本个数设置为 3, 并且只有两个 pod 与标签选择器匹配, 所以 ReplicationController 启动 kubia-2qneh pod(这个名字与实际跑的不一致忽略即可), 使总数回到了三. kubiad-mdck pod 现在是完全独立的, 并且会一直运行直到你手动删除它(现在可以这样做, 因为不再需要它).</p> <p><img src="/img/image-20240227231829-4e3r828.png" alt="image" title="图4.5 通过更改标签从 ReplicationController 的作用域中删除一个 pod"></p> <blockquote><p>从控制器删除 pod</p></blockquote> <p>当你想操作特定的 pod 时, 从 ReplicationController 管理范围中移除 pod 的操作很管用. 例如, 你可能有一个 bug 导致 pod 在特定时间或特定事件后开始出问题. 如果你知道某个 pod 发生了故障, 就可以将它从 Replication-Controller 的管理范围中移除, <strong>让控制器将它替换为新 pod, 接着这个 pod 就任你处置了, 比如可以用于继续排查问题. 完成后删除该 pod 即可</strong>.</p> <blockquote><p>更改 ReplicationController 的标签选择器</p></blockquote> <p>这里有个练习, 看看你是否完全理解了 ReplicationController: <strong>如果不是更改某个 pod 的标签而是修改了 ReplicationController 的标签选择器, 你认为会发生什么</strong>?</p> <p>如果你的答案是 &quot;<strong>它会让所有的 pod 脱离 ReplicationController 的管理, 导致它创建三个新的 pod</strong>&quot;, 那么恭喜你, 答对了. 这表明你了解了 ReplicationController 的工作方式. Kubernetes 确实允许更改 ReplicationController 的标签选择器, 但这不适用于本章后半部分中介绍的其他资源(也是用来管理 pod 的).</p> <p>你永远不会修改控制器的标签选择器, 但你会<strong>时不时会更改它的 pod 模板</strong>. 下面来了解一下吧.</p> <h6 id="_4-2-5-修改pod模板"><a href="#_4-2-5-修改pod模板" class="header-anchor">#</a> 4.2.5 修改pod模板</h6> <p><strong>ReplicationController 的 pod 模板可以随时修改</strong>. 更改 pod 模板就<strong>像用一个曲奇刀替换另一个</strong>. 它只会影响你之后切出的曲奇, 并且<strong>不会影响你已经剪切的曲奇</strong>(见图 4.6). 要修改旧的 pod, 需要删除它们, 并让 ReplicationController 根据新模板将其替换为新的 pod.</p> <p><img src="/img/image-20240227231849-5av1lvo.png" alt="image" title="图4.6 更改 ReplicationController 的 pod 模板只影响之后创建的 pod, 并且不会影响现有的 pod"></p> <p>可以试着编辑 ReplicationController 并向 pod 模板添加标签. 使用以下<strong>命令编辑</strong> ReplicationController:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit rc kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这将在文本编辑器中<strong>打开 ReplicationController 的 YAML 配置</strong>. 找到 pod 模板部分并向元数据<strong>添加一个新的标签</strong>. 保存更改并退出编辑器后, kubectl 将更新 ReplicationController 并打印以下消息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>replicationcontroller <span class="token string">&quot;kubia&quot;</span> edited
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在可以再次列出 pod 及其标签, 并确认它们未发生变化. 但是<strong>如果删除了这个 pod 并等待其替代 pod 创建, 你会看到新的标签</strong>.</p> <p><mark><strong>像这样编辑一个 ReplicationController, 来更改容器模板中的容器图像, 删除现有的容器, 并让它们替换为新模板中的新容器, 可以用于升级 pod</strong></mark>, 后面将在第 9 章学到更好的方法.</p> <h6 id="_4-2-6-水平缩放pod"><a href="#_4-2-6-水平缩放pod" class="header-anchor">#</a> 4.2.6 水平缩放pod</h6> <p>你已经看到了 ReplicationController 如何确保持续运行的 pod 实例数量保持不变. 因为改变副本的所需数量非常简单, 所以这也意味着<strong>水平缩放 pod 很简单</strong>.</p> <p><strong>放大或者缩小 pod 的数量规模就和在 ReplicationController 资源中更改 Replicas 字段的值一样简单</strong>. 更改之后, ReplicationController 将会看到存在太多的 pod 并删除其中的一部分(缩容时), 或者看到它们数目太少并创建 pod(扩容时).</p> <blockquote><p>ReplicationController 扩容</p></blockquote> <p>ReplicationController 一直保持三个 pod 实例在运行的状态. 现在要把这个数字提高到 10. 前面已经在第 2 章中扩容了 ReplicationController. 可以使用和之前相同的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl scale rc kubia <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">10</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>但这次你的做法会不一样.</p> <blockquote><p>通过编辑定义来缩放 ReplicationController</p></blockquote> <p>不使用 kubectl scale 命令, 而是通过<strong>以声明的形式编辑 ReplicationController 的定义</strong>对其进行缩放:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit rc kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>当文本编辑器打开时, <strong>找到 spec.replicas 字段并将其值更改为 5</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-4.7 运行 kubectl edit 在文本编辑器中编辑 RC</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span>
<span class="token comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span>
<span class="token comment"># reopened with the relevant failures.</span>
<span class="token comment">#</span>
apiVersion: v1
kind: ReplicationController
metadata:
  creationTimestamp: <span class="token string">&quot;2024-02-03T02:44:08Z&quot;</span>
  generation: <span class="token number">2</span>
  labels:
    app: kubia
  name: kubia
  namespace: default
  resourceVersion: <span class="token string">&quot;16334&quot;</span>
  uid: 20a9a67d-db7c-4b84-af74-f37c89861b36
spec:
  <span class="token comment"># 这里副本数改成5</span>
  replicas: <span class="token number">5</span>
  selector:
    app: kubia
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia
        imagePullPolicy: Always
        name: kubia
        ports:
        - containerPort: <span class="token number">8080</span>
          protocol: TCP
        resources: <span class="token punctuation">{</span><span class="token punctuation">}</span>
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: <span class="token punctuation">{</span><span class="token punctuation">}</span>
      terminationGracePeriodSeconds: <span class="token number">30</span>
status:
  availableReplicas: <span class="token number">5</span>
  fullyLabeledReplicas: <span class="token number">5</span>
  observedGeneration: <span class="token number">2</span>
<span class="token comment"># ......</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br></div></div><p>保存该文件并关闭编辑器, ReplicationController 会<strong>更新并立即将 pod 的数量</strong>增加到 5:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   <span class="token number">5</span>         <span class="token number">5</span>         <span class="token number">5</span>       49m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods <span class="token parameter variable">-L</span> app
NAME          READY   STATUS    RESTARTS   AGE    APP
kubia-45f7s   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          116s   kubia
kubia-bttsd   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          24m    kubia
kubia-lbjxr   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          49m    kubia
kubia-lgfcw   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          47m    kubia
kubia-vhhqc   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          116s   kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>就是这样. 如果 kubectl scale 命令看起来好像是你在告诉 Kubernetes 要做什么, 现在就更清晰了, 你是在<strong>声明对 ReplicationController 的目标状态的更改</strong>, 而不是告诉 Kubernetes 它要做的事情.</p> <blockquote><p>用 kubectl scale 命令缩容</p></blockquote> <p>现在将副本数目减小到 3. 可以使用 kubectl scale 命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl scale rc kubia <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">3</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>执行完后, 会有两个 terminating 状态的 pod.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods <span class="token parameter variable">-L</span> app
NAME          READY   STATUS        RESTARTS   AGE     APP
kubia-45f7s   <span class="token number">1</span>/1     Terminating   <span class="token number">0</span>          3m25s   kubia
kubia-bttsd   <span class="token number">1</span>/1     Running       <span class="token number">0</span>          25m     kubia
kubia-lbjxr   <span class="token number">1</span>/1     Running       <span class="token number">0</span>          51m     kubia
kubia-lgfcw   <span class="token number">1</span>/1     Running       <span class="token number">0</span>          49m     kubia
kubia-vhhqc   <span class="token number">1</span>/1     Terminating   <span class="token number">0</span>          3m25s   kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>等一会之后就只有三个 Running 的 pod 了.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pods <span class="token parameter variable">-L</span> app
NAME          READY   STATUS    RESTARTS   AGE   APP
kubia-bttsd   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          26m   kubia
kubia-lbjxr   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          51m   kubia
kubia-lgfcw   <span class="token number">1</span>/1     Running   <span class="token number">0</span>          49m   kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>所有这些命令都会<strong>修改 ReplicationController 定义的 spec.replicas 字段</strong>, 就像通过 kubectl edit 进行更改一样.</p> <blockquote><p>伸缩集群的声明式方法</p></blockquote> <p>在 Kubernetes 中水平伸缩 pod 是陈述式的: &quot;<strong>我想要运行 x 个实例</strong>.&quot; 你<mark><strong>不是告诉 Kubernetes 做什么或如何去做, 只是指定了期望的状态</strong></mark>.</p> <p>这种声明式的方法使得与 Kubernetes 集群的交互变得容易. 设想一下, 如果必须手动确定当前运行的实例数量, 然后明确告诉 Kubernetes 需要再多运行多少个实例的话, 工作更多且更容易出错, 改变一个简单的数字要容易得多. 在第 15 章中, 会发现如果启用 pod 水平自动缩放, 那么即使是 Kubernetes 本身也可以完成.</p> <h6 id="_4-2-7-删除一个replicationcontroller"><a href="#_4-2-7-删除一个replicationcontroller" class="header-anchor">#</a> 4.2.7 删除一个ReplicationController</h6> <p>当<strong>通过 kubectl delete 删除 ReplicationController 时, pod 也会被删除</strong>. 但是由于由 ReplicationController 创建的 pod 不是 ReplicationController 的组成部分, 只是由其进行管理, 因此可以只删除 ReplicationController 并保持 pod 运行, 如图 4.7 所示.</p> <p>当你最初拥有一组由 ReplicationController 管理的 pod, 然后决定用 <strong>ReplicaSet</strong> (接下来会知道)替换 ReplicationController 时, 这就很有用. 可以在不影响 pod 的情况下执行此操作, 并在替换管理它们的 ReplicationController 时保持 pod 不中断运行.</p> <p><img src="/img/image-20240227231918-yy9rtpr.png" alt="image" title="图4.7 使用--cascade=false 删除 ReplicationController 使托架不受管理"></p> <p>当使用 kubectl delete 删除 ReplicationController 时, 可以通过给命令增加 <code>--cascade=false</code>​ 选项来<strong>保持 pod 的运行</strong>. 马上试试看:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete rc kubia <span class="token parameter variable">--cascade</span><span class="token operator">=</span>false
warning: <span class="token parameter variable">--cascade</span><span class="token operator">=</span>false is deprecated <span class="token punctuation">(</span>boolean value<span class="token punctuation">)</span> and can be replaced with <span class="token parameter variable">--cascade</span><span class="token operator">=</span>orphan.
replicationcontroller <span class="token string">&quot;kubia&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>你已经删除了 ReplicationController, 所以<strong>这些 pod 独立了, 它们不再被管理</strong>. 但是你始终可以使用适当的标签选择器创建新的 ReplicationController, 并再次将它们管理起来.</p> <h5 id="_4-3-使用replicaset而不是replicationcontroller"><a href="#_4-3-使用replicaset而不是replicationcontroller" class="header-anchor">#</a> 4.3 使用ReplicaSet而不是ReplicationController</h5> <p>最初, ReplicationController 是用于复制和在异常时重新调度节点的唯一 Kubernetes 组件, 后来又<mark><strong>引入了一个名为 ReplicaSet 的类似资源. 它是新一代的 ReplicationController, 并且将其完全替换掉(ReplicationController 最终将被弃用)</strong></mark> .</p> <p>你本可以通过创建一个 ReplicaSet 而不是一个 ReplicationController 来开始本章, 但是笔者觉得从 Kubernetes 最初提供的组件开始是个好主意. 另外, 仍然可以看到使用中的 ReplicationController, 所以最好知道它们. 也就是说从现在起, <mark><strong>应该始终创建 ReplicaSet 而不是 ReplicationController. 它们几乎完全相同, 所以你不会碰到任何麻烦</strong></mark>.</p> <p>你通常不会直接创建它们, 而是<strong>在创建更高层级的 Deployment 资源时(在第 9 章中会学到)自动创建它们</strong>. 无论如何, 你应该了解 ReplicaSet, 所以下面来看看它们与 ReplicationController 的区别.</p> <h6 id="_4-3-1-比较replicaset和replicationcontroller"><a href="#_4-3-1-比较replicaset和replicationcontroller" class="header-anchor">#</a> 4.3.1 比较ReplicaSet和ReplicationController</h6> <p><strong>ReplicaSet 的行为与 ReplicationController 完全相同, 但 pod 选择器的表达能力更强</strong>. 虽然 ReplicationController 的标签选择器只允许包含某个标签的匹配 pod, 但 <strong>ReplicaSet 的选择器还允许匹配缺少某个标签的 pod, 或包含特定标签名的 pod, 不管其值如何</strong>.</p> <p>另外, 举个例子, 单个 ReplicationController 无法将 pod 与标签 env=production 和 env=devel 同时匹配. 它只能匹配带有 env=devel 标签的 pod 或带有 env=devel 标签的 pod. 但是<strong>一个 ReplicaSet 可以匹配两组 pod 并将它们视为一个大组</strong>.</p> <p>同样, 无论 ReplicationController 的值如何, ReplicationController 都无法仅基于标签名的存在来匹配 pod, 而 ReplicaSet 则可以. 例如, ReplicaSet 可匹配所有包含名为 env 的标签的 pod, 无论 ReplicaSet 的实际值是什么(可以理解为 env=*).</p> <h6 id="_4-3-2-定义replicaset"><a href="#_4-3-2-定义replicaset" class="header-anchor">#</a> 4.3.2 定义ReplicaSet</h6> <p>现在要创建一个 ReplicaSet, 并看看先前<strong>由 ReplicationController 创建稍后又被抛弃的无主 pod, 现在如何被 ReplicaSet 管理</strong>. 首先, 创建一个名为 kubia-replicaset.yaml 的新文件将你的 ReplicationController 改写为 ReplicaSet, 其中包含以下代码清单中的内容.</p> <p><strong>代码清单-4.8 ReplicaSet 的 YAML 定义: kubia-replicaset.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1beta2    <span class="token comment"># 版本号需要换一下</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ReplicaSet     <span class="token comment"># 指定创建的资源类型</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia   
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>   
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>   
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token comment"># 使用了更简单的 matchLabels 选择器, 非常类似于rc的选择器</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
  <span class="token key atrule">template</span><span class="token punctuation">:</span>           <span class="token comment"># 模板与rc中的模板一样</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
        <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>首先要注意的是 ReplicaSet 不是 v1 API 的一部分, 因此需要确保在创建资源时指定正确的 apiVersion. 你正在创建一个类型为 ReplicaSet 的资源, 它的内容与之前创建的 ReplicationController 的内容大致相同.</p> <p><strong>唯一的区别在选择器中. 不必在 selector 属性中直接列出 pod 需要的标签, 而是在 selector.matchLabels 下指定它们</strong>. 这是在 ReplicaSet 中定义标签选择器的更简单(也更不具表达力)的方式. 之后, 你会看到表达力更强的选项.</p> <blockquote><p>关于 API 版本的属性</p></blockquote> <p>这是你第一次有机会看到 apiVersion 属性指定的两件事情:</p> <ul><li>API 组(在这种情况下是 apps)</li> <li>实际的 API 版本(v1beta2)</li></ul> <p>你将在整本书中看到某些 Kubernetes 资源位于所谓的核心 API 组中, 该组并不需要在 apiVersion 字段中指定(只需指定版本--例如, 你已经在定义 pod 资源时使用过 apiVersion:v1). 在后续的 Kubernetes 版本中引入其他资源, 被分为几个 API 组.</p> <p><strong>因为你仍然有三个 pod 匹配从最初运行的 app=kubia 选择器, 所以创建此 ReplicaSet 不会触发创建任何新的 pod. ReplicaSet 将把它现有的三个 pod 归为自己的管辖范围.</strong></p> <h6 id="_4-3-3-创建和检查replicaset"><a href="#_4-3-3-创建和检查replicaset" class="header-anchor">#</a> 4.3.3 创建和检查ReplicaSet</h6> <p><strong>使用 kubectl create 命令根据 YAML 文件创建 ReplicaSet</strong>. 之后, 可以使用 kubectl get 和 kubectl describe 来检查 ReplicaSet:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rs
NAME         DESIRED         CURRENT     READY     AGE
kubia        <span class="token number">3</span>               <span class="token number">3</span>           <span class="token number">3</span>         3s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>提示: <strong>rs 是 replicaset 的简写</strong>.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe rs
Name: kubia
Namespace: default
Selector: <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
Labels: <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
Annotations: <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Replicas: <span class="token number">3</span> current / <span class="token number">3</span> desired
Pods Status: <span class="token number">3</span> Running / <span class="token number">0</span> Waiting / <span class="token number">0</span> Succeeded / <span class="token number">0</span> Failed
Pod Template:
  Labels: <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>如你所见, ReplicaSet 与 ReplicationController 没有任何区别. 显示有三个与选择器匹配的副本. 如果列出所有 pod, 你会发现它们仍然是你以前的三个 pod. <strong>ReplicaSet 没有创建任何新的 pod</strong>.</p> <h6 id="_4-3-4-使用replicaset的更富表达力的标签选择器"><a href="#_4-3-4-使用replicaset的更富表达力的标签选择器" class="header-anchor">#</a> 4.3.4 使用ReplicaSet的更富表达力的标签选择器</h6> <p><mark><strong>ReplicaSet 相对于 ReplicationController 的主要改进是它更具表达力的标签选择器</strong></mark>. 之前故意在第一个 ReplicaSet 示例中, 用较简单的 matchLabels 选择器来确认 ReplicaSet 与 ReplicationController 没有区别. 现在, 你将用更强大的 matchExpressions 属性来<strong>重写选择器</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-4.9 一个 matchExpressions 选择器:kubia-replicasetmatchexpressions.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">selector</span><span class="token punctuation">:</span>
  <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app    <span class="token comment"># 此选择器要求该pod包含名为&quot;app&quot;的标签</span>
      <span class="token key atrule">operator</span><span class="token punctuation">:</span> In
      <span class="token key atrule">values</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> kubia    <span class="token comment"># 标签的值必须是&quot;kubia&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>注意: 仅显示了选择器. 你会在本书的代码档案中找到整个 ReplicaSet 定义.</p> <p>可以给选择器添加额外的表达式. 如示例, 每个表达式都必须包含一个 key, 一个 operator(运算符), 并且可能还有一个 values 的列表(取决于运算符). 你会看到四个有效的运算符:</p> <ul><li><strong>In:Label</strong> 的值必须与其中一个指定的 values 匹配.</li> <li><strong>NotIn:Label</strong> 的值与任何指定的 values 不匹配.</li> <li><strong>Exists:pod</strong> 必须包含一个指定名称的标签(值不重要). 使用此运算符时, 不应指定 values 字段.</li> <li><strong>DoesNotExist:pod</strong> 不得包含有指定名称的标签. values 属性不得指定.</li></ul> <p><strong>如果指定了多个表达式, 则所有这些表达式都必须为 true 才能使选择器与 pod 匹配. 如果同时指定 matchLabels 和 matchExpressions, 则所有标签都必须匹配, 并且所有表达式必须计算为 true 以使该 pod 与选择器匹配</strong>.</p> <h6 id="_4-3-5-replicaset小结"><a href="#_4-3-5-replicaset小结" class="header-anchor">#</a> 4.3.5 ReplicaSet小结</h6> <p>这是对 ReplicaSet 的快速介绍, <mark><strong>将其作为 ReplicationController 的替代</strong></mark>. 请记住, 始终使用它而不是 ReplicationController, 但你仍可以在其他人的部署中找到 ReplicationController.</p> <p>现在, 删除 ReplicaSet 以清理你的集群. 可以像删除 ReplicationController 一样删除 ReplicaSet:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete rs kubia
replicaset <span class="token string">&quot;kubia&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>删除 ReplicaSet 会删除所有的 pod. 这种情况下是需要列出 pod 来确认的.</p> <h5 id="_4-4-使用daemonset在每个节点上运行一个pod"><a href="#_4-4-使用daemonset在每个节点上运行一个pod" class="header-anchor">#</a> 4.4 使用DaemonSet在每个节点上运行一个pod</h5> <p>Replicationcontroller 和 ReplicaSet 都用于在 Kubernetes 集群上运行部署特定数量的 pod. 但是, 当你<strong>希望 pod 在集群中的每个节点上运行</strong>时(并且每个节点都需要正好一个运行的 pod 实例, 如图 4.8 所示), 就会出现某些情况.</p> <p>这些情况包括 pod 执行系统级别的与基础结构相关的操作. 例如, 希望在每个节点上运行日志收集器和资源监控器. 另一个典型的例子是 Kubernetes 自己的 kube-proxy 进程, 它需要<strong>运行在所有节点上</strong>才能使服务工作.</p> <p><img src="/img/image-20240227232000-jlq0adq.png" alt="image" title="图4.8 DaemonSet 在每个节点上只运行一个 pod 副本, 而副本则将它们随机地分布在整个集群中"></p> <p>在 Kubernetes 之外, 此类进程通常<strong>在节点启动期间通过系统初始化脚本或 systemd 守护进程启动</strong>. 在 Kubernetes 节点上, 仍然可以使用 systemd 运行系统进程, 但这样就不能利用所有的 Kubernetes 特性了.</p> <h6 id="_4-4-1-使用daemonset在每个节点上运行一个pod"><a href="#_4-4-1-使用daemonset在每个节点上运行一个pod" class="header-anchor">#</a> 4.4.1 使用DaemonSet在每个节点上运行一个pod</h6> <p><mark><strong>要在所有集群节点上运行一个 pod, 需要创建一个 DaemonSet 对象</strong></mark>, 这很像一个 ReplicationController 或 ReplicaSet, 除了由 DaemonSet 创建的 pod, 已经有一个指定的目标节点并跳过 Kubernetes 调度程序. 它们不是随机分布在集群上的.</p> <p>DaemonSet 确保创建足够的 pod, 并在自己的节点上部署每个 pod, 如图 4.8 所示.</p> <p>尽管 ReplicaSet(或 ReplicationController)确保集群中存在期望数量的 pod 副本, 但 <strong>DaemonSet 并没有期望的副本数的概念. 它不需要, 因为它的工作是确保一个 pod 匹配它的选择器并在每个节点上运行</strong>.</p> <p>如果节点下线, DaemonSet 不会在其他地方重新创建 pod. 但是, <strong>当将一个新节点添加到集群中时, DaemonSet 会立刻部署一个新的 pod 实例</strong>. 如果有人无意中删除了一个 pod, 那么它也会重新创建一个新的 pod. 与 ReplicaSet 一样, DaemonSet 从配置的 pod 模板创建 pod.</p> <h6 id="_4-4-2-使用daemonset只在特定的节点上运行pod"><a href="#_4-4-2-使用daemonset只在特定的节点上运行pod" class="header-anchor">#</a> 4.4.2 使用DaemonSet只在特定的节点上运行pod</h6> <p>DaemonSet 将 pod 部署到集群中的所有节点上, 除非指定这些 pod 只在部分节点上运行. 这是通过 pod 模板中的 nodeSelector 属性指定的, 这是 DaemonSet 定义的一部分(类似于 ReplicaSet 或 ReplicationController 中的 pod 模板).</p> <p>在第 3 章中, 已经使用了<strong>节点选择器将 pod 部署到特定的节点</strong>上. DaemonSet 中的节点选择器与之相似--它定义了 DaemonSet 必须将其 pod 部署到的节点.</p> <p>注意: 在本书的后面, 你将了解到节点可以被设置为不可调度的, 防止 pod 被部署到节点上. DaemonSet 甚至会将 pod 部署到这些节点上, 因为无法调度的属性只会被调度器使用, 而 DaemonSet 管理的 pod 则完全绕过调度器. 这是预期的, 因为 DaemonSet 的目的是运行系统服务, 即使是在不可调度的节点上, 系统服务通常也需要运行.</p> <blockquote><p>用一个例子来解释 DaemonSet</p></blockquote> <p>假设有一个名为 ssd-monitor 的守护进程, 它需要在<strong>包含固态驱动器(SSD)的所有节点</strong>上运行. 你将创建一个 DaemonSet, 它在标记为具有 SSD 的所有节点上运行这个守护进程. 集群管理员已经<strong>向所有此类节点添加了 disk=ssd 的标签</strong>, 因此你将使用节点选择器创建 DaemonSet, 该选择器只选择具有该标签的节点, 如图 4.9 所示.</p> <p><img src="/img/image-20240227232023-pf1we77.png" alt="image" title="图4.9 使用含有节点选择器的 DaemonSet 在特定的节点上部署 pod"></p> <blockquote><p>创建一个 DaemonSet YAML 定义文件</p></blockquote> <p>下面将创建一个运行模拟的 ssd-monitor 监控器进程的 DaemonSet, 该进程每 5 秒会将 &quot;SSD OK&quot; 打印到标准输出. 笔者已经准备了模拟容器镜像并将它推到 Docker Hub, 因此无须构建而直接使用. 为 DaemonSet 创建一个 YAML 文件, 如下面的代码清单所示.</p> <p><strong>代码清单-4.10 一个 DaemonSet 的 YAML:ssd-monitor-daemonset.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1  <span class="token comment"># 原文是apps/v1beta2,但是已经废弃了,所以用这个版本</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> ssd<span class="token punctuation">-</span>monitor
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">app</span><span class="token punctuation">:</span> ssd<span class="token punctuation">-</span>monitor
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> ssd<span class="token punctuation">-</span>monitor
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>    <span class="token comment"># pod模板包含一个节点选择器, 会选择有disk=ssd标签的节点</span>
        <span class="token key atrule">disk</span><span class="token punctuation">:</span> ssd
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
        <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/ssd<span class="token punctuation">-</span>monitor
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>你正在定义一个 DaemonSet, 它将运行一个基于 <code>luksa/ssd-monitor</code>​ 容器镜像的单容器 pod. 该 pod 的实例将在每个具有 <code>disk=ssd</code>​ 标签的节点上创建.</p> <blockquote><p>创建DaemonSet</p></blockquote> <p>创建一个 DaemonSet 就像从 YAML 文件创建资源那样:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> ssd-monitor-daemonset.yaml
daemonset.apps/ssd-monitor created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>来看一下创建的 DaemonSet:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   <span class="token number">0</span>         <span class="token number">0</span>         <span class="token number">0</span>       <span class="token number">0</span>            <span class="token number">0</span>           <span class="token assign-left variable">disk</span><span class="token operator">=</span>ssd        2m54s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这些 0 看起来很奇怪. DaemonSet 不应该部署 pod 吗? 现在列出 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
No resources found.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>pod 在哪里呢? 知道发生了什么吗? 是的, 忘记给节点打上 <code>disk=ssd</code>​ 标签了. 打上标签之后, DaemonSet 将检测到节点的标签已经更改, 并将 pod 部署到有匹配标签的所有节点. 下面来看一下.</p> <blockquote><p>向节点上添加所需的标签</p></blockquote> <p>无论你使用的是 Minikube, GKE 或其他多节点集群, 都需要首先列出节点, 因为在<strong>标记时需要知道节点的名称</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">node</span>
NAME         STATUS     AGE     VERSION
minikube     Ready      4d      v1.6.0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>现在像这样给节点添加 <code>disk=ssd</code>​ 标签:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl label <span class="token function">node</span> minikube <span class="token assign-left variable">disk</span><span class="token operator">=</span>ssd
<span class="token function">node</span> <span class="token string">&quot;minikube&quot;</span> labeled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 如果你没有使用 Minikube, 用你的节点名替换 minikube.</p> <p>DaemonSet 现在应该已经创建 pod 了, 来看一下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME                 READY     STATUS     RESTARTS     AGE
ssd-monitor-hgxwq    <span class="token number">1</span>/1       Running    <span class="token number">0</span>            35s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>现在看起来一切正常. 如果你有多个节点并且其他的节点也加上了同样的标签, 将会看到 DaemonSet 在每个节点上都启动 pod.</p> <blockquote><p>从节点上删除所需的标签</p></blockquote> <p>现在假设给其中一个节点打错了标签. 它的硬盘是磁盘而不是 SSD. 这时你修改了标签会发生什么呢?</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl label <span class="token function">node</span> minikube <span class="token assign-left variable">disk</span><span class="token operator">=</span>hdd <span class="token parameter variable">--overwrite</span>
<span class="token function">node</span> <span class="token string">&quot;minikube&quot;</span> labeled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>看一下更改是否影响了运行在节点上的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME                 READY         STATUS         RESTARTS         AGE
ssd-monitor-hgxwq    <span class="token number">1</span>/1           Terminating    <span class="token number">0</span>                4m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>pod 如预期中正在被终止. 这里对 DaemonSet 的探索就要结束了, 因此可能想要删除 ssd-monitor DaemonSet. 如果还有其他的 pod 在运行, 删除 DaemonSet 也会一起删除这些 pod.</p> <h5 id="_4-5-运行执行单个任务的pod"><a href="#_4-5-运行执行单个任务的pod" class="header-anchor">#</a> 4.5 运行执行单个任务的pod</h5> <p>到目前为止, 只谈论了需要持续运行的 pod. 你会遇到<strong>只想运行完成工作后就终止任务</strong>的情况. <strong>ReplicationController, ReplicaSet 和 DaemonSet 会持续运行任务, 永远达不到完成态. 这些 pod 中的进程在退出时会重新启动. 但是在一个可完成的任务中, 其进程终止后, 不应该再重新启动</strong>.</p> <h6 id="_4-5-1-介绍job资源"><a href="#_4-5-1-介绍job资源" class="header-anchor">#</a> 4.5.1 介绍Job资源</h6> <p>Kubernetes 通过 Job 资源提供了对此的支持, 这与本章中讨论的其他资源类似, 但它允许你运行一种 pod, <strong>该 pod 在内部进程成功结束时, 不重启容器</strong>. 一旦任务完成, pod 就被认为处于完成状态.</p> <p>在发生节点故障时, 该节点上由 Job 管理的 pod 将按照 ReplicaSet 的 pod 的方式, 重新安排到其他节点. 如果进程本身异常退出(进程返回错误退出代码时), 可以将 Job 配置为重新启动容器.</p> <p>图 4.10 显示了如果一个 Job 所创建的 pod, <strong>在最初被调度节点上异常退出后, 被重新安排到一个新节点上的情况. 该图还显示了托管的 pod(未重新安排)和由 ReplicaSet 管理的 pod(被重新安排)</strong> .</p> <p>例如, Job 对于临时任务很有用, 关键是任务要以正确的方式结束. 可以在未托管的 pod 中运行任务并等待它完成, 但是如果发生节点异常或 pod 在执行任务时被从节点中逐出, 则需要手动重新创建该任务. 手动做这件事并不合理--特别是如果任务需要几个小时才能完成.</p> <p>这样的任务的一个例子是, 如果有数据存储在某个地方, 需要转换并将其导出到某个地方. 你将通过运行构建在 busybox 镜像上的容器镜像来模拟此操作, 该容器将调用 sleep 命令两分钟. 笔者已经构建了镜像并将其推送到 Docker Hub, 你可以在本书的代码档案中查看它的 Dockerfile.</p> <p><img src="/img/image-20240227232053-cxr9v25.png" alt="image" title="图4.10 由 Job 管理的 pod 会一直被重新安排, 直到它们成功完成任务"></p> <h6 id="_4-5-2-定义job资源"><a href="#_4-5-2-定义job资源" class="header-anchor">#</a> 4.5.2 定义Job资源</h6> <p>按照下面的代码清单创建 Job manifest.</p> <p><strong>代码清单-4.11 Job 的 YAML 定义: exporter.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> batch<span class="token punctuation">-</span>job
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token comment"># 没有指定pod选择器, 它将根据pod模板中的标签创建</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> batch<span class="token punctuation">-</span>job
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure    <span class="token comment"># Job不能使用默认为Always的重新启动策略</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> main
        <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/batch<span class="token punctuation">-</span>job
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>Job 是 batch API 组 v1 API 版本的一部分. YAML 定义了一个 Job 类型的资源, 它将运行 <code>luksa/batch-job</code>​ 镜像, 该镜像调用一个运行 120 秒的进程, 然后退出.</p> <p>在一个 pod 的定义中, <strong>可以指定在容器中运行的进程结束时</strong>, Kubernetes 会做什么. 这是通过 pod 配置的属性 <strong>restartPolicy</strong> 完成的, 默认为 <strong>Always</strong>. <strong>Job pod 不能使用默认策略, 因为它们不是要无限期地运行</strong>. 因此, 需要明确地将重启策略设置为 OnFailure 或 Never. 此设置防止容器在完成任务时重新启动(pod 被 Job 管理时并不是这样的).</p> <h6 id="_4-5-3-看job运行一个pod"><a href="#_4-5-3-看job运行一个pod" class="header-anchor">#</a> 4.5.3 看Job运行一个pod</h6> <p>在使用 kubectl create 命令创建此作业后, 应该看到它立即启动一个 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">jobs</span>
NAME         DESIRED         SUCCESSFUL     AGE
batch-job    <span class="token number">1</span>               <span class="token number">0</span>              2s

$ kubectl get po
NAME                 READY     STATUS     RESTARTS     AGE
batch-job-28qf4      <span class="token number">1</span>/1       Running    <span class="token number">0</span>            4s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>两分钟过后, pod 将不再出现在 pod 列表中, 工作将<strong>被标记为已完成</strong>. 默认情况下, 除非使用 --show-all(或-a)开关, 否则在列出 pod 时<strong>不显示已完成的 pod</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-a</span>
NAME             READY     STATUS     RESTARTS     AGE
batch-job-28qf4  <span class="token number">0</span>/1       Completed  <span class="token number">0</span>            2m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>完成后 pod 未被删除的原因是<strong>允许你查阅其日志</strong>. 例如:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs batch-job-28qf4
Fri Apr <span class="token number">29</span> 09:58:22 UTC <span class="token number">2016</span> Batch job starting
Fri Apr <span class="token number">29</span> <span class="token number">10</span>:00:22 UTC <span class="token number">2016</span> Finished succesfully
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>pod 可以被直接删除, 或者在删除创建它的 Job 时被删除. 在删除它之前, 再看一下 Job 资源:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get job
NAME         DESIRED     SUCCESSFUL     AGE
batch-job    <span class="token number">1</span>           <span class="token number">1</span>              9m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>作业显示已成功完成. 但为什么这样的信息显示为一个数字而不是 yes 或 true? DESIRED 列表示什么意思?</p> <h6 id="_4-5-4-在job中运行多个pod实例"><a href="#_4-5-4-在job中运行多个pod实例" class="header-anchor">#</a> 4.5.4 在Job中运行多个pod实例</h6> <p>作业可以配置为创建多个 pod 实例, 并以<strong>并行或串行</strong>方式运行它们. 这是通过在 Job 配置中<strong>设置 completions 和 parallelism 属性</strong>来完成的.</p> <blockquote><p>顺序运行Job pod</p></blockquote> <p>如果需要一个 Job 运行多次, 则可以将 completions 设为希望作业的 pod 运行多少次. 下面的代码清单显示了一个例子.</p> <p><strong>代码清单-4.12 需要多次完成的 Job:multi-completion-batch-job.yaml</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: <span class="token number">5</span>    <span class="token comment"># 设置此作业顺序运行五个pod</span>
  template:    <span class="token comment"># template与前面的一致</span>
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure   
      containers:
        name: main
        image: luksa/batch-job
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p><strong>Job 将一个接一个地运行五个 pod</strong>. 它最初创建一个 pod, 当 pod 的容器运行完成时, 它创建第二个 pod, 以此类推, 直到五个 pod 成功完成. 如果其中一个 pod 发生故障, 工作会创建一个新的 pod, 所以 Job 总共可以创建五个以上的 pod.</p> <blockquote><p>并行运行Job pod</p></blockquote> <p>不必一个接一个地运行单个 Job pod, 也可以让<strong>该 Job 并行运行多个 pod</strong>. 可以通过 parallelism Job 配置属性, 指定允许多少个 pod 并行执行, 如下面的代码清单所示.</p> <p><strong>代码清单-4.13 并行运行 Job pod:multi-completion-parallel-batch-job.yaml</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: <span class="token number">5</span>    <span class="token comment"># 设置此作业顺序运行五个pod</span>
  parallelism: <span class="token number">2</span>    <span class="token comment"># 最多两个任务并行</span>
  template:    <span class="token comment"># template与前面的一致</span>
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure   
      containers:
        name: main
        image: luksa/batch-job
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>通过将 parallelism 设置为 2, Job 创建两个 pod 并行运行它们:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME                             READY     STATUS      RESTARTS      AGE
multi-completion-batch-job-lmmnk <span class="token number">1</span>/1       Running     <span class="token number">0</span>             21s
multi-completion-batch-job-qx4nq <span class="token number">1</span>/1       Running     <span class="token number">0</span>             21s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>只要其中一个 pod 完成任务, 工作将运行下一个 pod, 直到五个 pod 都成功完成任务.</p> <blockquote><p>Job的缩放</p></blockquote> <p>甚至可以在 Job <strong>运行时更改 Job 的 parallelism 属性</strong>. 这与缩放 ReplicaSet 或 ReplicationController 类似, 可以使用 kubectl scale 命令完成:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl scale job multi-completion-batch-job <span class="token parameter variable">--replicas</span> <span class="token number">3</span>
job <span class="token string">&quot;multi-completion-batch-job&quot;</span> scaled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>由于将 parallelism 从 2 增加到 3, 另一个 pod 立即启动, 因此现在有三个 pod 在运行.</p> <h6 id="_4-5-5-限制job-pod完成任务的时间"><a href="#_4-5-5-限制job-pod完成任务的时间" class="header-anchor">#</a> 4.5.5 限制Job pod完成任务的时间</h6> <p>关于 Job 需要讨论最后一件事. <strong>Job 要等待一个 pod 多久来完成任务</strong>? 如果 pod 卡住并且根本无法完成(或者无法足够快完成), 该怎么办?</p> <p><strong>通过在 pod 配置中设置 activeDeadlineSeconds 属性, 可以限制 pod 的时间. 如果 pod 运行时间超过此时间, 系统将尝试终止 pod, 并将 Job 标记为失败</strong>.</p> <p>注意: 通过指定 Job manifest 中的 spec.backoffLimit 字段, 可以配置 Job 在被标记为失败之前可以重试的次数. 如果没有明确指定它, 则默认为 6.</p> <h5 id="_4-6-安排job定期运行或在将来运行一次"><a href="#_4-6-安排job定期运行或在将来运行一次" class="header-anchor">#</a> 4.6 安排Job定期运行或在将来运行一次</h5> <p><strong>Job 资源在创建时会立即运行 pod</strong>. 但是许多批处理任务需要在<strong>特定的时间运行, 或者在指定的时间间隔内重复运行</strong>. 在 Linux 和类 UNIX 操作系统中, 这些任务通常被称为 cron 任务. Kubernetes 也支持这种任务.</p> <p><mark><strong>Kubernetes 中的 cron 任务通过创建 CronJob 资源进行配置</strong></mark>. 运行任务的时间表以知名的 cron 格式指定, 所以如果你熟悉常规 cron 任务, 你将在几秒钟内了解 Kubernetes 的 CronJob.</p> <p>在配置的时间, <strong>Kubernetes 将根据在 CronJob 对象中配置的 Job 模板创建 Job 资源</strong>. 创建 Job 资源时, 将根据任务的 pod 模板创建并启动一个或多个 pod 副本, 如你在前一部分中所了解的那样.</p> <p>来看看如何创建 CronJob.</p> <h6 id="_4-6-1-创建一个cronjob"><a href="#_4-6-1-创建一个cronjob" class="header-anchor">#</a> 4.6.1 创建一个CronJob</h6> <p>想象一下, 你需要每 15 分钟运行一次前一个示例中的批处理任务. 为此可以使用以下规范创建一个 CronJob 资源.</p> <p><strong>代码清单-4.14 CronJob 资源的 YAML:cronjob.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob    <span class="token comment"># 资源类型为CronJob</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> batch<span class="token punctuation">-</span>job<span class="token punctuation">-</span>every<span class="token punctuation">-</span>fifteen<span class="token punctuation">-</span>minutes
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">schedule</span><span class="token punctuation">:</span> <span class="token string">&quot;0,15,30,45 * * * *&quot;</span>    <span class="token comment"># 定义cron表达式</span>
  <span class="token key atrule">jobTemplate</span><span class="token punctuation">:</span>    <span class="token comment"># 配置CronJob创建Job资源会用到的模板</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">template</span><span class="token punctuation">:</span>
        <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
          <span class="token key atrule">labels</span><span class="token punctuation">:</span>
            <span class="token key atrule">app</span><span class="token punctuation">:</span> periodic<span class="token punctuation">-</span>batch<span class="token punctuation">-</span>job
        <span class="token key atrule">spec</span><span class="token punctuation">:</span>
          <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure
          <span class="token key atrule">containers</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
            <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/batch<span class="token punctuation">-</span>job
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>正如你所看到的, 它不是太复杂. 你已经指定了<strong>创建 Job 对象的时间表和模板</strong>.</p> <blockquote><p>配置Job模板</p></blockquote> <p>CronJob 通过 CronJob 规范中配置的 <strong>jobTemplate 属性创建任务资源</strong>, 更多有关如何配置它的信息, 请参阅 4.5 节.</p> <h6 id="_4-6-2-了解计划任务的运行方式"><a href="#_4-6-2-了解计划任务的运行方式" class="header-anchor">#</a> 4.6.2 了解计划任务的运行方式</h6> <p><strong>在计划的时间内, CronJob 资源会创建 Job 资源, 然后 Job 创建 pod</strong>.</p> <p>可能发生 Job 或 pod 创建并运行得相对较晚的情况. 你可能对这项工作有很高的要求, 任务开始不能落后于预定的时间过多. 在这种情况下, 可以通过指定 CronJob 规范中的 <strong>startingDeadlineSeconds</strong> 字段来指定截止日期, 如下面的代码清单所示.</p> <p><strong>代码清单-4.15 为 CronJob 指定一个 startingDeadlineSeconds</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob    <span class="token comment"># 资源类型为CronJob</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> batch<span class="token punctuation">-</span>job<span class="token punctuation">-</span>every<span class="token punctuation">-</span>fifteen<span class="token punctuation">-</span>minutes
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">schedule</span><span class="token punctuation">:</span> <span class="token string">&quot;0,15,30,45 * * * *&quot;</span>    <span class="token comment"># 定义cron表达式</span>
  <span class="token key atrule">startingDeadlineSeconds</span><span class="token punctuation">:</span> <span class="token number">15</span>    <span class="token comment"># pod最迟必须在预定时间后15秒开始运行</span>
  <span class="token key atrule">jobTemplate</span><span class="token punctuation">:</span>    <span class="token comment"># 配置CronJob创建Job资源会用到的模板</span>
    <span class="token punctuation">...</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>在代码清单 4.15 的例子中, 工作运行的时间应该是 10:30:00. 如果因为任何原因 10:30:15 不启动, 任务将<strong>不会运行</strong>, 并将显示为 Failed.</p> <p>在正常情况下, CronJob 总是为计划中配置的每个执行创建一个 Job, 但可能会同时创建两个 Job, 或者根本没有创建. 为了解决第一个问题, 任务应该是幂等的(多次而不是一次运行不会得到不希望的结果). 对于第二个问题, 请确保下一个任务运行完成本应该由上一次的(错过的)运行完成的任何工作.</p> <h5 id="_4-7-本章小结"><a href="#_4-7-本章小结" class="header-anchor">#</a> 4.7 本章小结</h5> <p>你现在已经学会了如何让 pod 保持运行, 并在发生节点故障时重新安排它们. 你现在应该知道:</p> <ul><li><strong>使用存活探针, 让 Kubernetes 在容器不再健康的情况下立即重启它(应用程序定义了健康的条件)</strong> .</li> <li><strong>不应该直接创建 pod, 因为如果它们被错误地删除, 它们正在运行的节点异常, 或者它们从节点中被逐出时, 它们将不会被重新创建.</strong></li> <li>ReplicationController 始终保持所需数量的 pod 副本正在运行.</li> <li>水平缩放 pod 与在 ReplicationController 上更改所需的副本个数一样简单.</li> <li><strong>pod 不属于 ReplicationController</strong>, 如有必要可以在它们之间移动.</li> <li>ReplicationController 将从 pod 模板创建新的 pod. 更改模板对现有的 pod 没有影响.</li> <li><mark><strong>ReplicationController 应该替换为 ReplicaSet 和 Deployment, 它们提供相同的能力, 但具有额外的强大功能</strong></mark>.</li> <li><strong>ReplicationController 和 ReplicaSet 将 pod 安排到随机集群节点, 而 DaemonSet 确保每个节点都运行一个 DaemonSet 中定义的 pod 实例</strong>.</li> <li><strong>执行批处理任务的 pod 应通过 Kubernetes Job 资源创建, 而不是直接或通过 ReplicationController 或类似对象创建</strong>.</li> <li>需要在未来某个时候运行的 Job 可以通过 <strong>CronJob</strong> 资源创建.</li></ul> <h4 id="_5-服务-让客户端发现pod并与之通信"><a href="#_5-服务-让客户端发现pod并与之通信" class="header-anchor">#</a> 5.服务:让客户端发现pod并与之通信</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>创建服务资源, 利用单个地址访问一组 pod</li> <li>发现集群中的服务</li> <li>将服务公开给外部客户端</li> <li>从集群内部连接外部服务</li> <li>控制 pod 是与服务关联</li> <li>排除服务故障</li></ul> <p>现在已经学习过了 pod, 以及如何通过 ReplicaSet 和类似资源部署运行. 尽管特定的 pod 可以独立地应对外部刺激, 现在大多数应用都需要<strong>根据外部请求做出响应</strong>. 例如, 就微服务而言, pod 通常需要对来自集群内部其他 pod, 以及来自集群外部的客户端的 HTTP 请求做出响应.</p> <p>pod 需要一种寻找其他 pod 的方法来使用其他 pod 提供的服务, 不像在没有 Kubernetes 的世界, 系统管理员要在用户端配置文件中明确指出服务的精确的 IP 地址或者主机名来配置每个客户端应用, 但是同样的方式在 Kubernetes 中并不适用, 因为:</p> <ul><li><strong>pod 是短暂的</strong>. 它们随时会启动或者关闭, 无论是为了给其他 pod 提供空间而从节点中被移除, 或者是减少了 pod 的数量, 又或者是因为集群中存在节点异常.</li> <li><strong>Kubernetes 在 pod 启动前会给已经调度到节点上的 pod 分配 IP 地址</strong>. 因此客户端不能提前知道提供服务的 pod 的 IP 地址.</li> <li><strong>水平伸缩意味着多个 pod 可能会提供相同的服务</strong>. 每个 pod 都有自己的 IP 地址, 客户端无须关心后端提供服务 pod 的数量, 以及各自对应的 IP 地址. 它们无须记录每个 pod 的 IP 地址. 相反, 所有的 pod 可以通过一个单一的 IP 地址进行访问.</li></ul> <p>为了解决上述问题, Kubernetes 提供了一种资源类型--<mark><strong>服务(service)</strong></mark> , 在本章中将对其进行介绍.</p> <h5 id="_5-1-介绍服务"><a href="#_5-1-介绍服务" class="header-anchor">#</a> 5.1 介绍服务</h5> <p><mark><strong>Kubernetes 服务是一种为一组功能相同的 pod 提供单一不变的接入点的资源. 当服务存在时, 它的 IP 地址和端口不会改变. 客户端通过 IP 地址和端口号建立连接, 这些连接会被路由到提供该服务的任意一个 pod 上. 通过这种方式, 客户端不需要知道每个单独的提供服务的 pod 的地址, 这样这些 pod 就可以在集群中随时被创建或移除.</strong></mark></p> <blockquote><p>结合实例解释服务</p></blockquote> <p>回顾一下有前端 web 服务器和后端数据库服务器的例子. 有很多 pod 提供前端服务, 而只有一个 pod 提供后台数据库服务. 需要解决两个问题才能使系统发挥作用.</p> <ul><li>外部客户端无须关心服务器数量而连接到前端 pod 上.</li> <li>前端的 pod 需要连接后端的数据库. 由于数据库运行在 pod 中, 它可能会在集群中移来移去, 导致 IP 地址变化. 当后台数据库被移动时, 无须对前端 pod 重新配置.</li></ul> <p><strong>通过为前端 pod 创建服务, 并且将其配置成可以在集群外部访问, 可以暴露一个单一不变的 IP 地址让外部的客户端连接 pod</strong>. 同理, 可以为后台数据库 pod 创建服务, 并为其分配一个固定的 IP 地址. 尽管 pod 的 IP 地址会改变, 但是服务的 IP 地址固定不变. 另外, 通过创建服务, 能够让前端的 pod 通过环境变量或 DNS 以及服务名来访问后端服务. 系统中所有的元素都在图 5.1 中展示出来(两种服务, 支持这些服务的两套 pod, 以及它们之间的相互依赖关系).</p> <p><img src="/img/image-20240227232130-aqqh8a2.png" alt="image" title="图5.1 内部和外部客户端通常通过服务连接到 pod"></p> <p>到目前为止了解了服务背后的基本理念. 那么现在, 去深入研究如何创建它们.</p> <h6 id="_5-1-1-创建服务"><a href="#_5-1-1-创建服务" class="header-anchor">#</a> 5.1.1 创建服务</h6> <p>服务的后端可以有不止一个 pod. 服务的连接对所有的后端 pod 是<strong>负载均衡</strong>的. 但是要如何准确地定义哪些 pod 属于服务哪些不属于呢?</p> <p>或许还记得在 ReplicationController 和其他的 pod 控制器中<strong>使用标签选择器来指定哪些 pod 属于同一组. 服务使用相同的机制</strong>, 可以参考图 5.2.</p> <p>在前面的章节中, 通过创建 ReplicationController 运行了三个包含 Node.js 应用的 pod. 再次创建 ReplicationController 并且确认 pod 启动运行, 在这之后将会为这三个 pod 创建一个服务.</p> <p><img src="/img/image-20240227232153-dplau0u.png" alt="image" title="图5.2 标签选择器决定哪些 pod 属于服务"></p> <blockquote><p>通过 kubectl expose 创建服务</p></blockquote> <p>创建服务的最简单的方法是通过 <code>kubectl expose</code>​, 在第 2 章中曾使用这种方法来暴露创建的 ReplicationController. 像创建 ReplicationController 时使用的 pod 选择器那样, <strong>利用 expose 命令和 pod 选择器来创建服务资源, 从而通过单个的 IP 和端口来访问所有的 pod</strong>.</p> <p>现在, 除了使用 expose 命令, 可以<mark><strong>通过将配置的 YAML 文件传递到 Kubernetes API 服务器来手动创建服务</strong></mark>.</p> <blockquote><p>通过 YAML 描述文件来创建服务</p></blockquote> <p>使用以下代码清单中的内容创建一个名为 kubia-svc.yaml 的文件.</p> <p><strong>代码清单-5.1 服务的定义: kubia-svc.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service  <span class="token comment"># 资源类型为服务</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia  
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>    <span class="token comment"># 该服务的可用端口</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token comment"># 服务将连接转发到的容器端口</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia    <span class="token comment"># 具有app=kubia标签的pod都属于该服务</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p><strong>创建了一个名叫 kubia 的服务, 它将在端口 80 接收请求并将连接路由到具有标签选择器是 app=kubia 的 pod 的 8080 端口上</strong>.</p> <p>接下来通过使用 <strong>kubectl create 发布文件来创建服务</strong>.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-svc.yml 
service/kubia created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>检测新的服务</p></blockquote> <p>在发布完 YAML 文件后, 可以在命名空间下<strong>列出来所有的服务资源</strong>, 并可以发现新的服务已经被分配了一个内部集群 IP.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>   AGE
kubernetes   ClusterIP   <span class="token number">10.96</span>.0.1       <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">443</span>/TCP   19d
kubia        ClusterIP   <span class="token number">10.111</span>.249.153   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">80</span>/TCP    37s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>列表显示分配给服务的 IP 地址是 10.111.249.153. 因为只是<strong>集群的 IP 地址, 只能在集群内部可以被访问</strong>. 服务的主要目标就是使集群内部的其他 pod 可以访问当前这组 pod, 但通常也希望对外暴露服务. 如何实现将在之后讲解. 现在, 从集群内部使用创建好的服务并了解服务的功能.</p> <blockquote><p>从内部集群测试服务</p></blockquote> <p>可以通过以下几种方法<strong>向服务发送请求</strong>:</p> <ul><li>显而易见的方法是<strong>创建一个 pod</strong>, 它将请求发送到服务的集群 IP 并记录响应. 可以通过查看 pod 日志检查服务的响应.</li> <li>使用 ssh 远程登录到其中一个 Kubernetes 节点上, 然后使用 curl 命令.</li> <li>可以通过 <strong>kubectl exec 命令在一个已经存在的 pod 中执行 curl 命令</strong>.</li></ul> <p>下面来学习最后一种方法--如何在已有的 pod 中运行命令.</p> <blockquote><p>在运行的容器中远程执行命令</p></blockquote> <p>可以使用 <strong>kubectl exec 命令</strong>远程地在一个已经存在的 pod 容器上执行任何命令. 这样就可以很方便地了解 pod 的内容, 状态及环境. <strong>用 kubectl get pod 命令列出所有的 pod, 并且选择其中一个作为 exec 命令的执行目标</strong>(在下述例子中, 选择 kubia-7nog1 pod 作为目标). 也可以获得服务的集群 IP(比如使用 kubectl get svc 命令), 当执行下述命令时, 请确保替换对应 pod 的名称及服务 IP 地址.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> kubia-7nog1 -- <span class="token function">curl</span> <span class="token parameter variable">-s</span> http://10.111.249.153
You’ve hit kubia-gzwli
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果之前使用过 ssh 命令登录到一个远程系统, 会发现 kubectl exec 没有特别大的不同之处.</p> <p><strong>双横杠(--)代表着 kubectl 命令项的结束. 在两个横杠之后的内容是指在 pod 内部需要执行的命令</strong>. 如果需要执行的命令并没有以横杠开始的参数, 横杠也不是必需的. 如下情况, 如果这里不使用横杠号, -s 选项会被解析成 kubectl exec 选项, 会导致结果异常和歧义错误.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> kubia-7nog1 <span class="token function">curl</span> <span class="token parameter variable">-s</span> http://10.111.249.153
The connection to the server <span class="token number">10.111</span>.249.153 was refused – did you specify the right <span class="token function">host</span> or port?
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>服务除拒绝连接外什么都不做. 这是因为 kubectl 并不能连接到位于 10.111.249.153 的 API 服务器(-s 选项用来告诉 kubectl 需要连接一个不同的 API 服务器而不是默认的).</p> <p>回顾一下在运行命令时发生了什么. 图 5.3 展示了事件发生的顺序. <strong>在一个 pod 容器上, 利用 Kubernetes 去执行 curl 命令. curl 命令向一个后端有三个 pod 服务的 IP 发送了 HTTP 请求, Kubernetes 服务代理截取的该连接, 在三个 pod 中任意选择了一个 pod, 然后将请求转发给它</strong>. Node.js 在 pod 中运行处理请求, 并返回带有 pod 名称的 HTTP 响应. 接着, curl 命令向标准输出打印返回值, 该返回值被 kubectl 截取并打印到宕主机的标准输出.</p> <p><img src="/img/image-20240227232227-pzj4t3u.png" alt="image" title="图5.3 使用 kubectl exec 通过在一个 pod 中运行 curl 命令来测试服务是否连通"></p> <p>在之前的例子中, <strong>在 pod 主容器中以独立进程的方式执行了 curl 命令</strong>. 这与容器真正的主进程和服务通信并没有什么区别.</p> <blockquote><p>配置服务上的会话亲和性</p></blockquote> <p>如果多次执行同样的命令, 每次调用执行应该在<strong>不同的 pod 上</strong>. 因为服务代理通常将每个连接<strong>随机</strong>指向选中的后端 pod 中的一个, 即使连接来自于同一个客户端.</p> <p>另一方面, <strong>如果希望特定客户端产生的所有请求每次都指向同一个 pod, 可以设置服务的 sessionAffinity 属性为 ClientIP</strong>(而不是 None,None 是默认值), 如下面的代码清单所示.</p> <p><strong>代码清单-5.2 会话亲和性被设置成 ClientIP 的服务的例子</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">sessionAffinity</span><span class="token punctuation">:</span> ClientIP
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这种方式将会<strong>使服务代理将来自同一个 client IP 的所有请求转发至同一个 pod 上</strong>.</p> <p>Kubernetes 仅仅支持两种形式的会话亲和性服务: None 和 ClientIP. 你或许惊讶竟然不支持基于 cookie 的会话亲和性的选项, 但是你要了解 <mark><strong>Kubernetes 服务不是在 HTTP 层面上工作. 服务处理 TCP 和 UDP 包, 并不关心其中的载荷内容. 因为 cookie 是 HTTP 协议中的一部分, 服务并不知道它们, 这就解释了为什么会话亲和性不能基于 cookie.</strong></mark></p> <blockquote><p>同一个服务暴露多个端口</p></blockquote> <p>创建的服务可以暴露一个端口, 也可以<strong>暴露多个端口</strong>. 比如, 你的 pod 监听两个端口, 比如 HTTP 监听 8080 端口, HTTPS 监听 8443 端口, 可以<strong>使用一个服务从端口 80 和 443 转发至 pod 端口 8080 和 8443</strong>. 在这种情况下, 无须创建两个不同的服务. 通过一个<strong>集群 IP</strong>, 使用一个服务就可以将多个端口全部暴露出来.</p> <p>注意: 在创建一个<strong>有多个端口的服务的时候, 必须给每个端口指定名字</strong>.</p> <p>以下代码清单中展示了多端口服务的规格.</p> <p><strong>代码清单-5.3 在服务定义中指定多端口</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>  <span class="token comment"># pod的8080端口映射成80端口</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8443</span>  <span class="token comment"># pod的8443端口映射成443端口</span>
<span class="token key atrule">selector</span><span class="token punctuation">:</span>
  <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia    <span class="token comment"># 标签选择器适用于整个服务</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>注意: <strong>标签选择器应用于整个服务, 不能对每个端口做单独的配置</strong>. 如果不同的 pod 有不同的端口映射关系, 需要创建两个服务.</p> <p>之前创建的 kubia pod 不在多个端口上侦听, 因此可以练习创建一个多端口服务和一个多端口 pod.</p> <blockquote><p>使用命名的端口</p></blockquote> <p>在这些例子中, 通过数字来指定端口, 但是在<strong>服务 spec 中也可以给不同的端口号命名, 通过名称来指定</strong>. 这样对于一些不是众所周知的端口号, 使得服务 spec 更加清晰.</p> <p>举个例子, 假设你的 pod 端口定义命名如下面的代码清单所示.</p> <p><strong>代码清单-5.4 在 pod 的定义中指定 port 名称</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
      <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>   <span class="token comment"># 端口8080被命名为http</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https
      <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8443</span>   <span class="token comment"># 端口8443被命名为https</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>可以<strong>在服务 spec 中按名称引用这些端口</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-5.5 在服务中引用命名 pod</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> http   <span class="token comment"># 将端口80映射到中被称为http的端口</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> https  <span class="token comment"># 将端口443映射到中被称为https的端口</span>
<span class="token key atrule">selector</span><span class="token punctuation">:</span>
  <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>为什么要采用命名端口的方式? 最大的好处就是<strong>即使更换端口号也无须更改服务 spec</strong>. 你的 pod 现在对 http 服务用的是 8080, 但是假设过段时间你决定将端口更换为 80 呢?</p> <p>如果采用了命名的端口, 仅仅需要做的就是<strong>改变 spec pod 中的端口号</strong>(当然端口号的名称没有改变). 在 pod 向新端口更新时, 根据 pod 收到的连接(8080 端口在旧的 pod 上, 80 端口在新的 pod 上), 用户连接将会转发到对应的端口号上.</p> <h6 id="_5-1-2-服务发现"><a href="#_5-1-2-服务发现" class="header-anchor">#</a> 5.1.2 服务发现</h6> <p><strong>通过创建服务, 现在就可以通过一个单一稳定的 IP 地址访问到 pod. 在服务整个生命周期内这个地址保持不变. 在服务后面的 pod 可能删除重建, 它们的 IP 地址可能改变, 数量也会增减, 但是始终可以通过服务的单一不变的 IP 地址访问到这些 pod.</strong></p> <p>但客户端 pod 如何知道服务的 IP 和端口? 是否需要先创建服务, 然后手动查找其 IP 地址并将 IP 传递给客户端 pod 的配置选项? 当然不是. <mark><strong>Kubernetes 还为客户端提供了发现服务的 IP 和端口的方式</strong></mark>.</p> <blockquote><p>通过环境变量发现服务</p></blockquote> <p><strong>在 pod 开始运行的时候, Kubernetes 会初始化一系列的环境变量指向现在存在的服务</strong>. 如果你创建的服务早于客户端 pod 的创建, pod 上的进程可以<strong>根据环境变量</strong>获得服务的 IP 地址和端口号. 可以理解为, 服务创建完成之后会把自己的服务信息写入环境变量, 这样之后创建的 pod 就能读取这些环境变量了.</p> <p>在一个运行 pod 上检查环境, 去了解这些环境变量. 现在已经了解了通过 kubectl exec 命令在 pod 上运行一个命令, 但是由于服务的创建晚于 pod 的创建, 那么关于这个服务的环境变量并没有设置, 这个问题也需要解决.</p> <p>在查看服务的环境变量之前, 首先需要删除所有的 pod 使得 ReplicationController <strong>创建全新的 pod</strong>. 在无须知道 pod 的名字的情况下就能删除所有的 pod, 就像这样:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po <span class="token parameter variable">--all</span>
pod <span class="token string">&quot;kubia-7nog1&quot;</span> deleted
pod <span class="token string">&quot;kubia-bf50t&quot;</span> deleted
pod <span class="token string">&quot;kubia-gzwli&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>现在<strong>列出所有新的 pod, 然后选择一个作为 kubectl exec 命令的执行目标</strong>. 一旦选择了目标 pod, 通过在容器中运行 env 来列出所有的环境变量, 如下面的代码清单所示.</p> <p><strong>代码清单-5.6 容器中和服务相关的环境变量</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> kubia-3inly <span class="token function">env</span>
<span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<span class="token assign-left variable"><span class="token environment constant">HOSTNAME</span></span><span class="token operator">=</span>kubia-3inly
<span class="token assign-left variable">KUBERNETES_SERVICE_HOST</span><span class="token operator">=</span><span class="token number">10.111</span>.240.1
<span class="token assign-left variable">KUBERNETES_SERVICE_PORT</span><span class="token operator">=</span><span class="token number">443</span>
<span class="token punctuation">..</span>.
<span class="token assign-left variable">KUBIA_SERVICE_HOST</span><span class="token operator">=</span><span class="token number">10.111</span>.249.153    <span class="token comment"># 这是服务的集群IP</span>
<span class="token assign-left variable">KUBIA_SERVICE_PORT</span><span class="token operator">=</span><span class="token number">80</span>                <span class="token comment"># 这是服务所在的端口</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>在集群中定义了两个服务: <strong>kubernetes 和 kubia</strong>(之前在用 kubectl get svc 命令的时候应该见过); 所以, 列表中显示了和这两个服务相关的环境变量. 在本章开始部分, 创建了 kubia 服务, 在和其有关的环境变量中有 <strong>KUBIA_SERVICE_HOST 和 KUBIA_SERVICE_PORT, 分别代表了 kubia 服务的 IP 地址和端口号</strong>.</p> <p>回顾本章开始部分的前后端的例子, 当前端 pod 需要后端数据库服务 pod 时, 可以通过名为 backend-database 的服务将后端 pod 暴露出来, 然后前端 pod 通过环境变量 BACKEND_DATABASE_SERVICE_HOST 和 BACKEND_DATABASE_SERVICE_PORT 去获得 IP 地址和端口信息.</p> <p>注意: 服务名称中的横杠被转换为下画线, 并且当服务名称用作环境变量名称中的前缀时, 所有的字母都是<strong>大写</strong>的.</p> <p>环境变量是获得服务 IP 地址和端口号的一种方式, 为什么不用 DNS 域名? 为什么 Kubernetes 中没有 DNS 服务器, 并且允许通过 DNS 来获得所有服务的 IP 地址? 事实证明, 它的确如此!</p> <blockquote><p>通过 DNS 发现服务</p></blockquote> <p>还记得第 3 章中在 kube-system 命名空间下列出的所有 pod 的名称吗? 其中一个 pod 被称作 <strong>kube-dns</strong>, 当前的 kube-system 的命名空间中也包含了一个具有相同名字的响应服务.</p> <p>就像名字的暗示, <mark><strong>这个 pod 运行 DNS 服务, 在集群中的其他 pod 都被配置成使用其作为 dns(Kubernetes 通过修改每个容器的/etc/resolv.conf 文件实现). 运行在 pod 上的进程 DNS 查询都会被 Kubernetes 自身的 DNS 服务器响应, 该服务器知道系统中运行的所有服务.</strong></mark></p> <p>注意: <strong>pod 是否使用内部的 DNS 服务器是根据 pod 中 spec 的 dnsPolicy 属性</strong>来决定的.</p> <p>每个服务从内部 DNS 服务器中获得一个 DNS 条目, 客户端的 pod 在知道服务名称的情况下可以通过全限定域名(FQDN)来访问, 而不是诉诸于环境变量.</p> <blockquote><p>通过 FQDN 连接服务</p></blockquote> <p>再次回顾前端-后端的例子, 前端 pod 可以通过打开以下 FQDN 的连接来访问后端数据库服务:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>backend-database.default.svc.cluster.local
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>backend-database 对应于服务名称, default 表示服务在其中定义的名称空间, 而 svc.cluster.local 是在所有集群本地服务名称中使用的可配置集群域后缀.</p> <p>注意: 客户端仍然必须知道服务的端口号. 如果服务使用标准端口号(例如, HTTP 的 80 端口或 Postgres 的 5432 端口), 这样是没问题的. 如果并不是标准端口, 客户端可以从环境变量中获取端口号.</p> <p>连接一个服务可能比这更简单. 如果前端 pod 和数据库 pod 在同一个命名空间下, 可以省略 svc.cluster.local 后缀, 甚至命名空间. 因此可以使用 backend-database 来指代服务. 这简单到不可思议, 不是吗?</p> <p>尝试一下. 尝试使用 FQDN 来代替 IP 去访问 kubia 服务. 另外, 必须在一个存在的 pod 上才能这样做. 已经知道如何通过 kubectl exec 在一个 pod 的容器上去执行一个简单的命令, 但是这一次不是直接运行 curl 命令, 而是运行 bash shell, 这样可以在容器上运行多条命令. 在第2章中, 当想进入容器启动 Docker 时, 调用 docker exec-it bash 命令, 这与此很相似.</p> <blockquote><p>在pod容器中运行shell</p></blockquote> <p>可<mark><strong>以通过 kubectl exec 命令在一个 pod 容器上运行 bash(或者其他形式的 shell)</strong></mark> . 通过这种方式, 可以随意浏览容器, 而无须为每个要运行的命令执行 kubectl exec.</p> <p>注意: shell 的二进制可执行文件必须在容器镜像中可用才能使用.</p> <p>为了正常地使用 shell,kubectl exec 命令需要<strong>添加 –it 选项</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> kubia-3inly <span class="token function">bash</span>
root@kubia-3inly:/<span class="token comment">#</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在进入容器内部, 根据下述的任何一种方式<strong>使用 curl 命令来访问 kubia 服务</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@kubia-3inly:/<span class="token comment"># curl http://kubia.default.svc.cluster.local</span>
You’ve hit kubia-5asi2
root@kubia-3inly:/<span class="token comment"># curl http://kubia.default</span>
You’ve hit kubia-3inly
root@kubia-3inly:/<span class="token comment"># curl http://kubia</span>
You’ve hit kubia-8awf3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>在请求的 URL 中, 可以<strong>将服务的名称作为主机名来访问服务</strong>. 因为根据每个 pod 容器 DNS 解析器配置的方式, 可以将命名空间和 svc.cluster.local 后缀省略掉. 查看一下容器中的 <code>/etc/resilv.conf</code>​ 文件就明白了.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@kubia-3inly:/<span class="token comment"># cat /etc/resolv.conf</span>
search default.svc.cluster.local svc.cluster.local cluster.local <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>无法ping通服务IP的原因</p></blockquote> <p>在继续之前还有最后一问题. 了解了如何创建服务, 很快地去自己创建一个. 但是, 不知道什么原因, 无法访问创建的服务.</p> <p>大家可能会尝试通过进入现有的 pod, 并尝试像上一个示例那样访问该服务来找出问题所在. 然后, 如果仍然无法使用简单的 curl 命令访问服务, 也许会<strong>尝试 ping 服务 IP</strong> 以查看服务是否已启动. 现在来尝试一下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@kubia-3inly:/<span class="token comment"># ping kubia</span>
PING kubia.default.svc.cluster.local <span class="token punctuation">(</span><span class="token number">10.111</span>.249.153<span class="token punctuation">)</span>: <span class="token number">56</span> data bytes
^C--- kubia.default.svc.cluster.local <span class="token function">ping</span> statistics ---
<span class="token number">54</span> packets transmitted, <span class="token number">0</span> packets received, <span class="token number">100</span>% packet loss
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>嗯, curl 这个服务是工作的, 但是却 ping 不通. 这是因为<strong>服务的集群 IP 是一个虚拟 IP, 并且只有在与服务端口结合时才有意义</strong>. 将在第 11 章中解释这意味着什么, 以及服务是如何工作的. 在这里提到这个问题, 因为这是用户在尝试调试异常服务时会做的第一件事(ping 服务的 IP), 而服务的 IP 无法 ping 通会让大多数人措手不及.</p> <h5 id="_5-2-连接集群外部的服务"><a href="#_5-2-连接集群外部的服务" class="header-anchor">#</a> 5.2 连接集群外部的服务</h5> <p>到现在为止, 已经讨论了后端是集群中运行的一个或多个 pod 的服务. 但也存在<mark><strong>希望通过 Kubernetes 服务特性暴露外部服务的情况. 不要让服务将连接重定向到集群中的 pod, 而是让它重定向到外部 IP 和端口</strong></mark>.</p> <p>这样做可以让你充分利用服务负载平衡和服务发现. 在集群中运行的客户端 pod 可以像连接到内部服务一样连接到外部服务.</p> <h6 id="_5-2-1-介绍服务endpoint"><a href="#_5-2-1-介绍服务endpoint" class="header-anchor">#</a> 5.2.1 介绍服务endpoint</h6> <p>在进入如何做到这一点之前, 先阐述一下服务. <strong>服务并不是和 pod 直接相连的</strong>. 相反, 有一种资源介于两者之间--它就是 <mark><strong>Endpoint 资源</strong></mark>. 如果之前在服务上运行过 kubectl describe, 可能已经注意到了 endpoint, 如下面的代码清单所示.</p> <p><strong>代码清单-5.7 用 kubectl describe 展示服务的全部细节</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe svc kubia
Name: kubia
Namespace: default
Labels: <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token comment"># 用于创建endpoint列表的服务pod选择器</span>
Selector: <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia  
Type: ClusterIP
IP: <span class="token number">10.111</span>.249.153
Port: <span class="token operator">&lt;</span>unset<span class="token operator">&gt;</span> <span class="token number">80</span>/TCP
<span class="token comment"># 代表服务endpoint的pod的IP和端口列表</span>
Endpoints: <span class="token number">10.108</span>.1.4:8080,10.108.2.5:8080,10.108.2.6:8080 
Session Affinity: None
No events.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><mark><strong>Endpoint 资源就是暴露一个服务的 IP 地址和端口的列表</strong></mark>, Endpoint 资源和其他 Kubernetes 资源一样, 所以可以使用 kubectl info 来获取它的基本信息.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get endpoints kubia
NAME     ENDPOINTS                                         AGE
kubia    <span class="token number">10.108</span>.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>尽管在 spec 服务中定义了 pod 选择器, 但在重定向传入连接时不会直接使用它. 相反, <strong>选择器用于构建 IP 和端口列表, 然后存储在 Endpoint 资源中. 当客户端连接到服务时, 服务代理选择这些 IP 和端口对中的一个, 并将传入连接重定向到在该位置监听的服务器</strong>.</p> <h6 id="_5-2-2-手动配置服务的endpoint"><a href="#_5-2-2-手动配置服务的endpoint" class="header-anchor">#</a> 5.2.2 手动配置服务的endpoint</h6> <p>或许已经意识到这一点, 服务的 endpoint 与服务解耦后, 可以分别手动配置和更新它们.</p> <p>如果<strong>创建了不包含 pod 选择器的服务, Kubernetes 将不会创建 Endpoint 资源</strong>(毕竟, 缺少选择器, 将不会知道服务中包含哪些 pod). 这样就需要创建 Endpoint 资源来指定该服务的 endpoint 列表.</p> <p>要使用手动配置 endpoint 的方式创建服务, 需要创建服务和 Endpoint 资源.</p> <blockquote><p>创建没有选择器的服务</p></blockquote> <p>首先为服务创建一个 YAML 文件, 如下面的代码清单所示.</p> <p><strong>代码清单-5.8 不含 pod 选择器的服务: external-service.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> external<span class="token punctuation">-</span>service  <span class="token comment"># 服务的名字必须和Endpoint对象的名字相匹配</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>     <span class="token comment"># 服务中没有定义选择器</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>定义一个名为 external-service 的服务, 它将接收端口 80 上的传入连接. 并没有为服务定义一个 pod 选择器.</p> <blockquote><p>为没有选择器的服务创建 Endpoint 资源</p></blockquote> <p>Endpoint 是一个单独的资源并不是服务的一个属性. 由于创建的资源中并不包含选择器, 相关的 Endpoints 资源并没有自动创建, 所以必须<strong>手动创建</strong>. 如下所示的代码清单中列出了 YAML manifest.</p> <p><strong>代码清单-5.9 手动创建 Endpoint 资源: external-service-endpoints.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token comment"># 资源类型为Endpoints</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Endpoints
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> external<span class="token punctuation">-</span>service  <span class="token comment"># endpoint的名称必须和服务的名称相匹配</span>
<span class="token key atrule">subsets</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">addresses</span><span class="token punctuation">:</span>            <span class="token comment"># 服务将连接重定向到endpoint的IP地址</span>
    <span class="token punctuation">-</span> <span class="token key atrule">ip</span><span class="token punctuation">:</span> 11.11.11.11
    <span class="token punctuation">-</span> <span class="token key atrule">ip</span><span class="token punctuation">:</span> 22.22.22.22
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>  
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>            <span class="token comment"># endpoint的目标端口</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>Endpoint 对象需要与服务具有相同的名称, 并包含该服务的目标 IP 地址和端口列表</strong>. 服务和 Endpoint 资源都发布到服务器后, 这样服务就可以像具有 pod 选择器那样的服务正常使用. 在服务创建后创建的容器将包含服务的环境变量, 并且与其 <code>IP:port</code>​ 对的所有连接都将在服务端点之间进行负载均衡.</p> <p>图 5.4 显示了三个 pod 连接到具有外部 endpoint 的服务.</p> <p>如果稍后决定将外部服务迁移到 Kubernetes 中运行的 pod, 可以为服务添加选择器, 从而对 Endpoint 进行自动管理. 反过来也是一样的--将选择器从服务中移除, Kubernetes 将停止更新 Endpoints. 这意味着服务的 IP 地址可以保持不变, 同时服务的实际实现却发生了改变.</p> <p><img src="/img/image-20240221212052-bx9z56l.png" alt="image" title="图5.4 pod 关联到具有两个外部 endpoint 的服务上"></p> <h6 id="_5-2-3-为外部服务创建别名"><a href="#_5-2-3-为外部服务创建别名" class="header-anchor">#</a> 5.2.3 为外部服务创建别名</h6> <p>除了手动配置服务的 Endpoint 来代替公开外部服务方法, 有一种更简单的方法, 就是**通过其完全限定域名(FQDN)访问外部服务. **</p> <blockquote><p>创建ExternalName类型的服务</p></blockquote> <p>要创建一个<strong>具有别名的外部服务</strong>时, 要将创建服务资源的一个 type 字段设置为 <strong>ExternalName</strong>. 例如, 设想一下在 api.somecompany.com 上有公共可用的 API, 可以定义一个指向它的服务, 如下面的代码清单所示.</p> <p><strong>代码清单-5.10 ExternalName 类型的服务: external-service-externalname.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> external<span class="token punctuation">-</span>service
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> ExternalName    <span class="token comment"># 代码的type被设置成ExeternalName</span>
  <span class="token key atrule">externalName</span><span class="token punctuation">:</span> someapi.somecompany.com   <span class="token comment"># 实际服务的完全限定域名</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>服务创建完成后, <strong>pod 可以通过 external-service.default.svc.cluster.local 域名(甚至是 external-service)连接到外部服务</strong>, 而不是使用服务的实际 FQDN. 这隐藏了实际的服务名称及其使用该服务的 pod 的位置, 允许修改服务定义, 并且在以后如果将其指向不同的服务, 只需简单地修改 externalName 属性, 或者将类型重新变回 ClusterIP 并为服务创建 Endpoint--无论是手动创建, 还是对服务上指定标签选择器使其自动创建.</p> <p>ExternalName 服务仅在 DNS 级别实施--为服务创建了简单的 CNAME DNS 记录. 因此, 连接到服务的客户端将直接连接到外部服务, 完全绕过服务代理. 出于这个原因, 这些类型的服务甚至不会获得集群 IP.</p> <p>注意: CNAME 记录指向完全限定的域名而不是数字 IP 地址.</p> <h5 id="_5-3-将服务暴露给外部客户端"><a href="#_5-3-将服务暴露给外部客户端" class="header-anchor">#</a> 5.3 将服务暴露给外部客户端</h5> <p>到目前为止, 只讨论了<strong>集群内服务如何被 pod 使用</strong>; 但是, 还需要<strong>向外部公开某些服务</strong>. 例如前端 web 服务器, 以便外部客户端可以访问它们, 就像图 5.5 描述的那样.</p> <p><img src="/img/image-20240227232324-erviptt.png" alt="image" title="图5.5 将服务暴露给外部客户端"></p> <p>有几种方式可以<strong>在外部访问服务</strong>:</p> <ul><li><strong>将服务的类型设置成 NodePort</strong>. 每个集群节点都会在节点上打开一个端口, 对于 NodePort 服务, 每个集群节点在节点本身(因此得名叫 NodePort)上打开一个端口, 并将在该端口上接收到的流量重定向到基础服务. 该服务仅在内部集群 IP 和端口上才可访问, 但也可通过所有节点上的专用端口访问.</li> <li><strong>将服务的类型设置成 LoadBalance, NodePort 类型的一种扩展</strong>. 这使得服务可以通过一个<strong>专用的负载均衡器</strong>来访问, 这是由 Kubernetes 中正在运行的云基础设施提供的. 负载均衡器将流量重定向到跨所有节点的节点端口. 客户端通过负载均衡器的 IP 连接到服务.</li> <li><strong>创建一个 Ingress 资源, 这是一个完全不同的机制, 通过一个 IP 地址公开多个服务</strong>. 它运行在 HTTP 层(网络协议第 7 层)上, 因此可以提供比工作在第 4 层的服务更多的功能. 后面将在 5.4 节介绍 Ingress 资源.</li></ul> <h6 id="_5-3-1-使用nodeport类型的服务"><a href="#_5-3-1-使用nodeport类型的服务" class="header-anchor">#</a> 5.3.1 使用NodePort类型的服务</h6> <p><strong>将一组 pod 公开给外部客户端的第一种方法是创建一个服务并将其类型设置为 NodePort. 通过创建 NodePort 服务, 可以让 Kubernetes 在其所有节点上保留一个端口(所有节点上都使用相同的端口号), 并将传入的连接转发给作为服务部分的 pod</strong>.</p> <p>这与常规服务类似(它们的实际类型是 ClusterIP), 但是不仅可以通过服务的内部集群 IP 访问 NodePort 服务, 还可以通过任何节点的 IP 和预留节点端口访问 NodePort 服务.</p> <p>当尝试与 NodePort 服务交互时, 意义更加重大.</p> <blockquote><p>创建NodePort类型的服务</p></blockquote> <p>现在将创建一个 NodePort 服务, 以查看如何使用它. 下面的代码清单显示了服务的 YAML.</p> <p><strong>代码清单-5.11 NodePort 服务定义: kubia-svc-nodeport.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>nodeport
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token comment"># 为NodePort设置服务类型</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token comment"># 服务集群IP和端口号</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>  <span class="token comment"># 背后pod的目标端口号</span>
    <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">30123</span>   <span class="token comment"># 通过集群节点的30123端口可以访问该服务</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p><strong>将类型设置为 NodePort 并指定该服务应该绑定到的所有集群节点的节点端口</strong>. 指定端口不是强制性的. 如果忽略它, Kubernetes 将选择一个随机端口.</p> <p>注意: 当在 GKE 中创建服务时, kubectl 打印出一个关于必须配置防火墙规则的警告. 接下来的章节将讲述如何处理.</p> <blockquote><p>查看 NodePort 类型的服务</p></blockquote> <p>查看该服务的基础信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc kubia-nodeport
NAME             CLUSTER-IP         EXTERNAL-IP         PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>          AGE
kubia-nodeport   <span class="token number">10.111</span>.254.223     <span class="token operator">&lt;</span>nodes<span class="token operator">&gt;</span>             <span class="token number">80</span>:30123/TCP     2m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>看看 EXTERNAL-IP 列. 它显示 nodes, <strong>表明服务可通过任何集群节点的 IP 地址访问</strong>. PORT(S) 列显示集群 IP(80)的内部端口和节点端口(30123), 可以通过以下地址访问该服务:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">10.11</span>.254.223:80
<span class="token operator">&lt;</span>1st node’s IP<span class="token operator">&gt;</span>:30123
<span class="token operator">&lt;</span>2nd node’s IP<span class="token operator">&gt;</span>:30123 等等
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>图 5.6 显示了<strong>服务暴露在两个集群节点的端口 30123 上</strong>(这适用于在 GKE 上运行的情况; Minikube 只有一个节点, 但原理相同). 到达任何一个端口的传入连接将被重定向到一个随机选择的 pod, 该 pod 是否位于接收到连接的节点上是不确定的.</p> <p><img src="/img/image-20240227232357-738f2bt.png" alt="image" title="图5.6 外部客户端通过节点1或者节点2连接到 NodePort 服务"></p> <p>在第一个节点的端口 30123 收到的连接, 可以被重定向到第一节点个上运行的 pod, 也可能是第二个节点上运行的 pod.</p> <blockquote><p>更改防火墙规则, 让外部客户端访问我们的 NodePort 服务</p></blockquote> <p>如前所述, 在通过节点端口访问服务之前, 需要配置谷歌云平台的防火墙, 以允许外部连接到该端口上的节点, 如下所示.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud compute firewall-rules create kubia-svc-rule <span class="token parameter variable">--allow</span><span class="token operator">=</span>tcp:30123
Created <span class="token punctuation">[</span>https://www.googleapis.com/compute/v1/projects/kubia-
<span class="token number">1295</span>/global/firewalls/kubia-svc-rule<span class="token punctuation">]</span>.
NAME NETWORK SRC_RANGES RULES SRC_TAGS TARGET_TAGS
kubia-svc-rule default <span class="token number">0.0</span>.0.0/0 tcp:30123
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以通过其中一个节点的 IP 的端口 30123 访问服务, 但是需要首先找出节点的 IP. 请参阅补充内容了解如何做到这一点.</p> <blockquote><p>使用 JSONPath 获取所有节点的 IP</p></blockquote> <p>可以在节点的 JSON 或 YAML 描述符中找到 IP. 但并不是在很大的 JSON 中筛选, 而是可以<strong>利用 kubectl 只打印出节点 IP</strong> 而不是整个服务的定义.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>kubectl get nodes <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">'{.items[*].status.addresses[?(@.type==&quot;ExternalIP&quot;)].address}'</span>
<span class="token number">130.211</span>.97.55 <span class="token number">130.211</span>.99.206
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>通过指定 kubectl 的 JSONPath, 使得其只输出需要的信息. 你可能已经熟悉 XPath, 并且知道如何使用 XML, JSONPath 基本上是 JSON 的 XPath. 上例中的 JSONPath 指示 kubectl 执行以下操作:</p> <ul><li>浏览 item 属性中的所有元素.</li> <li>对于每个元素, 输入 status 属性.</li> <li>过滤 address 属性的元素, 仅包含那些具有将 type 属性设置为 ExternalIP 的元素.</li> <li>最后, 打印过滤元素的 address 属性.</li></ul> <p>一旦知道了节点的 IP, 就可以尝试通过以下方式访问服务:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://130.211.97.55:30123
You<span class="token string">'ve hit kubia-ym8or
$ curl http://130.211.99.206:30123
You'</span>ve hit kubia-xueq1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>提示: 使用 Minikube 时, 可以运行 <code>minikube sevrvice &lt;service-name&gt;[-n&lt;namespace&gt;]</code>​ 命令, 通过浏览器轻松访问 NodePort 服务.</p> <p>正如所看到的, 现在<strong>整个互联网可以通过任何节点上的 30123 端口访问到你的 pod</strong>. 客户端发送请求的节点并不重要. 但是, 如果只将客户端指向第一个节点, 那么当该节点发生故障时, 客户端无法再访问该服务. 这就是为什么将负载均衡器放在节点前面以确保发送的请求传播到所有健康节点, 并且从不将它们发送到当时处于脱机状态的节点的原因.</p> <p>如果 Kubernetes 集群支持它(当 Kubernetes 部署在云基础设施上时, 大多数情况都是如此), 那么可以通过创建一个 Load Badancer 而不是 NodePort 服务自动生成负载均衡器. 接下来介绍此部分.</p> <h6 id="_5-3-2-通过负载均衡器将服务暴露出来"><a href="#_5-3-2-通过负载均衡器将服务暴露出来" class="header-anchor">#</a> 5.3.2 通过负载均衡器将服务暴露出来</h6> <p>在云提供商上运行的 Kubernetes 集群<strong>通常支持从云基础架构自动提供负载平衡器</strong>. 所有需要做的就是设置<strong>服务的类型为 Load Badancer</strong> 而不是 NodePort. 负载均衡器拥有自己<strong>独一无二</strong>的可公开访问的 IP 地址, 并将所有连接重定向到服务. 可以通过负载均衡器的 IP 地址访问服务.</p> <p>如果 Kubernetes 在不支持 Load Badancer 服务的环境中运行, 则不会调配负载平衡器, 但该服务仍将表现得像一个 NodePort 服务. 这是因为 Load Badancer 服务是 NodePort 服务的扩展. 可以在支持 Load Badancer 服务的 Google Kubernetes Engine 上运行此示例. Minikube 没有, 至少在写作本书的时候.</p> <blockquote><p>创建 LoadBalance 服务</p></blockquote> <p>要使用服务前面的负载均衡器, 请按照以下 YAML manifest 创建服务, 代码清单如下所示.</p> <p><strong>代码清单-5.12 Load Badancer 类型的服务: kubia-svc-loadbalancer.yaml</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer  <span class="token comment"># 该服务从Kubernetes集群的基础架构获取负载均衡器</span>
  ports:
  - port: <span class="token number">80</span>
    targetPort: <span class="token number">8080</span>
  selector:
    app: kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>服务类型设置为 <strong>LoadBalancer</strong> 而不是 NodePort. 如果没有指定特定的节点端口, Kubernetes 将会选择一个端口.</p> <blockquote><p>通过负载均衡器连接服务</p></blockquote> <p>创建服务后, 云基础架构需要一段时间才能创建负载均衡器并将其 IP 地址写入服务对象. 一旦这样做了, <strong>IP 地址将被列为服务的外部 IP 地址</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc kubia-loadbalancer
NAME                 CLUSTER-IP         EXTERNAL-IP     PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>         AGE
kubia-loadbalancer   <span class="token number">10.111</span>.241.153     <span class="token number">130.211</span>.53.173  <span class="token number">80</span>:32143/TCP    1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在这种情况下, 负载均衡器的 IP 地址为 130.211.53.173, 因此现在可以通过该 IP 地址访问该服务:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://130.211.53.173
You've hit kubia-xueq1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>成功了! 可能像你已经注意到的那样, 这次不需要像以前使用 NodePort 服务那样来关闭防火墙.</p> <blockquote><p>会话亲和性和 Web 浏览器</p></blockquote> <p>由于服务现在已暴露在外, 因此可以尝试使用网络浏览器访问它. 但是会看到一些可能觉得奇怪的东西--<strong>每次浏览器都会碰到同一个 pod</strong>. 此时服务的会话亲和性是否发生变化? 使用 kubectl explain, 可以再次检查服务的会话亲缘性是否仍然设置为 None, 那么为什么不同的浏览器请求不会碰到不同的 pod, 就像使用 curl 时那样?</p> <p>现在阐述为什么会这样. <strong>浏览器使用 keep-alive 连接, 并通过单个连接发送所有请求, 而 curl 每次都会打开一个新连接</strong>. 服务在连接级别工作, 所以当首次打开与服务的连接时, 会选择一个随机集群, 然后将属于该连接的所有网络数据包全部发送到单个集群. 即使会话亲和性设置为 None, 用户也会始终使用相同的 pod(直到连接关闭).</p> <p>请参阅图 5.7, 了解 HTTP 请求如何传递到该 pod. 外部客户端(可以使用 curl)连接到负载均衡器的 80 端口, 并路由到其中一个节点上的隐式分配节点端口. 之后该连接被转发到一个 pod 实例.</p> <p>如前所述, <strong>LoadBalancer 类型的服务是一个具有额外的基础设施提供的负载平衡器 NodePort 服务</strong>. 如果使用 kubectl describe 来显示有关该服务的其他信息, 则会看到为该服务选择了一个节点端口. 如果要为此端口打开防火墙, 就像在上一节中对 NodePort 服务所做的那样, 也可以通过节点 IP 访问服务.</p> <p>提示: 如果使用的是 Minikube, 尽管负载平衡器不会被分配, 仍然可以通过节点端口(位于 Minikube VM 的 IP 地址)访问该服务.</p> <p><img src="/img/image-20240227232429-3for7ax.png" alt="image" title="图 5.7 外部客户端连接一个 LoadBalancer 服务"></p> <h6 id="_5-3-3-了解外部连接的特性"><a href="#_5-3-3-了解外部连接的特性" class="header-anchor">#</a> 5.3.3 了解外部连接的特性</h6> <p>你必须了解与服务的外部发起的<strong>连接</strong>有关的几件事情.</p> <blockquote><p>了解并防止不必要的网络跳数</p></blockquote> <p><strong>当外部客户端通过节点端口连接到服务时(这也包括先通过负载均衡器时的情况), 随机选择的 pod 并不一定在接收连接的同一节点上运行</strong>. 可能需要额外的网络跳转才能到达 pod, 但这种行为并不符合期望.</p> <p>可以通过将服务配置为仅将外部通信重定向到接收连接的节点上运行的 pod 来阻止此额外跳数. 这是通过在服务的 spec 部分中设置 externalTrafficPolicy 字段来完成的:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>spec:
  externalTrafficPolicy: Local
  <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>如果服务定义包含此设置, 并且通过服务的节点端口打开外部连接, 则服务代理将选择本地运行的 pod. 如果没有本地 pod 存在, 则连接将挂起(它不会像不使用注解那样, 将其转发到随机的全局 pod). 因此, 需要确保负载平衡器将连接转发给至少具有一个 pod 的节点.</p> <p>使用这个注解还有其他缺点. 通常情况下, 连接均匀分布在所有的 pod 上, 但使用此注解时, 情况就不再一样了.</p> <p>想象一下两个节点有三个 pod. 假设节点 A 运行一个 pod, 节点 B 运行另外两个 pod. 如果负载平衡器在两个节点间均匀分布连接, 则节点 A 上的 pod 将接收所有连接的 50%, 但节点 B 上的两个 pod 每个只能接收25%, 如图 5.8 所示.</p> <p><img src="/img/image-20240227232454-pih7w1i.png" alt="image" title="图5.8 使用 local 外部流量策略的服务可能会导致跨 pod 的负载分布不均衡"></p> <blockquote><p>记住客户端 IP 是不记录的</p></blockquote> <p>通常, 当集群内的客户端连接到服务时, 支持服务的 pod 可以<strong>获取客户端的 IP 地址</strong>. 但是, 当通过节点端口接收到连接时, 由于对数据包执行了源网络地址转换(SNAT), 因此数据包的<strong>源 IP 将发生更</strong>改.</p> <p><strong>后端的 pod 无法看到实际的客户端 IP</strong>, 这对于某些需要了解客户端 IP 的应用程序来说可能是个问题. 例如, 对于 Web 服务器, 这意味着访问日志无法显示浏览器的 IP.</p> <p>上一节中描述的 local 外部流量策略会影响客户端 IP 的保留, 因为在接收连接的节点和托管目标 pod 的节点之间没有额外的跳跃(不执行 SNAT).</p> <h5 id="_5-4-通过ingress暴露服务"><a href="#_5-4-通过ingress暴露服务" class="header-anchor">#</a> 5.4 通过Ingress暴露服务</h5> <p>现在已经介绍了向集群外部的客户端公开服务的两种方法, 还有另一种方法--<mark><strong>创建 Ingress 资源</strong></mark>.</p> <p>定义: <strong>Ingress(名词)——进入或进入的行为; 进入的权利; 进入的手段或地点; 入口</strong>.</p> <p>接下来解释为什么需要另一种方式从外部访问 Kubernetes 服务.</p> <blockquote><p>为什么需要Ingress</p></blockquote> <p>一个重要的原因是每个 LoadBalancer 服务都需要自己的负载均衡器, 以及独有的公有 IP 地址, 而 <mark><strong>Ingress 只需要一个公网 IP 就能为许多服务提供访问. 当客户端向 Ingress 发送 HTTP 请求时, Ingress 会根据请求的主机名和路径决定请求转发到的服务</strong></mark>, 如图 5.9 所示.</p> <p><img src="/img/image-20240221215624-hixcwvm.png" alt="image" title="图5.9 通过一个 Ingress 暴露多个服务"></p> <p>Ingress 在网络栈(HTTP)的应用层操作, 并且可以<strong>提供一些服务不能实现的功能</strong>, 诸如基于 cookie 的会话亲和性(session affinity)等功能.</p> <blockquote><p>Ingress控制器是必不可少的</p></blockquote> <p>在介绍 Ingress 对象提供的功能之前, 必须强调<mark><strong>只有 Ingress 控制器在集群中运行, Ingress 资源才能正常工作. 不同的 Kubernetes 环境使用不同的控制器实现, 但有些并不提供默认控制器</strong></mark>.</p> <p>例如, Google Kubernetes Engine 使用 Google Cloud Platform 带有的 HTTP 负载平衡模块来提供 Ingress 功能. 最初, Minikube 没有提供可以立即使用的控制器, 但它现在包含一个可以启用的附加组件, 可以试用 Ingress 功能. 请根据下面的补充信息里的说明确保 Ingress 功能已启用.</p> <blockquote><p>在 minikube 上启动 Ingress 的扩展功能</p></blockquote> <p>如果使用 Minikube 运行本书中的示例, 则需要确保<strong>已启用 Ingress 附加组件</strong>. 可以通过列出所有附件来检查 Ingress 是否已启动:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube addons list
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: disabled    <span class="token comment"># Ingress组件没有启动</span>
- registry-creds: disabled
- addon-manager: enabled
- dashboard: enabled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>通过本书可以了解这些附加组件, 但应该对 dashboard 和 kube-dns 附件的用途十分清楚. 启用 Ingress 附加组件, 并查看正在运行的 Ingress:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube addons <span class="token builtin class-name">enable</span> ingress
ingress was successfully enabled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这应该会<strong>在另一个 pod 上运行一个 Ingress 控制器</strong>. 控制器 pod 很可能位于 kube-system 命名空间中, 但也不一定是这样, 所以使用 --all-namespaces 选项列出所有命名空间中正在运行的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po --all-namespaces
NAMESPACE         NAME                             READY   STATUS     RESTARTS  AGE
default           kubia-rsv5m                      <span class="token number">1</span>/1     Running    <span class="token number">0</span>         13h
default           kubia-fe4ad                      <span class="token number">1</span>/1     Running    <span class="token number">0</span>         13h
default           kubia-ke823                      <span class="token number">1</span>/1     Running    <span class="token number">0</span>         13h
kube-system       default-http-backend-5wb0h       <span class="token number">1</span>/1     Running    <span class="token number">0</span>         18m
kube-system       kube-addon-manager-minikube      <span class="token number">1</span>/1     Running    <span class="token number">3</span>         6d
kube-system       kube-dns-v20-101vq               <span class="token number">3</span>/3     Running    <span class="token number">9</span>         6d
kube-system       kubernetes-dashboard-jxd9l       <span class="token number">1</span>/1     Running    <span class="token number">3</span>         6d
kube-system       nginx-ingress-controller-gdts0   <span class="token number">1</span>/1     Running    <span class="token number">0</span>         18m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>在输出的底部, 会<strong>看到 Ingress 控制器 pod. 该名称暗示 Nginx(一种开源 HTTP 服务器并可以做反向代理)用于提供 Ingress 功能</strong>.</p> <p>提示: 当不知道 pod(或其他类型的资源)所在的命名空间, 或者是否希望跨所有命名空间列出资源时, 利用补充说明中提到的 --all-namespaces 选项非常方便.</p> <h6 id="_5-4-1-创建ingress资源"><a href="#_5-4-1-创建ingress资源" class="header-anchor">#</a> 5.4.1 创建Ingress资源</h6> <p>已经确认集群中正在运行 Ingress 控制器, 因此现在可以<strong>创建一个 Ingress 资源</strong>. 下面的代码清单显示了 Ingress 的示例 YAML:</p> <p><strong>代码清单-5.13 Ingress 资源的定义: kubia-ingress.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token comment"># 资源类型为Ingress</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">rules</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> kubia.example.com    <span class="token comment"># Ingress将域名kubia.example.com映射到你的服务</span>
    <span class="token key atrule">http</span><span class="token punctuation">:</span>
      <span class="token key atrule">paths</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /
        <span class="token key atrule">backend</span><span class="token punctuation">:</span>        <span class="token comment"># 将所有的请求发送到kubia-nodeport服务的80端口</span>
          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>nodeport
          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p><strong>上面定义了一个单一规则的 Ingress, 确保 Ingress 控制器收到的所有请求主机 kubia.example.com 的 HTTP 请求, 将被发送到端口 80 上的 kubia-nodeport 服务</strong>.</p> <p>注意: 云供应商的 Ingress 控制器(例如 GKE)要求 Ingress 指向一个 NodePort 服务. 但 Kubernetes 并没有这样的要求.</p> <h6 id="_5-4-2-通过ingress访问服务"><a href="#_5-4-2-通过ingress访问服务" class="header-anchor">#</a> 5.4.2 通过Ingress访问服务</h6> <p>要通过 http://kubia.example.com 访问服务, 需要确保<strong>域名解析为 Ingress 控制器的 IP</strong>.</p> <blockquote><p>获取 Ingress 的 IP 地址</p></blockquote> <p>要查找 IP, 需要列出 Ingress:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get ingresses
NAME     HOSTS             ADDRESS         PORTS     AGE
kubia    kubia.example.com <span class="token number">192.168</span>.99.100  <span class="token number">80</span>        29m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: 在云提供商的环境上运行时, 地址可能需要一段时间才能显示, 因为 Ingress 控制器在幕后调配负载均衡器.</p> <p>IP 在 ADDRESS 列中显示出来.</p> <blockquote><p>确保在 Ingress 中配置的 Host 指向 Ingress 的 IP 地址</p></blockquote> <p>一旦知道 IP 地址, 通过<strong>配置 DNS 服务器</strong>将 kubia.example.com 解析为此 IP 地址, 或者在 <code>/ect/hosts</code>​ 文件(Windows 系统为 <code>C:\windows\system32\drivers\etc\hosts</code>​)中添加下面一行内容:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token number">192.168</span>.99.100 kubia.example.com
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>通过 Ingress 访问 pod</p></blockquote> <p>环境都已经建立完毕, 可以通过 http://kubia.example.com 地址访问服务(使用浏览器或者 curl 命令):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://kubia.example.com
You've hit kubia-ke823
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在已经通过 Ingress 成功访问了该服务, 接下来对其展开深层次的研究.</p> <blockquote><p>了解 Ingress 的工作原理</p></blockquote> <p>图 5.10 显示了客户端如何通过 Ingress 控制器连接到其中一个 pod. <mark><strong>客户端首先对 kubia.example.com 执行 DNS 查找, DNS 服务器(或本地操作系统)返回了 Ingress 控制器的 IP. 客户端然后向 Ingress 控制器发送 HTTP 请求, 并在 Host 头中指定 kubia.example.com. 控制器从该头部确定客户端尝试访问哪个服务, 通过与该服务关联的 Endpoint 对象查看 pod IP, 并将客户端的请求转发给其中一个 pod</strong></mark>.</p> <p>如你所见, Ingress 控制器不会将请求转发给该服务, 只用它来<strong>选择一个 pod</strong>. 大多数(即使不是全部)控制器都是这样工作的.</p> <p><img src="/img/image-20240227232527-vk7vt50.png" alt="image" title="图5.10 通过 Ingress 访问 pod"></p> <h6 id="_5-4-3-通过相同的ingress暴露多个服务"><a href="#_5-4-3-通过相同的ingress暴露多个服务" class="header-anchor">#</a> 5.4.3 通过相同的Ingress暴露多个服务</h6> <p>如果仔细查看 Ingress 规范, 则会看到 rules 和 paths 都是<strong>数组</strong>, 因此它们可以包含多个条目. <mark><strong>一个 Ingress 可以将多个主机和路径映射到多个服务</strong></mark>, 先来看看 paths 字段.</p> <blockquote><p>将不同的服务映射到相同主机的不同路径</p></blockquote> <p>将不同的服务映射到相同主机的不同 paths, 以下面的代码清单为例.</p> <p><strong>代码清单-5.14 在同一个主机, 不同的路径上, Ingress 暴露出多个服务</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token comment"># 资源类型为Ingress</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">rules</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> kubia.example.com
      <span class="token key atrule">http</span><span class="token punctuation">:</span>
        <span class="token key atrule">paths</span><span class="token punctuation">:</span>    <span class="token comment"># 对kubia.example.com/kubia的请求将会转发至kubia服务</span>
        <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /kubia  
          <span class="token key atrule">backend</span><span class="token punctuation">:</span>
            <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> kubia
            <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
        <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /bar    <span class="token comment"># 对kubia.example.com/bar的请求将会转发至bar服务</span>
          <span class="token key atrule">backend</span><span class="token punctuation">:</span>
            <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> bar
            <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>​​在这种情况下, <strong>根据请求的 URL 中的路径, 请求将发送到两个不同的服务</strong>. 因此, <mark><strong>客户端可以通过一个 IP 地址(Ingress 控制器的 IP 地址)访问两种不同的服务</strong></mark>.</p> <blockquote><p>将不同的服务映射到不同的主机上</p></blockquote> <p>同样, 可以<mark><strong>使用 Ingress 根据 HTTP 请求中的主机而不是(仅)路径映射到不同的服务</strong></mark>, 如下面的代码清单所示.</p> <p><strong>代码清单-5.15 Ingress 根据不同的主机(host)暴露出多种服务</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token comment"># 资源类型为Ingress</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">rules</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> foo.example.com    <span class="token comment"># 对foo.example.com的请求将会转发至foo服务</span>
    <span class="token key atrule">http</span><span class="token punctuation">:</span>
      <span class="token key atrule">paths</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /
        <span class="token key atrule">backend</span><span class="token punctuation">:</span>
          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> foo
          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> bar.example.com    <span class="token comment"># 对bar.example.com的请求将会转发至bar服务</span>
    <span class="token key atrule">http</span><span class="token punctuation">:</span>
      <span class="token key atrule">paths</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /
        <span class="token key atrule">backend</span><span class="token punctuation">:</span>
          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> bar
          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p><mark><strong>根据请求中的 Host 头(虚拟主机在网络服务器中处理的方式), 控制器收到的请求将被转发到 foo 服务或 bar 服务. DNS 需要将 foo.example.com 和 bar.example.com 域名都指向 Ingress 控制器的 IP 地址.</strong></mark></p> <h6 id="_5-4-4-配置ingress处理tls传输"><a href="#_5-4-4-配置ingress处理tls传输" class="header-anchor">#</a> 5.4.4 配置Ingress处理TLS传输</h6> <p>前面已经知道 Ingress 如何<strong>转发 HTTP 流量</strong>. 但是 HTTPS 呢? 接下来了解一下<strong>如何配置 Ingress 以支持 TLS</strong>.</p> <blockquote><p>为 Ingress 创建 TLS 认证</p></blockquote> <p>当客户端创建到 Ingress 控制器的 TLS 连接时, 控制器将<strong>终止 TLS 连接</strong>. 客户端和控制器之间的通信是加密的, 而控制器和后端 pod 之间的通信则不是. <strong>运行在 pod 上的应用程序不需要支持 TLS</strong>. 例如, <mark><strong>如果 pod 运行 web 服务器, 则它只能接收 HTTP 通信, 并让 Ingress 控制器负责处理与 TLS 相关的所有内容. 要使控制器能够这样做, 需要将证书和私钥附加到 Ingress</strong></mark>. 这两个必需资源存储在称为 <strong>Secret</strong> 的 Kubernetes 资源中, 然后<strong>在 Ingress manifest 中引用它</strong>. 后面将在第 7 章中详细介绍 Secret. 现在, 只需创建 Secret, 而不必太在意.</p> <p>首先, 需要创建私钥和证书:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ openssl genrsa <span class="token parameter variable">-out</span> tls.key <span class="token number">2048</span>
$ openssl req <span class="token parameter variable">-new</span> <span class="token parameter variable">-x509</span> <span class="token parameter variable">-key</span> tls.key <span class="token parameter variable">-out</span> tls.cert <span class="token parameter variable">-days</span> <span class="token number">360</span> -subj/CN<span class="token operator">=</span>kubia.example.com
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>像下述两个文件一样创建 Secret:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create secret tls tls-secret <span class="token parameter variable">--cert</span><span class="token operator">=</span>tls.cert <span class="token parameter variable">--key</span><span class="token operator">=</span>tls.key
secret <span class="token string">&quot;tls-secret&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>通过 CertificateSigningRequest 资源签署证书</p></blockquote> <p>可以不通过自己签署证书, 而是<strong>通过创建 CertificateSigningRequest(CSR)资源来签署</strong>. 用户或他们的应用程序可以创建一个常规证书请求, 将其放入 CSR 中, 然后由人工操作员或自动化程序批准请求, 像这样:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl certificate approve <span class="token operator">&lt;</span>name of the CSR<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后可以从 CSR 的 status.certificate 字段中检索签名的证书.</p> <p>请注意, <strong>证书签署者组件必须在集群中运行, 否则创建 CertificateSigningRequest 以及批准或拒绝将不起作用</strong>.</p> <p><strong>私钥和证书现在存储在名为 tls-secret 的 Secret 中</strong>. 现在, 可以更新 Ingress 对象, 以便它也接收 kubia.example.com 的 HTTPS 请求. Ingress 现在看起来应该像下面的代码清单.</p> <p><strong>代码清单-5.16 Ingress 处理 TLS 传输: kubia-ingress-tls.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">tls</span><span class="token punctuation">:</span>        <span class="token comment"># 这个属性下包含了所有的TLS的配置</span>
  <span class="token punctuation">-</span> <span class="token key atrule">hosts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> kubia.example.com    <span class="token comment"># 将接收来自kubia.example.com主机的TLS连接</span>
    <span class="token key atrule">secretName</span><span class="token punctuation">:</span> tls<span class="token punctuation">-</span>secret    <span class="token comment"># 从tls-secret中获得之前创立的私钥和证书</span>
  <span class="token key atrule">rules</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> kubia.example.com
    <span class="token key atrule">http</span><span class="token punctuation">:</span>
      <span class="token key atrule">paths</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> /
        <span class="token key atrule">backend</span><span class="token punctuation">:</span>
          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>nodeport
          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>提示: 可以调用 <code>kubectl apply-f kubia-ingress-tls.yaml</code>​ 使用文件中指定的内容来更新 Ingress 资源, 而不是通过删除并从新文件重新创建的方式.</p> <p>现在可以使用 HTTPS 通过 Ingress 访问服务:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-k</span> <span class="token parameter variable">-v</span> https://kubia.example.com/kubia
* About to connect<span class="token punctuation">(</span><span class="token punctuation">)</span> to kubia.example.com port <span class="token number">443</span> <span class="token punctuation">(</span><span class="token comment">#0)</span>
<span class="token punctuation">..</span>.
* Server certificate:
* subject: <span class="token assign-left variable">CN</span><span class="token operator">=</span>kubia.example.com
<span class="token punctuation">..</span>.
<span class="token operator">&gt;</span> GET /kubia HTTP/1.1
<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.
You've hit kubia-xueq1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>该命令的输出显示应用程序的响应, 以及配置的 Ingress 的<strong>证书服务器的响应</strong>.</p> <p>注意: 对 Ingress 功能的支持因不同的 Ingress 控制器实现而异, 因此请检查特定实现的文档以确定支持的内容.</p> <h5 id="_5-5-pod就绪后发出信号"><a href="#_5-5-pod就绪后发出信号" class="header-anchor">#</a> 5.5 pod就绪后发出信号</h5> <p>还有一件关于 Service 和 Ingress 的事情需要考虑. 已经了解到, <strong>如果 pod 的标签与服务的 pod 选择器相匹配, 那么 pod 就将作为服务的后端</strong>. 只要创建了具有适当标签的新 pod, 它就成为服务的一部分, 并且请求开始被重定向到 pod. 但<mark><strong>如果 pod 没有准备好, 如何处理服务请求呢</strong></mark>?</p> <p>该 pod 可能需要时间来加载配置或数据, 或者可能需要执行预热过程以防止第一个用户请求时间太长影响了用户体验. 在这种情况下, <strong>不希望该 pod 立即开始接收请求, 尤其是在运行的实例可以正确快速地处理请求的情况下. 不要将请求转发到正在启动的 pod 中, 直到完全准备就绪</strong>.</p> <h6 id="_5-5-1-介绍就绪探针"><a href="#_5-5-1-介绍就绪探针" class="header-anchor">#</a> 5.5.1 介绍就绪探针</h6> <p>在之前的章节中, 了解了存活探针, 以及它们如何通过确保异常容器自动重启来保持应用程序的正常运行. 与存活探针类似, <strong>Kubernetes 还允许为容器定义准备就绪探针</strong>.</p> <p><mark><strong>就绪探测器会定期调用, 并确定特定的 pod 是否接收客户端请求. 当容器的准备就绪探测返回成功时, 表示容器已准备好接收请求</strong></mark>.</p> <p>这个准备就绪的概念显然是<strong>每个容器特有</strong>的东西. Kubernetes 只能检查在容器中运行的应用程序是否响应一个简单的 <code>GET /</code>​ 请求, 或者它可以响应特定的 URL 路径(该 URL 导致应用程序执行一系列检查以确定它是否准备就绪). 考虑到应用程序的具体情况, 这种确切的准备就绪的判定是应用程序开发人员的责任.</p> <blockquote><p>就绪探针的类型</p></blockquote> <p>像存活探针一样, 就绪探针有三种类型:</p> <ul><li><strong>Exec 探针, 执行进程的地方</strong>. 容器的状态由进程的退出状态代码确定.</li> <li><strong>HTTP GET 探针</strong>, 向容器发送 HTTP GET 请求, 通过响应的 HTTP 状态代码判断容器是否准备好.</li> <li><strong>TCP socket 探针</strong>, 它打开一个 TCP 连接到容器的指定端口. 如果连接已建立, 则认为容器已准备就绪.</li></ul> <blockquote><p>了解就绪探针的操作</p></blockquote> <p>启动容器时, 可以为 Kubernetes 配置一个<strong>等待时间</strong>, 经过等待时间后才可以执行第一次准备就绪检查. 之后, 它会<strong>周期性地调用探针, 并根据就绪探针的结果采取行动</strong>. 如果某个 pod 报告它尚未准备就绪, 则会从该服务中删除该 pod. 如果 pod 再次准备就绪, 则重新添加 pod.</p> <p><mark><strong>与存活探针不同, 如果容器未通过准备检查, 则不会被终止或重新启动. 这是存活探针与就绪探针之间的重要区别. 存活探针通过杀死异常的容器并用新的正常容器替代它们来保持 pod 正常工作, 而就绪探针确保只有准备好处理请求的 pod 才可以接收它们(请求). 这在容器启动时最为必要, 当然在容器运行一段时间后也是有用的.</strong></mark></p> <p>如图 5.11 所示, 如果一个容器的就绪探测失败, 则将该容器<strong>从端点对象中移除</strong>. 连接到该服务的客户端不会被重定向到 pod. 这和 pod 与服务的标签选择器完全不匹配的效果相同.</p> <p><img src="/img/image-20240227232606-3rkru08.png" alt="image" title="图 5.11 就绪探针失败的 pod 从服务的 endpoint 中移除"></p> <blockquote><p>了解就绪探针的重要性</p></blockquote> <p>设想一组 pod(例如, 运行应用程序服务器的 pod)取决于另一个 pod(例如, 后端数据库)提供的服务. 如果任何一个前端连接点出现连接问题并且无法再访问数据库, 那么就绪探针可能会告知 Kubernetes 该 pod 没有准备好处理任何请求. 如果其他 pod 实例没有遇到类似的连接问题, 则它们可以正常处理请求. <strong>就绪探针确保客户端只与正常的 pod 交互, 并且永远不会知道系统存在问题</strong>.</p> <h6 id="_5-5-2-向pod添加就绪探针"><a href="#_5-5-2-向pod添加就绪探针" class="header-anchor">#</a> 5.5.2 向pod添加就绪探针</h6> <p>接下来, 将通过修改 Replication Controller 的 pod 模板来为<strong>现有的 pod 添加就绪探针</strong>.</p> <blockquote><p>向 pod template 添加就绪探针</p></blockquote> <p>可以通过 <code>kubectl edit</code>​ 命令来向已存在的 ReplicationController 中的 pod 模板添加探针.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit rc kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>当在文本编辑器中打开 ReplicationController 的 YAML 时, 在 pod 模板中查找容器规格, 并将以下就绪探针定义添加到 <code>spec.template.spec.containers</code>​ 下的第一个容器. YAML 看起来应该就像下面的代码清单.</p> <p><strong>代码清单-5.17 RC 创建带有就绪探针的 pod:kubia-rc-readinessprobe.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ReplicationController
<span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token punctuation">...</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
        <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
        <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>    <span class="token comment"># pod中的每个容器都会有一个就绪探针</span>
          <span class="token key atrule">exec</span><span class="token punctuation">:</span>
            <span class="token key atrule">command</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> ls
            <span class="token punctuation">-</span> /var/ready
        <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>就绪探针将定期在容器内执行 <code>ls/var/ready</code>​ 命令. <strong>如果文件存在, 则 ls 命令返回退出码 0, 否则返回非零的退出码. 如果文件存在, 则就绪探针将成功; 否则, 它会失败</strong>.</p> <p>定义这样一个奇怪的就绪探针的原因是, 可以通过创建或删除有问题的文件来触发结果. 该文件尚不存在, 所以所有的 pod 现在应该报告没有准备好, 是这样的吗? 其实并不完全是, 正如在前面章节中了解的那样, 更改 ReplicationController 的 pod 模板对现有的 pod 没有影响.</p> <p>换句话说, 现有的所有 pod 仍没有定义准备就绪探针. 可以通过使用 <code>kubectl get pods</code>​ 列出 pod 并查看 READY 列. 需要删除 pod 并让它们通过 ReplicationController 重新创建. 新的 pod 将进行就绪检查会一直失败, 并且不会将其作为服务的端点, 直到在每个 pod 中创建 <code>/var/ready</code>​ 文件.</p> <blockquote><p>观察并修改 pod 就绪状态</p></blockquote> <p>再次列出 pod 并检查它们是否准备好:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME            READY       STATUS      RESTARTS      AGE
kubia-2r1qb     <span class="token number">0</span>/1         Running     <span class="token number">0</span>             1m
kubia-3rax1     <span class="token number">0</span>/1         Running     <span class="token number">0</span>             1m
kubia-3yw4s     <span class="token number">0</span>/1         Running     <span class="token number">0</span>             1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p><strong>READY 列显示出没有一个容器准备好</strong>. 现在通过<code>创建 /var/ready 文件使其中一个文件的就绪探针返回成功</code>​, 该文件的存在可以模拟就绪探针成功:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> kubia-2r1qb -- <span class="token function">touch</span> /var/ready
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>使用 kubectl exec 命令在 kubia-2r1qb 的 pod 容器内<strong>执行 touch 命令</strong>. 如果文件尚不存在, touch 命令会创建该文件. 就绪探针命令现在应该返回退出码 0, 这意味着探测成功, 并且现在应该显示 pod 已准备就绪. 现在去查看其状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po kubia-2r1qb
NAME         READY     STATUS     RESTARTS  AGE
kubia-2r1qb  <span class="token number">0</span>/1       Running    <span class="token number">0</span>         2m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>该 pod 还没有准备好</strong>. 有什么不对或者这是预期的结果吗? 用 kubectl describe 来获得更详细的关于 pod 的信息. 输出应该包含以下内容:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>Readiness: <span class="token builtin class-name">exec</span> <span class="token punctuation">[</span>ls /var/ready<span class="token punctuation">]</span> <span class="token assign-left variable">delay</span><span class="token operator">=</span>0s <span class="token assign-left variable">timeout</span><span class="token operator">=</span>1s <span class="token assign-left variable">period</span><span class="token operator">=</span>10s <span class="token comment">#success=1#failure=3</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>准备就绪探针会定期检查——默认情况下每 10 秒检查一次. 由于尚未调用就绪探针, 因此容器未准备好. 但是最晚 10 秒钟内, 该 pod 应该已经准备就绪, 其 IP 应该列为 service 的 endpoint(运行 kubectl get endpoint kubialoadbalancer 来确认).</p> <blockquote><p>服务打向单独的 pod</p></blockquote> <p>现在可以点击几次服务网址, 查看每个请求都被重定向到这个 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://130.211.53.173
You’ve hit kubia-2r1qb
$ <span class="token function">curl</span> http://130.211.53.173
You’ve hit kubia-2r1qb
<span class="token punctuation">..</span>.
$ <span class="token function">curl</span> http://130.211.53.173
You’ve hit kubia-2r1qb
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>即使有三个 pod 正在运行, 但只有一个 pod 报告已准备好, 因此是唯一的 pod 接收请求. 如果现在删除该文件, 则将再次从该服务中删除该容器.</strong></p> <h6 id="_5-5-3-了解就绪探针的实际作用"><a href="#_5-5-3-了解就绪探针的实际作用" class="header-anchor">#</a> 5.5.3 了解就绪探针的实际作用</h6> <p>此模拟就绪探针仅用于演示就绪探针的功能. 在实际应用中, 应用程序是否可以(并且希望)接收客户端请求, 决定了就绪探测应该返回成功或失败.</p> <p><strong>应该通过删除 pod 或更改 pod 标签而不是手动更改探针来从服务中手动移除 pod.</strong></p> <p>提示: 如果想要从某个服务中手动添加或删除 pod, 请将 <code>enabled=true</code>​ 作为标签添加到 pod, 以及服务的标签选择器中. 当想要从服务中移除 pod 时, 删除标签.</p> <blockquote><p>务必定义就绪探针! ! ! ! !</p></blockquote> <p>在总结本节之前, 有两个关于就绪探针的要点, 需要强调. <mark><strong>首先, 如果没有将就绪探针添加到 pod 中, 它们几乎会立即成为服务端点. 如果应用程序需要很长时间才能开始监听传入连接, 则在服务启动但尚未准备好接收传入连接时, 客户端请求将被转发到该 pod. 因此, 客户端会看到&quot;连接被拒绝&quot;类型的错误</strong></mark>.</p> <p>提示: <mark><strong>应该始终定义一个就绪探针, 即使它只是向基准 URL 发送 HTTP 请求一样简单</strong></mark>.</p> <blockquote><p>不要将停止 pod 的逻辑纳入就绪探针中</p></blockquote> <p>需要提及的另一件事情涉及 pod 生命周期结束(pod 关闭), 并且也与客户端出现连接错误相关.</p> <p><strong>当一个容器关闭时, 运行在其中的应用程序通常会在收到终止信号后立即停止接收连接</strong>. 因此, 可能认为只要启动关机程序, 就需要让就绪探针返回失败, 以确保从所有服务中删除该 pod. 但这不是必需的, 因为只要删除该容器, Kubernetes 就会从所有服务中移除该容器.</p> <h5 id="_5-6-使用headless服务来发现独立的pod"><a href="#_5-6-使用headless服务来发现独立的pod" class="header-anchor">#</a> 5.6 使用headless服务来发现独立的pod</h5> <p>已经看到如何使用服务来提供稳定的 IP 地址, 从而允许客户端连接到支持服务的每个 pod(或其他端点). 到服务的每个连接都被转发到一个随机选择的 pod 上. 但是如果<strong>客户端需要链接到所有的 pod 呢</strong>? 如果后端的 pod 都需要连接到所有其他 pod 呢? 通过服务连接显然不是这样的, 那是怎样的呢?</p> <p><strong>要让客户端连接到所有 pod, 需要找出每个 pod 的 IP</strong>. 一种选择是让客户端调用 Kubernetes API 服务器并通过 API 调用获取 pod 及其 IP 地址列表, 但由于应始终努力保持应用程序与 Kubernetes 无关, 因此使用 API 服务器并不理想.</p> <p>幸运的是, <strong>Kubernetes 允许客户通过 DNS 查找发现 pod IP</strong>. 通常, 当执行服务的 DNS 查找时, DNS 服务器会返回单个 IP--服务的集群 IP. 但是, 如果告诉 Kubernetes, 不需要为服务提供集群 IP(通过在服务 spec 中将 clusterIP 字段设置为 None 来完成此操作), 则 DNS 服务器将返回 pod IP 而不是单个服务 IP.</p> <p>DNS 服务器不会返回单个 DNS A 记录, 而是会为该服务返回多个 A 记录, 每个记录指向当时支持该服务的单个 pod 的 IP. <strong>客户端因此可以做一个简单的 DNS A 记录查找并获取属于该服务一部分的所有 pod 的 IP</strong>. 客户端可以使用该信息连接到其中的一个, 多个或全部.</p> <h6 id="_5-6-1-创建headless服务"><a href="#_5-6-1-创建headless服务" class="header-anchor">#</a> 5.6.1 创建headless服务</h6> <p><strong>将服务 spec 中的 clusterIP 字段设置为 None 会使服务成为 headless 服务, 因为 Kubernetes 不会为其分配集群 IP, 客户端可通过该 IP 将其连接到支持它的 pod</strong>.</p> <p>现在将创建一个名为 kubia-headless 的 headless 服务. 以下代码清单显示了它的定义.</p> <p><strong>代码清单-5.18 一个 headless 服务: kubia-svc-headless.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>headless
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">clusterIP</span><span class="token punctuation">:</span> None    <span class="token comment"># 该定义使得服务成为headless服务</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>在使用 kubectl create 创建服务之后, 可以通过 kubectl get 和 kubectl describe 来查看服务, 你会发现它<strong>没有集群 IP</strong>, 并且它的后端包含与 pod 选择器匹配的(部分)pod. &quot;部分&quot; 是因为 pod 包含就绪探针, 所以只有准备就绪的 pod 会被列出作为服务的后端文件来确保至少有两个 pod 报告已准备就绪, 如上例所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token operator">&lt;</span>pod name<span class="token operator">&gt;</span> -- <span class="token function">touch</span> /var/ready
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h6 id="_5-6-2-通过dns发现pod"><a href="#_5-6-2-通过dns发现pod" class="header-anchor">#</a> 5.6.2 通过DNS发现pod</h6> <p>准备好 pod 后, 现在可以尝试执行 DNS 查找以查看是否获得了实际的 pod IP. 需要从其中一个 pod 中执行查找. 不幸的是, kubia 容器镜像不包含 nslookup(或 dig)二进制文件, 因此无法使用它执行 DNS 查找.</p> <p>所要做的就是在集群中运行的一个 pod 中执行 DNS 查询. 为什么不寻找一个包含所需二进制文件的镜像来运行新的容器? 要执行与 DNS 相关的操作, 可以使用 Docker Hub 上提供的 <code>tutum/dnsutils</code>​ 容器镜像, 它包含 nslookup 和 dig 二进制文件. 要运行 pod, 可以完成创建 YAML 清单并将其传给 kubectl create 的整个过程. 但是太烦琐了, 对吗? 幸运的是, 有一个更快的方法.</p> <blockquote><p>不通过 YAML 文件运行 pod</p></blockquote> <p>在第 1 章中, 已经使用 <code>kubectl run</code>​ 命令在没有 YAML 清单的情况下创建了 pod. 但是这次只想创建一个 pod, 不需要创建一个 ReplicationController 来管理 pod. 可以这样做:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run dnsutils <span class="token parameter variable">--image</span><span class="token operator">=</span>tutum/dnsutils <span class="token parameter variable">--generator</span><span class="token operator">=</span>run-pod/v1 <span class="token parameter variable">--command</span> -- <span class="token function">sleep</span> infinity
pod <span class="token string">&quot;dnsutils&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>诀窍在 <code>--generator=run-pod/v1</code>​ 选项中, 该选项让 kubectl 直接创建 pod, 而不需要通过 ReplicationController 之类的资源来创建.</p> <blockquote><p>理解 headless 服务的 DNS A 记录解析</p></blockquote> <p>使用新创建的 pod 执行 DNS 查找:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> dnsutils <span class="token function">nslookup</span> kubia-headless
<span class="token punctuation">..</span>.
Name: kubia-headless.default.svc.cluster.local
Address: <span class="token number">10.108</span>.1.4
Name: kubia-headless.default.svc.cluster.local
Address: <span class="token number">10.108</span>.2.5
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>DNS 服务器为 <code>kubia-headless.default.svc.cluster.local FQDN</code>​ 返回两个不同的 IP. 这些是报告准备就绪的两个 pod 的 IP. 可以通过使用 <code>kubectl get pods -o wide</code>​ 列出 pod 来确认此问题, 该清单显示了 pod 的 IP.</p> <p>这与常规(非 headless 服务)服务返回的 DNS 不同, 比如 kubia 服务, 返回的 IP 是服务的集群 IP:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> dnsutils <span class="token function">nslookup</span> kubia
<span class="token punctuation">..</span>.
Name: kubia.default.svc.cluster.local
Address: <span class="token number">10.111</span>.249.153
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>尽管 headless 服务看起来可能与常规服务不同, 但在客户的视角上它们并无不同. 即使使用 headless 服务, 客户也可以通过连接到服务的 DNS 名称来连接到 pod 上, 就像使用常规服务一样. <strong>但是对于 headless 服务, 由于 DNS 返回了 pod 的 IP, 客户端直接连接到该 pod, 而不是通过服务代理</strong>.</p> <p>注意: headless 服务仍然提供跨 pod 的负载平衡, 但是通过 DNS 轮询机制不是通过服务代理.</p> <h6 id="_5-6-3-发现所有的pod-包括未就绪的pod"><a href="#_5-6-3-发现所有的pod-包括未就绪的pod" class="header-anchor">#</a> 5.6.3 发现所有的pod—包括未就绪的pod</h6> <p><strong>只有准备就绪的 pod 能够作为服务的后端</strong>. 但<strong>有时希望即使 pod 没有准备就绪, 服务发现机制也能够发现所有匹配服务标签选择器的 pod</strong>.</p> <p>幸运的是, 不必通过查询 Kubernetes API 服务器, 可以<strong>使用 DNS 查找机制来查找那些未准备好的 pod</strong>. 要告诉 Kubernetes 无论 pod 的准备状态如何, 希望将所有 pod 添加到服务中. 必须将以下注解添加到服务中:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
    <span class="token key atrule">service.alpha.kubernetes.io/tolerate-unready-endpoints</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span> <span class="token comment"># 添加这个注解</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>警告: 就像说的那样, 注解名称表明了这是一个 alpha 功能. Kubernetes Service API 已经支持一个名为 publishNotReadyAddresses 的新服务规范字段, 它将替换 tolerate-unready-endpoints 注解. 在  Kubernetes 1.9.0 版本中, 这个字段还没有实现(这个注解决定了未准备好的 endpoints 是否在 DNS 的记录中). 检查文档以查看是否已更改.</p> <h5 id="_5-7-排除服务故障"><a href="#_5-7-排除服务故障" class="header-anchor">#</a> 5.7 排除服务故障</h5> <p>服务是 Kubernetes 的一个重要概念, 也是让许多开发人员感到困扰的根源. 许多开发人员为了弄清楚无法通过服务 IP 或 FQDN 连接到他们的 pod 的原因花费了大量时间. 出于这个原因, 了解一下如何排除服务故障是很有必要的:</p> <p>如果<strong>无法通过服务访问 pod</strong>, 应该根据下面的列表进行排查:</p> <ul><li>首先, 确保<strong>从集群内连接到服务的集群 IP</strong>, 而不是从外部.</li> <li>不要通过 ping 服务 IP 来判断服务是否可访问(请记住, <mark><strong>服务的集群 IP 是虚拟 IP, 是无法 ping 通的</strong></mark>).</li> <li><mark><strong>如果已经定义了就绪探针, 请确保它返回成功; 否则该 pod 不会成为服务的一部分</strong></mark>.</li> <li><strong>要确认某个容器是服务的一部分, 请使用 kubectl get endpoints 来检查相应的端点对象</strong>.</li> <li>如果尝试通过 FQDN 或其中一部分来访问服务(例如, myservice.mynamespace.svc.cluster.local 或 myservice.mynamespace), 但并不起作用, 请查看是否可以使用其集群 IP 而不是 FQDN 来访问服务.</li> <li><strong>检查是否连接到服务公开的端口, 而不是目标端口</strong>.</li> <li>尝试直接连接到 pod IP 以确认 pod 正在接收正确端口上的连接.</li> <li>如果甚至无法通过 pod 的 IP 访问应用, 请确保应用不是仅绑定到本地主机.</li></ul> <p>这应该可以帮助解决大部分与服务相关的问题. 将在第 11 章中了解更多有关服务如何工作的内容. 通过了解它们的实现方式, 应该可以更轻松地对它们进行故障排除.</p> <h5 id="_5-8-本章小结"><a href="#_5-8-本章小结" class="header-anchor">#</a> 5.8 本章小结</h5> <p>在本章中, 已经学习了如何创建 Kubernetes 服务资源来暴露应用程序中可用的服务, 无论每个服务后端有多少 pod 实例. 你已经学会了 Kubernetes 关于服务的用法:</p> <ul><li>在一个固定的 IP 地址和端口下暴露匹配到某个标签选择器的多个 pod</li> <li>服务在集群内默认是可访问的, 通过将服务的类型设置为 NodePort 或 LoadBalancer, 使得服务也可以从集群外部访问</li> <li>让 pod 能够通过查找环境变量发现服务的 IP 地址和端口</li> <li>允许通过创建服务资源而不指定选择器来发现驻留在集群外部的服务并与之通信, 方法是创建关联的 Endpoint 资源</li> <li>为具有 ExternalName 服务类型的外部服务提供 DNS CNAME 别名</li> <li><strong>通过单个 Ingress 公开多个 HTTP 服务(使用单个 IP)</strong></li> <li><strong>使用 pod 容器的</strong>​<mark><strong>就绪探针</strong></mark>​<strong>来确定是否应该将 pod 包含在服务 endpoints 内</strong></li> <li><strong>通过创建 headless 服务让 DNS 发现 pod IP</strong></li></ul> <p>随着对服务的深入理解, 也学习到了下面的内容:</p> <ul><li>故障排查</li> <li>修改 Google Kubernetes/Compute Engine 中的防火墙规则</li> <li>通过 kubectl exec 在 pod 容器中执行命令在现有容器的容器中运行一个 bash shell</li> <li>通过 kubectl apply 命令修改 Kubernetes 资源</li> <li>使用 <code>kubectl run--generator=run-pod/v1</code>​ 运行临时的 pod</li></ul> <h4 id="_6-卷-将磁盘挂载到容器"><a href="#_6-卷-将磁盘挂载到容器" class="header-anchor">#</a> 6.卷:将磁盘挂载到容器</h4> <blockquote><p>本章内容包括</p></blockquote> <ul><li>创建多容器 pod</li> <li>创建一个可在容器间共享磁盘存储的卷在 pod 中使用 git 仓库</li> <li>将持久性存储(如 GCE 持久磁盘)挂载到 pod</li> <li>使用预先配置的持久性存储</li> <li>动态调配持久存储</li></ul> <p>在前面三个章节中, 介绍了 pod 和与之交互的其他 Kubernetes 资源, 即: ReplicationController(复制控制器), ReplicaSet(副本服务器), DaemonSet(守护进程集), 作业和服务. 现在回到 pod 中, 来<mark><strong>了解容器是如何访问外部磁盘存储的, 以及如何在它们之间共享存储空间</strong></mark>.</p> <p>之前说过, pod 类似逻辑主机, 在逻辑主机中运行的进程共享诸如 CPU, RAM, 网络接口等资源. 人们会期望进程也能共享磁盘, 但事实并非如此. 需要谨记一点, <mark><strong>pod 中的每个容器都有自己独立的文件系统, 因为文件系统来自容器镜像</strong></mark>.</p> <p><strong>每个新容器都是通过在构建镜像时加入的详细配置文件来启动的</strong>. 将此与 pod 中容器重新启动的现象结合起来(也许是因为进程崩溃, 也许是存活探针向 Kubernetes 发送了容器状态异常的信号), 就会意识到新容器并不会识别前一个容器写入文件系统内的任何内容, 即使新启动的容器运行在同一个 pod 中. 在某些场景下, 可能希望新的容器可以在之前容器结束的位置继续运行, 比如在物理机上重启进程. 可能不需要(或者不想要)整个文件系统被持久化, 但又希望能保存实际数据的目录.</p> <p>Kubernetes 通过<strong>定义存储卷</strong>来满足这个需求, 它们不像 pod 这样的顶级资源, 而是被<strong>定义为 pod 的一部分, 并和 pod 共享相同的生命周期</strong>. 这意味着<strong>在 pod 启动时创建卷, 并在删除 pod 时销毁卷</strong>. 因此, 在容器重新启动期间, 卷的内容将保持不变, <strong>在重新启动容器之后, 新容器可以识别前一个容器写入卷的所有文件. 另外, 如果一个 pod 包含多个容器, 那这个卷可以同时被所有的容器使用</strong>.</p> <h5 id="_6-1-介绍卷"><a href="#_6-1-介绍卷" class="header-anchor">#</a> 6.1 介绍卷</h5> <p><mark><strong>Kubernetes 的卷是 pod 的一个组成部分, 因此像容器一样在 pod 的规范中就定义了. 它们不是独立的 Kubernetes 对象, 也不能单独创建或删除. pod 中的所有容器都可以使用卷, 但必须先将它挂载在每个需要访问它的容器中. 在每个容器中, 都可以在其文件系统的任意位置挂载卷.</strong></mark></p> <h6 id="_6-1-1-卷的应用示例"><a href="#_6-1-1-卷的应用示例" class="header-anchor">#</a> 6.1.1 卷的应用示例</h6> <p>假设有一个带有三个容器的 pod(如图 6.1 所示), 一个容器运行了一个 web 服务器, 该 web 服务器的 HTML 页面目录位于 <code>/var/htdocs</code>​, 并将站点访问日志存储到 <code>/var/logs</code>​ 目录中. 第二个容器运行了一个代理来创建 HTML 文件, 并将它们存放在 <code>/var/html</code>​ 中, 第三个容器处理在 <code>/var/logs</code>​ 目录中找到的日志(转换, 压缩, 分析它们或者做其他处理).</p> <p><img src="/img/image-20240227232720-343v87g.png" alt="image" title="图6.1 同一个 pod 的三个容器没有共享存储"></p> <p>每个容器都有一个很明确的用途, 但是每个容器单独使用就没有多大用处了. 在没有共享磁盘存储的情况下, 用这三个容器创建一个 pod 没有任何意义. 因为内容生成器(content generator)会在自己的容器中存放生成的 HTML 文件, 而 web 服务器无法访问这些文件, 因为它运行在一个隔离的独立容器内. 正好相反, 它会托管放置在容器镜像的 <code>/var/htdocs</code>​ 目录下的任意内容, 或者是放置在容器镜像中 <code>/var/htdocs</code>​ 路径下的任意内容. 同样, 日志转换器(logrotator)也无事可做, 因为它的 <code>/var/logs</code>​ 目录始终是空的, 并没有日志写入. 一个有这三个容器而没有挂载卷的 pod 基本上什么也做不了.</p> <p>但是, <strong>如果将两个卷添加到 pod 中, 并在三个容器的适当路径上挂载它们</strong>, 如图 6.2 所示, 就已经创建出一个比其各个部分之和更完善的系统. Linux 允许在文件树中的任意位置挂载文件系统, 当这样做的时候, 挂载的文件系统内容在目录中是可以访问的. <strong>通过将相同的卷挂载到两个容器中, 它们可以对相同的文件进行操作</strong>. 在这个例子中, 只需要在三个容器中挂载两个卷, 这样三个容器将可以一起工作, 并发挥作用.</p> <p><img src="/img/image-20240227232740-kxxf9x3.png" alt="image" title="图6.2 三个容器共享挂载在不同安装路径的两个卷上"></p> <p>下面解释一下:</p> <p>首先, pod 有一个<strong>名为 publicHtml 的卷</strong>, 这个卷被挂载在 WebServer 容器的 <code>/var/htdocs</code>​ 中, 因为这是 web 服务器的服务目录. 在 ContentAgent 容器中也挂载了相同的卷, 但在 <code>/var/html</code>​ 中, 因为代理将文件写入 <code>/var/html</code>​ 中. 通过这种方式挂载这个卷, web 服务器现在将为 content agent 生成的内容提供服务.</p> <p>同样, pod 还拥有一个<strong>名为 logVol 的卷</strong>, 用于存放日志, 此卷在 WebServer 和 LogRotator 容器中的 <code>/var/log</code>​ 中挂载, 注意, 它没有挂载在 ContentAgent 容器中, 这个容器不能访问它的文件, 即使容器和卷是同一个 pod 的一部分, 在 pod 的规范中定义卷是不够的. 如果我们希望容器能够访问它, 还需要在容器的规范中定义一个 VolumeMount.</p> <p>在本例中, 两个卷最初都是空的, 因此可以使用一种<strong>名为 emptyDir 的卷</strong>. Kubernetes 还支持其他类型的卷, 这些卷要么是在从外部源初始化卷时填充的, 要么是在卷内挂载现有目录. 这个填充或装入卷的过程是在 pod 内的容器启动之前执行的.</p> <p><strong>卷被绑定到 pod 的 lifecycle(生命周期)中, 只有在 pod 存在时才会存在, 但取决于卷的类型, 即使在 pod 和卷消失之后, 卷的文件也可能保持原样, 并可以挂载到新的卷中</strong>. 下面来看看卷有哪些类型.</p> <h6 id="_6-1-2-介绍可用的卷类型"><a href="#_6-1-2-介绍可用的卷类型" class="header-anchor">#</a> 6.1.2 介绍可用的卷类型</h6> <p>有多种卷类型可供选择. 其中一些是通用的, 而另一些则相对于当前常用的存储技术有较大差别. 如果从来没有听说过这些技术, 也别太担心--其中至少一半笔者也没有听说过. 你有可能只会用到那些自己熟悉和曾经用过的卷技术. 以下是几种可用卷类型的列表:</p> <ul><li><strong>emptyDir</strong>——用于<strong>存储临时数据的简单空目录</strong>.</li> <li><strong>hostPath</strong> ——用于<strong>将目录从工作节点的文件系统挂载到 pod 中</strong>.</li> <li><strong>gitRepo</strong>——通过<strong>检出 Git 仓库的内容来初始化的卷</strong>.</li> <li><strong>nfs</strong>——挂载到 pod 中的 NFS 共享卷.</li> <li>gcePersistentDisk(Google 高效能型存储磁盘卷), awsElastic BlockStore(AmazonWeb 服务弹性块存储卷), azureDisk(Microsoft Azure 磁盘卷)——用于挂载云服务商提供的特定存储类型.</li> <li>cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-Volume, photonPersistentDisk, scaleIO——用于挂载其他类型的网络存储.</li> <li>configMap, secret, downwardAPI——用于将 Kubernetes 部分资源和集群信息公开给 pod 的特殊类型的卷.</li> <li><strong>persistentVolumeClaim</strong>——一种使用预置或者动态配置的持久存储类型(后面将在本章的最后一节对此展开讨论).</li></ul> <p>这些卷类型有各种用途. 将在下面的部分中了解其中一些内容. 特殊类型的卷(secret, downwardAPI, configMap)将在接下来的两章中讨论, 因为它们不是用于存储数据, 而是用于将 Kubernetes 元数据公开给运行在 pod 中的应用程序.</p> <p><strong>单个容器可以同时使用不同类型的多个卷, 而且正如前面提到的, 每个容器都可以装载或不装载卷.</strong></p> <h5 id="_6-2-通过卷在容器之间共享数据"><a href="#_6-2-通过卷在容器之间共享数据" class="header-anchor">#</a> 6.2 通过卷在容器之间共享数据</h5> <p>尽管一个卷即使被单个容器使用也可能很有用, 但是首先要关注它是<mark><strong>如何用于在一个 pod 的多个容器之间共享数据</strong></mark>的.</p> <h6 id="_6-2-1-使用emptydir卷"><a href="#_6-2-1-使用emptydir卷" class="header-anchor">#</a> 6.2.1 使用emptyDir卷</h6> <p>最简单的卷类型是 emptyDir 卷, 所以作为第一个例子来看看<strong>如何在 pod 中定义卷</strong>. 顾名思义, 卷从一个空目录开始, 运行在 pod 内的应用程序可以写入它需要的任何文件. <strong>因为卷的生存周期与 pod 的生存周期相关联, 所以当删除 pod 时, 卷的内容就会丢失</strong>.</p> <p>一个 emptyDir 卷对于在同一个 pod 中运行的容器之间共享文件特别有用. 但是它也可以被单个容器用于将数据临时写入磁盘, 例如在大型数据集上执行排序操作时, 没有那么多内存可供使用. 数据也可以写入容器的文件系统本身(还记得容器的顶层读写层吗?), 但是这两者之间存在着细微的差别. <strong>容器的文件系统甚至可能是不可写的(将在书的末尾讨论这个问题), 所以写到挂载的卷可能是唯一的选择</strong>.</p> <blockquote><p>在 pod 中使用 emptyDir 卷</p></blockquote> <p>重新回顾一下前面的例子, 其中 web 服务器, 内容代理和日志转换器共享两个卷, 简化一下, 现在将<strong>构建一个仅有 web 服务器容器内容代理和单独 HTML 的卷的 pod</strong>.</p> <p>将使用 Nginx 作为 Web 服务器和 UNIX fortune 命令来生成 HTML 内容, fortune 命令每次运行时都会输出一个随机引用, 可以创建一个脚本每 10 秒调用一次执行, 并将其输出<strong>存储在 index.html 中</strong>, 在 Docker Hub 上可以找到一个现成的 Nginx 镜像, 但是需要自己创建 fortune 镜像, 或者使用笔者已经构建并推送到 Docker Hub <code>luksa/Fortune</code>​ 下的镜像.</p> <blockquote><p>构建 fortune 容器镜像</p></blockquote> <p>这里描述如何创建镜像, 创建一个名为 fortune 的新目录, 然后在其中创建一个具有以下内容的 fortuneloop.sh 的 shell 脚本:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token shebang important">#!/bin/bash</span>
<span class="token builtin class-name">trap</span> <span class="token string">&quot;exit&quot;</span> SIGINT
<span class="token function">mkdir</span> /var/htdocs
<span class="token keyword">while</span> <span class="token builtin class-name">:</span>
<span class="token keyword">do</span>
    <span class="token builtin class-name">echo</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">date</span><span class="token variable">)</span></span> Writing fortune to /var/htdocs/index.html
    /usr/games/fortune <span class="token operator">&gt;</span> /var/htdocs/index.html
    <span class="token function">sleep</span> <span class="token number">10</span>
<span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>然后, 在同一个目录中, 创建一个名为 Dockerfile 的文件, 其中包含以下内容:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FROM ubuntu:latest
RUN <span class="token function">apt-get</span> update <span class="token punctuation">;</span> <span class="token function">apt-get</span> <span class="token parameter variable">-y</span> <span class="token function">install</span> fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT /bin/fortuneloop.sh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>该镜像基于 ubuntu:latest 镜像, 默认情况下不包括 fortune 二进制文件. 这就是为什么在 Dockerfile 的第二行中, 需要<strong>使用 apt-get 安装它</strong>的原因. 之后, 可以<strong>向镜像的 /bin 文件夹中添加 fortuneloop.sh 脚本</strong>. 在 Dockerfile 的最后一行中, 指定镜像启动时执行 fortuneloop.sh 脚本.</p> <p>准备好这两个文件之后, 使用以下两个命令(用自己的 Docker Hub 用户 ID 替换 luksa)构建镜像并上传到 Docker Hub:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> build <span class="token parameter variable">-t</span> luksa/fortune <span class="token builtin class-name">.</span>
$ <span class="token function">docker</span> push luksa/fortune
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>创建 pod</p></blockquote> <p>现在有两个镜像需要运行在 pod 上, 是时候创建 pod 的 manifest 了, 创建一个名为 fortune-pod.yaml 的文件, 其内容包含在下面的代码清单中.</p> <p><strong>代码清单-6.1 一个 pod 中有两个共用同一个卷的容器 : fortune-pod.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/fortune     <span class="token comment"># 第一个容器名称为html-generator, 运行luksa/fortune镜像</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> html<span class="token punctuation">-</span>generator
      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token comment"># 名为html的卷挂载在容器的/var/htdocs中</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
        <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/htdocs
    <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>alpine      <span class="token comment"># 第二个容器名称为web-server, 运行nginx:alpine镜像</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>server
      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token comment"># 与上面相同的卷挂载在/usr/share/nginx/html上, 设为只读</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
        <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html
        <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
      <span class="token key atrule">ports</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
        <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
    <span class="token key atrule">volumes</span><span class="token punctuation">:</span>            <span class="token comment"># 一个名为html的单独emptyDir卷, 挂载在上面的两个容器中</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
      <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><p>上面的 <strong>pod 定义包含两个容器和一个挂载在两个容器中的共用的卷, 但在不同的路径上</strong>. 当 html-generator 容器启动时, 它每 10 秒启动一次 fortune 命令输出到 <code>/var/htdocs/index.html</code>​ 文件. 因为卷是在 <code>/var/htdocs</code>​ 上挂载的, 所以 index.html 文件被写入卷中, 而不是容器的顶层. 一旦 web-server 容器启动, 它就开始为 <code>/usr/share/nginx/html</code>​ 目录中的任意 HTML 文件提供服务(这是 Nginx 服务的默认服务文件目录). 因为将卷挂载在那个确切的位置, Nginx 将为运行 fortune 循环的容器输出的 index.html 文件提供服务. 最终的效果是, 一个客户端向 pod 上的 80 端口发送一个 HTTP 请求, 将接收当前的 fortune 消息作为响应.</p> <blockquote><p>查看 pod 状态</p></blockquote> <p>为了查看 fortune 消息, 需要启用对 pod 的访问, 可以尝试将端口从本地机器转发到 pod 来实现:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl port-forward fortune <span class="token number">8080</span>:80
Forwarding from <span class="token number">127.0</span>.0.1:8080 -<span class="token operator">&gt;</span> <span class="token number">80</span>
Forwarding from <span class="token punctuation">[</span>::1<span class="token punctuation">]</span>:8080 -<span class="token operator">&gt;</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: 作为练习, 还可以通过服务来访问该 pod, 而不是单纯使用端口转发.</p> <p>现在可以通过本地计算机的 8080 端口来访问 Nginx 服务器. 通过执行 curl 命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://localhost:8080
Beware of a tall blond <span class="token function">man</span> with one black shoe.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果等待几秒发送另一个请求, 则应该会接收到另一条信息. 通过组合两个容器, 就创建了一个简单的应用, 通过这个应用可以观察到<strong>卷是如何将两个容器组合在一起</strong>, 并分别增强它们各自的功能的.</p> <blockquote><p>指定用于 EMPTYDIR 的介质</p></blockquote> <p>作为<strong>卷来使用的 emptyDir, 是在承载 pod 的工作节点的实际磁盘上创建的, 因此其性能取决于节点的磁盘类型</strong>. 但可以通知 Kubernetes 在 tmfs 文件系统(存在内存而非硬盘)上创建 emptyDir. 因此, 将 emptyDir 的 medium 设置为 Memory:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span>
      <span class="token key atrule">medium</span><span class="token punctuation">:</span> Memory    <span class="token comment"># emptyDir的文件将会存储在内存中</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><strong>emptyDir 卷是最简单的卷类型, 但是其他类型的卷都是在它的基础上构建的, 在创建空目录后, 它们会用数据填充它</strong>. 有一种称作 gitRepo 的卷类型, 将在下面进行介绍.</p> <h6 id="_6-2-2-使用git仓库作为存储卷"><a href="#_6-2-2-使用git仓库作为存储卷" class="header-anchor">#</a> 6.2.2 使用Git仓库作为存储卷</h6> <p>gitRepo 卷基本上也是一个 emptyDir 卷, 它<strong>通过克隆 Git 仓库并在 pod 启动时(但在创建容器之前)检出特定版本来填充数据</strong>, 如图 6.3 所示.</p> <p><img src="/img/image-20240227232825-c3x4zbv.png" alt="image" title="图6.3 gitRepo 卷是一个 emptyDir 卷, 最初填充了 Git 仓库的内容"></p> <p>注意: 在创建 gitRepo 卷后, 它<strong>并不能和对应 repo 保持同步</strong>. 当向 Git 仓库推送新增的提交时, 卷中的文件将不会被更新. 然而, 如果所用的 pod 是由 ReplicationController 管理的, 删除这个 pod 将<strong>触发新建一个新的 pod, 而这个新 pod 的卷中将包含最新的提交</strong>.</p> <p>例如, <strong>可以使用 Git 仓库来存放网站的静态 HTML 文件, 并创建一个包含 web 服务器容器和 gitRepo 卷的 pod. 每当 pod 创建时, 它会拉取网站的最新版本并开始托管网站. 唯一的缺点是, 每次将更改推送到 gitRepo 时, 都需要删除 pod, 才能托管新版本的网站</strong>. 这个拿来做博客网站就很香?</p> <p>现在开始, 这跟以前做的很接近.</p> <blockquote><p>从一个克隆的 Git 仓库中运行 web 服务器 pod 的服务文件</p></blockquote> <p>在创建 pod 之前, 需要有一个<strong>包含 HTML 文件并实际可用的 Git 仓库</strong>, 笔者在 GitHub 创建了一个 repo, 链接为: https://github.com/luksa/kubia-website-example.git. 需要 fork 这个项目(在 github 上创建自己的 repo 副本), 这样就可以在后面对其进行变更修改.</p> <p>当完成了 fork 操作, 就可以继续创建 pod 了. 这次, 只<strong>需要一个 Nginx 容器和一个 gitRepo 卷</strong>(确保已将 gitRepo 卷指向 fork 来的 repo 副本), 如下面的代码清单所示.</p> <p><strong>代码清单-6.2 使用 gitRepo 卷的 pod: gitrepo-volume-pod.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> gitrepo<span class="token punctuation">-</span>volume<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>alpine
    <span class="token key atrule">name</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>server
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
    <span class="token key atrule">gitRepo</span><span class="token punctuation">:</span>    <span class="token comment"># 创建一个gitRepo卷</span>
      <span class="token key atrule">repository</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//github.com/luksa/kubia<span class="token punctuation">-</span>website<span class="token punctuation">-</span>example.git    <span class="token comment"># 这个卷克隆一个git仓库</span>
      <span class="token key atrule">revision</span><span class="token punctuation">:</span> master    <span class="token comment"># 检出主分支</span>
      <span class="token key atrule">directory</span><span class="token punctuation">:</span> .        <span class="token comment"># 将repo克隆到卷的根目录</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p><strong>在创建 pod 时, 首先将卷初始化为一个空目录, 然后将制定的 Git 仓库克隆到其中</strong>. 如果没有将目录设置为.(句点), 存储库将会被克隆到 kubia-website-example 示例目录中, 这不是我们想要的结果. 我们预期将 repo 克隆到卷的根目录中. 在设置存储库时, 还需要<strong>指明让 Kubernetes 切换到 master 分支所在的版本来创建存储卷修订变更</strong>.</p> <p>在 pod 运行时, 可以尝试通过端口转发, 服务或在 pod(或集群中的任意其他 pod)中运行 curl 命令来访问 pod.</p> <blockquote><p>确认文件未与 Git 仓库保持同步</p></blockquote> <p>现在将对 Github 项目中的 index.html 文件进行更改. Git 仓库的主分支现在包含对 HTML 文件所做的更改. 而这些更改在 Nginx web 服务器上不可见, 因为 gitrepo 卷与 Git 仓库<strong>未能保持同步</strong>. 可以通过再次访问 pod 来确认这一点.</p> <p>要查看新版本的站点, 需要<strong>删除 pod 并重建, 每次进行更改时, 没必要每次都删除 pod, 可以运行一个附加进程来使卷与 Git 仓库保持同步</strong>. 在这里不详细解释如何实现, 相反, 建议自己多做练习, 这里可以给到一些指引.</p> <blockquote><p>介绍 sidecar 容器</p></blockquote> <p><strong>Git 同步进程不应该运行在与 Nginx 站点服务器相同的容器中, 而是在第二个容器: sidecar container</strong>. 它是一种容器, 增加了对 pod 主容器的操作. 可以将一个 sidecar 添加到 pod 中, 这样就可以<strong>使用现有的容器镜像</strong>, 而不是将附加逻辑填入主应用程序的代码中, 这会使它过于复杂和不可复用.</p> <p>为了找到一个保持本地目录与 Git 仓库同步的现有容器镜像, 转到 Docker Hub 并搜索 &quot;git syc&quot;, 可以看到很多可以实现的镜像. 然后在示例中, 从 pod 的一个新容器使用镜像, 挂载 pod 现有的 gitRepo 卷到新容器中, 并配置 Git 同步容器来保持文件与 Git repo 同步. 如果正确设置了所有的内容, 应该能看到 web 服务器正在加载的文件与 GitHub repo 同步.</p> <p>注意: 第 18 章中的一个例子包含了类似的 Git 同步容器, 所以可以等读到第 18 章, 再跟着分步说明做这个练习.</p> <blockquote><p>使用带有专用 Git 仓库的 gitRepo 卷</p></blockquote> <p>另外还有一个原因, 使得我们必须<strong>依赖于 Git sync sidecar 容器</strong>. 还没有讨论过是否可以使用对应私有 Git repo 的 gitRepo 卷, 其实不可行. Kubernetes 开发人员的共识是保持 gitRepo 卷的简单性, 而不添加任何通过 SSH 协议克隆私有存储库的支持, 因为这需要向 gitRepo 卷添加额外的配置选项.</p> <p>如果想要将私有的 Git repo 克隆到容器中, 则应该使用 gitsync sidecar 或类似的方法, 而不是使用 gitRepo 卷.</p> <h5 id="_6-3-访问工作节点文件系统上的文件"><a href="#_6-3-访问工作节点文件系统上的文件" class="header-anchor">#</a> 6.3 访问工作节点文件系统上的文件</h5> <p>gitRepo 容器就像 emptyDir 卷一样, 基本上是一个<strong>专用目录</strong>, 专门用于包含卷的容器并单独使用. 当 pod 被删除时, 卷及其内容被删除. 然而, <mark><strong>其他类型的卷并不创建新目录, 而是将现有的外部目录挂载到 pod 的容器文件系统中</strong></mark>. 该卷的内容可以保存多个 pod 实例化, 接下来将了解这些类型的卷.</p> <p>大多数 pod 应该忽略它们的主机节点, 因此它们不应该访问节点文件系统上的任何文件. 但是<strong>某些系统级别的 pod(切记, 这些通常由 DaemonSet 管理)确实需要读取节点的文件或使用节点文件系统来访问节点设备</strong>. Kubernetes <strong>通过 hostPath 卷</strong>实现了这一点.</p> <h6 id="_6-3-1-介绍hostpath卷"><a href="#_6-3-1-介绍hostpath卷" class="header-anchor">#</a> 6.3.1 介绍hostPath卷</h6> <p><mark><strong>hostPath 卷指向节点文件系统上的特定文件或目录</strong></mark>(请参见图6.4). <mark><strong>在同一个节点上运行并在其 hostPath 卷中使用相同路径的 pod 可以看到相同的文件</strong></mark>.</p> <p><img src="/img/image-20240227232849-scln0cx.png" alt="image" title="图6.4 hostPath 卷将工作节点上的文件或目录挂载到容器的文件系统中"></p> <p>hostPath 卷是介绍的第一种类型的<mark><strong>持久性存储</strong></mark>, 因为 gitRepo 和 emptyDir 卷的内容都会在 pod 被删除时被删除, 而 hostPath 卷的内容则不会被删除. 如果删除了一个 pod, 并且下一个 pod 使用了指向主机上相同路径的 hostPath 卷, 则新 pod 将会<strong>发现上一个 pod 留下的数据</strong>, 但前提是必须将其调度到与第一个 pod 相同的节点上.</p> <p>如果你正在考虑使用 hostPath 卷作为存储数据库数据的目录, 请重新考虑. 因为卷的内容存储在特定节点的文件系统中, 所以当数据库 pod 被重新安排在另一个节点时, 会找不到数据. 这解释了为什么对常规 pod 使用 hostPath 卷不是一个好主意, 因为这会<strong>使 pod 对预定规划的节点很敏感</strong>.</p> <h6 id="_6-3-2-检查使用hostpath卷的系统pod"><a href="#_6-3-2-检查使用hostpath卷的系统pod" class="header-anchor">#</a> 6.3.2 检查使用hostPath卷的系统pod</h6> <p>下面看看如何正确地使用 hostPath 卷. 先看一下是否有系统层面的 pod 已经在使用这种类型的卷, 而不是直接创建一个新 pod. 你可能还记得前面一章中, 有几个这样的 pod 正在 kube-system 命名空间中运行, 再次列出它们:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pod s <span class="token parameter variable">--namespace</span> kube-system
NAME                         READY     STATUS     RESTARTS     AGE
fluentd-kubia-4ebc2f1e-9a3e  <span class="token number">1</span>/1       Running    <span class="token number">1</span>            4d
fluentd-kubia-4ebc2f1e-e2vz  <span class="token number">1</span>/1       Running    <span class="token number">1</span>            31d
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>选择第一个并<strong>查看其使用的卷大小</strong>(在下面的代码清单中显示).</p> <p><strong>代码清单-6.3 使用 hostPath 卷访问节点日志的 pod</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e <span class="token parameter variable">--namespace</span> kube-system
Name: fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e
Namespace: kube-system
<span class="token punctuation">..</span>.
Volumes:
varlog:
    Type: HostPath <span class="token punctuation">(</span>bare <span class="token function">host</span> directory volume<span class="token punctuation">)</span>
    Path: /var/log
varlibdockercontainers:
    Type: HostPath <span class="token punctuation">(</span>bare <span class="token function">host</span> directory volume<span class="token punctuation">)</span>
    Path: /var/lib/docker/containers
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>提示: 如果你使用的是 Minikube, 试试 kube-addon-manager-minikube pod.</p> <p>pod 使用两个 hostPath 卷来访问节点的 <code>/var/log</code>​ 和 <code>/var/lib/docker/containers</code>​ 目录. 也许你认为在第一次尝试时就找到一个在使用 hostPath 卷的 pod 很幸运, 但实际上并不是这样(至少在 GKE 不是). 检查其他文件, 将能看到大多数情况下都<mark><strong>使用这种类型的卷来访问节点的日志文件, kubeconfig(Kubernetes 配置文件)或 CA 证书</strong></mark>.</p> <p>如果检查其他 pod, 则会看到其中没有一个使用 hostPath 卷来存储自己的数据, 都是使用这种卷来访问节点的数据. 但是, 正如在本章后面将看到的, hostPath 卷通常用于<strong>尝试单节点集群中的持久化存储,</strong>  譬如 Minikube 创建的集群. 继续阅读将了解即使在多节点集群中也应该使用哪些类型的卷来正确地存储持久化数据.</p> <p>提示: <mark><strong>请记住仅当需要在节点上读取或写入系统文件时才使用 hostPath, 切勿使用它们来持久化跨 pod 的数据</strong></mark>.</p> <h5 id="_6-4-使用持久化存储"><a href="#_6-4-使用持久化存储" class="header-anchor">#</a> 6.4 使用持久化存储</h5> <p><strong>当运行在一个 pod 中的应用程序需要将数据保存到磁盘上, 并且即使该 pod 重新调度到另一个节点时也要求具有相同的数据可用</strong>. 这就不能使用到目前为止提到的任何卷类型, 由于这些数据需要可以从任何集群节点访问, 因此必须<mark><strong>将其存储在某种类型的网络存储(NAS)中</strong></mark>.</p> <p>要了解允许保存数据的卷, 将创建一个<strong>运行 MongoDB(文件类型 NoSQL 数据库)的 pod</strong>. 除了测试目的, 运行没有卷或非持久卷的数据库 pod 没有任何意义, 所以需要为该 pod 添加适当类型的卷并将其挂载在 MongoDB 容器中.</p> <h6 id="_6-4-1-使用gce持久磁盘作为pod存储卷"><a href="#_6-4-1-使用gce持久磁盘作为pod存储卷" class="header-anchor">#</a> 6.4.1 使用GCE持久磁盘作为pod存储卷</h6> <p>如果是在 Google Kubernetes Engine 中运行这些示例, 那么由于集群节点是运行在 Google Compute Engine(GCE)之上, 则将使用 GCE 持久磁盘作为底层存储机制.</p> <p>在早期版本中, Kubernetes 没有自动配置底层存储, 必须手动执行此操作. 自动配置现在已经可以实现, 将在本章的后面部分进一步了解. 首先需要手动配置存储, 这样可以让你有机会了解背后发生了什么.</p> <blockquote><p>创建 GCE 持久磁盘</p></blockquote> <p>首先创建 GCE 持久磁盘. 需要在同一区域的 Kubernetes 集群中创建它, 如果你不记得是在哪个区域创建了集群, 可以通过使用 gcloud 命令来查看:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud container clusters list
NAME     ZONE             MASTER_VERSION     MASTER_IP <span class="token punctuation">..</span>.
kubia    europe-west1-b   <span class="token number">1.2</span>.5              <span class="token number">104.155</span>.84.137 <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>以上输出说明已经在 europe-west1-b 区域中创建了集群, 因此也需要在同一区域中创建 GCE 持久磁盘. 可以像这样创建磁盘:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud compute disks create <span class="token parameter variable">--size</span><span class="token operator">=</span>1GiB <span class="token parameter variable">--zone</span><span class="token operator">=</span>europe-west1-b mongodb
WARNING: You have selected a disk size of under <span class="token punctuation">[</span>200GB<span class="token punctuation">]</span>. This may result <span class="token keyword">in</span>
poor I/O performance. For <span class="token function">more</span> information, see:
https://developers.google.com/compute/docs/disks<span class="token comment">#pdperformance.</span>
Created <span class="token punctuation">[</span>https://www.googleapis.com/compute/v1/projects/rapid-pivot-
<span class="token number">136513</span>/zones/europe-west1-b/disks/mongodb<span class="token punctuation">]</span>.
NAME ZONE SIZE_GB TYPE STATUS
mongodb europe-west1-b <span class="token number">1</span> pd-standard READY
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>这个命令创建了一个 1GiB 容量并<strong>命名为 mongodb 的 GCE 持久磁盘</strong>. 可以忽略有关磁盘大小的告警, 因为我们无须关心用于测试的磁盘性能.</p> <blockquote><p>创建一个使用 GCE 持久磁盘卷的 pod</p></blockquote> <p>现在已经正确设置了物理存储, 可以<strong>在 MongoDB pod 的卷中使用它</strong>. 着手为 pod 准备 YAML, 如下面的代码清单所示.</p> <p><strong>代码清单-6.4 一个使用 gce Persistent Disk 卷的 pod: mongodb-podgcepd.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data    <span class="token comment"># 卷的名称, 在挂载卷时也会引用</span>
    <span class="token key atrule">gcePersistentDisk</span><span class="token punctuation">:</span>    <span class="token comment"># 卷类型是GCE持久磁盘</span>
      <span class="token key atrule">pdName</span><span class="token punctuation">:</span> mongodb     <span class="token comment"># 持久磁盘的名称必须与先前创建的实际PD一致</span>
      <span class="token key atrule">fsType</span><span class="token punctuation">:</span> ext4        <span class="token comment"># 文件系统类型为EXT4</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> mongo
    <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /data/db    <span class="token comment"># MongoDB数据存放的路径</span>
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">27017</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>提示: 如果要使用 Minikube, 就不能使用 GCE 持久磁盘, 但是可以部署 mongodb-pod-hostpath.yaml, 这个使用的是 hostpath 卷而不是 GCE 持久磁盘.</p> <p>pod 包含一个容器和一个卷, 被之前创建的 GCE 持久磁盘支持(如图 6.5 所示). 因为 MongoDB 就是在 <code>/data/db</code>​ 上存储数据的, 所以<strong>容器中的卷也要挂载在这个路径</strong>上.</p> <p><img src="/img/image-20240227233311-i8gj553.png" alt="image" title="图6.5 带有单个运行 Mongodb 的容器的 pod, 该容器挂载引用外部的 GCE 持久磁盘"></p> <blockquote><p>通过向 MongoDB 数据库添加文档来将数据写入持久化存储</p></blockquote> <p>现在已经创建了 pod 并且容器也已经启动, 可以在容器中运行 MongoDB shell, 从而向数据存储写入一些数据.</p> <p>如下面的代码清单所示执行 shlle 命令.</p> <p><strong>代码清单-6.5 在 mongodb pod 中执行 MongoDB shell</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> mongodb mongo
MongoDB shell version: <span class="token number">3.2</span>.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
For interactive help, <span class="token builtin class-name">type</span> <span class="token string">&quot;help&quot;</span><span class="token builtin class-name">.</span>
For <span class="token function">more</span> comprehensive documentation, see
http://docs.mongodb.org/
Questions? Try the support group
http://groups.google.com/group/mongodb-user
<span class="token punctuation">..</span>.
<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>MongoDB 允许存储 JSON 文档, 所以将存放一个文档, 以查看其是否被持久化存储, 并且可以在<strong>重新创建 pod 后检索到</strong>. 使用以下命令插入一个新的 JSON 文档:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token operator">&gt;</span> use mystore
switched to db mystore
<span class="token operator">&gt;</span> db.foo.insert<span class="token punctuation">(</span><span class="token punctuation">{</span>name:<span class="token string">'foo'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
WriteResult<span class="token punctuation">(</span><span class="token punctuation">{</span> <span class="token string">&quot;nInserted&quot;</span> <span class="token builtin class-name">:</span> <span class="token number">1</span> <span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>你已经插入了一个简单的 JSON 文档(name:'foo'), 现在可以通过 find()命令来查看插入的文档:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token operator">&gt;</span> db.foo.<span class="token function-name function">find</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span> <span class="token string">&quot;_id&quot;</span> <span class="token builtin class-name">:</span> ObjectId<span class="token punctuation">(</span><span class="token string">&quot;57a61eb9de0cfd512374cc75&quot;</span><span class="token punctuation">)</span>, <span class="token string">&quot;name&quot;</span> <span class="token builtin class-name">:</span> <span class="token string">&quot;foo&quot;</span> <span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>文档现在已经被存储在 GCE Persistent Disk 中了.</p> <blockquote><p>重新创建 pod 并验证其可以读取由前一个 pod 保存的数据</p></blockquote> <p>现在可以退出 mongodb shell(输入 exit 并按 Enter 键), 然后<strong>删除 pod 并重建</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete pod mongodb
pod <span class="token string">&quot;mongodb&quot;</span> deleted
<span class="token comment"># 创建pod</span>
$ kubectl create <span class="token parameter variable">-f</span> mongodb-pod-gcepd.yaml
pod <span class="token string">&quot;mongodb&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p><strong>新的 pod 使用与前一个 pod 完全相同的 GCE Persistent Disk, 所以运行在其中的 MongoDB 容器应该会看到完全相同的数据, 即便将 pod 调度到不同的节点也是一样的</strong>.</p> <p>提示: 可以通过执行 kubectl get po-owide 来查看 pod 被调度到哪个节点上.</p> <p>容器启动后, 可以再次运行 MongoDB shell 来检查是否还可以检索之前存储的文档, 如下面的代码清单所示.</p> <p><strong>代码清单-6.6 在新 pod 中检索 MongoDB 的持久化数据</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> mongodb mongo
MongoDB shell version: <span class="token number">3.2</span>.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
<span class="token punctuation">..</span>.
<span class="token operator">&gt;</span> use mystore
switched to db mystore
<span class="token operator">&gt;</span> db.foo.<span class="token function-name function">find</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span> <span class="token string">&quot;_id&quot;</span> <span class="token builtin class-name">:</span> ObjectId<span class="token punctuation">(</span><span class="token string">&quot;57a61eb9de0cfd512374cc75&quot;</span><span class="token punctuation">)</span>, <span class="token string">&quot;name&quot;</span> <span class="token builtin class-name">:</span> <span class="token string">&quot;foo&quot;</span> <span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>符合预期, <strong>数据仍然存在, 即便删除了 pod 并重建. 这证实了可以使用 GCE 持久磁盘在多个 pod 实例中持久化数据</strong>.</p> <p>这里完成了 MongoDB pod 的操作, 所以继续清理这个 pod, 但是不要删除底层的 GCE 持久磁盘, 这将在本章后面再次用到.</p> <h6 id="_6-4-2-通过底层持久化存储使用其他类型的卷"><a href="#_6-4-2-通过底层持久化存储使用其他类型的卷" class="header-anchor">#</a> 6.4.2 通过底层持久化存储使用其他类型的卷</h6> <p>因为 Kubernetes 集群运行在 Google Kubernetes 引擎上所以需要创建 GCE persistent disk. 当在其他地方运行 Kubernetes 集群时, 应该<strong>根据不同的基础设施使用其他类型的卷</strong>.</p> <p>例如, 如果 Kubernetes 集群运行在 Amazon 的 AWS EC2上, 就可以使用 awsElasticBlockStore 卷的 pod 提供持久化存储. 如果集群在 Microsoft Azure 上运行, 则可以使用 azureFile 或者 azureDisk 卷. 这里无法在这里详细介绍如何去实现, 实际上与前面的示例是一样的. 首先, <strong>需要创建实际的底层存储, 然后在卷定义中设置适当的属性</strong>.</p> <blockquote><p>使用 AWS 弹性块存储卷</p></blockquote> <p>例如, 要使用 AWS 弹性块存储(Aws Elastic Block Store)而不是 GCE 持久磁盘, 只需要更改卷定义. 如下面的代码清单所示(请参阅以粗体标注的行).</p> <p><strong>代码清单-6.7 使用 awsElastic Block Store 卷的 pod: mongodb-podaws.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data
    <span class="token key atrule">awsElasticBlockStore</span><span class="token punctuation">:</span>  <span class="token comment"># 使用awsElasticBlockStore替换了gcePersistentDisk</span>
      <span class="token key atrule">volumeId</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>volume  <span class="token comment"># 指定创建的EBS卷的ID</span>
      <span class="token key atrule">fsType</span><span class="token punctuation">:</span> ext4         <span class="token comment"># 文件系统类型还是EXT4</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><blockquote><p>使用 NFS 卷</p></blockquote> <p>如果集群是运行在<strong>自有的一组服务器</strong>上, 那么就有大量其他可移植的选项用于在卷内挂载外部存储. 例如, 要挂载一个简单的 NFS 共享, 只需<strong>指定 NFS 服务器和共享路径</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-6.8 使用 NFS 的 pod: mongodb-pod-nfs.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data
  <span class="token key atrule">nfs</span><span class="token punctuation">:</span>            <span class="token comment"># 这个卷受NFS共享支持</span>
    <span class="token key atrule">server</span><span class="token punctuation">:</span> 1.2.3.4    <span class="token comment"># NFS服务器的IP</span>
    <span class="token key atrule">path</span><span class="token punctuation">:</span> /some/path    <span class="token comment"># 服务器提供的路径</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><blockquote><p>使用其他存储技术</p></blockquote> <p>其他的支持选项包括用于挂载 ISCSI 磁盘资源的 iscsi, 用于挂载 GlusterFS 的 glusterfs, 适用于 RADOS 块设备的 rbd, 还有 fiexVolume, cinder, cephfs, fiocker, fc(光纤通道)等. rbd 如果你不会使用到它们, 就不需要知道所有的信息. 这里提到是为了<strong>展示 Kubernetes 支持广泛的存储技术, 并且可以使用喜欢和习惯的任何存储技术</strong>.</p> <p><strong>要了解每个卷类型设置需要哪些属性的详细信息, 可以转到 Kubernetes API 引用中的 Kubernetes API 定义, 或者通过第三章展示的通过 kubectl explain 查找信息</strong>. 如果你已经熟悉了一种特定的存储技术, 那么使用 explain 命令可以让你轻松地了解如何挂载一个适当类型的卷, 并在 pod 中使用它.</p> <p>但是开发人员需要知道所有信息吗? 开发人员在创建 pod 时, 应该处理与基础设施相关的存储细节, 还是应该留给集群管理员处理?</p> <p>通过 pod 的卷来隐藏真实的底层基础设施, 不就是 Kubernetes 存在的意义吗? 举个例子, 让研发人员来指定 NFS 服务器的主机名会是一件感觉很糟糕的事情. 而这还不是最糟糕的.</p> <p><strong>将这种涉及基础设施类型的信息塞到一个 pod 设置中, 意味着 pod 设置与特定的 Kubernetes 集群有很大耦合度. 这就不能在另一个 pod 中使用相同的设置了. 所以使用这样的卷并不是在 pod 中附加持久化存储的最佳实践</strong>. 在下一节中将学习如何改进这一点.</p> <h5 id="_6-5-从底层存储技术解耦pod"><a href="#_6-5-从底层存储技术解耦pod" class="header-anchor">#</a> 6.5 从底层存储技术解耦pod</h5> <p>到目前为止, 探索过的所有持久卷类型都要求 pod 的开发人员了解集群中可用的真实网络存储的基础结构. 例如, 要创建支持 NFS 协议的卷, 开发人员必须知道 NFS 节点所在的实际服务器. 这违背了 Kubernetes 的基本理念, 这个理念<strong>旨在向应用程序及其开发人员隐藏真实的基础设施, 使他们不必担心基础设施的具体状态, 并使应用程序可在大量云服务商和数据企业之间进行功能迁移</strong>.</p> <p>理想的情况是, 在 Kubernetes 上部署应用程序的开发人员不需要知道底层使用的是哪种存储技术, 同理他们也不需要了解应该使用哪些类型的物理服务器来运行 pod, 与基础设施相关的交互是集群管理员独有的控制领域.</p> <p>当开发人员需要一定数量的持久化存储来进行应用时, 可以向 Kubernetes 请求, 就像在创建 pod 时可以请求 CPU, 内存和其他资源一样. 系统管理员可以对集群进行配置让其可以为应用程序提供所需的服务.</p> <h6 id="_6-5-1-介绍持久卷和持久卷声明"><a href="#_6-5-1-介绍持久卷和持久卷声明" class="header-anchor">#</a> 6.5.1 介绍持久卷和持久卷声明</h6> <p><mark><strong>在 Kubernetes 集群中为了使应用能够正常请求存储资源, 同时避免处理基础设施细节, 引入了两个新的资源, 分别是持久卷和持久卷声明</strong></mark>, 这名字可能有点误导, 因为正如在前面几节中看到的, 甚至常规的 Kubernetes 卷也可以用来存储持久性数据.</p> <p>在 pod 中使用 <strong>PersistentVolume</strong>(持久卷, 简称 PV)要比使用常规的 pod 卷复杂一些, 所以通过图 6.6 来说明持久卷, 持久卷声明和真实底层存储是如何相互关联的.</p> <p><img src="/img/image-20240227233357-yvsdwjt.png" alt="image" title="图6.6 持久卷由集群管理员提供, 并被 pod 通过持久卷声明来消费"></p> <p><strong>研发人员无须向他们的 pod 中添加特定技术的卷, 而是由集群管理员设置底层存储, 然后通过 Kubernetes API 服务器创建持久卷并注册. 在创建持久卷时, 管理员可以指定其大小和所支持的访问模式.</strong></p> <p>当集群用户需要在其 pod 中使用持久化存储时, 他们首先<strong>创建持久卷声明</strong>(PersistentVolumeClaim, 简称 PVC)清单, 指定所需要的<strong>最低容量要求和访问模式</strong>, 然后用户将持久卷声明清单提交给 Kubernetes API 服务器, Kubernetes 将找到可匹配的持久卷并将其绑定到持久卷声明.</p> <p><strong>持久卷声明可以当作 pod 中的一个卷来使用, 其他用户不能使用相同的持久卷, 除非先通过删除持久卷声明绑定来释放</strong>.</p> <h6 id="_6-5-2-创建持久卷"><a href="#_6-5-2-创建持久卷" class="header-anchor">#</a> 6.5.2 创建持久卷</h6> <p>下面重新讨论 MongoDB 示例, 但与之前操作不同的是, 这次不会直接引用在 pod 中的 GCE 持久磁盘. 相反, 你将首先承担<strong>集群管理员</strong>的角色, 并<strong>创建一个支持 GCE 持久磁盘的持久卷</strong>. 然后, 你将承担应用程序研发人员的角色, 首先<strong>声明持久卷, 然后在 pod 中使用</strong>.</p> <p>在 6.4.1 节中, 通过使用 GCE 持久磁盘来设置物理存储, 这次不用再这么操作. 这次需要做的就是在 Kubernetes 中创建持久卷, 方法是准备如下所示的代码清单, 并将其提交给 API 服务器.</p> <p><strong>代码清单-6.9 一个 gcePersistentDisk 持久卷: mongodb-pv-gcepd.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>pv
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>   
    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi      <span class="token comment"># 定义PersistentVolume的大小</span>
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>        <span class="token comment"># 可以被单个客户端挂载为读写模式或者被多个客户端挂载为只读模式</span>
  <span class="token punctuation">-</span> ReadWriteOnce
  <span class="token punctuation">-</span> ReadOnlyMany
  <span class="token key atrule">persistentVolumeReclaimPolicy</span><span class="token punctuation">:</span> Retain  <span class="token comment"># 当声明被释放后, PersistentVolume将会被保留(不清理和删除)</span>
  <span class="token key atrule">gcePersistentDisk</span><span class="token punctuation">:</span>    <span class="token comment"># PersistentVolume指定支持之前创建的GCE持久磁盘</span>
    <span class="token key atrule">pdName</span><span class="token punctuation">:</span> mongodb
    <span class="token key atrule">fsType</span><span class="token punctuation">:</span> ext4
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>注意: 如果在用 Minikube, 请用 mongodb-pv-hostpath.yaml 文件创建 PV.</p> <p>在创建持久卷时, <strong>管理员需要告诉 Kubernetes 其对应的容量需求, 以及它是否可以由单个节点或多个节点同时读取或写入</strong>. 管理员还需要告诉 Kubernetes 如何处理 PersistentVolume(当持久卷声明的绑定被删除时). 最后, 无疑也很重要的事情是, 管理员需要指定持久卷支持的实际存储类型, 位置和其他属性. 如果仔细观察, 当直接在 pod 卷中引用 GCE 持久磁盘时, 最后一部分配置与前面完全相同(在下面的代码清单中再次显示).</p> <p><strong>代码清单-6.10 在 pod 卷中引用 GCE PD</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data
    <span class="token key atrule">gcePersistentDisk</span><span class="token punctuation">:</span>
      <span class="token key atrule">pdName</span><span class="token punctuation">:</span> mongodb
      <span class="token key atrule">fsType</span><span class="token punctuation">:</span> ext4
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>在使用 kubectl create 命令创建持久卷之后, 应该可以声明它</strong>了. 看看是否列出了所有的持久卷:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">pv</span>
NAME           CAPACITY     RECLAIMPOLICY     ACCESSMODES     STATUS         CLAIM
mongodb-pv     1Gi          Retain            RWO,ROX         Available
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: 部分省略, <strong>同时 pv 也用作 persistentvolume 的简写</strong>.</p> <p>正如预期的那样, 持久卷显示为可用, 因为你<strong>还没创建持久卷声明</strong>.</p> <p>注意: <mark><strong>持久卷不属于任何命名空间(见图6.7), 它跟节点一样是集群层面的资源</strong></mark>.</p> <p><img src="/img/image-20240227233425-ut90oh0.png" alt="image" title="图6.7 和集群节点一样, 持久卷不属于任何命名空间, 区别于 pod 和持久卷声明"></p> <h6 id="_6-5-3-通过创建持久卷声明来获取持久卷"><a href="#_6-5-3-通过创建持久卷声明来获取持久卷" class="header-anchor">#</a> 6.5.3 通过创建持久卷声明来获取持久卷</h6> <p><strong>假设现在需要部署一个需要持久化存储的 pod, 将要用到之前创建的持久卷, 但是不能直接在 pod 内使用, 需要先声明一个</strong>.</p> <p><strong>声明一个持久卷和创建一个 pod 是相对独立的过程, 因为即使 pod 被重新调度(切记, 重新调度意味着先前的 pod 被删除并且创建了一个新的 pod), 我们也希望通过相同的持久卷声明来确保可用.</strong></p> <blockquote><p>创建持久卷声明</p></blockquote> <p>现在开始创建一个声明. 先参考下面的代码清单所示的内容来<strong>准备一个持久卷声明</strong>清单, 并通过 kubectl create 将其发布到 Kubernetes API.</p> <p><strong>代码清单-6.11 PersistentColumeClaim: mongodb-pvc.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim    <span class="token comment"># 资源类型为持久卷声明</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>pvc    <span class="token comment"># 声明的名称-稍后将声明当做pod的卷使用时需要用到</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>    <span class="token comment"># 申请1G的存储空间</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>   <span class="token comment"># 允许单个客户端访问(同时支持读取和写入操作) </span>
  <span class="token punctuation">-</span> ReadWriteOnce  
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> <span class="token string">&quot;&quot;</span>    <span class="token comment"># 将在关于动态配置的章节中了解到此处设置的用意</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>当创建好声明, Kubernetes 就会找到适当的持久卷并将其绑定到声明, 持久卷的容量必须足够大以满足声明的需求, 并且卷的访问模式必须包含声明中指定的访问模式</strong>. 在该示例中, 声明请求 1 GiB 的存储空间和 ReadWriteOnce 访问模式. 之前创建的持久卷符合刚刚声明中的这两个条件, 所以它被绑定到对应的声明中. 可以通过检查声明来查看.</p> <blockquote><p>列举持久卷声明</p></blockquote> <p>列举出所有的持久卷声明来查看 PVC 的状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pvc
NAME         STATUS VOLUME         CAPACITY    ACCESSMODES  AGE
mongodb-pvc  Bound  mongodb-pv     1Gi         RWO,ROX      3s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: <strong>使用 pvc 来代称 persistentvolumeclaim</strong>.</p> <p>PVC 状态显示已与持久卷的 mongodb-pv 绑定. 请留意访问模式的简写:</p> <ul><li><strong>RWO</strong>(ReadWriteOnce): 仅允许单个节点挂载读写.</li> <li><strong>ROX</strong>(ReadOnlyMany): 允许多个节点挂载只读.</li> <li><strong>RWX</strong>(ReadWriteMany): 允许多个节点挂载读写这个卷.</li></ul> <p>注意: RWO, ROX, RWX 涉及可以同时使用卷的工作节点的数量而并非 pod 的数量.</p> <blockquote><p>列举持久卷</p></blockquote> <p>通过使用 kubectl get 命令, 还可以看到<strong>持久卷现在已经 Bound</strong>, 并且不再是 Available.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">pv</span>
NAME         CAPACITY ACCESSMODES     STATUS     CLAIM                 AGE
mongodb-pv   1Gi      RWO,ROX         Bound      default/mongodb-pvc   1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><mark><strong>持久卷显示被绑定在 default/mongodb-pvc 的声明上, 这个 default 部分是声明所在的命名空间(在默认命名空间中创建的声明), 之前有提到过持久卷是集群范围的, 因此不能在特定的命名空间中创建, 但是持久卷声明又只能在特定的命名空间创建, 所以持久卷和持久卷声明只能被同一命名空间内的 pod 创建使用</strong></mark>.</p> <h6 id="_6-5-4-在pod中使用持久卷声明"><a href="#_6-5-4-在pod中使用持久卷声明" class="header-anchor">#</a> 6.5.4 在pod中使用持久卷声明</h6> <p><strong>持久卷现在已经可用了, 除非先释放掉卷, 否则没有人可以申明相同的卷</strong>. 要在 pod 中使用持久卷, 需要<strong>在 pod 的卷中引用持久卷声明名称</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-6.12 使用 PVC 卷的 pod: mongodb-pod-pvc.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod    <span class="token comment"># 定义pod</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> mongo
    <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /data/db
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">27017</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>    <span class="token comment"># 定义卷</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>data
    <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span>    <span class="token comment"># 在pod卷中通过名称应用持久卷声明</span>
      <span class="token key atrule">claimName</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>pvc
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>继续创建 pod, 现在检查这个 pod 是否确实在使用<strong>相同的持久卷和底层 GCE PD</strong>. 通过再次运行 MongoDB shell, 应该可以看到之前存储的数据, 如下面的代码清单所示.</p> <p><strong>代码清单-6.13 在已使用 PVC 和 PV 的 pod 中检索 MongoDB 的持久化数据</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> mongodb mongo
MongoDB shell version: <span class="token number">3.2</span>.8
connecting to: mongodb://127.0.0.1:27017
Welcome to the MongoDB shell.
<span class="token punctuation">..</span>.
<span class="token operator">&gt;</span> use mystore
switched to db mystore
<span class="token operator">&gt;</span> db.foo.<span class="token function-name function">find</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span> <span class="token string">&quot;_id&quot;</span> <span class="token builtin class-name">:</span> ObjectId<span class="token punctuation">(</span><span class="token string">&quot;57a61eb9de0cfd512374cc75&quot;</span><span class="token punctuation">)</span>, <span class="token string">&quot;name&quot;</span> <span class="token builtin class-name">:</span> <span class="token string">&quot;foo&quot;</span> <span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>符合预期, 可以检索之前存储到 MongoDB 的文档.</p> <h6 id="_6-5-5-了解使用持久卷和持久卷声明的好处"><a href="#_6-5-5-了解使用持久卷和持久卷声明的好处" class="header-anchor">#</a> 6.5.5 了解使用持久卷和持久卷声明的好处</h6> <p>通过图 6.8, 展示了 pod 可以<strong>直接使用, 或者通过持久卷和持久卷声明, 这两种方式使用 GCE 持久磁盘</strong>.</p> <p><img src="/img/image-20240227233455-bjk9etv.png" alt="image" title="图6.8 直接使用通过 PVC 和 PV 使用 GCE 持久磁盘"></p> <p>考虑如何使用<strong>这种间接方法从基础设施获取存储, 对于应用程序开发人员(或者集群用户)来说更加简单</strong>. 是的, 这需要额外的步骤来创建持久卷和持久卷声明, 但是<strong>研发人员不需要关心底层实际使用的存储技术</strong>.</p> <p>此外, 现在可以在许多不同的 Kubernetes 集群上使用相同的 pod 和持久卷声明清单, 因为它们不涉及任何特定依赖于基础设施的内容. 声明说: &quot;我需要 x 存储量, 并且我需要能够支持一个客户端同时读取和写入.&quot; 然后 pod 通过<strong>其中一个卷的名称来引用声明</strong>.</p> <h6 id="_6-5-6-回收持久卷"><a href="#_6-5-6-回收持久卷" class="header-anchor">#</a> 6.5.6 回收持久卷</h6> <p>在结束关于持久卷的本节前, 先做一个快速实验, 删除 pod 和持久卷声明:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 删除pod</span>
$ kubectl delete pod mongodb
pod <span class="token string">&quot;mongodb&quot;</span> deleted
<span class="token comment"># 删除持久卷声明</span>
$ kubectl delete pvc mongodb-pvc
persistentvolumeclaim <span class="token string">&quot;mongodb-pvc&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><strong>如果再次创建持久卷声明会怎样? 它是否会被绑定到持久卷?</strong>  在创建声明后, kubectl get pvc 命令返回的结果是什么?</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pvc
NAME         STATUS   VOLUME  CAPACITY ACCESSMODES  AGE
mongodb-pvc  Pending                                13s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这个<strong>持久卷声明的状态显示为 Pending</strong>, 有趣. 之前创建声明的时候, 它<strong>立即绑定到了持久卷</strong>, 那么为什么现在不绑定呢? 也许列出持久卷可以看得更清楚一些:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">pv</span>
NAME         CAPACITY     ACCESSMODES     STATUS     CLAIM                 REASON  AGE
mongodb-pv   1Gi          RWO,ROX         Released   default/mongodb-pvc           5m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>STATUS 列显示持久卷的状态是 <strong>Released</strong>, 不像之前那样是 Available. 原因在于<strong>之前已经使用过这个卷, 所以它可能包含前一个声明人的数据, 如果集群管理员还没来得及清理, 那么不应该将这个卷绑定到全新的声明中</strong>. 除此之外, 通过使用相同的持久卷, 新的 pod 可以读取由前一个 pod 存放的数据, 即使声明和 pod 是在不同的命名空间中创建的(因此有可能属于不同的集群租户).</p> <blockquote><p>手动回收持久卷</p></blockquote> <p>通过将 persistentVolumeReclaimPolicy 设置为 <strong>Retain</strong> 从而通知到 Kubernetes, 我们希望在创建持久卷后将其持久化, 让 Kubernetes 可以在持久卷从持久卷声明中释放后仍然能保留它的卷和数据内容. 据我所知, 手动回收持久卷并使其恢复可用的唯一方法是<strong>删除和重新创建持久卷资源</strong>. 当这样操作时, 你将决定如何处理底层存储中的文件: 可以删除这些文件, 也可以闲置不用, 以便在下一个 pod 中复用它们.</p> <blockquote><p>自动回收持久卷</p></blockquote> <p>存在两种其他<strong>可行的回收策略: Recycle 和 Delete</strong>. 第一种删除卷的内容并使卷可用于再次声明, 通过这种方式, 持久卷可以被不同的持久卷声明和 pod 反复使用, 如图 6.9 所示.</p> <p><img src="/img/image-20240227233515-zxe94h8.png" alt="image" title="图6.9 持久卷和持久卷声明的生命周期, 以及在 pod 中的使用"></p> <p>而另一边, <strong>Delete 策略删除底层存储</strong>. 需要注意当前 GCE 持久磁盘无法使用 Recycle 选项. 这种类型的持久卷只支持 Retain 和 Delete 策略, 其他类型的持久磁盘可能支持这些选项, 也可能不支持这些选项. 因此, 在创建自己的持久卷之前, 一定要检查卷中所用到的特定底层存储支持什么回收策略.</p> <p>提示: 可以在现有的持久卷上更改持久卷回收策略. 比如, 如果最初将其设置为 Delete, 则可以轻松地将其更改为 Retain, 以防止丢失有价值的数据.</p> <h5 id="_6-6-持久卷的动态卷配置"><a href="#_6-6-持久卷的动态卷配置" class="header-anchor">#</a> 6.6 持久卷的动态卷配置</h5> <p>使用持久卷和持久卷声明可以轻松获得持久化存储资源, 无须研发人员处理下面实际使用的存储技术, 但这<strong>仍然需要一个集群管理员来支持实际的存储</strong>. 幸运的是, Kubernetes 还可以<strong>通过动态配置持久卷来自动执行此任务</strong>.</p> <p><strong>集群管理员可以创建一个持久卷配置, 并定义一个或多个 StorageClass 对象, 从而让用户选择他们想要的持久卷类型而不仅仅只是创建持久卷. 用户可以在其持久卷声明中引用 StorageClass, 而配置程序在配置持久存储时将采用这一点.</strong></p> <p>注意: 与持久卷类似, StorageClass 资源并非命名空间.</p> <p>Kubernetes 包括最流行的云服务提供商的置备程序 provisioner, 所以管理员并不总是需要创建一个置备程序. 但是如果 Kubernetes 部署在本地, 则需要配置定制的置备程序.</p> <p>与管理员预先提供一组持久卷不同的是, 它们需要定义一个或两个(或多个)StorageClass, 并允许系统在每次通过持久卷声明请求时创建一个新的持久卷. 最重要的是, 不可能耗尽持久卷(很明显, 你可以用完存储空间).</p> <h6 id="_6-6-1-通过storageclass资源定义可用存储类型"><a href="#_6-6-1-通过storageclass资源定义可用存储类型" class="header-anchor">#</a> 6.6.1 通过StorageClass资源定义可用存储类型</h6> <p>在用户创建持久卷声明之前, 管理员需要<strong>创建一个或多个 StorageClass 资源, 然后才能创建新的持久卷</strong>. 来看下面代码清单中的一个例子.</p> <p><strong>代码清单-6.14 一个 StorageClass 定义: storageclass-fast-gcepd.yaml</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>apiVersion: storage.k8s.io/v1
kind: StorageClass  <span class="token comment"># 资源类型为StorageClass</span>
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd    <span class="token comment"># 用于配置持久卷的卷插件</span>
parameters:    
  type: pd-ssd    <span class="token comment"># 传递给parameters的参数</span>
  zone: europe-west1-b
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>注意: 如果使用 Minikube, 请部署文件 storageclass-fast-hostpath.yaml.</p> <p><strong>StorageClass 资源指定当久卷声明请求此 StorageClass 时应使用哪个置备程序来提供持久卷</strong>. StorageClass 定义中定义的参数将传递给置备程序, 并具体到每个供应器插件. StorageClass 使用 GCE 持久磁盘的预配置器, 这意味着当 Kubernetes 在 GCE 中运行时可供使用. 对于其他云提供商, 需要使用其他的置备程序.</p> <h6 id="_6-6-2-请求持久卷声明中的存储类"><a href="#_6-6-2-请求持久卷声明中的存储类" class="header-anchor">#</a> 6.6.2 请求持久卷声明中的存储类</h6> <p>创建 StorageClass 资源后, <strong>用户可以在其持久卷声明中按名称引用存储类</strong>.</p> <blockquote><p>创建一个请求特定存储类的 PVC 定义</p></blockquote> <p>可以修改 mongodb-pvc 以使用动态配置. 以下代码清单显示了 PVC 中更新后的 YAML 定义.</p> <p><strong>代码清单-6.15 一个采用动态配置的 PVC:mongodb-pvc-dp.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>pvc
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> fast    <span class="token comment"># 该PVC请求自定义存储类</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 100Mi
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> ReadWriteOnce  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>除了指定大小和访问模式, 持久卷声明现在还会指定<strong>要使用的存储类别</strong>. 在创建声明时, 持久卷由 fast StorageClass 资源中引用的 provisioner 创建. 即使现有手动设置的持久卷与持久卷声明匹配, 也可以使用 provisioner.</p> <p>注意: 如果在 PVC 中引用一个不存在的存储类, 则 PV 的配置将失败(在 PVC 上使用 kubectl describe 时, 将会看到 ProvisioningFailed 事件).</p> <blockquote><p>检查所创建的 PVC 和动态配置的 PV</p></blockquote> <p>接着, 创建 PVC, 然后使用 kubectl get 进行查看:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pvc mongodb-pvc
NAME         STATUS VOLUME         CAPACITY     ACCESSMODES   STORAGECLASS
mongodb-pvc  Bound  pvc-1e6bc048   1Gi          RWO           fast
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>VOLUME 列显示了与<strong>此声明绑定的持久卷</strong>(实际名称比上面显示的长). 现在可以尝试列出持久卷, 看看是否确实自动创建了一个新的 PV:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get <span class="token function">pv</span>
NAME         CAPACITY     ACCESSMODES     RECLAIMPOLICY  STATUS     STORAGECLASS
mongodb-pv   1Gi          RWO,ROX         Retain         Released
pvc-1e6bc048 1Gi          RWO             Delete         Bound      fast
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>注意: 仅显示相关的列.</p> <p>可以看到动态配置的持久卷其容量和访问模式是在 PVC 中所要求的. 它的回收策略是 Delete, 这意味着当 PVC 被删除时, 持久卷也将被删除. 除了 PV, 置备程序还提供了真实的存储空间, fast StorageClass 被配置为使用 <code>kubernetes.io/gce-pd</code>​ 从而提供了 GCE 持久磁盘. 可以使用以下命令查看磁盘:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ gcloud compute disks list
NAME                         ZONE             SIZE_GB     TYPE         STATUS
gke-kubia-dyn-pvc-1e6bc048   europe-west1-d   <span class="token number">1</span>           pd-ssd       READY
gke-kubia-default-pool-71df  europe-west1-d   <span class="token number">100</span>         pd-standard  READY
gke-kubia-default-pool-79cd  europe-west1-d   <span class="token number">100</span>         pd-standard  READY
gke-kubia-default-pool-blc4  europe-west1-d   <span class="token number">100</span>         pd-standard  READY
mongodb                      europe-west1-d   <span class="token number">1</span>           pd-standard  READY
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>如你所见, <strong>第一个持久磁盘的名称表明它是动态配置的, 同时它的类型显示为一个 SSD, 正如在前面创建的存储类中所指定的那样</strong>.</p> <blockquote><p>了解存储类的使用</p></blockquote> <p>集群管理员可以创建具有不同性能或其他特性的多个存储类, 然后研发人员再决定对应每一个声明最适合的存储类.</p> <p><strong>StorageClasses 的好处在于, 声明是通过名称引用它们的</strong>. 因此, 只要 StorageClass 名称在所有这些名称中相同, PVC 定义便可跨不同集群移植. 要自己查看这个可移植性, 可以尝试在 Minikube 上运行相同的示例, 假设你一直在使用 GKE. 作为集群管理员, 你必须创建一个不同的存储类(但名称相同). storageclass-fast-hostpath.yaml 文件中定义的存储类是专用于 Minikube 的. 然后, 一旦部署了存储类, 作为集群用户, 就可以像以前一样部署完全相同的 PVC 清单和完全相同的 pod 清单. 这展示了 pod 和 PVC 在不同集群间的移植性.</p> <h6 id="_6-6-3-不指定存储类的动态配置"><a href="#_6-6-3-不指定存储类的动态配置" class="header-anchor">#</a> 6.6.3 不指定存储类的动态配置</h6> <p>正如本章中所做的那样, 将持久性存储附加到 pod 上变得越来越简单. 本章中的章节反映了存储配置是如何从早期的 Kubernetes 版本发展到现在的. 在最后一节中, 将看看<strong>将持久卷附加到 pod</strong> 的最新和最简单的方法.</p> <blockquote><p>列出存储类</p></blockquote> <p>当创建名为 fast 的自定义存储类时, 并未检查集群中是否已定义任何现有存储类. 现在为什么不这样试试? 以下是 GKE 中可用的存储类:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get sc
NAME         TYPE
fast         kubernetes.io/gce-pd
standard     <span class="token punctuation">(</span>default<span class="token punctuation">)</span> kubernetes.io/gce-pd
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>注意: 使用 sc 作为 storageclass 的简写.</p> <p>除了自己创建的 fast 存储类, 还存在 standard 存储类并标记为<strong>默认</strong>存储类. 很快就会知道其含义了, 下面列举 Minikube 中可用的存储类, 以便进行比较:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get sc
NAME         TYPE
fast         k8s.io/minikube-hostpath
standard     <span class="token punctuation">(</span>default<span class="token punctuation">)</span> k8s.io/minikube-hostpath
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>再来看看, fast 存储类是由你创建的, 并且此处也存在默认的 standard 存储类, 比较两个列表中的 TYPE 列, 会看到 GKE 正在使用 kubernetes.io/gce-pd 置备程序, 而 Minikube 正在使用 k8s.io/minikube-hostpath.</p> <blockquote><p>检查默认存储类</p></blockquote> <p>使用 kubectl get 可查看有关 GKE 集群中标准存储类的更多信息, 如下面的代码清单所示.</p> <p><strong>代码清单-6.16 GKE 上的标准存储类的定义</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get sc standard <span class="token parameter variable">-o</span> yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    <span class="token comment"># 此注释将存储类标记为默认</span>
    storageclass.beta.kubernetes.io/is-default-class: <span class="token string">&quot;true&quot;</span>
  creationTimestamp: <span class="token number">2017</span>-05-16T15:24:11Z
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
    kubernetes.io/cluster-service: <span class="token string">&quot;true&quot;</span>
  name: standard
  resourceVersion: <span class="token string">&quot;180&quot;</span>
  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard
  uid: b6498511-3a4b-11e7-ba2c-42010a840014
parameters:
  type: pd-standard
provisioner: kubernetes.io/gce-pd  <span class="token comment"># GCE持久磁盘配置器被用于配置此类的PV</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>如果仔细观察清单的顶部, 会看到存储类定义会包含一个注释, 这会使其成为默认的存储类. 如果持久卷声明没有明确指出要使用哪个存储类, 则默认存储类会用于动态提供持久卷的内容.</p> <blockquote><p>创建一个没有指定存储类别的持久卷声明</p></blockquote> <p>可以在<strong>不指定 storageClassName 属性的情况下创建 PVC</strong>, 并且(在 Google Kubernetes 引擎上)将为你提供一个 <strong>pd-standard</strong> 类型的 GCE 持久磁盘. 试试通过下面的代码清单中的 YAML 来创建一个声明.</p> <p><strong>代码清单-6.17 不指定存储类别的 PVC: mongodb-pvc-dp-nostorageclass.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">-</span>pvc2
<span class="token key atrule">spec</span><span class="token punctuation">:</span>        <span class="token comment"># 没有指定storageClassName属性(与前面的示例不同)</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 100Mi
  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> ReadWriteOnce
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>此 PVC 定义仅包含存储大小请求和所需访问模式, 并不包含存储级别. 在创建 PVC 时, 将使用任何标记为默认的存储类. 可以通过如下代码确认:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get pvc mongodb-pvc2
NAME             STATUS     VOLUME         CAPACITY    ACCESSMODES  STORAGECLASS
mongodb-pvc2     Bound      pvc-95a5ec12   1Gi         RWO          standard
$ kubectl get <span class="token function">pv</span> pvc-95a5ec12
NAME             CAPACITY    ACCESSMODES     RECLAIMPOLICY  STATUS   STORAGECLASS
pvc-95a5ec12     1Gi         RWO             Delete         Bound    standard
$ gcloud compute disks list
NAME                         ZONE             SIZE_GB     TYPE         STATUS
gke-kubia-dyn-pvc-95a5ec12   europe-west1-d   <span class="token number">1</span>           pd-standard  READY
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><blockquote><p>强制将持久卷声明绑定到预配置的其中一个持久卷</p></blockquote> <p>这最后会告诉我们为什么要在代码清单 6.11 中将 storageClassName 设置为一个<strong>空字符串</strong>(当你想让 PVC 绑定到你手动配置的 PV 时). 在这里回顾一下这个 PVC 定义的相关行:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> <span class="token string">&quot;&quot;</span> <span class="token comment"># 将空字符串指定为存储类名可确保PVC绑定到预先配置的PV, 而不是动态配置新的PV</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>​​如果尚未将 storageClassName 属性设置为空字符串, 则尽管已存在适当的预配置持久卷, 但动态卷置备程序仍将配置新的持久卷. 此时, 笔者想演示一个声明如何绑定到手动预先配置的持久卷, 同时不希望置备程序干涉.</p> <p>提示: <strong>如果希望 PVC 使用预先配置的 PV, 请将 storageClassName 显式设置为 &quot;&quot;</strong> .</p> <blockquote><p>了解动态持久卷供应的全貌</p></blockquote> <p>总而言之, <mark><strong>将持久化存储附加到一个容器的最佳方式是仅创建 PVC(如果需要, 可以使用明确指定的 storgeClassName)和容器(其通过名称引用 PVC), 其他所有内容都由动态持久卷置备程序处理</strong></mark>.</p> <p>要全面了解获取动态的持久卷所涉及的步骤, 请查看图 6.10.</p> <p><img src="/img/image-20240227233547-kbsn2hn.png" alt="image" title="图6.10 持久卷动态配置的完整图示"></p> <h5 id="_6-7-本章小结"><a href="#_6-7-本章小结" class="header-anchor">#</a> 6.7 本章小结</h5> <p>本章展示了如何<strong>使用卷来为 pod 的容器提供临时或持久存储</strong>. 你已经学会了如何:</p> <ul><li>创建一个多容器 pod, 并通过为 pod 添加一个卷并将其挂载到每个容器中, 来让 pod 中的容器操作相同的文件</li> <li>使用 emptyDir 卷存储临时的非持久数据</li> <li>使用 gitRepo 卷可以在 pod 启动时使用 Git 库的内容轻松填充目录</li> <li>使用 hostPath 卷从主机节点访问文件</li> <li>将外部存储装载到卷中, 以便在 pod 重启之前保持 pod 数据读写</li> <li>通过使用持久卷和持久卷声明解耦 pod 与存储基础架构</li> <li>为每个持久卷声明动态设置所需(或缺省)存储类的持久卷</li> <li>当需要将持久卷声明绑定到预配置的持久卷时, 防止动态置备程序干扰</li></ul> <p>在下一章中, 将看到 Kubernetes 提供了什么机制来将<strong>配置数据, 机密信息, 以及有关 pod 和容器的元数据提供给在 pod 内运行的进程</strong>. 这是通过本章中提到的<strong>特殊类型的卷</strong>完成的.</p> <h4 id="_7-configmap和secret-配置应用程序"><a href="#_7-configmap和secret-配置应用程序" class="header-anchor">#</a> 7.ConfigMap和Secret:配置应用程序</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>更改容器的主进程</li> <li>将命令行选项传递给应用程序</li> <li>设置暴露给应用程序的环境变量</li> <li>通过 ConfigMap 配置应用程序</li> <li>通过 Secret 传递敏感配置信息</li></ul> <p>到目前为止<strong>尚未传递过任何配置数据</strong>给示例中的应用. 几乎所有的应用都需要配置信息(不同部署示例间的区分设置, 访问外部系统的证书等), 并且这些<strong>配置数据不应该被嵌入应用本身</strong>. 让我们来看一下<mark><strong>如何传递配置选项给运行在 Kubernetes 上的应用程序</strong></mark>.</p> <h5 id="_7-1-配置容器化应用程序"><a href="#_7-1-配置容器化应用程序" class="header-anchor">#</a> 7.1 配置容器化应用程序</h5> <p>回顾如何传递配置数据给运行在 Kubernetes 中的应用程序之前, 首先来<strong>看一下容器化应用通常是如何被配置</strong>的.</p> <p>开发一款新应用程序的初期, 除了将配置嵌入应用本身, 通常会以命令行参数的形式配置应用. 随着配置选项数量的逐渐增多, 将<strong>配置文件化</strong>.</p> <p>另一种通用的传递配置选项给容器化应用程序的方法是借助<strong>环境变量</strong>. 应用程序主动查找某一特定环境变量的值, 而非读取配置文件或者解析命令行参数. 例如, MySQL 官方镜像内部通过环境变量 MYSQL_ROOT_PASSWORD 设置超级用户 root 的密码.</p> <p>为何环境变量的方案会在容器环境下如此常见? 通常直接在 Docker 容器中采用配置文件的方式是有些许困难的, 往往需要将配置文件打入容器镜像, 抑或是挂载包含该文件的卷. 显然, 前者类似于在应用程序源代码中硬编码配置, 每次修改完配置之后需要重新构建镜像. 除此之外, 任何拥有镜像访问权限的人可以看到配置文件中包含的敏感信息, 如证书和密钥. 相比之下, 挂载卷的方式更好, 然而在容器启动之前需确保配置文件已写入响应的卷中.</p> <p>如果你已经阅读过前面的章节, 可能会想到采用 gitRepo 卷作为配置源. 这并不是一个坏主意, 通过它可以保持配置的版本化, 并且能比较容易地按需回滚配置. 然而有一种更加简便的方法能将配置数据置于 Kubernetes 的顶级资源对象中, 并可与其他资源定义存入同一 Git 仓库或者基于文件的存储系统中. <strong>用以存储配置数据的 Kubernetes 资源称为 ConfigMap</strong>. 将会在本章学习如何使用它.</p> <p>无论你是否在使用 ConfigMap 存储配置数据, 以下方法均可被用作配置应用程序:</p> <ul><li><mark><strong>向容器传递命令行参数</strong></mark></li> <li><mark><strong>为每个容器设置自定义环境变量</strong></mark></li> <li><mark><strong>通过特殊类型的卷将配置文件挂载到容器中</strong></mark></li></ul> <p>接下来的几节中将会介绍这些方法. 开始介绍之前, 首先从<strong>安全角度</strong>观察一下配置选项. 尽管绝大多数配置选项并未包含敏感信息, 少量配置依旧可能含有证书, 私钥, 以及其他需要保持安全的相似数据. 该类型数据需要被特殊对待. 这也是为何 Kubernetes 提供另一种称作 Secret 的一级对象的原因. 将在本章节末尾学习到它.</p> <h5 id="_7-2-向容器传递命令行参数"><a href="#_7-2-向容器传递命令行参数" class="header-anchor">#</a> 7.2 向容器传递命令行参数</h5> <p>迄今为止所有示例中<strong>容器运行的命令都是镜像中默认定义</strong>的. Kubernetes 可在 pod 的容器中定义并<strong>覆盖命令</strong>以满足运行不同的可执行程序, 或者是以不同的命令 Kubernetes 可在 pod 的容器中定义并覆盖命令来运行不同的可执行程序, 或者是以不同的命令行参数集运行. 现在来看一下应该如何操作.</p> <h6 id="_7-2-1-在docker中定义命令与参数"><a href="#_7-2-1-在docker中定义命令与参数" class="header-anchor">#</a> 7.2.1 在Docker中定义命令与参数</h6> <p>首先需要阐明的是, 容器中运行的完整指令由两部分组成: <strong>命令与参数</strong>.</p> <blockquote><p>了解 ENTRYPOINT 与 CMD</p></blockquote> <p>Dockerfile 中的两种指令分别定义命令与参数这两个部分:</p> <ul><li><strong>ENTRYPOINT 定义容器启动时被调用的可执行程序.</strong></li> <li><strong>CMD 指定传递给 ENTRYPOINT 的参数.</strong></li></ul> <p>尽管可以直接使用 CMD 指令指定镜像运行时想要执行的命令, 正确的做法依旧是借助 ENTRYPOINT 指令, 仅仅用 CMD 指定所需的默认参数. 这样, 镜像可以直接运行, 无须添加任何参数:</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>$ docker run <span class="token generics"><span class="token punctuation">&lt;</span>image<span class="token punctuation">&gt;</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>或者是添加一些参数, <strong>覆盖 Dockerile 中任何由 CMD 指定的默认参数值</strong>:</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>$ docker run <span class="token generics"><span class="token punctuation">&lt;</span>image<span class="token punctuation">&gt;</span></span> <span class="token generics"><span class="token punctuation">&lt;</span>arguments<span class="token punctuation">&gt;</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>了解 shell 与 exec 形式的区别</p></blockquote> <p>上述两条指令均支持以下两种形式:</p> <ul><li><strong>shell 形式</strong>: 如 <code>ENTRYPOINT node app.js</code>​.</li> <li><strong>exec 形式</strong>: 如 <code>ENTRYPOINT[&quot;node&quot;,&quot;app.js&quot;]</code>​.</li></ul> <p>两者的区别在于指定的命令是否是在 shell 中被调用.</p> <p>对于第 2 章中创建的 kubia 镜像, 如果使用 exec 形式的 ENTRYPOINT 指令:</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token constant">ENTRYPOINT</span> <span class="token punctuation">[</span><span class="token string">&quot;node&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;app.js&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以从容器中的运行进程列表看出: 这里是直接运行 node 进程, 而并非在 shell 中执行.</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>$ docker exec <span class="token number">4675d</span> ps x
<span class="token constant">PID</span> <span class="token constant">TTY</span> <span class="token constant">STAT</span> <span class="token constant">TIME</span> <span class="token constant">COMMAND</span>
<span class="token number">1</span> <span class="token operator">?</span> <span class="token class-name">Ssl</span> <span class="token number">0</span><span class="token operator">:</span><span class="token number">00</span> node app<span class="token punctuation">.</span>js
<span class="token number">12</span> <span class="token operator">?</span> <span class="token class-name">Rs</span> <span class="token number">0</span><span class="token operator">:</span><span class="token number">00</span> ps x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>如果采用 shell 形式(ENTRYPOINT node app.js), 容器进程如下所示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> e4bad <span class="token function">ps</span> x
PID TTY STAT TIME COMMAND
<span class="token number">1</span> ? Ss <span class="token number">0</span>:00 /bin/sh <span class="token parameter variable">-c</span> <span class="token function">node</span> app.js    <span class="token comment"># 主进程是shell进程</span>
<span class="token number">7</span> ? Sl <span class="token number">0</span>:00 <span class="token function">node</span> app.js
<span class="token number">13</span> ? Rs+ <span class="token number">0</span>:00 <span class="token function">ps</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以看出, <strong>主进程(PID 1)是 shell 进程而非 node 进程, node 进程(PID 7)于 shell 中启动. shell 进程往往是多余的, 因此通常可以直接采用 exec 形式的 ENTRYPOINT 指令</strong>.</p> <blockquote><p>可配置化 fortune 镜像中的间隔参数</p></blockquote> <p>下面通过修改 fortune 脚本与镜像 Dockerfile 使循环的延迟间隔可配置. 如下面这段代码所示, 在 fortune 脚本中<strong>添加 VARIABLE 变量并用第一个命令行参数对其初始化</strong>.</p> <p><strong>代码清单-7.1 通过参数可配置化 fortune 脚本中的循环间隔: fortune-args/fortuneloop.sh</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token shebang important">#!/bin/bash</span>
<span class="token builtin class-name">trap</span> <span class="token string">&quot;exit&quot;</span> SIGINT
<span class="token assign-left variable">INTERVAL</span><span class="token operator">=</span><span class="token variable">$1</span>    <span class="token comment"># 添加变量</span>
<span class="token builtin class-name">echo</span> Configured to generate new fortune every <span class="token variable">$INTERVAL</span> seconds
<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /var/htdocs
<span class="token keyword">while</span> <span class="token builtin class-name">:</span>
<span class="token keyword">do</span>
    <span class="token builtin class-name">echo</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">date</span><span class="token variable">)</span></span> Writing fortune to /var/htdocs/index.html
    /usr/games/fortune <span class="token operator">&gt;</span> /var/htdocs/index.html
    <span class="token function">sleep</span> <span class="token variable">$INTERVAL</span>    
<span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>现在修改 Dockerfile, 采用 exec 形式的 ENTRYPOINT 指令, 以及利用 CMD 设置间隔的默认值为 10</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-7.2 修改 fortune 镜像的 Dockerfile: fortune-args/Dockerfile</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FROM ubuntu:latest
RUN <span class="token function">apt-get</span> update <span class="token punctuation">;</span> <span class="token function">apt-get</span> <span class="token parameter variable">-y</span> <span class="token function">install</span> fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT <span class="token punctuation">[</span><span class="token string">&quot;/bin/fortuneloop.sh&quot;</span><span class="token punctuation">]</span>    <span class="token comment"># exec形式的ENTRYPOINT</span>
CMD <span class="token punctuation">[</span><span class="token string">&quot;10&quot;</span><span class="token punctuation">]</span>        <span class="token comment"># 可执行程序的默认参数</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>现在可以重新构建镜像并推送至 Docker Hub. 这里将镜像的 tag 由 latest 修改为 args:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> build <span class="token parameter variable">-t</span> docker.io/luksa/fortune:args <span class="token builtin class-name">.</span>
$ <span class="token function">docker</span> push docker.io/luksa/fortune:args
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以用 Docker 在本地启动该镜像并进行测试:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-it</span> docker.io/luksa/fortune:args
Configured to generate new fortune every <span class="token number">10</span> seconds
Fri May <span class="token number">19</span> <span class="token number">10</span>:39:44 UTC <span class="token number">2017</span> Writing fortune to /var/htdocs/index.html
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>也可以<strong>传递一个间隔参数覆盖默认睡眠间隔值</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> run <span class="token parameter variable">-it</span> docker.io/luksa/fortune:args <span class="token number">15</span>
Configured to generate new fortune every <span class="token number">15</span> seconds
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在可以确保镜像能够正确应用传递给它的参数. 来看一下在 pod 中如何使用它.</p> <h6 id="_7-2-2-在kubernetes中覆盖命令和参数"><a href="#_7-2-2-在kubernetes中覆盖命令和参数" class="header-anchor">#</a> 7.2.2 在Kubernetes中覆盖命令和参数</h6> <p>在 Kubernetes 中<mark><strong>定义容器时, 镜像的 ENTRYPOINT 和 CMD 均可以被覆盖, 仅需在容器定义中设置属性 command 和 args 的值</strong></mark>, 如下面的代码清单所示.</p> <p><strong>代码清单-7.3 指定自定义命令与参数的 pod 定义</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> some/image
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/command&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;arg1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;arg2&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;arg3&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>绝大多数情况下, 只需要<strong>设置自定义参数</strong>. 命令一般很少被覆盖, 除非针对一些未定义 ENTRYPOINT 的通用镜像, 例如 busybox.</p> <p>注意: <strong>command 和 args 字段在 pod 创建后无法被修改</strong>.</p> <p>上述的两条 Dockerfile 指令与<strong>等同</strong>的 pod 规格字段如表 7.1 所示.</p> <p><strong>表-7.1 在 Docker 与 Kubernetes 中指定可执行程序及其参数</strong></p> <table><thead><tr><th>Docker</th> <th>Kubernetes</th> <th>描述</th></tr></thead> <tbody><tr><td>ENTRYPOINT</td> <td>commnad</td> <td>容器中运行的可执行文件</td></tr> <tr><td>CMD</td> <td>args</td> <td>传递给可执行文件的参数</td></tr></tbody></table> <blockquote><p>用自定义间隔值运行 fortune pod</p></blockquote> <p>为了能够用自定义的延迟间隔值运行 fortune pod, 首先复制文件 fortune-pod.yaml 并重命名为 fortune-pod-args.yaml, 然后修改它, 如下面的代码清单所示.</p> <p><strong>代码清单-7.4 在 pod 定义中传递参数值: fortune-pod-args.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune2s    <span class="token comment"># 修改pod名称</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/fortune<span class="token punctuation">:</span>args    <span class="token comment"># fortune:latest替换为fortune:args</span>
    <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;2&quot;</span><span class="token punctuation">]</span>                  <span class="token comment"># 该参数值使得脚本每隔两秒生成一个新fortune</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> html<span class="token punctuation">-</span>generator
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/htdocs
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>现在已经在容器定义中<strong>添加了 args 数组参数, 可以尝试创建该 pod</strong>. 数组值会在 pod 运行时作为命令行参数传递给容器.</p> <p>少量参数值的设置可以使用上述的数组表示. 多参数值情况下可以采用如下标记:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">args</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> foo
<span class="token punctuation">-</span> bar
<span class="token punctuation">-</span> <span class="token string">&quot;15&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>提示: 字符串值无须用引号标记, <strong>数值需要</strong>.</p> <h5 id="_7-3-为容器设置环境变量"><a href="#_7-3-为容器设置环境变量" class="header-anchor">#</a> 7.3 为容器设置环境变量</h5> <p>通过命令行参数指定参数值是给容器传递配置选项的其中一种手段. 接下来将学习<mark><strong>如何通过环境变量完成配置</strong></mark>.</p> <p>如前所述, <mark><strong>容器化应用通常会使用环境变量作为配置源. Kubernetes 允许为 pod 中的每一个容器都指定自定义的环境变量集合</strong></mark>, 如图 7.1 所示. 尽管从 pod 层面定义环境变量同样有效, 然而当前并未提供该选项.</p> <p>注意: 与容器的命令和参数设置相同, <strong>环境变量列表无法在 pod 创建后被修改</strong>.</p> <p><img src="/img/image-20240227233620-hlrki96.png" alt="image" title="图7.1 每个容器都可设置环境变量"></p> <blockquote><p>通过环境变量配置化 fortune 镜像中的间隔值</p></blockquote> <p>来看一下如何通过环境变量使 fortuneloop.sh 脚本中的睡眠间隔值可配置化, 具体如下面的代码清单所示.</p> <p><strong>代码清单-7.5 通过环境变量配置化 fortune 脚本中的间隔值: fortune-env/fortuneloop.sh</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token shebang important">#!/bin/bash</span>
<span class="token builtin class-name">trap</span> <span class="token string">&quot;exit&quot;</span> SIGINT
<span class="token builtin class-name">echo</span> Configured to generate new fortune every <span class="token variable">$INTERVAL</span> seconds
<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /var/htdocs
<span class="token keyword">while</span> <span class="token builtin class-name">:</span>
<span class="token keyword">do</span>
    <span class="token builtin class-name">echo</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">date</span><span class="token variable">)</span></span> Writing fortune to /var/htdocs/index.html
    /usr/games/fortune <span class="token operator">&gt;</span> /var/htdocs/index.html
    <span class="token function">sleep</span> <span class="token variable">$INTERVAL</span>
<span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>当前的应用仅是一个简单的 bash 脚本, 只需要移除脚本中 INTERVAL 初始化所在的行即可. 如果应用由 Java 编写, 需要使用 <code>System.getenv(&quot;INTERVAL&quot;)</code>​, 同样地, 对应到 Node.JS 与 Python 中分别是 <code>process.env.INTERVAL</code>​ 与 <code>os.environ[′INTERVAL′]</code>​.</p> <h6 id="_7-3-1-在容器定义中指定环境变量"><a href="#_7-3-1-在容器定义中指定环境变量" class="header-anchor">#</a> 7.3.1 在容器定义中指定环境变量</h6> <p>构建完新镜像(镜像的 tag 变更为 luksa/fortune:env)并推送至 Docker Hub 之后, 可以通过创建一个新 pod 来运行它. 如下面的代码清单所示, 在<strong>容器定义中写入环境变量以传递给脚本</strong>.</p> <p><strong>代码清单-7.6 在 pod 中指定环境变量: fortune-pod-env.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/fortune<span class="token punctuation">:</span>env
    <span class="token key atrule">env</span><span class="token punctuation">:</span>        <span class="token comment"># 在环境变量列表中添加一个新变量</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> INTERVAL
      <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;30&quot;</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> html<span class="token punctuation">-</span>generator
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>正如前面提到的, <mark><strong>环境变量被设置在 pod 的容器定义中, 并非是 pod 级别</strong></mark>.</p> <p>注意: 不要忘记在每个容器中, <strong>Kubernetes 会自动暴露相同命名空间下每个 service 对应的环境变量. 这些环境变量基本上可以被看作自动注入的配置</strong>.</p> <h6 id="_7-3-2-在环境变量值中引用其他环境变量"><a href="#_7-3-2-在环境变量值中引用其他环境变量" class="header-anchor">#</a> 7.3.2 在环境变量值中引用其他环境变量</h6> <p>在前面的示例中, <strong>环境变量的值是固定的</strong>. 可以采用 <code>$(VAR)</code>​ 语法在环境变量值中引用其他的环境变量. 假设定义了两个环境变量, 第二个变量定义中可包含第一个环境变量的值, 如下面的代码清单所示.</p> <p><strong>代码清单-7.7 在环境变量值中引用另一个变量</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">env</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FIRST_VAR
  <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;foo&quot;</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SECOND_VAR
  <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">&quot;$(FIRST_VAR)bar&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>SECOND_VAR 的值是 &quot;foobar&quot;. 7.2 节中介绍的 command 和 args 属性值同样可以像这样引用环境变量, 这将在 7.4.5 节中被使用到.</p> <h6 id="_7-3-3-硬编码环境变量的不足之处"><a href="#_7-3-3-硬编码环境变量的不足之处" class="header-anchor">#</a> 7.3.3 硬编码环境变量的不足之处</h6> <p>pod 定义硬编码意味着需要有效区分生产环境与开发过程中的 pod 定义. 为了能在<strong>多个环境</strong>下复用 pod 的定义, 需要将配置从 pod 定义描述中解耦出来. 幸运的是, 可以通过一种叫作 <strong>ConfigMap 的资源对象完成解耦, 用 valueFrom 字段替代 value 字段使 ConfigMap 成为环境变量值的来源</strong>. 接下来将学习到这一用法.</p> <h5 id="_7-4-利用configmap解耦配置"><a href="#_7-4-利用configmap解耦配置" class="header-anchor">#</a> 7.4 利用ConfigMap解耦配置</h5> <p><mark><strong>应用配置的关键在于能够在多个环境中区分配置选项, 将配置从应用程序源码中分离, 可频繁变更配置值</strong></mark>. 如果将 pod 定义描述看作是应用程序源代码, 显然需要<strong>将配置移出 pod 定义</strong>. 微服务架构下正是如此, 该架构定义了如何将多个个体组件组合成功能系统.</p> <h6 id="_7-4-1-configmap介绍"><a href="#_7-4-1-configmap介绍" class="header-anchor">#</a> 7.4.1 ConfigMap介绍</h6> <p><strong>Kubernetes 允许将配置选项分离到单独的资源对象 ConfigMap 中, 本质上就是一个键/值对映射, 值可以是短字面量, 也可以是完整的配置文件</strong>.</p> <p><strong>应用无须直接读取 ConfigMap, 甚至根本不需要知道其是否存在. 映射的内容通过环境变量或者卷文件(如图 7.2 所示)的形式传递给容器, 而并非直接传递给容器. 命令行参数的定义中可以通过 $(ENV_VAR) 语法引用环境变量, 因而可以达到将 ConfigMap 的条目当作命令行参数传递给进程的效果.</strong></p> <p><img src="/img/image-20240227233655-xv9trf9.png" alt="image" title="图7.2 pod 通过环境变量与 ConfigMap 卷使用 ConfigMap"></p> <p>当然, 应用程序同样可以通过 <strong>Kubernetes Rest API</strong> 按需直接读取 ConfigMap 的内容. 不过除非是需求如此, 应<strong>尽可能使应用保持对 Kubernetes 的无感知</strong>.</p> <p>不管应用具体是如何使用 ConfigMap 的, 将配置存放在独立的资源对象中有助于在不同环境(开发, 测试, 质量保障和生产等)下拥有<strong>多份同名配置清单</strong>. pod 是通过名称引用 ConfigMap 的, 因此可以在多环境下使用相同的 pod 定义描述, 同时保持不同的配置值以适应不同环境(如图 7.3 所示).</p> <p><img src="/img/image-20240227233723-fi0evc7.png" alt="image" title="图7.3 不同环境下的同名 ConfigMap"></p> <h6 id="_7-4-2-创建configmap"><a href="#_7-4-2-创建configmap" class="header-anchor">#</a> 7.4.2 创建ConfigMap</h6> <p>了解一下如何在 pod 中使用 ConfigMap. 首先从最简单的例子开始, 先创建一个仅包含单一键的映射, 并用它填充之前示例中的环境变量 INTERVAL. 这里将使用<strong>指令 kubectl create configmap 创建 ConfigMap</strong>, 而非通用指令 <code>kubectl create -f</code>​.</p> <blockquote><p>使用指令 kubectl 创建 ConfigMap</p></blockquote> <p>利用 kubectl 创建 ConfigMap 的映射条目时可以<strong>指定字面量或者存储在磁盘上的文件</strong>. 先创建一个简单的字面量条目:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create configmap fortune-config --from-literal<span class="token operator">=</span>sleep-interval<span class="token operator">=</span><span class="token number">25</span>
configmap <span class="token string">&quot;fortune-config&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: ConfigMap 中的键名必须是一个合法的 DNS 子域, 仅包含数字字母, 破折号, 下画线以及圆点. 首位的圆点符号是可选的.</p> <p>通过这条命令创建了一个叫作 fortune-config 的 ConfigMap, 仅包含单映射条目 sleep-interval=25(如图7.4所示).</p> <p><img src="/img/image-20240223093348-ct7pq0l.png" alt="image" title="图7.4 ConfigMap fortune-config 包含单映射条目"></p> <p>ConfigMap 一般包含多个映射条目. 通过添加多个  <strong>--from-literal</strong> 参数可创建包含多条目的 ConfigMap:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create configmap myconfigmap --from-literal<span class="token operator">=</span>foo<span class="token operator">=</span>bar --from-literal<span class="token operator">=</span>bar<span class="token operator">=</span>baz --from-literal<span class="token operator">=</span>one<span class="token operator">=</span>two
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>观察一下通过 kubectl 创建的 ConfigMap 的 YAML 格式的定义描述, 如下所示.</p> <p><strong>代码清单-7.8 ConfigMap 定义</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get configmap fortune-config <span class="token parameter variable">-o</span> yaml
apiVersion: v1
data:
    sleep-interval: <span class="token string">&quot;25&quot;</span>    <span class="token comment"># 映射中唯一的条目</span>
kind: ConfigMap             <span class="token comment"># 描述符定义了一个ConfigMap          </span>
metadata:
    creationTimestamp: <span class="token number">2016</span>-08-11T20:31:08Z
    name: fortune-config    <span class="token comment"># 映射的名称(通过这个名称可以引用ConfigMap)</span>
    namespace: default
    resourceVersion: <span class="token string">&quot;910025&quot;</span>
    selfLink: /api/v1/namespaces/default/configmaps/fortune-config
    uid: 88c4167e-6002-11e6-a50d-42010af00237
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>这没有什么特别的. 编写这个 YAML 文件很容易, 除了 metadata 中的名称无须指定其他字段, 然后通过 Kubernetes API 创建对应的 ConfigMap:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> fortune-config.yaml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><blockquote><p>从文件内容创建 ConfigMap 条目</p></blockquote> <p><strong>ConfigMap 同样可以存储粗粒度的配置数据, 比如完整的配置文件. kubectl create configmap 命令支持从磁盘上读取文件, 并将文件内容单独存储为 ConfigMap 中的条目:</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create configmap my-config --from-file<span class="token operator">=</span>config-file.conf
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>运行上述命令时, kubectl 会在当前目录下查找 <strong>config-file.conf 文件</strong>, 并将文件内容存储在 ConfigMap 中以 config-file.conf 为键名的条目下. 当然也可以手动指定键名:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create configmap my-config --from-file<span class="token operator">=</span>customkey<span class="token operator">=</span>config-file.conf
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这条命令会将文件内容存在键名为 customkey 的条目下. 与使用字面量时相同, 多次使用  <strong>--from-file</strong> 参数可增加多个文件条目.</p> <blockquote><p>从文件夹创建 ConfigMap</p></blockquote> <p>除单独引入每个文件外, 甚至可以引入<strong>某一文件夹中的所有文件</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create configmap my-config --from-file<span class="token operator">=</span>/path/to/dir
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这种情况下, kubectl 会为文件夹中的每个文件单独创建条目, 仅限于那些文件名可作为合法 ConfigMap 键名的文件.</p> <blockquote><p>合并不同选项</p></blockquote> <p>创建 ConfigMap 时可以<strong>混合使用这里提到的所有选项</strong>(注意这里的文件并未包含在本书的代码归档中--如果想要尝试这条命令需自行创建):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create configmap my-config
 --from-file<span class="token operator">=</span>foo.json            <span class="token comment"># 单独的文件</span>
 --from-file<span class="token operator">=</span>bar<span class="token operator">=</span>foobar.conf     <span class="token comment"># 自定义键名条目下的文件</span>
 --from-file<span class="token operator">=</span>config-opts/        <span class="token comment"># 完整的文件夹</span>
 --from-literal<span class="token operator">=</span>some<span class="token operator">=</span>thing       <span class="token comment"># 字面量</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这里的 ConfigMap 创建自多种选项: 完整文件夹, 单独文件, 自定义键名的条目下的文件(替代文件名作键名)以及字面量. 图 7.5 显示了所有源选项以及最终的 ConfigMap.</p> <p><img src="/img/image-20240223093838-gl4lfa5.png" alt="image" title="图7.5 从文件, 文件夹以及字面量创建 ConfigMap"></p> <h6 id="_7-4-3-给容器传递configmap条目作为环境变量"><a href="#_7-4-3-给容器传递configmap条目作为环境变量" class="header-anchor">#</a> 7.4.3 给容器传递ConfigMap条目作为环境变量</h6> <p>如何将映射中的值传递给 pod 的容器? 有三种方法. 首先尝试最为简单的一种--<strong>设置环境变量</strong>, 将会使用到 7.5.3 节中提到的 <strong>valueFrom 字段</strong>. pod 的定义描述如下面的代码清单所示.</p> <p><strong>代码清单-7.9 通过配置文件注入环境变量的 pod:fortune-pod-env-configmap.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>env<span class="token punctuation">-</span>from<span class="token punctuation">-</span>configmap
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/fortune<span class="token punctuation">:</span>env
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> INTERVAL    <span class="token comment"># 设置环境变量INTREVAL</span>
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>        <span class="token comment"># 用ConfigMap初始化, 不设定固定值</span>
        <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config    <span class="token comment"># 引用ConfigMap的名称</span>
          <span class="token key atrule">key</span><span class="token punctuation">:</span> sleep<span class="token punctuation">-</span>interval     <span class="token comment"># 环境变量被设置为ConfigMap下对应键的值</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>这里<strong>定义了一个环境变量 INTERVAL, 并将其值设置为 fortune-config ConfigMap 中键名为 sleep-interval 对应的值</strong>. 运行在 html-generator 容器中的进程读取到环境变量 INTERVAL 的值为 25(如图 7.6 所示).</p> <p><img src="/img/image-20240227233840-mvr7387.png" alt="image" title="图7.6 给容器的环境变量传递 ConfigMap 的条目"></p> <blockquote><p>在 pod 中引用不存在的 ConfigMap</p></blockquote> <p>你可能会好奇如果<strong>创建 pod 时引用的 ConfigMap 不存在会发生什么</strong>? Kubernetes 会正常调度 pod 并尝试运行所有的容器. 然而<mark><strong>引用不存在的 ConfigMap 的容器会启动失败, 其余容器能正常启动</strong></mark>. 如果之后创建了这个缺失的 ConfigMap, 失败容器会<strong>自动启动</strong>, 无须重新创建 pod.</p> <p>注意: 可以标记对 ConfigMap 的引用是可选的(设置 configMapKeyRef.optional: true). 这样, 即便 ConfigMap 不存在, 容器也能正常启动.</p> <p>这个例子展示了如何将配置从 pod 定义中分离. 这样能使所有的配置项较为集中(甚至多个 pod 也是如此), 而不是分散在各处(或者冗余复制于多个 pod 定义清单).</p> <h6 id="_7-4-4-一次性传递configmap的所有条目作为环境变量"><a href="#_7-4-4-一次性传递configmap的所有条目作为环境变量" class="header-anchor">#</a> 7.4.4 一次性传递ConfigMap的所有条目作为环境变量</h6> <p>如果 ConfigMap 包含不少条目, 为每个条目单独设置环境变量的过程是单调乏味且容易出错的. 幸运的是, 1.6版本的 Kubernetes 提供了<strong>暴露 ConfigMap 的所有条目作为环境变量</strong>的手段.</p> <p>假设一个 ConfigMap 包含 FOO, BAR 和 FOO-BAR 三个键. 可以通过 envFrom 属性字段将所有条目暴露作为环境变量, 而非使用前面例子中的 env 字段. 示例代码如下所示.</p> <p><strong>代码清单-7.10 pod 包含来源于 ConfigMap 所有条目的环境变量</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> some<span class="token punctuation">-</span>image
    <span class="token key atrule">envFrom</span><span class="token punctuation">:</span>             <span class="token comment"># 使用envFrom字段而不是env字段</span>
    <span class="token punctuation">-</span> <span class="token key atrule">prefix</span><span class="token punctuation">:</span> CONFIG_    <span class="token comment"># 所有环境变量均包含前缀CONFIG_</span>
      <span class="token key atrule">configMapRef</span><span class="token punctuation">:</span>    
        <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>config<span class="token punctuation">-</span>map    <span class="token comment"># 引用名为my-config-map的ConfigMap</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>如你所见, 可以<strong>为所有的环境变量设置前缀</strong>, 如本例中的 CONFIG_, 容器中两个环境变量的名称为: <strong>CONFIG_FOO 与 CONFIG_BAR</strong>.</p> <p>注意: 前缀设置是可选的, 若不设置前缀值, 环境变量的名称与 ConfigMap 中的<strong>键名相同</strong>.</p> <p>是否注意到前面说的是两个环境变量, 然而 ConfigMap 拥有三个条目(FOO, BAR 和 FOO-BAR)? 为何没有对应 FOO-BAR 条目的环境变量呢?</p> <p>原因在于 CONFIG_FOO-BAR 包含破折号, 这并不是一个合法的环境变量名称. Kubernetes 不会主动转换键名(例如不会将破折号转换为下画线). <strong>如果 ConfigMap 的某键名格式不正确, 创建环境变量时会忽略对应的条目</strong>(忽略时不会发出事件通知).</p> <h6 id="_7-4-5-传递configmap条目作为命令行参数"><a href="#_7-4-5-传递configmap条目作为命令行参数" class="header-anchor">#</a> 7.4.5 传递ConfigMap条目作为命令行参数</h6> <p>现在来看一下<mark><strong>如何将 ConfigMap 中的值作为参数值传递给运行在容器中的主进程</strong></mark>. 在<strong>字段 pod.spec.containers.args 中无法直接引用 ConfigMap 的条目, 但是可以利用 ConfigMap 条目初始化某个环境变量, 然后再在参数字段中引用该环境变量</strong>, 具体如图 7.7 所示.</p> <p><img src="/img/image-20240227233903-9d335hy.png" alt="image" title="图7.7 传递 ConfigMap 的条目作为命令行参数"></p> <p>代码清单 7.11 展示了如何在 YAML 文件中做到这一点.</p> <p><strong>代码清单-7.11 使用 ConfigMap 条目作为参数值: fortune-pod-args-configmap.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>args<span class="token punctuation">-</span>from<span class="token punctuation">-</span>configmap
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/fortune<span class="token punctuation">:</span>args   <span class="token comment"># 使用从第一个参数读取间隔值的镜像, 而不是读取环境变量的镜像</span>
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> INTERVAL            <span class="token comment"># 与之前环境变量的定义相同</span>
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config
          <span class="token key atrule">key</span><span class="token punctuation">:</span> sleep<span class="token punctuation">-</span>interval
    <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;$(INTERVAL)&quot;</span><span class="token punctuation">]</span>    <span class="token comment"># 在参数设置中引用环境变量</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>环境变量的定义与之前相同, 需通过 <code>$(ENV_VARIABLE_NAME)</code>​ 将环境变量的值注入参数值.</p> <h6 id="_7-4-6-使用configmap卷将条目暴露为文件"><a href="#_7-4-6-使用configmap卷将条目暴露为文件" class="header-anchor">#</a> 7.4.6 使用configMap卷将条目暴露为文件</h6> <p>环境变量或者命令行参数值作为配置值通常适用于变量值较短的场景. 由于 ConfigMap 中可以包含完整的配置文件内容, 当想要将其暴露给容器时, 可以借助前面章节提到过的一种称为 <strong>configMap 卷</strong>的特殊卷格式.</p> <p><strong>configMap 卷会将 ConfigMap 中的每个条目均暴露成一个文件. 运行在容器中的进程可通过读取文件内容获得对应的条目值.</strong></p> <p>尽管这种方法主要适用于<strong>传递较大的配置文件</strong>给容器, 同样可以用于传递较短的变量值.</p> <blockquote><p>创建 ConfigMap</p></blockquote> <p>这里不再修改脚本 fortuneloop.sh, 将尝试另一个不同的示例, 使用配置文件配置运行在 fortune pod 的 Web 服务器容器中的 Nginx web 服务器. 如果想要让 Nginx 服务器压缩传递给客户端的响应, Nginx 的配置文件需开启压缩配置, 如下面的代码清单所示.</p> <p><strong>代码清单-7.12 开启 gzip 压缩的 Nginx 配置文件: my-nginx-config.conf</strong></p> <div class="language-nginx line-numbers-mode"><pre class="language-nginx"><code><span class="token directive"><span class="token keyword">server</span></span> <span class="token punctuation">{</span>
  <span class="token directive"><span class="token keyword">listen</span> <span class="token number">80</span></span><span class="token punctuation">;</span>
  <span class="token directive"><span class="token keyword">server_name</span> www.kubia-example.com</span><span class="token punctuation">;</span>
  
  <span class="token directive"><span class="token keyword">gzip</span> <span class="token boolean">on</span></span><span class="token punctuation">;</span>        <span class="token comment"># 开启对文本文件与XML文件的gzip压缩</span>
  <span class="token directive"><span class="token keyword">gzip_types</span> text/plain application/xml</span><span class="token punctuation">;</span>
  
  <span class="token directive"><span class="token keyword">location</span> /</span> <span class="token punctuation">{</span>
    <span class="token directive"><span class="token keyword">root</span> /usr/share/nginx/html</span><span class="token punctuation">;</span>
    <span class="token directive"><span class="token keyword">index</span> index.html index.htm</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>现在首先通过 kubectl delete configmap fortune-config 删除现有的 ConfigMap fortune-config, 然后<strong>用存储在本地磁盘上的 Nginx 配置文件创建一个新的 ConfigMap</strong>.</p> <p>创建一个新文件夹 confimap-files 并将上面的配置文件存储于 configmap-files/my-nginx-config.conf 中. 另外在该文件夹中<strong>添加一个名为 sleep-interval 的文本文件</strong>, 写入值为 25, 使 ConfigMap 同样包含条目 sleep-interval, 如图 7.8 所示.</p> <p><img src="/img/image-20240223214207-x2xcve7.png" alt="image" title="图7.8 configmap-files 文件夹及文件的内容"></p> <p>从文件夹创建 ConfigMap:</p> <div class="language-nginx line-numbers-mode"><pre class="language-nginx"><code>$ kubectl create configmap fortune-config --from-file=configmap-files
configmap &quot;fortune-config&quot; created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>下面的代码清单展示了 ConfigMap 的 YAML 格式内容.</p> <p><strong>代码清单-7.13 查看一下从文件创建的 ConfigMap 的 YAML 格式定义</strong></p> <div class="language-nginx line-numbers-mode"><pre class="language-nginx"><code>$ <span class="token directive"><span class="token keyword">kubectl</span> get configmap fortune-config -o yaml
apiVersion: v1
  data:
    my-nginx-config.conf: |     <span class="token comment"># 条目中包含了Nginx配置文件的内容</span>
    server</span> <span class="token punctuation">{</span>
      <span class="token directive"><span class="token keyword">listen</span> <span class="token number">80</span></span><span class="token punctuation">;</span>
      <span class="token directive"><span class="token keyword">server_name</span> www.kubia-example.com</span><span class="token punctuation">;</span>

      <span class="token directive"><span class="token keyword">gzip</span> <span class="token boolean">on</span></span><span class="token punctuation">;</span>
      <span class="token directive"><span class="token keyword">gzip_types</span> text/plain application/xml</span><span class="token punctuation">;</span>

      <span class="token directive"><span class="token keyword">location</span> /</span> <span class="token punctuation">{</span>
        <span class="token directive"><span class="token keyword">root</span> /usr/share/nginx/html</span><span class="token punctuation">;</span>
        <span class="token directive"><span class="token keyword">index</span> index.html index.htm</span><span class="token punctuation">;</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  sleep-interval: |        <span class="token comment"># 条目sleep-interval</span>
    25
kind: ConfigMap
...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>注意: 所有条目第一行最后的管道符号(|)表示后续的条目值是<strong>多行字面量</strong>.</p> <p><strong>ConfigMap 包含两个条目, 条目的键名与文件名相同</strong>. 接下来将在 pod 的容器中使用该 ConfigMap.</p> <blockquote><p>在卷内使用 ConfigMap 的条目</p></blockquote> <p><strong>创建包含 ConfigMap 条目内容的卷只需要创建一个引用 ConfigMap 名称的卷并挂载到容器中</strong>. 已经学会了如何创建及挂载卷, 接下来要学习的仅是如何用 ConfigMap 的条目初始化卷.</p> <p>Nginx 需读取配置文件 <code>/etc/nginx/nginx.conf</code>​, 而 Nginx 镜像内的这个文件包含默认配置, 并不想完全覆盖这个配置文件. 幸运的是, 默认配置文件会自动嵌入子文件夹 <code>/etc/nginx/conf.d/</code>​ 下的所有 .conf 文件, 因此只需要将配置文件置于该<strong>子文件夹</strong>中即可. 图 7.9 展示了如何做到这一点.</p> <p><img src="/img/image-20240227233932-bmfqtkt.png" alt="image" title="图7.9 ConfigMap 条目作为容器卷中的文件"></p> <p>pod 的定义描述如代码清单 7.14 所示(省略无关部分).</p> <p><strong>代码清单-7.14 pod 挂载 ConfigMap 条目作为文件: fortune-pod-configmap-volume.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>configmap<span class="token punctuation">-</span>volume
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>alpine
    <span class="token key atrule">name</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>server
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/nginx/conf.d    <span class="token comment"># 挂载configMap卷至这个位置</span>
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token punctuation">...</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">...</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config
    <span class="token key atrule">configMap</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config    <span class="token comment"># 卷定义引用fortune-config ConfigMap</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>pod 定义中包含了引用 fortune-config ConfigMap 的卷, 需要被挂载到文件夹 <code>/etc/nginx/conf.d</code>​ 下让 Nginx 服务器使用它.</p> <blockquote><p>检查 Nginx 是否使用被挂载的配置文件</p></blockquote> <p>现在的 web 服务器应该已经被配置为会压缩响应, 可以将 localhost:8080 转发到 pod 的 80 端口, 利用 curl 检查服务器响应来验证配置是否生效, 如下面的代码清单所示.</p> <p><strong>代码清单-7.15 观察 nginx 响应是否被压缩</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl port<span class="token punctuation">-</span>forward fortune<span class="token punctuation">-</span>configmap<span class="token punctuation">-</span>volume 8080<span class="token punctuation">:</span>80 &amp;
Forwarding from 127.0.0.1<span class="token punctuation">:</span>8080 <span class="token punctuation">-</span><span class="token punctuation">&gt;</span> 80
Forwarding from <span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>8080 <span class="token punctuation">-</span><span class="token punctuation">&gt;</span> 80
<span class="token key atrule">$ curl -H &quot;Accept-Encoding</span><span class="token punctuation">:</span> gzip&quot; <span class="token punctuation">-</span>I localhost<span class="token punctuation">:</span><span class="token number">8080</span>
HTTP/1.1 200 OK
<span class="token key atrule">Server</span><span class="token punctuation">:</span> nginx/1.11.1
<span class="token key atrule">Date</span><span class="token punctuation">:</span> Thu<span class="token punctuation">,</span> 18 Aug 2016 11<span class="token punctuation">:</span>52<span class="token punctuation">:</span>57 GMT
<span class="token key atrule">Content-Type</span><span class="token punctuation">:</span> text/html
<span class="token key atrule">Last-Modified</span><span class="token punctuation">:</span> Thu<span class="token punctuation">,</span> 18 Aug 2016 11<span class="token punctuation">:</span>52<span class="token punctuation">:</span>55 GMT
<span class="token key atrule">Connection</span><span class="token punctuation">:</span> keep<span class="token punctuation">-</span>alive
<span class="token key atrule">ETag</span><span class="token punctuation">:</span> W/&quot;57b5a197<span class="token punctuation">-</span>37&quot;
<span class="token key atrule">Content-Encoding</span><span class="token punctuation">:</span> gzip    <span class="token comment"># 这里说明响应被压缩</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><blockquote><p>检查被挂载的 configMap 卷的内容</p></blockquote> <p>服务器响应说明配置成功生效. 现在来看一下文件夹 <code>/etc/nginx/conf.d</code>​ 下的内容:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> fortune-configmap-volume <span class="token parameter variable">-c</span> web-server <span class="token function">ls</span> /etc/nginx/conf.d
my-nginx-config.conf
sleep-interval
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>ConfigMap 的两个条目均作为文件置于这一文件夹下</strong>. 条目 sleep-interval 对应的文件也被包含在内, 然而它只会被 fortuneloop 容器所使用. 可以创建两个不同的 ConfigMap, 一个用以配置容器 fortuneloop, 另一个用来配置 webserver, 然而采用多个 ConfigMap 去分别配置同一 pod 中的不同容器的做法是不好的. 毕竟同一 pod 中的容器是紧密联系的, 需要被当作整体单元来配置.</p> <blockquote><p>卷内暴露指定的 ConfigMap 条目</p></blockquote> <p>幸运的是, 可以创建仅包含 ConfigMap 中<strong>部分条目的 configMap 卷</strong>---本示例中的条目 my-nginx-config.conf. 这样容器 fortuneloop 不会受到影响, 条目 sleep-interval 会作为环境变量传递给容器而不是以卷的方式.</p> <p>通过<strong>卷的 items 属性能够指定哪些条目会被暴露作为 configMap 卷中的文件</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-7.16 ConfigMap 的指定条目挂载至 pod 的文件夹: fortune-pod-configmap-volume-with-itmes.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">volumes</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config
  <span class="token key atrule">configMap</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config
    <span class="token key atrule">items</span><span class="token punctuation">:</span>               <span class="token comment"># 选择包含在卷中的条目</span>
    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>nginx<span class="token punctuation">-</span>config.conf    <span class="token comment"># 该键对应的条目被包含</span>
      <span class="token key atrule">path</span><span class="token punctuation">:</span> gzip.conf    <span class="token comment"># 条目的值被存储在该文件中</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>指定单个条目时需同时设置条目的键名称以及对应的文件名. 如果采用上面的配置文件创建 pod, <code>/etc/nginx/conf.d</code>​ 文件夹是比较干净的, 仅包含所需的 gzip.conf 文件.</p> <blockquote><p>挂载某一文件夹会隐藏该文件夹中已存在的文件</p></blockquote> <p>这里有一件重要的事情需要讨论. 在当前与此前的示例中, 将卷挂载至某个文件夹, 意味着容器镜像中 <code>/etc/nginx/conf.d</code>​ 文件夹下原本存在的任何文件都会被隐藏.</p> <p><strong>Linux 系统挂载文件系统至非空文件夹时通常表现如此.</strong>  文件夹中只会包含被挂载文件系统中的文件, 即便文件夹中原本的文件是不可访问的也是同样如此.</p> <p>本示例中, 这种现象并不会带来比较糟糕的副作用. 不过假设挂载文件夹是 <code>/etc</code>​, 该文件夹通常包含不少重要文件. 由于 /etc 下的所有文件不存在, 容器极大可能会损坏. <strong>如果你希望添加文件至某个文件夹如 /etc, 绝不能采用这种方法</strong>.</p> <blockquote><p>ConfigMap独立条目作为文件被挂载且不隐藏文件夹中的其他文件</p></blockquote> <p>顺理成章, 你会好奇<strong>如何能挂载 ConfigMap 对应文件至现有文件夹的同时不会隐藏现有文件</strong>. volumeMount 额外的 <strong>subPath 字段</strong>可以被用作挂载卷中的某个独立文件或者是文件夹, 无须挂载完整卷. 图 7.10 的形象化解释可能更加容易理解.</p> <p>假设拥有一个包含文件 myconfig.conf 的 configMap 卷, 希望能将其添加为 /etc 文件夹下的文件 someconfig.conf. 通过属性 subPath 可以将该文件挂载的同时又不影响文件夹中的其他文件. pod 定义中的相关部分如下面的代码清单所示.</p> <p><img src="/img/image-20240227234015-1f4htia.png" alt="image" title="图7.10 挂载卷中的单独文件"></p> <p><strong>代码清单-7.17 pod 挂载 ConfigMap 的指定条目至特定文件</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> some/image
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myvolume
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/someconfig.conf    <span class="token comment"># 挂载至某一文件, 而不是文件夹</span>
      <span class="token key atrule">subPath</span><span class="token punctuation">:</span> myconfig.conf    <span class="token comment"># 仅挂载指定的条目myconfig.conf, 并非完整的卷</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>挂载任意一种卷时均可以使用 subPath 属性. 可以选择挂载部分卷而不是挂载完整的卷. 不过这种独立文件的挂载方式会带来文件更新上的缺陷, 你会在接下来的小节中学习到更多的相关知识, 在这里还是先要说一些文件权限问题对 configMap 卷的讨论进行收尾.</p> <blockquote><p>为 configMap 卷中的文件设置权限</p></blockquote> <p><strong>configMap 卷中所有文件的权限默认被设置为644(-rw-r-r--)</strong> . 可以通过卷规格定义中的 defaultMode 属性改变默认权限, 如下面的代码清单所示.</p> <p><strong>代码清单-7.18 设置权限: fortune-pod-configmap-volume-defaultMode.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">volumes</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config
  <span class="token key atrule">configMap</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config
    <span class="token key atrule">defaultMode</span><span class="token punctuation">:</span> <span class="token string">&quot;6600&quot;</span>    <span class="token comment"># 设置所有文件的权限为-rw-rw------</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>ConfigMap 通常被用作存储非敏感数据, 不过依旧可能希望仅限于文件拥有者的用户和组可读写, 正如上面的例子所示.</p> <h6 id="_7-4-7-更新应用配置且不重启应用程序"><a href="#_7-4-7-更新应用配置且不重启应用程序" class="header-anchor">#</a> 7.4.7 更新应用配置且不重启应用程序</h6> <p>在此之前提到过, 使用环境变量或者命令行参数作为配置源的弊端在于无法在进程运行时更新配置. <mark><strong>将 ConfigMap 暴露为卷可以达到配置热更新的效果, 无须重新创建 pod 或者重启容器</strong></mark>.</p> <p><strong>ConfigMap 被更新之后, 卷中引用它的所有文件也会相应更新, 进程发现文件被改变之后进行重载. Kubernetes 同样支持文件更新之后手动通知容器</strong>.</p> <p>警告: 请注意笔者在写这段的时候, 更新 ConfigMap 之后对应文件的更新耗时会出人意料地长(往往需要数分钟).</p> <blockquote><p>修改 ConfigMap</p></blockquote> <p>现在来瞧一瞧<strong>如何修改 ConfigMap, 同时运行在 pod 中的进程会重载 configMap 卷中对应的文件</strong>. 你需要修改前面示例中的 Nginx 配置文件, 使得 Nginx 能够在不重启 pod 的前提下应用新配置. 尝试用 kubectl edit 命令修改 ConfigMap fortune-config 来关闭 gzip 压缩:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit configmap fortune-config
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>编辑器打开, 行 gzip on 改为 gzip off, 保存文件后关闭编辑器. <strong>ConfigMap 被更新不久之后会自动更新卷中的对应文件</strong>. 用 kubectl exec 命令打印出该文件内容进行确认:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> fortune-configmap-volume <span class="token parameter variable">-c</span> web-server <span class="token function">cat</span> /etc/nginx/conf.d/my-nginx-config.conf
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>若尚未看到文件内容被更新, 可稍等一会儿后重试. <strong>文件更新过程需要一段时间. 最终你会看到配置文件的变化</strong>, 然而发现这对 Nginx 并没有什么影响, 这是因为 Nginx 不会去监听文件的变化并自动重载.</p> <blockquote><p>通知 Nginx 重载配置</p></blockquote> <p>Nginx 会持续压缩响应直到你通过以下命令<strong>主动通知它</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> fortune-configmap-volume <span class="token parameter variable">-c</span> web-server -- nginx <span class="token parameter variable">-s</span> reload
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在再次用 curl 命令访问服务器后会发现响应不再被压缩(响应头中未包含 Content-Encoding: gzip). 在无须重启容器或者重建 pod 的同时有效修改了应用配置.</p> <blockquote><p>了解文件被自动更新的过程</p></blockquote> <p>你可能会疑惑在 Kubernetes 更新完 configMap 卷中的所有文件之前, <strong>应用是否会监听到文件变化并主动进行重载</strong>. 幸运的是, 这不会发生, <strong>所有的文件会被自动一次性更新. Kubernetes 通过符号链接做到这一点</strong>. 如果尝试列出 configMap 卷挂载位置的所有文件, 会看到如下内容.</p> <p><strong>代码清单-7.19 被挂载的 configMap 卷中的文件</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> fortune-configmap-volume <span class="token parameter variable">-c</span> web-server -- <span class="token function">ls</span> <span class="token parameter variable">-lA</span> /etc/nginx/conf.d
total <span class="token number">4</span>
drwxr-xr-x <span class="token punctuation">..</span>. <span class="token number">12</span>:15 <span class="token punctuation">..</span>4984_09_04_12_15_06.865837643
lrwxrwxrwx <span class="token punctuation">..</span>. <span class="token number">12</span>:15 <span class="token punctuation">..</span>data -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>4984_09_04_12_15_06.865837643
lrwxrwxrwx <span class="token punctuation">..</span>. <span class="token number">12</span>:15 my-nginx-config.conf -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>data/my-nginx-config.conf
lrwxrwxrwx <span class="token punctuation">..</span>. <span class="token number">12</span>:15 sleep-interval -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>data/sleep-interval
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, 被挂载的 configMap 卷中的文件是 ..data 文件夹中文件的<strong>符号链接</strong>, 而 ..data 文件夹同样是 ..4984_09_04_something 的符号链接. <strong>每当 ConfigMap 被更新后, Kubernetes 会创建一个这样的文件夹, 写入所有文件并重新将符号 ..data 链接至新文件夹, 通过这种方式可以一次性修改所有文件</strong>.</p> <blockquote><p>挂载至已存在文件夹的文件不会被更新</p></blockquote> <p>涉及到更新 configMap 卷需要提出一个警告: 如果挂载的是容器中的单个文件而不是完整的卷, ConfigMap 更新之后对应的文件不会被更新! 至少在写本章节的时候表现如此.</p> <p>如果现在需要挂载单个文件并且在修改源 ConfigMap 的同时会自动修改这个文件, 一种方案是挂载完整卷至不同的文件夹并创建指向所需文件的符号链接. 符号链接可以原生创建在容器镜像中, 也可以在容器启动时创建.</p> <blockquote><p>了解更新 ConfigMap 的影响</p></blockquote> <p><strong>容器的一个比较重要的特性是其不变性, 从同一镜像启动的多个容器之间不存在任何差异</strong>. 那么通过修改被运行容器所使用的 ConfigMap 来打破这种不变性的行为是否是错误的?</p> <p>关键点在于应用是否支持重载配置. ConfigMap 更新之后创建的 pod 会使用新配置, 而之前的 pod 依旧使用旧配置, 这会导致运行中的不同实例的配置不同. 这也不仅限于新 pod, 如果 pod 中的容器因为某种原因重启了, 新进程同样会使用新配置. 因此, <strong>如果应用不支持主动重载配置, 那么修改某些运行 pod 所使用的 ConfigMap 并不是一个好主意</strong>.</p> <p>如果应用支持主动重载配置, 那么修改 ConfigMap 的行为就算不了什么. 不过有一点仍需注意, 由于 configMap 卷中文件的更新行为对于所有运行中示例而言<strong>不是同步的</strong>, 因此不同 pod 中的文件可能会在长达一分钟的时间内出现不一致的情况.</p> <h5 id="_7-5-使用secret给容器传递敏感数据"><a href="#_7-5-使用secret给容器传递敏感数据" class="header-anchor">#</a> 7.5 使用Secret给容器传递敏感数据</h5> <p>到目前为止传递给容器的所有信息都是比较常规的非敏感数据. 然而正如本章开头提到的, <mark><strong>配置通常会包含一些敏感数据, 如证书和私钥, 需要确保其安全性</strong></mark>.</p> <h6 id="_7-5-1-介绍secret"><a href="#_7-5-1-介绍secret" class="header-anchor">#</a> 7.5.1 介绍Secret</h6> <p>为了存储与分发此类信息, <strong>Kubernetes 提供了一种称为 Secret 的单独资源对象. Secret 结构与 ConfigMap 类似, 均是键/值对的映射</strong>. Secret 的使用方法也与 ConfigMap 相同, 可以</p> <ul><li><strong>将 Secret 条目作为环境变量传递给容器</strong></li> <li><strong>将 Secret 条目暴露为卷中的文件</strong></li></ul> <p><mark><strong>Kubernetes 通过仅仅将 Secret 分发到需要访问 Secret 的 pod 所在的机器节点来保障其安全性. 另外, Secret 只会存储在节点的内存中, 永不写入物理存储, 这样从节点上删除 Secret 时就不需要擦除磁盘了.</strong></mark></p> <p>对于主节点本身(尤其是 etcd), Secret 通常以非加密形式存储, 这就需要保障主节点的安全从而确保存储在 Secret 中的敏感数据的安全性. 这种保障不仅仅是对 etcd 存储的安全性保障, 同样包括防止未授权用户对 API 服务器的访问, 这是因为任何人都能<strong>通过创建 pod 并将 Secret 挂载来获得此类敏感数据</strong>. 从 Kubernetes 1.7开始, etcd 会以加密形式存储 Secret, 某种程度提高了系统的安全性. 正因为如此, 从 Secret 与 ConfigMap 中做出正确选择是势在必行的, 选择依据相对简单:</p> <ul><li><strong>采用 ConfigMap 存储非敏感的文本配置数据</strong>.</li> <li><strong>采用 Secret 存储天生敏感的数据, 通过键来引用</strong>. 如果一个配置文件同时包含敏感与非敏感数据, 该文件应该被存储在 Secret 中.</li></ul> <p>第 5 章中已经使用过 Secret 以存储 Ingress 资源的 TLS 证书. 接下来将更深入地探讨 Secret 的细节.</p> <h6 id="_7-5-2-默认令牌secret介绍"><a href="#_7-5-2-默认令牌secret介绍" class="header-anchor">#</a> 7.5.2 默认令牌Secret介绍</h6> <p>首先来分析一种<strong>默认被挂载至所有容器的 Secret</strong>, 对任意一个 pod 使用命令 kubectl describe pod, 输出往往包含如下信息:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>Volumes:
  default-token-cfee9:
    Type: Secret <span class="token punctuation">(</span>a volume populated by a Secret<span class="token punctuation">)</span>
    SecretName: default-token-cfee9
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>每个 pod 都会被自动挂载上一个 secret 卷, 这个卷引用的是前面 kubectl describe 输出中的一个叫作 default-token-cfee9 的 Secret. 由于 Secret 也是<strong>资源对象</strong>, 因此可以通过 kubectl get secrets 命令从 Secret 列表中找到这个 default-token Secret:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get secrets
NAME                     TYPE                                 DATA     AGE
default-token-cfee9      kubernetes.io/service-account-token  <span class="token number">3</span>        39d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>同样可以使用 kubectl describe 多了解一下这个 Secret, 如下面的代码清单所示.</p> <p><strong>代码清单-7.20 描述一个 Secret</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe secrets
Name: default-token-cfee9
Namespace: default
Labels: <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
Annotations: kubernetes.io/service-account.name<span class="token operator">=</span>default
kubernetes.io/service-account.uid<span class="token operator">=</span>cc04bb39-b53f-42010af00237
Type: kubernetes.io/service-account-token
Data
<span class="token operator">&lt;</span>mark<span class="token operator">&gt;</span><span class="token operator">&lt;</span>/mark<span class="token operator">&gt;</span>
ca.crt: <span class="token number">1139</span> bytes
namespace: <span class="token number">7</span> bytes
token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9<span class="token punctuation">..</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>可以看出<strong>这个 Secret 包含三个条目——ca.crt, namespace 与 token, 包含了从 pod 内部安全访问 Kubernetes API 服务器所需的全部信息</strong>. 尽管你希望做到应用程序对 Kubernetes 的完全无感知, 然而在除了直连 Kubernetes 别无他法的情况下, 你将会使用到 secret 卷提供的文件.</p> <p>kubectl describe pod 命令会显示 secret 卷被挂载的位置:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>Mounts:
    /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: default-token Secret 默认会被<strong>挂载至每个容器</strong>. 可以通过设置 pod 定义中的 automountServiceAccountToken 字段为 false, 或者设置 pod 使用的服务账户中的相同字段为 false 来关闭这种默认行为(本书后面会对服务账户进行讲解).</p> <p>图 7.11 能够帮助你更形象地理解默认令牌 Secret 的挂载行为.</p> <p><img src="/img/image-20240227234044-ro6g0sh.png" alt="image" title="图7.11 default-tokenSecret 被自动创建且对应的卷被自动挂载到每个 pod 上"></p> <p>前面说过 Secret 类似于 ConfigMap, 由于该 Secret 包含三个条目, 可通过 kubectl exec 观察到被 secret 卷挂载的文件夹下<strong>包含三个文件</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> mypod <span class="token function">ls</span> /var/run/secrets/kubernetes.io/serviceaccount/
ca.crt
namespace
token
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>下一章中将会看到应用程序是<strong>如何使用这些文件来访问 API 服务器</strong>的.</p> <h6 id="_7-5-3-创建secret"><a href="#_7-5-3-创建secret" class="header-anchor">#</a> 7.5.3 创建Secret</h6> <p>现在你将创建自己地小型 Secret. 改进 fortune-serving 的 Nginx 容器的配置, 使其能够服务于 <strong>HTTPS</strong> 流量. 你需要创建私钥和证书, 由于<strong>需要确保私钥的安全性, 可将其与证书同时存入 Secret</strong>.</p> <p>首先在本地机器上生成证书与私钥文件, 当然也可以直接使用本书代码归档中的相应文件(fortune-https 文件夹下的证书与密钥文件):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ openssl genrsa <span class="token parameter variable">-out</span> https.key <span class="token number">2048</span>
$ openssl req <span class="token parameter variable">-new</span> <span class="token parameter variable">-x509</span> <span class="token parameter variable">-key</span> https.key <span class="token parameter variable">-out</span> https.cert <span class="token parameter variable">-days</span> <span class="token number">3650</span> <span class="token parameter variable">-subj</span>
/CN<span class="token operator">=</span>www.kubia-example.com
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>现在为了更好地理解 Secret, 额外创建一个内容为字符串 bar 的虚拟文件 foo. 过会儿你就会理解为何要这样做:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token builtin class-name">echo</span> bar <span class="token operator">&gt;</span> foo
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在<strong>使用 kubectl create secret 命令由这三个文件创建 Secret</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create secret generic fortune-https --from-file<span class="token operator">=</span>https.key --from-file<span class="token operator">=</span>https.cert --from-file<span class="token operator">=</span>foo
secret <span class="token string">&quot;fortune-https&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>与创建 ConfigMap 的过程类似, 这里创建了一个名为 fortune-https 的 generic Secret, 它包含有两个条目: https.key 和 https.cert, 分别对应于两个同名文件的内容. 如前所述, 同样可以用 --from-file=fortune-https 囊括整个文件夹中的<strong>所有文件</strong>, 替代单独指定每个文件的创建方式.</p> <p>注意: 这里创建了一个 generic Secret, 在此之前你可能在第 5 章通过 kubectl create secret tls 创建过一个 tls Secret. 两种方式创建的 Secret 的条目名称不同.</p> <h6 id="_7-5-4-对比configmap与secret"><a href="#_7-5-4-对比configmap与secret" class="header-anchor">#</a> 7.5.4 对比ConfigMap与Secret</h6> <p>Secret 与 ConfigMap 仍有比较大的差别, 这也是为何 Kubernetes 开发者们在支持了 Secret 一段时间之后仍会选择创建 ConfigMap. 创建的 Secret 的 YAML 格式定义如下面的代码清单所示.</p> <p><strong>代码清单-7.21 Secret 的 YAML 格式定义</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get secret fortune-https <span class="token parameter variable">-o</span> yaml
apiVersion: v1
data:
    foo: <span class="token assign-left variable">YmFyCg</span><span class="token operator">==</span>
    https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ<span class="token punctuation">..</span>.
    https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE<span class="token punctuation">..</span>.
kind: Secret
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>将其与之前创建的 ConfigMap 的 YAML 格式定义做对比:</p> <p><strong>代码清单-7.22 Config 的 YAML 格式定义</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get configmap fortune-config <span class="token parameter variable">-o</span> yaml
apiVersion: v1
data:
    my-nginx-config.conf: <span class="token operator">|</span>
        server <span class="token punctuation">{</span>
            <span class="token punctuation">..</span>.
        <span class="token punctuation">}</span>
    sleep-interval: <span class="token operator">|</span>
        <span class="token number">25</span>
kind: ConfigMap
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>注意到两者的区别了吗? <strong>Secret 条目的内容会被以 Base64格式编码, 而 ConfigMap 直接以纯文本展示</strong>. 这种区别导致在处理 YAML 和 JSON 格式的 Secret 时会稍许有些麻烦, 需要在设置和读取相关条目时对内容进行编解码.</p> <blockquote><p>为二进制数据创建Secret</p></blockquote> <p>采用 Base64 编码的原因很简单. Secret 的条目可以涵盖二进制数据, 而不仅仅是纯文本. Base64 编码可以将二进制数据转换为纯文本, 以 YAML 或 JSON 格式展示.</p> <p>提示: Secret 甚至可以被用来存储非敏感二进制数据. 不过值得注意的是, Secret 的大小限于 1MB.</p> <blockquote><p>stringData字段介绍</p></blockquote> <p>由于并非所有的敏感数据都是二进制形式, Kubernetes 允许<strong>通过 Secret 的 stringData 字段设置条目的纯文本值</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-7.23 通过 stringData 字段向 Secret 添加纯文本条目值</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">stringData</span><span class="token punctuation">:</span>
  <span class="token key atrule">foo</span><span class="token punctuation">:</span> plain text
<span class="token key atrule">data</span><span class="token punctuation">:</span>
  <span class="token key atrule">https.cert</span><span class="token punctuation">:</span> LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ<span class="token punctuation">...</span>
  <span class="token key atrule">https.key</span><span class="token punctuation">:</span> LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>stringData 字段是只写的(注意: 是只写, 非只读), 可以被用来设置条目值. 通过 <code>kubectl get -o yaml</code>​ 获取 Secret 的 YAML 格式定义时, 不会显示 stringData 字段. 相反, stringData 字段中的所有条目(如上面示例中的 foo 条目)会被 Base64 编码之后展示在 data 字段下.</p> <blockquote><p>在 pod 中读取 Secret 条目</p></blockquote> <p>通过 secret 卷将 Secret 暴露给容器之后, Secret 条目的值会被解码并以真实形式(纯文本或二进制)写入对应的文件. 通过环境变量暴露 Secret 条目亦是如此. 在这两种情况下, <strong>应用程序均无须主动解码, 可直接读取文件内容或者查找环境变量</strong>.</p> <h6 id="_7-5-5-在pod中使用secret"><a href="#_7-5-5-在pod中使用secret" class="header-anchor">#</a> 7.5.5 在pod中使用Secret</h6> <p><strong>fortune-https Secret 已经包含了证书与密钥文件</strong>, 接下来需要做的是<strong>配置 Nginx 服务器去使用它们</strong>.</p> <blockquote><p>修改 fortune-config ConfigMap 以开启 HTTPS</p></blockquote> <p>为了开启 HTTPS, 需要再次修改这个 ConfigMap 对应的配置条目:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit configmap fortune-config
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>文本编辑器打开后, 修改条目 my-nginx-config.con 的内容, 如下面的代码清单所示.</p> <p><strong>代码清单-7.24 修改 fortune-config ConfigMap 的数据</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
data:
  my-nginx-config.conf: <span class="token operator">|</span>
    server <span class="token punctuation">{</span>
      listen <span class="token number">80</span><span class="token punctuation">;</span>
      listen <span class="token number">443</span> ssl<span class="token punctuation">;</span>
      server_name www.kubia-example.com<span class="token punctuation">;</span>
      ssl_certificate certs/https.cert<span class="token punctuation">;</span>        <span class="token comment"># 证书</span>
      ssl_certificate_key certs/https.key<span class="token punctuation">;</span>  
      ssl_protocols TLSv1 TLSv1.1 TLSv1.2<span class="token punctuation">;</span>
      ssl_ciphers HIGH:<span class="token operator">!</span>aNULL:<span class="token operator">!</span>MD5<span class="token punctuation">;</span>
  
      location / <span class="token punctuation">{</span>
        root /usr/share/nginx/html<span class="token punctuation">;</span>
        index index.html index.htm<span class="token punctuation">;</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  sleep-interval: <span class="token operator">|</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>上面配置了服务器从 <code>/etc/nginx/certs</code>​​ 中读取<strong>证书与密钥文件</strong>, 因此之后<strong>需要将 secret 卷挂载于此</strong>.</p> <blockquote><p>挂载 fortune-secret 至 pod</p></blockquote> <p>接下来需要创建一个新的 fortune-https pod, <strong>将含有证书与密钥的 secret 卷挂载至 pod 中的 web-server 容器</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-7.25 fortune-https pod 的 YAML 格式定义: fortune-pod-https.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>https
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/fortune<span class="token punctuation">:</span>env
    <span class="token key atrule">name</span><span class="token punctuation">:</span> html<span class="token punctuation">-</span>generator
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> INTERVAL
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config
          <span class="token key atrule">key</span><span class="token punctuation">:</span> sleep<span class="token punctuation">-</span>interval
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/htdocs
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>alpine
  <span class="token key atrule">name</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>server
  <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
    <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html
    <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/nginx/conf.d
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> certs    <span class="token comment"># 配置Nginx从/etc/nginx/certs中读取证书和秘钥文件, 需将secret卷挂载于此</span>
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/nginx/certs/  
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">443</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> html
      <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config
      <span class="token key atrule">configMap</span><span class="token punctuation">:</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>config
        <span class="token key atrule">items</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>nginx<span class="token punctuation">-</span>config.conf
          <span class="token key atrule">path</span><span class="token punctuation">:</span> https.conf
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> certs    <span class="token comment"># 这里引用fortune-https Secret来定义secret卷</span>
      <span class="token key atrule">secret</span><span class="token punctuation">:</span>
        <span class="token key atrule">secretName</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>https
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br></div></div><p>图 7.12 形象化地展示了上述 YAML 格式定义中的各组件及其相互关系. <strong>Secret default-token 以及卷, 卷挂载并不包含在这一定义中, 因为这些组件被自动加入 pod 定义</strong>, 图中不予展示.</p> <p><img src="/img/image-20240227234111-bmownc0.png" alt="image" title="图7.12 组合了 ConligMap 录密钥运行 tortune-heaps pod"></p> <p>注意: 与 configMap 卷相同, secret 卷同样支持通过 defaultModes 属性指定卷中文件的默认权限.</p> <blockquote><p>测试 Nginx 是否正使用 Secret 中的证书与密钥</p></blockquote> <p>pod 运行之后, 开启端口转发隧道将 HTTPS 流量转发至 pod 的 443 端口, 并用 curl 向服务器发送请求:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl port-forward fortune-https <span class="token number">8443</span>:443 <span class="token operator">&amp;</span>
Forwarding from <span class="token number">127.0</span>.0.1:8443 -<span class="token operator">&gt;</span> <span class="token number">443</span>
Forwarding from <span class="token punctuation">[</span>::1<span class="token punctuation">]</span>:8443 -<span class="token operator">&gt;</span> <span class="token number">443</span>
$ <span class="token function">curl</span> https://localhost:8443 <span class="token parameter variable">-k</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>若服务器配置正确, 会得到一个响应, <strong>检查响应中服务器证书是否与之前生成的证书匹配</strong>. curl 命令添加选项 -v 开启详细日志, 如下面的代码清单所示.</p> <p><strong>代码清单-7.26 显示 Nginx 发送的服务器证书</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> https://localhost:8443 <span class="token parameter variable">-k</span> <span class="token parameter variable">-v</span>
* About to connect<span class="token punctuation">(</span><span class="token punctuation">)</span> to localhost port <span class="token number">8443</span> <span class="token punctuation">(</span><span class="token comment">#0)</span>
* Trying ::1<span class="token punctuation">..</span>.
* Connected to localhost <span class="token punctuation">(</span>::1<span class="token punctuation">)</span> port <span class="token number">8443</span> <span class="token punctuation">(</span><span class="token comment">#0)</span>
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
* subject: <span class="token assign-left variable">CN</span><span class="token operator">=</span>www.kubia-example.com    <span class="token comment"># 证书与之前创建并存储于Secret中的证书匹配</span>
* start date: aug <span class="token number">16</span> <span class="token number">18</span>:43:13 <span class="token number">2016</span> GMT
* expire date: aug <span class="token number">14</span> <span class="token number">18</span>:43:13 <span class="token number">2026</span> GMT
* common name: www.kubia-example.com
* issuer: <span class="token assign-left variable">CN</span><span class="token operator">=</span>www.kubia-example.com
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><blockquote><p>Secret卷存储于内存</p></blockquote> <p>通过挂载 secret 卷至文件夹 <code>/etc/nginx/certs</code>​ 将证书与私钥成功传递给容器. secret 卷采用内存文件系统列出容器的挂载点, 如下面的代码清单所示.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> fortune-https <span class="token parameter variable">-c</span> web-server -- <span class="token function">mount</span> <span class="token operator">|</span> <span class="token function">grep</span> certs
tmpfs on /etc/nginx/certs <span class="token builtin class-name">type</span> tmp
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>由于使用的是 tmpfs, <strong>存储在 Secret 中的数据不会写入磁盘</strong>, 这样就无法被窃取.</p> <blockquote><p>通过环境变量暴露 Secret 条目</p></blockquote> <p>除卷之外, Secret 的独立条目可作为<strong>环境变量</strong>被暴露, 就像 ConfigMap 中 sleep-interval 条目做的那样. 举个例子, 若想将 Secret 中的键 foo 暴露为环境变量 FOO_SECRET, 需要在容器定义中添加如下片段.</p> <p><strong>代码清单-7.27 Secret 条目暴露为环境变量</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">env</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FOO_SECRET    <span class="token comment"># 通过Secret条目设置环境变量</span>
  <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
    <span class="token key atrule">secretKeyRef</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> fortune<span class="token punctuation">-</span>https    <span class="token comment"># Secret的键</span>
      <span class="token key atrule">key</span><span class="token punctuation">:</span> foo               <span class="token comment"># Secret的名称</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>上面片段与设置 INTERVAL 环境变量的基本一致, 除了这里是<strong>使用 secretKeyRef 字段来引用 Secret, 而非 configMapKeyRef, 后者用以引用 ConfigMap</strong>.</p> <p><mark><strong>Kubernetes 允许通过环境变量暴露 Secret, 然而此特性的使用往往不是一个好主意</strong></mark>. 应用程序通常会在错误报告时转储环境变量, 或者是启动时打印在应用日志中, 无意中暴露了 Secret 信息. 另外, 子进程会继承父进程的所有环境变量, 如果是通过第三方二进制程序启动应用, 你并不知道它使用敏感数据做了什么.</p> <p>提示: 由于敏感数据可能在无意中被暴露, 通过环境变量暴露 Secret 给容器之前请再三思考. 为了<mark><strong>确保安全性, 请始终采用 secret 卷的方式暴露 Secret</strong></mark>.</p> <blockquote><p>了解镜像拉取 Secret</p></blockquote> <p>你已经学会了如何传递 Secret 给应用程序并使用它们包含的数据. Kubernetes 自身在有些时候希望大家能够传递证书给它, 比如从某个私有镜像仓库拉取镜像时. 这一点同样需通过 Secret 来做到.</p> <p>到目前为止所使用的容器镜像均存储在公共仓库, 从上面拉取镜像时无须任何特殊的证书. 不过大部分组织机构不希望它们的镜像开放给所有人, 因此会使用私有镜像仓库. 部署一个 pod 时, 如果容器镜像位于私有仓库, Kubernetes 需拥有拉取镜像所需的证书. 来看一下该怎么做.</p> <blockquote><p>在 Docker Hub 上使用私有镜像仓库</p></blockquote> <p>Docker Hub 除了是一个公共镜像仓库, 还支持在上面创建私有仓库. 通过浏览器登录 https://hub.docker.com, 找到对应的镜像仓库, 勾选指定的复选框, 将仓库标记为私有.</p> <p>运行一个镜像来源于私有仓库的 pod 时, 需要做以下两件事:</p> <ul><li>创建包含 Docker 镜像仓库证书的 Secret.</li> <li>pod 定义中的 imagePullSecrets 字段引用该 Secret.</li></ul> <blockquote><p>创建用于 Docker 镜像仓库鉴权的 Secret</p></blockquote> <p>创建一个包含 Docker 镜像仓库鉴权证书的 Secret 与 7.5.3 节中创建 generic Secret 并没有什么不同. 同样使用 kubectl create secret 命令, 仅仅是类型与参数选项的不同:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create secret docker-registry mydockerhubsecret <span class="token punctuation">\</span>
--docker-username<span class="token operator">=</span>myusername --docker-password<span class="token operator">=</span>mypassword <span class="token punctuation">\</span>
--docker-email<span class="token operator">=</span>my.email@provider.com
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这里创建了一个 docker-registry 类型的 mydockerhubsecret Secret, 创建时需指定 Docker Hub 的用户名, 密码以及邮箱. 通过 kubectl describe 观察新建 Secret 的内容时会发现仅有一个条目 .dockercfg, 相当于用户主目录下的 .dockercfg 文件. 该文件通常在运行 docker login 命令时由 Docker 自动创建.</p> <blockquote><p>在 pod 定义中使用 docker-registry Secret</p></blockquote> <p>为了 Kubernetes 从私有镜像仓库拉取镜像时能够使用 Secret, 需要在 pod 定义中指定 docker-registry Secret 的名称, 如下面的代码清单所示.</p> <p><strong>代码清单-7.28 指定镜像拉取 Secret 的 pod 定义: pod-with-private-image.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> private<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">imagePullSecrets</span><span class="token punctuation">:</span>    <span class="token comment"># 能够从私有镜像仓库中拉取镜像</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mydockerhubsecret
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> username/private<span class="token punctuation">:</span>tag
    <span class="token key atrule">name</span><span class="token punctuation">:</span> main
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>上述 pod 定义中, 字段 imagePullSecrets 引用了 mydockerhubsecret Secret. 建议尝试一下这个特性, 因为很可能在不久之后就会与私有镜像打交道.</p> <blockquote><p>不需要为每个 pod 指定镜像拉取 Secret</p></blockquote> <p>假设某系统中通常运行大量 pod, 你可能会好奇是否需要为每个 pod 都添加相同的镜像拉取 Secret. 幸运的是, 情况并非如此. 第 12 章中将会学习到如何通过添加 Secret 至 ServiceAccount 使所有 pod 都能自动添加上镜像拉取 Secret.</p> <h5 id="_7-6-本章小结"><a href="#_7-6-本章小结" class="header-anchor">#</a> 7.6 本章小结</h5> <p>本章展示了如何向容器传递配置数据. 读完这一章, 你应该知道如何:</p> <ul><li>在 pod 定义中覆盖容器镜像定义的默认命令</li> <li>传递命令行参数给容器主进程</li> <li><strong>为容器设置环境变量</strong></li> <li><strong>将配置从 pod 定义中分离并放入 ConfigMap</strong></li> <li><strong>通过 Secret 存储敏感数据并安全分发至容器</strong></li> <li>创建 docker-registry Secret 用以从私有镜像仓库拉取镜像</li></ul> <p>下一章将会学习到<strong>如何传递 pod 和容器的元数据给运行于其中的应用程序</strong>, 会了解到本章的默认令牌 Secret 是如何在 pod 中被用来访问 API 服务器的.</p> <h4 id="_8-从应用访问pod元数据以及其他资源"><a href="#_8-从应用访问pod元数据以及其他资源" class="header-anchor">#</a> 8.从应用访问pod元数据以及其他资源</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>通过 Downward APl 传递信息到容器</li> <li>探索 Kubernetes REST APl</li> <li>把授权和服务器验证交给 kubectl proxy</li> <li>从容器内部访问 APl 服务器</li> <li>理解 ambassador 容器模式</li> <li>使用 Kubernetes 客户端库</li></ul> <p><strong>应用往往需要获取所运行环境的一些信息, 包括应用自身以及集群中其他组件的信息</strong>. 前面已经了解到 Kubernetes 如何通过环境变量以及 DNS 进行服务发现, 但其他信息如何处理呢? 本章将了解特定的 <strong>pod 和容器元数据如何被传递到容器, 了解在容器中运行的应用如何便捷地与 Kubernetes API 服务器进行交互, 从而获取在集群中部署资源的信息, 并且进一步了解如何创建和修改这些资源</strong>.</p> <h5 id="_8-1-通过downward-api传递元数据"><a href="#_8-1-通过downward-api传递元数据" class="header-anchor">#</a> 8.1 通过Downward API传递元数据</h5> <p>在之前的章节已经了解到如何通过环境变量或者 configMap 和 secret 卷向应用传递配置数据. 这对于 pod 调度, 运行前预设的数据是可行的. 但是对于那些<mark><strong>不能预先知道的数据</strong></mark>​ <strong>, 比如 pod 的 IP, 主机名或者是 pod 自身的名称(当名称被生成, 比如当 pod 通过 ReplicaSet 或类似的控制器生成时)呢</strong>? 此外, 对于那些已经在别处定义的数据, 比如 pod 的标签和注解呢? 我们不想在多个地方重复保留同样的数据.</p> <p>对于此类问题, 可以通过<strong>使用 Kubernetes Downward API 解决</strong>. Downward API 允许通过<strong>环境变量或者文件</strong>(在 downwardAPI 卷中)传递 pod 的元数据. 不要对这个名称产生困惑, Downward API 的方式并不像 REST endpoint 那样需要通过访问的方式获取数据. 这种方式主要是<strong>将在 pod 的定义和状态中取得的数据作为环境变量和文件的值</strong>, 如图 8.1 所示.</p> <p><img src="/img/image-20240227234140-yhjtyaf.png" alt="image" title="图8.1 Downward API 通过环境变量或者文件对外暴露 pod 元数据"></p> <h6 id="_8-1-1-了解可用的元数据"><a href="#_8-1-1-了解可用的元数据" class="header-anchor">#</a> 8.1.1 了解可用的元数据</h6> <p><strong>Downward API 可以给在 pod 中运行的进程暴露 pod 的元数据</strong>. 目前可以给容器传递以下数据:</p> <ul><li><strong>pod 的名称</strong></li> <li><strong>pod 的 IP</strong></li> <li><strong>pod 所在的命名空间</strong></li> <li><strong>pod 运行节点的名称</strong></li> <li><strong>pod 运行所归属的服务账户的名称</strong></li> <li><strong>每个容器请求的 CPU 和内存的使用量</strong></li> <li><strong>每个容器可以使用的 CPU 和内存的限制</strong></li> <li><strong>pod 的标签</strong></li> <li><strong>pod 的注解</strong></li></ul> <p>这个清单中所列举的大部分项目, 除了还没有讲到的服务账户, CPU 和内存的请求和限制概念, 其他都无须进一步解释. 后面将在第 12 章详细讲解服务账户. 现在只需了解, <strong>服务账户是 pod 访问 API 服务器时用来进行身份验证的账户</strong>. CPU 和内存的请求和限制将在第 14 章进行说明, 它们代表了分配给一个容器的 CPU 和内存的使用量, 以及一个容器可以分配的上限.</p> <p>列表中的大部分项目既可以<strong>通过环境变量也可以通过 downwardAPI 卷传递给容器, 但是标签和注解只可以通过卷暴露</strong>. 部分数据可以通过其他方式获取(例如, 可以直接从操作系统获取), 但是 <mark><strong>Downward API 提供了一种更加便捷的方式</strong></mark>.</p> <p>来看一个<strong>向容器化的进程传递元数据</strong>的例子.</p> <h6 id="_8-1-2-通过环境变量暴露元数据"><a href="#_8-1-2-通过环境变量暴露元数据" class="header-anchor">#</a> 8.1.2 通过环境变量暴露元数据</h6> <p>首先来了解如何通过环境变量的方式将 pod 和容器的元数据传递到容器中. 下面根据如下列出的配置创建一个简单的单容器.</p> <p><strong>代码清单-8.1 在环境变量中使用 downward API: downward-api-env.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> downward
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;9999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 15m
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 100Ki
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 4Mi
    <span class="token key atrule">env</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> POD_NAME
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                             
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>            <span class="token comment"># 引用pod manifest中的元数据名称字段, 而不是设定一个具体的值           </span>
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.name           
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> POD_NAMESPACE
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.namespace
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> POD_IP
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> status.podIP
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NODE_NAME
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> spec.nodeName
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SERVICE_ACCOUNT
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> spec.serviceAccountName
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CONTAINER_CPU_REQUEST_MILLICORES
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                             
        <span class="token key atrule">resourceFieldRef</span><span class="token punctuation">:</span>                    
          <span class="token key atrule">resource</span><span class="token punctuation">:</span> requests.cpu  <span class="token comment"># 容器请求的CPU和内存使用量是引用resourceFieldRef字段而不是fieldRef字段            </span>
          <span class="token key atrule">divisor</span><span class="token punctuation">:</span> 1m      <span class="token comment"># 对于资源相关的字段, 定义一个基数单位, 从而生成每部分的值</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CONTAINER_MEMORY_LIMIT_KIBIBYTES
      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>
        <span class="token key atrule">resourceFieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">resource</span><span class="token punctuation">:</span> limits.memory
          <span class="token key atrule">divisor</span><span class="token punctuation">:</span> 1Ki
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br></div></div><p><strong>当进程在运行时, 它可以获取所有在 pod 的定义文件中设定的环境变量</strong>. 图 8-2 展示了所有的环境变量以及变量值的来源. pod 的名称, IP 和命名空间可以通过 <strong>pod_NAME, pod_IP 和 pod_NAMESPACE</strong> 这几个环境变量分别暴露. 容器运行的节点的名称可以通过 NODE_NAME 变量暴露. 同样, 服务账户可以使用环境变量 SERVICE_ACCOUNT. 也可以创建两个环境变量来保存容器请求使用的 CPU 的数量, 以及容器被最大允许使用的内存数量.</p> <p>对于暴露资源请求和使用限制的环境变量, 会设定一个基数单位. 实际的资源请求值和限制值除以这个基数单位, 所得的结果通过环境变量暴露出去. 在前面的例子中, 设定 CPU 请求的基数为 1m(即1 millicore, 也就是千分之一核 CPU). 当设置资源请求为 15m 时, 环境变量 CONTAINER_CPU_REQUEST_MILLICORES 的值就是 15. 同样, 设定内存的使用限制为 4Mi(4 mebibytes), 设定基数为 1Ki(1 Kibibyte), 则环境变量 CONTAINER_MEMORY_LIMIT_KIBIBYTES 的值就是 4096.</p> <p><img src="/img/image-20240224102941-x2pz6ml.png" alt="image" title="图8.2 pod 元数据与属性通过环境变量暴露给 pod"></p> <p>对于 CPU 资源请求量和使用限制可以被设定为 1, 也就意味着整颗 CPU 的计算能力, 也可以设定为 1m, 即千分之一核的计算能力. 对于内存的资源请求和使用限制可以设定为 1(字节), 也可以是 1k(kilobute)或1Ki(kibibute), 同样也可以设为 1M(megavyte)或者 1Mi(mebibyte), 等等.</p> <p>在完成创建 pod 后, 可以<strong>使用 kubectl exec 命令来查看容器中的所有环境变量</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-8.2 downward pod 中的环境变量</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> downward <span class="token function">env</span>
<span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<span class="token assign-left variable"><span class="token environment constant">HOSTNAME</span></span><span class="token operator">=</span>downward
<span class="token assign-left variable">CONTAINER_MEMORY_LIMIT_KIBIBYTES</span><span class="token operator">=</span><span class="token number">4096</span>  <span class="token comment"># 内存限制</span>
<span class="token assign-left variable">POD_NAME</span><span class="token operator">=</span>downward  
<span class="token assign-left variable">POD_NAMESPACE</span><span class="token operator">=</span>default
<span class="token assign-left variable">POD_IP</span><span class="token operator">=</span><span class="token number">10.0</span>.0.10    <span class="token comment"># IP</span>
<span class="token assign-left variable">NODE_NAME</span><span class="token operator">=</span>gke-kubia-default-pool-32a2cac8-sgl7
<span class="token assign-left variable">SERVICE_ACCOUNT</span><span class="token operator">=</span>default
<span class="token assign-left variable">CONTAINER_CPU_REQUEST_MILLICORES</span><span class="token operator">=</span><span class="token number">15</span>
<span class="token assign-left variable">KUBERNETES_SERVICE_HOST</span><span class="token operator">=</span><span class="token number">10.3</span>.240.1
<span class="token assign-left variable">KUBERNETES_SERVICE_PORT</span><span class="token operator">=</span><span class="token number">443</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><strong>所有在这个容器中运行的进程都可以读取并使用它们需要的变量.</strong></p> <h6 id="_8-1-3-通过downwardapi卷来传递元数据"><a href="#_8-1-3-通过downwardapi卷来传递元数据" class="header-anchor">#</a> 8.1.3 通过downwardAPI卷来传递元数据</h6> <p><strong>如果更倾向于使用文件的方式而不是环境变量的方式暴露元数据, 可以定义一个 downwardAPI 卷并挂载到容器中</strong>. 由于不能通过环境变量暴露, 所以必须使用 downwardAPI 卷来暴露 pod 标签或注解. 后面将讨论原因.</p> <p>与环境变量一样, 需要显示地指定<strong>元器据字段</strong>来暴露给进程. 下面将把前面的示例从使用环境变量修改为<strong>使用存储卷</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-8.3 一个带有 dowanwardAPI 卷的 pod 示例: dowanward-apivolume.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> downward
  <span class="token key atrule">labels</span><span class="token punctuation">:</span>           <span class="token comment"># 通过downwardAPI卷来保留这些标签和注解</span>
    <span class="token key atrule">foo</span><span class="token punctuation">:</span> bar     
  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>           
    <span class="token key atrule">key1</span><span class="token punctuation">:</span> value1      
    <span class="token key atrule">key2</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token scalar string">               
      multi                
      line            
      value                </span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;9999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 15m
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 100Ki
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 4Mi
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>  <span class="token comment"># 在/etc/downward目录下挂载这个downward卷                         </span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> downward                      
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/downward            
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>       
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> downward   <span class="token comment"># 通过将卷的名字设定为downward来定义一个downwardAPI卷                       </span>
    <span class="token key atrule">downwardAPI</span><span class="token punctuation">:</span>                          
      <span class="token key atrule">items</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;podName&quot;</span>  <span class="token comment"># pod的名称(来自manifest文件中的metadata.name字段)将被写入podName文件中                  </span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                         
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.name         
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;podNamespace&quot;</span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.namespace
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;labels&quot;</span>    <span class="token comment"># pod的标签将被保存到/etc/downward/labels文件中                    </span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                              
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.labels           
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;annotations&quot;</span>  <span class="token comment"># pod的注解将被保存到/etc/downward/annotations文件中                 </span>
        <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                              
          <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> metadata.annotations      
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;containerCpuRequestMilliCores&quot;</span>
        <span class="token key atrule">resourceFieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">containerName</span><span class="token punctuation">:</span> main
          <span class="token key atrule">resource</span><span class="token punctuation">:</span> requests.cpu
          <span class="token key atrule">divisor</span><span class="token punctuation">:</span> 1m
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;containerMemoryLimitBytes&quot;</span>
        <span class="token key atrule">resourceFieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">containerName</span><span class="token punctuation">:</span> main
          <span class="token key atrule">resource</span><span class="token punctuation">:</span> limits.memory
          <span class="token key atrule">divisor</span><span class="token punctuation">:</span> <span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br></div></div><p>现在没有通过环境变量来传递元数据, 而是<strong>定义了一个叫作 downward 的卷</strong>, 并且通过 <code>/etc/downward</code>​ 目录挂载到容器中. <mark><strong>卷所包含的文件会通过卷定义中的 downwardAPI.items 属性来定义</strong></mark>.</p> <p>对于想要在文件中保存的每一个 pod 级的字段或者容器资源字段, 都分别在 <strong>downwardAPI.items</strong> 中说明了元数据被保存和引用的 path(文件名), 如图 8.3 所示.</p> <p><img src="/img/image-20240224103324-xfi66bc.png" alt="image" title="图8.3 使用 dowanward API 卷来传递元数据"></p> <p>从之前列表的 manifest 中删除原来的 pod, 并且新建一个 pod. 然后查看已挂载到 downwardAPI 卷目录的内容, 存储卷被挂载在 <code>/etc/downward/</code>​​ 目录下, 列出目录中的<strong>文件</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-8.4 downwordAPI 卷中的文件</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> downward <span class="token function">ls</span> <span class="token parameter variable">-lL</span> /etc/downward
-rw-r--r--   <span class="token number">1</span> root   root   <span class="token number">134</span> May <span class="token number">25</span> <span class="token number">10</span>:23 annotations
-rw-r--r--   <span class="token number">1</span> root   root     <span class="token number">2</span> May <span class="token number">25</span> <span class="token number">10</span>:23 containerCpuRequestMilliCores
-rw-r--r--   <span class="token number">1</span> root   root     <span class="token number">7</span> May <span class="token number">25</span> <span class="token number">10</span>:23 containerMemoryLimitBytes
-rw-r--r--   <span class="token number">1</span> root   root     <span class="token number">9</span> May <span class="token number">25</span> <span class="token number">10</span>:23 labels
-rw-r--r--   <span class="token number">1</span> root   root     <span class="token number">8</span> May <span class="token number">25</span> <span class="token number">10</span>:23 podName
-rw-r--r--   <span class="token number">1</span> root   root     <span class="token number">7</span> May <span class="token number">25</span> <span class="token number">10</span>:23 podNamespace
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>注意: 与 configMAp 和 secret 卷一样, 可以通过 pod 定义中 downwardAPI 卷的 defaultMode 属性来改变文件的访问权限设置.</p> <p><strong>每个文件都对应了卷定义中的一项</strong>. 文件的内容与之前例子中的元数据字段和值, 这里不再重复展示. 不过由于不能通过环境变量的方式暴露 label 和 annotation, 所以看一下暴露的这两个文件的代码清单.</p> <p><strong>代码清单-8.5 展示 downwardAPI 卷中的标签和注解</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> downward <span class="token function">cat</span> /etc/downward/labels
<span class="token assign-left variable">foo</span><span class="token operator">=</span><span class="token string">&quot;bar&quot;</span>

$ kubectl <span class="token builtin class-name">exec</span> downward <span class="token function">cat</span> /etc/downward/annotations
<span class="token assign-left variable">key1</span><span class="token operator">=</span><span class="token string">&quot;value1&quot;</span>
<span class="token assign-left variable">key2</span><span class="token operator">=</span><span class="token string">&quot;multi<span class="token entity" title="\n">\n</span>line<span class="token entity" title="\n">\n</span>value<span class="token entity" title="\n">\n</span>&quot;</span>
kubernetes.io/config.seen<span class="token operator">=</span><span class="token string">&quot;2016-11-28T14:27:45.664924282Z&quot;</span>
kubernetes.io/config.source<span class="token operator">=</span><span class="token string">&quot;api&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>正如上面看到的, 每一个标签和注解都以 <code>key=value</code>​ 的格式保存在单独的行中, 如对应多个值, 则写在同一行, 并且用回车符 <code>\n</code>​ 连接.</p> <blockquote><p>修改标签和注解</p></blockquote> <p><strong>可以在 pod 运行时修改标签和注解. 当标签和注解被修改后, Kubernetes 会更新存有相关信息的文件, 从而使 pod 可以获取最新的数据. 这也解释了为什么不能通过环境变量的方式暴露标签和注解, 在环境变量方式下, 一旦标签和注解被修改, 新的值将无法暴露</strong>.</p> <blockquote><p>在卷的定义中引用容器级的元数据</p></blockquote> <p>在结束这一部分的内容之前, 需要说明一点, 当暴露容器级的元数据时, 如容器可使用的资源限制或者资源请求(使用字段 resourceFieldRef), 必须指定<strong>引用资源字段对应的容器名称</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-8.6 在 downwardAPI 卷中引用容器级的元数据</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> downward
    <span class="token key atrule">downwardAPI</span><span class="token punctuation">:</span>
      <span class="token key atrule">items</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">&quot;containerCpuRequestMilliCores&quot;</span>
        <span class="token key atrule">resourceFieldRef</span><span class="token punctuation">:</span>
          <span class="token key atrule">containerName</span><span class="token punctuation">:</span> main    <span class="token comment"># 必须指定容器名称 </span>
          <span class="token key atrule">resource</span><span class="token punctuation">:</span> requests.cpu
          <span class="token key atrule">divisor</span><span class="token punctuation">:</span> 1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这样做的理由很明显, 因为<strong>对于卷的定义是基于 pod 级的, 而不是容器级的</strong>. 当引用卷定义某一个容器的资源字段时, 需要明确说明引用的容器的名称. 这个规则对于只包含单容器的 pod 同样适用.</p> <p>使用卷的方式来暴露容器的资源请求和使用限制比环境变量的方式稍显复杂, 但好处是如果有必要, <strong>可以传递一个容器的资源字段到另一个容器(当然两个容器必须处于同一个 pod)</strong> . 使用环境变量的方式, 一个容器只能传递它自身资源申请求和限制的信息.</p> <blockquote><p>何时使用 Dowanward API 方式</p></blockquote> <p>Downward API 方式并不复杂, 它使得应用独立于 Kubernetes. 这一点在处理部分<strong>数据已在环境变量中的现有应用</strong>时特别有用. Downward API 方式使得我们不必通过修改应用, 或者使用 shell 脚本获取数据再传递给环境变量的方式来暴露数据.</p> <p>不过通过 Downward API 的方式获取的元数据是相当有限的, 如果需要获取更多的元数据, 需要使用<strong>直接访问 Kubernetes API 服务器</strong>的方式. 将在接下来的部分了解这种方式.</p> <h5 id="_8-2-与kubernetes-api服务器交互"><a href="#_8-2-与kubernetes-api服务器交互" class="header-anchor">#</a> 8.2 与Kubernetes API服务器交互</h5> <p>Downward API 提供了一种简单的方式, 将 pod 和容器的元数据传递给在它们内部运行的进程. 但<strong>这种方式其实仅仅可以暴露一个 pod 自身的元数据, 而且只可以暴露部分元数据</strong>. 某些情况下, 应用需要知道<strong>其他 pod 的信息</strong>, 甚至是集群中其他资源的信息. 这种情况下 Downward API 方式将无能为力.</p> <p>正如书中提到的, 可以通过<strong>服务相关的环境变量或者 DNS 来获取服务和 pod 的信息, 但如果应用需要获取其他资源的信息或者获取最新的信息, 就需要直接与 API 服务器进行交互</strong>(如图8.4所示).</p> <p><img src="/img/image-20240227234215-y3hidjb.png" alt="image" title="图8.4 从 pod 内部与 API 服务器交互获取其他 API 对象的信息"></p> <p>在了解 pod 中的应用如何与 Kubernetes API 服务器交互之前, 先在自己的本机上研究一下<strong>服务器的 REST endpoit</strong>, 这样可以大致了解什么是 API 服务器.</p> <h6 id="_8-2-1-探究kubernetes-rest-api"><a href="#_8-2-1-探究kubernetes-rest-api" class="header-anchor">#</a> 8.2.1 探究Kubernetes REST API</h6> <p>前面已经了解了 Kubernetes 不同的资源类型. 但如果打算开发一个可以与 Kubernetes API 交互的应用, 要首先<strong>了解 API</strong>.</p> <p>先尝试直接访问 API 服务器, 可以通过运行 <code>kubectl cluster-info</code>​ 命令来得到服务器的 URL.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>因为服务器使用 HTTPS 协议并且需要授权, 所以与服务器交互并不是一件简单的事情, 可以尝试通过 curl 来访问它, 使用 curl 的 --insecure(或-k)选项来跳过服务器证书检查环节, 但这也不能让我们走得更远.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> https://192.168.99.100:8443 <span class="token parameter variable">-k</span>
Unauthorized
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>幸运的是, 可以执行 <strong>kubectl proxy 命令, 通过代理与服务器交互</strong>, 而不是自行来处理验证过程.</p> <blockquote><p>通过 Kubectl proxy 访问 API 服务器</p></blockquote> <p><strong>kubectl proxy 命令启动了一个代理服务来接收来自本机的 HTTP 连接并转发至 API 服务器, 同时处理身份认证, 所以不需要每次请求都上传认证凭证</strong>. 它也可以确保我们直接与真实的 API 服务器交互, 而不是一个中间人(通过每次验证服务器证书的方式).</p> <p>运行代理很简单, 所要做的就是运行以下命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl proxy
Starting to serve on <span class="token number">127.0</span>.0.1:8001
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>也无须传递其他任何参数, 因为 kubectl 已经知晓所需的所有参数(API 服务器 URL, 认证凭证等). 一旦启动, 代理服务器就将在本地端口 8001 接收连接请求, 看一下它是如何工作的:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> localhost:8001
<span class="token punctuation">{</span>
  <span class="token string">&quot;paths&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;/api&quot;</span>,
    <span class="token string">&quot;/api/v1&quot;</span>,
    <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这里发送请求给代理, 代理接着发送请求给 API 服务器, 然后代理将返回从服务器返回的所有信息, 现在开始研究.</p> <blockquote><p>通过 Kubectl proxy 研究 Kubernetes API</p></blockquote> <p>可以继续使用 curl, 或者打开浏览器并且指向 <code>http://localhost:8001</code>​, 看一下当访问这个基础的 URL 时, API 服务器会返回什么. 服务器的应答是一组路径的清单, 如下所示.</p> <p><strong>代码清单-8.7 API 服务器的 REST endpoint 清单: http://localhost:8001</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://localhost:8001
<span class="token punctuation">{</span>
  <span class="token string">&quot;paths&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;/api&quot;</span>,
    <span class="token string">&quot;/api/v1&quot;</span>,                  <span class="token comment"># 这里可以看到大部分的资源类型</span>
    <span class="token string">&quot;/apis&quot;</span>,
    <span class="token string">&quot;/apis/apps&quot;</span>,
    <span class="token string">&quot;/apis/apps/v1beta1&quot;</span>,
    <span class="token punctuation">..</span>.
    <span class="token string">&quot;/apis/batch&quot;</span>,              <span class="token comment"># batch API组以及它的两个版本</span>
    <span class="token string">&quot;/apis/batch/v1&quot;</span>,       
    <span class="token string">&quot;/apis/batch/v2alpha1&quot;</span>,   
    <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>这些路径对应了<strong>创建 Pod, Service 这些资源时定义的 API 组和版本信息</strong>.</p> <p>你或许已经发现在 /apis/batch/v1 路径下的 batch/V1 就是在第 4 章了解的 Job 资源 API 组和版本信息. 同样, /api/V1 对应 apiVersion: 这里所说的 V1 指的是创建的基础资源(Pod, Service, ReplicationController 等). 在 Kubernetes 最早期版本中提到的最基础的资源并不属于任何指定的组, 原因是 Kubernetes 初期并没有使用 API 组的概念, 这个概念是后期引入的.</p> <p>注意: 这些<strong>没有列入 API 组的初始资源类型现在一般被认为归属于核心的 API 组</strong>.</p> <blockquote><p>研究批量 API 组的 REST endpoint</p></blockquote> <p>下面来研究 Job 资源 API, 从路径 /apis/batch 下的内容开始(暂时忽略版本), 如下面的代码清单所示.</p> <p><strong>代码清单-8.8 在/apis/batch 目录下的 endpoint 清单: http://localhost:8001/apis/batch</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://localhost:8001/apis/batch
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;APIGroup&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch&quot;</span>,
  <span class="token string">&quot;versions&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token string">&quot;groupVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch/v1&quot;</span>,             <span class="token comment"># 批量API组包含两个版本</span>
      <span class="token string">&quot;version&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>                     
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{</span>
      <span class="token string">&quot;groupVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch/v2alpha1&quot;</span>,       <span class="token comment"># 批量API组包含两个版本</span>
      <span class="token string">&quot;version&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v2alpha1&quot;</span>               
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>,
  <span class="token string">&quot;preferredVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>                       <span class="token comment"># 客户应该使用V1版本而不是v2alpha1版本</span>
    <span class="token string">&quot;groupVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch/v1&quot;</span>,           
    <span class="token string">&quot;version&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>                       
  <span class="token punctuation">}</span>,
  <span class="token string">&quot;serverAddressByClientCIDRs&quot;</span><span class="token builtin class-name">:</span> null
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>这个响应消息展示了包括可用版本, 客户推荐使用版本在内的批量 API 组信息. 接着看一下 /apis/batch/V1 路径下的内容, 如下面的代码清单所示.</p> <p><strong>代码清单-8.9 在 batch/V1中的资源类型: http://localhost:8001/apis/batch/v1</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://localhost:8001/apis/batch/v1
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;APIResourceList&quot;</span>,              <span class="token comment"># 这里是在batch/v1API组中的API资源清单</span>
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token string">&quot;groupVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch/v1&quot;</span>,         
  <span class="token string">&quot;resources&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>          <span class="token comment"># 这个数据包含了这个组中所有的资源类型              </span>
    <span class="token punctuation">{</span>
      <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;jobs&quot;</span>,     <span class="token comment"># 这里描述了已经被指定了命名空间的Job资源</span>
      <span class="token string">&quot;namespaced&quot;</span><span class="token builtin class-name">:</span> true,             
      <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Job&quot;</span>,                  
      <span class="token string">&quot;verbs&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>            <span class="token comment"># 这里给出了资源对应可以使用的动词(可以创建JOB, </span>
        <span class="token string">&quot;create&quot;</span>,           <span class="token comment"># 可以删除单独的一个JOB或者其中的一个集合, 也可以恢复, 监听或者修改它们)           </span>
        <span class="token string">&quot;delete&quot;</span>,                     
        <span class="token string">&quot;deletecollection&quot;</span>,           
        <span class="token string">&quot;get&quot;</span>,                        
        <span class="token string">&quot;list&quot;</span>,                       
        <span class="token string">&quot;patch&quot;</span>,                      
        <span class="token string">&quot;update&quot;</span>,                     
        <span class="token string">&quot;watch&quot;</span>                       
      <span class="token punctuation">]</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{</span>
      <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;jobs/status&quot;</span>,  <span class="token comment"># 资源也有一个专门的REST endpoint来修改它们的状态</span>
      <span class="token string">&quot;namespaced&quot;</span><span class="token builtin class-name">:</span> true,
      <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Job&quot;</span>,
      <span class="token string">&quot;verbs&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>              <span class="token comment"># 状态可以被恢复, 打补丁或者修改</span>
        <span class="token string">&quot;get&quot;</span>,                        
        <span class="token string">&quot;patch&quot;</span>,                      
        <span class="token string">&quot;update&quot;</span>                      
      <span class="token punctuation">]</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br></div></div><p>像看到的一样, API 服务器返回了在 <strong>batch/V1 目录下 API 组中的资源类型以及 REST endpoint 清单</strong>. 除了资源的名称和相关的类型, API 服务器也包含了一些其他信息, 比如资源是否被指定了命名空间, 名称简写(如果有的话, 对于 Job 来说没有), 资源对应可以使用的动词列表等.</p> <p>返回的列表描述了在 API 服务器中暴露的 REST 资源. <code>&quot;name&quot;:&quot;jobs&quot;</code>​ 行的信息告诉了 API 包含了 /apis/batch/V1/jobs 的 endpoint, &quot;verbs&quot; 数组告诉了可以通过 endpoint 恢复, 修改以及删除 Job 资源. 对于某些特定的资源, API 服务器暴露了额外的 API endpoint(例如, 通过 jobs/status 路径可以修改 Job 的状态).</p> <blockquote><p>列举集群中所有的 Job 实例</p></blockquote> <p>通过在 <code>/apis/batch/v1/jobs</code>​ 路径运行一个 GET 请求, 可以<strong>获取集群中所有 Job 的清单</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-8.10 Job 清单: http://localhost:8001/apis/batch/v1/jobs</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://localhost:8001/apis/batch/v1/jobs
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;JobList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch/v1&quot;</span>,
  <span class="token string">&quot;metadata&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;selfLink&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/apis/batch/v1/jobs&quot;</span>,
    <span class="token string">&quot;resourceVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;225162&quot;</span>
  <span class="token punctuation">}</span>,
  <span class="token string">&quot;items&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token string">&quot;metadata&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;my-job&quot;</span>,
        <span class="token string">&quot;namespace&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default&quot;</span>,
        <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>如果在集群中没有部署 Job 资源, 那么 items 数组将是空的. 可以尝试在 Chapter08/my-job.yaml 中部署 Job, 然后再次访问 RESR endpoint 从而得到与代码清单 8.10 中相同的输出信息.</p> <blockquote><p>通过名称恢复一个指定的 Job 实例</p></blockquote> <p>前面的 endpoint 返回了<strong>跨命名空间的所有 Job 的清单</strong>, 如果想要返回指定的一个 Job, 需要<strong>在 URL 中指定它的名称和所在的命名空间</strong>. 为了恢复在之前清单中的一个 Job(name:my-job;namespace:dfault), 需要访问路径:  <code>/apis/batch/v1/namespaces/default/jobs/my-job</code>​, 如下面的代码清单所示.</p> <p><strong>代码清单-8.11 通过名称恢复一个指定命名空间下的资源</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;Job&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;batch/v1&quot;</span>,
  <span class="token string">&quot;metadata&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;name&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;my-job&quot;</span>,
    <span class="token string">&quot;namespace&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;default&quot;</span>,
    <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 这里得到了 my-job 这个 Job 资源的完整的 JSON 定义信息, 和运行  <strong>$kubetcl get job my-job-o json</strong> 命令得到的信息完全一致.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get job my-job <span class="token parameter variable">-o</span> json
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>虽然不使用任何特定的工具, 也可以访问 Kubernetes REST API 服务器, 但如果要全面地研究 REST API 并与之交互, 在本章最后会介绍更好的方式. 暂时来看, 像这样使用 curl 进行研究, 对理解一个应用如何在 pod 中运行并与 Kubernetes 交互已经足够.</p> <h6 id="_8-2-2-从pod内部与api服务器进行交互"><a href="#_8-2-2-从pod内部与api服务器进行交互" class="header-anchor">#</a> 8.2.2 从pod内部与API服务器进行交互</h6> <p>前面已经知道如何从本机通过使用 kubectl proxy 与 API 服务器进行交互. 现在来研究<strong>从一个 pod 内部访问它</strong>, 这种情况下通常没有 kubectl 可用. 因此想要从 pod 内部与 API 服务器进行交互, 需要关注以下三件事情:</p> <ul><li><strong>确定 API 服务器的位置</strong></li> <li><strong>确保是与 API 服务器进行交互</strong>, 而不是一个冒名者</li> <li>通过服务器的<strong>认证</strong>, 否则将不能查看任何内容以及进行任何操作</li></ul> <p>接下来看一下交互如何实现.</p> <blockquote><p>运行一个 pod 来尝试与 API 服务器进行通信</p></blockquote> <p>首先需要一个 pod 以便从它内部发起与 API 服务器的交互. 运行一个什么也不做的 pod(在它仅有的容器内部运行一个 sleep 命令), 然后<strong>通过 kubectl exec 在容器内部运行一个脚本, 接下来在脚本中使用 curl 尝试访问 API 服务器</strong>.</p> <p>因此, 需要使用一个包含 curl 二进制的容器镜像. 如果在 Docker Hub 中搜索, 就会发现 <code>tutum/curl</code>​ 镜像, 可以使用这个镜像(也可以使用任何包含 curl 二进制的已有镜像或者自己打包的镜像). pod 的定义如下面的代码清单所示.</p> <p><strong>代码清单-8.12 用来尝试与 API 服务器通信的 pod:curl.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> curl
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> tutum/curl    <span class="token comment"># 由于需要在容器中可以用curl, 使用tutum/curl镜像</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;9999999&quot;</span><span class="token punctuation">]</span>  <span class="token comment"># 运行一个长时间延迟的sleep目录来保持容器处于运行状态</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>在完成 pod 的创建后, 在容器中运行 kubectl exec 来启动一个 bash shell:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> <span class="token function">curl</span> <span class="token function">bash</span>
root@curl:/<span class="token comment">#</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>​​现在已经做好了与 API 服务器交互的准备.</p> <blockquote><p>发现 API 服务器地址</p></blockquote> <p>首先, <strong>需要找到 Kubernetes API 服务器的 IP 地址和端口</strong>. 这一点比较容易做到, 因为一个<strong>名为 kubernetes 的服务在默认的命名空间被自动暴露, 并被配置为指向 API 服务器</strong>. 也许你应该记得, 每次使用 kubectl get svc 命令显示所有服务清单时, 都会看到这个服务.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>   AGE
kubernetes   <span class="token number">10.0</span>.0.1     <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>        <span class="token number">443</span>/TCP   46d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在第 5 章中说过<strong>每个服务都被配置了对应的环境变量</strong>, 在容器内通过<strong>查询 KUBERNETES_SERVICE_HOST 和 KUBERNETES_SERVICE_PORT 这两个环境变量就可以获取 API 服务器的 IP 地址和端口</strong>.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># env | grep KUBERNETES_SERVICE</span>
<span class="token assign-left variable">KUBERNETES_SERVICE_PORT</span><span class="token operator">=</span><span class="token number">443</span>
<span class="token assign-left variable">KUBERNETES_SERVICE_HOST</span><span class="token operator">=</span><span class="token number">10.0</span>.0.1
<span class="token assign-left variable">KUBERNETES_SERVICE_PORT_HTTPS</span><span class="token operator">=</span><span class="token number">443</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>同样, 应该记得每个服务都可以获得一个 DNS 入口, 所以甚至没有必要去查询环境变量, 而只是<strong>简单地将 curl 指向 https://kubernetes</strong>. 公平地讲, 如果<strong>不知道服务在哪个端口是可用的, 既可以查询环境变量, 也可以查看 DNS SRV 记录来得到实际的端口号</strong>.</p> <p>之前展示的环境变量说明 API 服务器监听 HTTPS 协议默认的 443 端口, 所以尝试通过 HTTPS 协议来访问服务器.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># curl https://kubernetes</span>
curl: <span class="token punctuation">(</span><span class="token number">60</span><span class="token punctuation">)</span> SSL certificate problem: unable to get <span class="token builtin class-name">local</span> issuer certificate
<span class="token punctuation">..</span>.
If you<span class="token string">'d like to turn off curl'</span>s verification of the certificate, use
  the <span class="token parameter variable">-k</span> <span class="token punctuation">(</span>or --insecure<span class="token punctuation">)</span> option.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>虽然最简单的绕开这一步骤的方式是使用推荐的 -k 选项(这也是在手工操作 API 服务器时通常会使用的方式), 但还是来看一下更长(也是正确)的途径. 应该通过使用 curl 检查证书的方式验证 API 服务器的身份, 而不是盲目地相信连接的服务是可信的.</p> <p>提示: <strong>在真实的应用中, 永远不要跳过检查服务器证书的环节</strong>. 这样做会导致应用验证凭证暴露给采用中间人攻击方式的攻击者.</p> <blockquote><p>验证服务器身份</p></blockquote> <p>在之前的章节中, 在讨论 Secret 时, 可以看到一个名为 <strong>defalut-token-xyz</strong> 的 Secret 被自动创建, 并挂载到每个容器的 <code>/var/run/secrets/kubernetes.io/serviceaccount</code>​ 目录下. 来查看目录下的文件, 再次看一下 Secret 的内容.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># ls /var/run/secrets/kubernetes.io/serviceaccount/</span>
ca.crt    namespace    token
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>Secret 有三个入口(因此在 Secret 卷中有三个文件). 现在<strong>关注一下 ca.crt 文件. 该文件中包含了 CA 的证书, 用来对 Kubernetes API 服务器证书进行签名</strong>. 为了验证正在交互的 API 服务器, 需要检查服务器的证书是否是由 CA 签发. <strong>curl 允许使用 -cacert 选项来指定 CA 证书</strong>, 来尝试重新访问 API 服务器:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes</span>
Unauthorized
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 可能看到一个比 &quot;Unauthorized&quot; 更长的错误描述.</p> <p>到目前为止, 已经取得进展, 服务使用了信任的 CA 签名的证书, 所以 curl 验证通过了服务器的身份, 但 Unauthorized 这个响应提醒了需要关注<strong>授权</strong>的问题. 同时看一下如何通过设置 CURL_CA_BUNDLE 环境变量来简化操作, 从而不必在每次运行 curl 时都指定 --cacert 选项:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在可以不使用 --cacert 来访问 API 服务器:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># curl https://kubernetes</span>
Unauthorized
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这样操作相对便捷, 客户端(curl)现在信任 API 服务器, 但 API 服务器并不确认访问者的身份, 所以没有授权允许访问.</p> <blockquote><p>获得 API 服务器授权</p></blockquote> <p>我们需要<strong>获得 API 服务器的授权</strong>, 以便可以读取并进一步修改或删除部署在集群中的 API 对象. 为了获得授权, 需要认证的凭证, 幸运的是, 凭证可以使用之前提到的 <strong>default-token Secret</strong> 来产生, 同时凭证可以被存放在 secret 卷的 token 文件中. Secret 这个名字就说明了它主要的作用.</p> <p>可以使用凭证来访问 API 服务器, 第一步, 将凭证挂载到环境变量中:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>此时, <strong>凭证已经被存放在 TOKEN 环境变量中,</strong>  如下面的代码清单所示, 可以在向 API 服务器发送请求时使用它.</p> <p><strong>代码清单-8.13 获得 API 服务器的正确响应</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># curl -H &quot;Authorization: Bearer $TOKEN&quot; https://kubernetes</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;paths&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;/api&quot;</span>,
    <span class="token string">&quot;/api/v1&quot;</span>,
    <span class="token string">&quot;/apis&quot;</span>,
    <span class="token string">&quot;/apis/apps&quot;</span>,
    <span class="token string">&quot;/apis/apps/v1beta1&quot;</span>,
    <span class="token string">&quot;/apis/authorization.k8s.io&quot;</span>,
    <span class="token punctuation">..</span>.
    <span class="token string">&quot;/ui/&quot;</span>,
    <span class="token string">&quot;/version&quot;</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><blockquote><p>关闭基于角色的访问控制(RBAC)</p></blockquote> <p>如果正在使用一个带有 <strong>RBAC</strong> 机制的 Kubernetes 集群, 服务账户可能不会被授权访问 API 服务器(或只有部分授权). 将在第 12 章详细了解服务账户和 RBAC 机制. 目前最简单的方式就是运行下面的命令查询 API 服务器, 从而绕过 RBAC 方式.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrolebinding permissive-binding <span class="token punctuation">\</span>
  <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>cluster-admin <span class="token punctuation">\</span>
  <span class="token parameter variable">--group</span><span class="token operator">=</span>system:serviceaccounts
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这个命令赋予了所有服务账户(也可以说所有的 pod)的集群管理员权限, 允许它们执行任何需要的操作, 很明显这是一个危险的操作, 永远都不应该在生产的集群中执行, 对于测试来说是没有问题的.</p> <p>这里通过发送请求的 HTTP 头中的 <strong>Authorization 字段向 API 服务器传递了凭证</strong>, API 服务器识别确认凭证并返回正确的响应, 正如前面几个章节所做的, 现在可以探索集群中所有的资源.</p> <p>例如, 可以列出集群中所有的 pod, 但前提是知道运行 curl 的 pod 属于哪个命名空间.</p> <blockquote><p>获取当前运行 pod 所在的命名空间</p></blockquote> <p>在本章的第一部分了解了如何使用 Downward API 的方式将命名空间的属性传递到 pod. 如果你注意观察的话, 或许注意到 <strong>secret 卷中也包含了一个叫作命名空间的文件</strong>. 这个文件包含了当前运行 pod 所在的命名空间, 所以可以读取这个文件来获得命名空间信息, 而不是通过环境变量明确地传递信息到 pod. 文件内容挂载到 NS 环境变量中, 然后列出所有的 pod, 如下面的代码清单所示.</p> <p><strong>代码清单-8.14 获取当前 pod 所在命名空间中的所有 pod 清单</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl:/<span class="token comment"># NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)</span>
root@curl:/<span class="token comment"># curl -H &quot;Authorization: Bearer $TOKEN&quot; https://kubernetes/api/v1/namespaces/$NS/pods</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PodList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这就对了, 通过<strong>使用挂载在 secret 卷目录下的三个文件, 可以罗列出与当前 pod 运行在同一个命名空间下的所有 pod 的清单</strong>. 使用同样的方式不仅可以使用 GET 请求, 还可以使用 PUT 或者 PATCH 来检索和修改其他 API 对象.</p> <blockquote><p>简要说明 pod 如何与 Kubernetes 交互</p></blockquote> <p>来简单说明一下<strong>在 pod 中运行的应用如何正确访问 Kubernetes 的 API</strong>:</p> <ul><li><strong>应用应该验证 API 服务器的证书是否是证书机构所签发, 这个证书是在 ca.crt 文件中.</strong></li> <li><strong>应用应该将它在 token 文件中持有的凭证通过 Authorization 标头来获得 API 服务器的授权.</strong></li> <li>当对 pod 所在命名空间的 API 对象进行 CRUD 操作时, 应该<strong>使用 namespace 文件来传递命名空间信息到 API 服务器</strong>.</li></ul> <p>定义: CRUD 代表创建, 读取, 修改和删除操作, 与之对应的 HTTP 方法分别是 POST, GET, PATCH/PUT 以及 DELETE.</p> <p>与 API 服务器通信相关的 pod 的三个方面如图 8.5 所示.</p> <p><img src="/img/image-20240227234251-71t2ab2.png" alt="image" title="图8.5 通过 default-token Secret 中的文件与 API 服务器进行交互"></p> <h6 id="_8-2-3-通过ambassador容器简化与api服务器的交互"><a href="#_8-2-3-通过ambassador容器简化与api服务器的交互" class="header-anchor">#</a> 8.2.3 通过ambassador容器简化与API服务器的交互</h6> <p>使用 HTTPS, 证书和授权凭证, 对于开发者来说看上去有点复杂. 碰到过开发者在许多场景下关闭了对服务器证书验证的功能(当然笔者有时候也会这么做). 幸运的是, <strong>在保证安全性的前提下有办法简化通信的方式</strong>.</p> <p>还记得在 8.2.1 中提到过的 kubectl proxy 命令吗? 在本机上运行这个命令, 从而可以更加方便地访问 API 服务器. <strong>向代理而不是直接向 API 服务器发送请求, 通过代理来处理授权, 加密和服务器验证. 同样, 也可以在 pod 中这么操作</strong>.</p> <blockquote><p>ambassador容器模式介绍</p></blockquote> <p>想象一下, 如果一个应用需要查询 API 服务器(此外还有其他原因). 除了像之前章节讲到的直接与 API 服务器交互, 可以<strong>在主容器运行的同时, 启动一个 ambassador 容器, 并在其中运行 kubecctl proxy 命令, 通过它来实现与 API 服务器的交互</strong>.</p> <p>在这种模式下, <strong>运行在主容器中的应用不是直接与 API 服务器进行交互, 而是通过 HTTP 协议(不是 HTTPS 协议)与 ambassador 连接, 并且由 ambassador 通过 HTTPS 协议来连接 API 服务器, 对应用透明地来处理安全问题</strong>(见图8.6). 这种方式同样使用了默认凭证 Secret 卷中的文件.</p> <p><img src="/img/image-20240227234308-h96ul6t.png" alt="image" title="图8.6 使用 ambassador 连接 API 服务器"></p> <p>因为在同一个 pod 中的所有连接共享同样的回送网络接口, 所以我应用可以使用本地的端口来访问代理.</p> <blockquote><p>运行带有附加 ambassador 容器的 CURL pod</p></blockquote> <p>为了通过操作来理解 ambassador 容器模式, 下面像之前创建 curl pod 一样创建一个新的 pod, 但这次不是仅仅在 pod 中运行单个容器, 而是<strong>基于一个多用途的 kubectl-proxy 容器镜像</strong>来运行一个额外的 ambassador 容器, 这个镜像是之前创建的并已提交到 Docker Hub. 如果你想自己来编译它, 可以在代码存档中找到 Dockerfile 镜像(在 /Chapter08/kubectl-proxy/ 目录下).</p> <p>pod 的 manifest 文件如以下代码清单所示.</p> <p><strong>代码清单-8.15 带有 ambassador 容器的 pod:curl-with-ambassador.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> curl<span class="token punctuation">-</span>with<span class="token punctuation">-</span>ambassador
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> tutum/curl
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;9999999&quot;</span><span class="token punctuation">]</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ambassador     <span class="token comment"># ambassador容器, 运行kubectl-proxy镜像</span>
    <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubectl<span class="token punctuation">-</span>proxy<span class="token punctuation">:</span>1.6.2     
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>pod 的 spec 与之前非常类似, 但 pod 名称是不同的, 同时<strong>增加了一个额外的容器</strong>. 运行这个 pod, 并且通过以下命令进入主容器:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> curl-with-ambassador <span class="token parameter variable">-c</span> main <span class="token function">bash</span>
root@curl-with-ambassador:/<span class="token comment">#</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>现在 pod 包含两个容器</strong>, 我们希望在 main 容器中运行 bash, 所以使用 -c main 选项. 如果想在 pod 的第一个容器中运行该命令, 也无须明确地指定容器. 但如果想在任何其他的容器中运行这个命令, 就需要使用 -c 选项来说明容器的名称.</p> <blockquote><p>通过 ambassador 来与 API 服务器进行交互</p></blockquote> <p>接下来尝试<strong>通过 ambassador 容器来连接 API 服务器</strong>. 默认情况下, kubectl proxy 绑定 8001 端口, 由于 pod 中的两个容器共享包括回送地址在内的相同的网络接口, 可以如下面的代码清单所示, 将 curl 指向 localhost:8001.</p> <p><strong>代码清单-8.16 通过 ambassador 容器访问 API 服务器</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>root@curl-with-ambassador:/<span class="token comment"># curl localhost:8001</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;paths&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;/api&quot;</span>,
    <span class="token punctuation">..</span>.
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>成功了! curl 的输出打印结果与之前看到的响应相同, 但这次并<strong>不需要处理授权的凭证和服务器证书</strong>.</p> <p>想要清楚地了解处理的细节, 请参考图 8.7. <mark><strong>curl 向在 ambassador 容器内运行的代理发送普通的 HTTP 请求(不包含任何授权相关的标头), 然后代理向 API 服务器发送 HTTPS 请求, 通过发送凭证来对客户端授权, 同时通过验证证书来识别服务器的身份.</strong></mark></p> <p>这是一个很好的例子, 它说明了如何使用一个 ambassador 容器来屏蔽连接外部服务的复杂性, 从而简化在主容器中运行的应用. ambassador 容器可以跨多个应用复用, 而且与主应用使用的开发语言无关. 负面因素是需要运行额外的进程, 并且消耗资源.</p> <p><img src="/img/image-20240224110615-7pdlsda.png" alt="image" title="图8.7 将加密, 授权, 服务器验证工作交给 ambassador 容器中的 kubectl proxy"></p> <h6 id="_8-2-4-使用客户端库与api服务器交互"><a href="#_8-2-4-使用客户端库与api服务器交互" class="header-anchor">#</a> 8.2.4 使用客户端库与API服务器交互</h6> <p>在之前的例子中, 已经体验到了使用 ambassador 容器 kubectl-proxy 的好处, 如果应用仅仅需要在 API 服务器执行一些简单的操作, 往往可以使用一个标准的客户端库来执行简单的 HTTP 请求. <strong>但对于执行更复杂的 API 请求来说, 使用某个已有的 Kubernetes API 客户端库会更好一点</strong>.</p> <blockquote><p>使用已有的客户端库</p></blockquote> <p>目前, 存在由 API Machinery special interest group(SIG)支持的两个版本的 Kuberbetes API 客户端库.</p> <ul><li>Golang client—https://github.com/kubernetes/client-go</li> <li>Python—https://github.com/kubernetes-incubator/client-python</li></ul> <p>注意: Kubernetes 社区有大量的兴趣组和工作组, 这些小组分别关注着 Kubernetes 生态系统中的某个特定部分. 可以在 https://github.com/kubernetes/community/blob/master/sig-list.md 下看到它们的清单.</p> <p>除了官方支持的两个库, 这里列出了一些由用户贡献的针对不同语言的客户端库:</p> <ul><li>Fabric8 维护的 Java 客户端—https://github.com/fabric8io/kubernetes-client</li> <li>Amdatu 维护的 Java 客户端—https://bitbucket.org/amdatulabs/amdatu-kubernetes</li> <li>tenxcloud 维护的 Node.js 客户端—https://github.com/tenxcloud/node-kubernetesclient</li></ul> <p>这些库通常支持 HTTPS 协议, 并且<strong>可以处理授权操作</strong>, 所以不需要使用 ambassador 容器.</p> <blockquote><p>一个使用 Fabric8 Java 库与 Kubernetes 进行交互的例子</p></blockquote> <p>为了说明如何通过客户端库与 API 服务器进行交互, 以下的代码清单给出了一个例子说明如何使用 Fabric8 Kubernetes 客户端列出一个 Java 应用中的服务.</p> <p><strong>代码清单-8.17 使用 Fabric8 java 客户端列出, 创建, 更新和删除 pod</strong></p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">import</span> <span class="token import"><span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Arrays</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">io<span class="token punctuation">.</span>fabric8<span class="token punctuation">.</span>kubernetes<span class="token punctuation">.</span>api<span class="token punctuation">.</span>model<span class="token punctuation">.</span></span><span class="token class-name">Pod</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">io<span class="token punctuation">.</span>fabric8<span class="token punctuation">.</span>kubernetes<span class="token punctuation">.</span>api<span class="token punctuation">.</span>model<span class="token punctuation">.</span></span><span class="token class-name">PodList</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">io<span class="token punctuation">.</span>fabric8<span class="token punctuation">.</span>kubernetes<span class="token punctuation">.</span>client<span class="token punctuation">.</span></span><span class="token class-name">DefaultKubernetesClient</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">io<span class="token punctuation">.</span>fabric8<span class="token punctuation">.</span>kubernetes<span class="token punctuation">.</span>client<span class="token punctuation">.</span></span><span class="token class-name">KubernetesClient</span></span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>
  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">{</span>
    <span class="token class-name">KubernetesClient</span> client <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DefaultKubernetesClient</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// list pods in the default namespace</span>
    <span class="token class-name">PodList</span> pods <span class="token operator">=</span> client<span class="token punctuation">.</span><span class="token function">pods</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">inNamespace</span><span class="token punctuation">(</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    pods<span class="token punctuation">.</span><span class="token function">getItems</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">stream</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span>s <span class="token operator">-&gt;</span> <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Found pod: &quot;</span> <span class="token operator">+</span>
               s<span class="token punctuation">.</span><span class="token function">getMetadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// create a pod</span>
    <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Creating a pod&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token class-name">Pod</span> pod <span class="token operator">=</span> client<span class="token punctuation">.</span><span class="token function">pods</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">inNamespace</span><span class="token punctuation">(</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">createNew</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">withNewMetadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">withName</span><span class="token punctuation">(</span><span class="token string">&quot;programmatically-created-pod&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">endMetadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">withNewSpec</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">addNewContainer</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
          <span class="token punctuation">.</span><span class="token function">withName</span><span class="token punctuation">(</span><span class="token string">&quot;main&quot;</span><span class="token punctuation">)</span>
          <span class="token punctuation">.</span><span class="token function">withImage</span><span class="token punctuation">(</span><span class="token string">&quot;busybox&quot;</span><span class="token punctuation">)</span>
          <span class="token punctuation">.</span><span class="token function">withCommand</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;99999&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">endContainer</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">endSpec</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">done</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Created pod: &quot;</span> <span class="token operator">+</span> pod<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// edit the pod (add a label to it)</span>
    client<span class="token punctuation">.</span><span class="token function">pods</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">inNamespace</span><span class="token punctuation">(</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">withName</span><span class="token punctuation">(</span><span class="token string">&quot;programmatically-created-pod&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">edit</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">editMetadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">addToLabels</span><span class="token punctuation">(</span><span class="token string">&quot;foo&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;bar&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">endMetadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">done</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Added label foo=bar to pod&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Waiting 1 minute before deleting pod...&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token class-name">Thread</span><span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// delete the pod</span>
    client<span class="token punctuation">.</span><span class="token function">pods</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">inNamespace</span><span class="token punctuation">(</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">withName</span><span class="token punctuation">(</span><span class="token string">&quot;programmatically-created-pod&quot;</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Deleted the pod&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br></div></div><p>由于 Fabric8 客户端暴露了一种友好的流畅(fiuent)的 Domain-Specific-Language(DSL)API, 所以以上代码浅显易懂.</p> <blockquote><p>使用 Swagger 和 OpenAPI 打造你自己的库</p></blockquote> <p>如果选择的开发语言没有可用的客户端, 可以使用 Swagger API 框架生成客户端库和文档. Kubernetes API 服务器在 /swaggerapi 下暴露 Swagger API 定义, 在 /swagger.json 下暴露 OpenAPI 定义.</p> <blockquote><p>使用 Swagger UI 研究 API</p></blockquote> <p>在本章开头, 已经提到了一种更好的研究 Rest API 的方式, 而不是使用 curl 直接访问 REST endpoint. 正如在前面部分所提到的, Swagger 不仅是描述 API 的工具, 如果暴露了 Swagger API 定义, 还能够提供一个用于查看 REST API 的 web UI.</p> <p>Kubernetes 不仅暴露了 Swagger API, 同时也有与 API 服务器集成的 Swagger UI. Swagger UI 默认状态没有激活, 可以通过使用--enable-swagger-ui=true 选项运行 API 服务器对其进行激活.</p> <p>提示: 如果使用 Minikube, 可以在启动集群时, 使用 <strong>minikube start--extra-config=apiserver.Features.Enable-SwaggerUI=true</strong> 选项来激活 Swagger UI.</p> <p>在激活 UI 后, 可以通过以下地址在浏览器中打开它:</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token function">http</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token generics"><span class="token punctuation">&lt;</span>api server<span class="token punctuation">&gt;</span></span><span class="token operator">:</span><span class="token generics"><span class="token punctuation">&lt;</span>port<span class="token punctuation">&gt;</span></span><span class="token operator">/</span>swagger<span class="token operator">-</span>ui
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>强烈建议尝试使用 Swagger UI, 通过它不仅可以浏览 Kubernetes API, 也可以使用它来进行交互(例如, 可以 POST JSON 资源 manifest, PATCH 资源或者 DELETE 它们).</p> <h5 id="_8-3-本章小结"><a href="#_8-3-本章小结" class="header-anchor">#</a> 8.3 本章小结</h5> <p>在完成本章的学习后, 知道了在一个 pod 中运行的应用, 如何获取它自身的数据, 或者部署在集群中的其他 pod, 其他组件的数据. 也了解了以下内容:</p> <ul><li><strong>如何通过环境变量或者 downwardAPI 卷中的文件, 将 pod 的名称, 所在的命名空间信息以及其他元数据暴露给进程</strong></li> <li><strong>如何将 CPU 和内存的资源请求和使用限额以任何应用指定的单位传递给应用</strong></li> <li>pod 如何使用 downwardAPI 卷来获取最新的元数据, 而这些元数据可能在 pod(如标签和注解)的生命周期内发生变化</li> <li>如何使用 kubectl proxy 浏览 Kubernetes REST API</li> <li><strong>pod 如何使用环境变量或者 DNS 来找到 API 服务器的定位, 与此类似的还有 Kubernetes 中定义的服务</strong></li> <li><strong>在 pod 中运行的应用如何验证与之交互的服务器的身份, 以及如何获得授权</strong></li> <li>如何通过使用 ambassador 容器使得其中运行的应用与 API 服务器的交互更加简单</li> <li>如何在短时间内使用客户端库与 Kubernetes 交互</li></ul> <p>在本章了解了如何与 API 服务器进行交互, 接下来的环节将了解更多的<strong>工作原理</strong>, 将在第 11 章进行学习. 在深入了解这方面的细节之前, 仍然需要了解另外两种 Kubernetes 资源: Deployment 和 StatefulSet, 将在接下来的两章展开这方面的内容.</p> <h4 id="_9-deployment-声明式地升级应用"><a href="#_9-deployment-声明式地升级应用" class="header-anchor">#</a> 9.Deployment:声明式地升级应用</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li><strong>使用新版本替换 pod</strong></li> <li><strong>升级已有的 pod</strong></li> <li><strong>使用 Deployment 资源升级 pod</strong></li> <li><strong>执行滚动升级</strong></li> <li><strong>出错时自动停止滚动升级 pod</strong></li> <li><strong>控制滚动升级比例</strong></li> <li><strong>回滚 pod 到上个版本</strong></li></ul> <p>现在已经知道如何将应用程序组件打包进容器, 将它们分组到 pod 中, 并为它们提供临时存储或持续化存储, 将密钥或配置文件注入, 并可以使 pod 之间相互通信. 这正是微服务化: 如何将一个大规模系统拆分成各个独立运行的组件. 除此之外, 还有什么呢?</p> <p>之后, 你将会<strong>需要升级你的应用程序</strong>. 这一章讲述了<mark><strong>如何升级在 Kubernetes 集群中运行的应用程序, 以及 Kubernetes 如何帮助你实现真正的零停机升级过程</strong></mark>. 升级操作可以通过使用 ReplicationController 或者 ReplicaSet 实现, <strong>但 Kubernetes 提供了另一种基于 ReplicaSet 的资源 Deployment, 并支持声明式地更新应用程序</strong>. 如果还不完全理解这里的意思, 请继续阅读, 并没有想象中那么复杂.</p> <h5 id="_9-1-更新运行在pod内的应用程序"><a href="#_9-1-更新运行在pod内的应用程序" class="header-anchor">#</a> 9.1 更新运行在pod内的应用程序</h5> <p>从一个简单的例子开始. 有一组 pod 实例为其他 pod 或外部客户端提供服务. 在本书中其他章节已经介绍过, 这些 pod 是由 ReplicationController 或 <strong>ReplicaSet</strong> 来创建和管理的. 客户端(运行在其他 pod 或外部的客户端程序)通过该 Service 访问 pod. 这就是 Kubernetes 中一个典型应用程序的运行方式(如图 9.1 所示).</p> <p><img src="/img/image-20240224111113-yqra14t.png" alt="image" title="图9.1 应用在 Kubernetes 中的基本架构"></p> <p>假设 pod 一开始<strong>使用 v1 版本的镜像</strong>运行第一个版本的应用. 然后开发了一个新版本的应用打包成镜像, 并将其推送到镜像仓库, 标记为 v2, <strong>接下来想用这个新版本替换所有的 pod. 由于 pod 在创建之后, 不允许直接修改镜像, 只能通过删除原有 pod 并使用新的镜像创建新的 pod 替换</strong>.</p> <p>有以下两种方法可以更新所有 pod:</p> <ul><li>直接删除所有现有的 pod, 然后创建新的 pod.</li> <li>也可以先创建新的 pod, 并等待它们成功运行之后, 再删除旧的 pod. 可以先创建所有新的 pod, 然后一次性删除所有旧的 pod, 或者按顺序创建新的 pod, 然后逐渐删除旧的 pod.</li></ul> <p>这两种方法各有优缺点. 第一种方法将会导致应用程序在一定的时间内<strong>不可用</strong>. 使用第二种方法, 应用程序需要支持两个版本同时对外提供服务. 如果应用程序使用数据库存储数据, 那么新版本不应该对原有的数据格式或者数据本身进行修改, 从而导致之前的版本运行异常.</p> <p>如何在 Kubernetes 中使用上述的两种更新方法呢? 首先, 来看看如何进行手动操作; 一旦了解了手动流程中涉及的要点之后, 将继续学习如何让 Kubernetes 自动执行更新操作.</p> <h6 id="_9-1-1-先删除旧版本pod再使用新版本pod替换"><a href="#_9-1-1-先删除旧版本pod再使用新版本pod替换" class="header-anchor">#</a> 9.1.1 先删除旧版本pod再使用新版本pod替换</h6> <p>你已经知道如何使用 ReplicationController 将原有 pod 实例替换成新版本的 pod. 并且 ReplicationController 内的 pod 模板一旦发生了更新之后, <strong>ReplicationController 会使用更新后的 pod 模板来创建新的实例</strong>.</p> <p><strong>如果你有一个 ReplicationController 来管理一组 v1 版本的 pod, 可以直接通过将 pod 模板修改为 v2 版本的镜像, 然后删除旧的 pod 实例</strong>. ReplicationController 会检测到当前没有 pod 匹配它的标签选择器, 便会创建新的实例. 整个过程如图 9.2 所示.</p> <p><img src="/img/image-20240227234343-d9ywt8s.png" alt="image" title="图9.2 修改 ReplicationController 中的 pod 模板来升级并删除原有的 pod"></p> <p>如果可以接受从删除旧的 pod 到启动新 pod 之间短暂的服务不可用, 那这将是更新一组 pod 的最简单方式.</p> <h6 id="_9-1-2-先创建新pod再删除旧版本pod"><a href="#_9-1-2-先创建新pod再删除旧版本pod" class="header-anchor">#</a> 9.1.2 先创建新pod再删除旧版本pod</h6> <p>如果短暂的服务不可用完全不能接受, 并且应用程序支持多个版本同时对外服务, 那么可以<strong>先创建新的 pod 再删除原有的 pod</strong>. 这会需要更多的硬件资源, 因为你将在短时间内同时运行两倍数量的 pod.</p> <p>与前面的方法相比较, 这个方法稍微复杂一点, 可以结合之前已经介绍过的 ReplicationController 和 Service 的概念进行使用.</p> <blockquote><p>从旧版本立即切换到新版本</p></blockquote> <p><strong>pod 通常通过 Service 来暴露</strong>. 在运行新版本的 pod 之前, <strong>Service 只将流量切换到初始版本的 pod. 一旦新版本的 pod 被创建并且正常运行之后, 就可以修改服务的标签选择器并将 Service 的流量切换到新的 pod</strong>. 整个过程如图 9.3 所示, 这就是所谓的<strong>蓝绿部署</strong>. 在切换之后, 一旦确定了新版本的功能运行正常, 就可以通过删除旧的 ReplicationController 来删除旧版本的 pod.</p> <p>注意: <strong>可以使用 kubectl set selector 命令来修改 Service 的 pod 选择器</strong>.</p> <p><img src="/img/image-20240224112551-7bxyjyh.png" alt="image" title="图9.3 将 Service 流量从旧的 pod 切换到新的 pod"></p> <blockquote><p>执行滚动升级操作</p></blockquote> <p>还可以<strong>执行滚动升级操作来逐步替代原有的 pod, 而不是同时创建所有新的 pod 并一并删除所有旧的 pod</strong>. 可以通过逐步对旧版本的 ReplicationController 进行缩容并对新版本的进行扩容, 来实现上述操作. 在这个过程中, 你希望服务的 pod 选择器<strong>同时包含新旧两个版本的 pod, 因此它将请求切换到这两组 pod</strong>, 如图 9.4 所示.</p> <p>手动执行滚动升级操作非常烦琐, 而且容易出错. 根据副本数量的不同, 需要以正确的顺序运行十几条甚至更多的命令来执行整个升级过程. 但是<strong>实际上 Kubernetes 可以实现仅仅通过一个命令来执行滚动升级</strong>. 在下一节会介绍如何执行这个操作.</p> <p><img src="/img/image-20240224112723-cbjf1rz.png" alt="image" title="图9.4 使用两个 ReplicationController 做滚动升级"></p> <h5 id="_9-2-使用replicationcontroller实现自动的滚动升级"><a href="#_9-2-使用replicationcontroller实现自动的滚动升级" class="header-anchor">#</a> 9.2 使用ReplicationController实现自动的滚动升级</h5> <p>不用手动地创建 Replicationcontroller 来执行滚动升级, 可以<strong>直接使用 kubectl 来执行</strong>. 使用 kubectl 执行升级会使整个升级过程更容易. 虽然这是一种<strong>相对过时</strong>的升级方式, 但是这里会先介绍一下, 因为它是第一种实现自动滚动升级的方式, 并且允许在不引入太多额外概念的情况下了解整个过程.</p> <h6 id="_9-2-1-运行第一个版本的应用"><a href="#_9-2-1-运行第一个版本的应用" class="header-anchor">#</a> 9.2.1 运行第一个版本的应用</h6> <p>在更新一个应用之前, 需要先部署好一个应用. 将使用第 2 章中创建的 kubia NodeJS 应用作为基础, 并稍微修改版本作为应用的初始版本. 它就是一个简单的 web 应用程序, 在 HTTP 响应中返回 pod 的主机名.</p> <blockquote><p>创建 v1 版本的应用</p></blockquote> <p>你将修改这个应用, 让它在响应中返回版本号, 以便区分应用构建的不同版本. 笔者已经构建并将应用镜像推送到 Docker Hub 的 <strong>luksa/kubia:v1</strong> 中. 下面的代码清单就是应用的源代码.</p> <p><strong>代码清单-9.1 v1版本的应用: v1/app.js</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token keyword">const</span> http <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'http'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> os <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'os'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">&quot;Kubia server starting...&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">var</span> <span class="token function-variable function">handler</span> <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span> response</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">&quot;Received request from &quot;</span> <span class="token operator">+</span> request<span class="token punctuation">.</span>connection<span class="token punctuation">.</span>remoteAddress<span class="token punctuation">)</span><span class="token punctuation">;</span>
  response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;This is v1 running in pod &quot;</span> <span class="token operator">+</span> os<span class="token punctuation">.</span><span class="token function">hostname</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
<span class="token keyword">var</span> www <span class="token operator">=</span> http<span class="token punctuation">.</span><span class="token function">createServer</span><span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span class="token punctuation">;</span>
www<span class="token punctuation">.</span><span class="token function">listen</span><span class="token punctuation">(</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><blockquote><p>使用单个 YAML 文件运行应用并通过 Service 暴露</p></blockquote> <p><strong>通过创建一个 ReplicationController 来运行应用程序, 并创建 LoadBalancer 服务将应用程序对外暴露</strong>. 接下来, 不是分别创建这两个资源, 而是为它们创建一个 YAML 文件, 并使用一个 kubectl create 命令调用 Kubernetes API. YAML manifest 可以使用包含三个横杠(---)的行来分隔多个对象, 如下面的代码清单所示.</p> <p><strong>代码清单-9.2 单个 YAML 文件同时包含一个 RC 和 一个 Service: kubia-rcand-service-v1.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ReplicationController
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>v1
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>            
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia    <span class="token comment"># Service指向所有由ReplicationController创建的pod</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">:</span>v1   <span class="token comment"># 使用ReplicationController来创建pod并允许实例</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> nodejs
<span class="token punctuation">---</span>              <span class="token comment"># yml文件可以包含多种资源定义, 并通过---来分行</span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> LoadBalancer
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>                     
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia    <span class="token comment"># Service指向所有由ReplicationController创建的pod</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p>这个 YAML 定义了一个名为 kubia-v1 的 ReplicationController 和一个名为 kubia 的 Service. 将此 YAML 发布到 Kubernetes 之后, <strong>三个 v1 pod 和负载均衡器都会开始工作</strong>. 如下面的代码清单所示, 可以通过查找服务的外部 IP 并使用 curl 命令来访问服务.</p> <p><strong>代码清单-9.3 查找到 Service IP 并使用 curl 循环调用服务接口</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get svc kubia
NAME      CLUSTER-IP     EXTERNAL-IP       PORT<span class="token punctuation">(</span>S<span class="token punctuation">)</span>         AGE
kubia     <span class="token number">10.3</span>.246.195   <span class="token number">130.211</span>.109.222   <span class="token number">80</span>:32143/TCP    5m
$ <span class="token keyword">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">curl</span> http://130.211.109.222<span class="token punctuation">;</span> <span class="token keyword">done</span>
This is v1 running <span class="token keyword">in</span> pod kubia-v1-qr192
This is v1 running <span class="token keyword">in</span> pod kubia-v1-kbtsk
This is v1 running <span class="token keyword">in</span> pod kubia-v1-qr192
This is v1 running <span class="token keyword">in</span> pod kubia-v1-2321o
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>注意: 如果使用 Minikube 或者其他不支持 LoadBalancer 的 Kubernetes 集群, 需要使用 Service 的节点端口方式来访问你的应用. 在第 5 章中有提到这一点.</p> <h6 id="_9-2-2-使用kubectl来执行滚动式升级"><a href="#_9-2-2-使用kubectl来执行滚动式升级" class="header-anchor">#</a> 9.2.2 使用kubectl来执行滚动式升级</h6> <p>接下来创建 v2 版本的应用, 修改之前的应用程序, 使得其请求返回 &quot;This is v2&quot;:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>response.end<span class="token punctuation">(</span><span class="token string">&quot;This is v2 running in pod &quot;</span> + os.hostname<span class="token punctuation">(</span><span class="token punctuation">)</span> + <span class="token string">&quot;<span class="token entity" title="\n">\n</span>&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个新的镜像已经推到了 Docker Hub 的 luksa/kubia:v2 中, 所以不需要自己从头构建这个镜像.</p> <blockquote><p>使用同样的 Tag 推送更新过后的镜像</p></blockquote> <p>虽然在开发过程中经常推送修改后的应用到同一个镜像 tag, 但是这种做法并不可取. 如果修改了 latest 的 tag 的话是可行的, 但如果使用一个不同的 tag 名(比如是 v1 而不是 lastest), 等计算节点拉取过镜像之后, 便会将镜像存储在节点上. 如果使用该镜像启动新的 pod, 便不会重新拉取镜像(至少这是默认的拉取镜像策略).</p> <p>这也意味着, <strong>如果将对更改过后的镜像推到相同的 tag, 会导致镜像不被重新拉取</strong>. 如果一个新的 pod 被调度到同一个节点上, Kubelet 直接使用旧的镜像版本来启动 pod. 另一方面, 没有运行过旧版本的节点将拉取并运行新镜像, 因此最后可能有两个不同版本的 pod 同时运行. 为了确保这种情况不会发生, 需要<strong>将容器的 imagePullPolicy 属性设置为 Always</strong>.</p> <p>另外默认的 imagePullPolicy 策略也依赖于镜像的 tag. 如果容器使用 latest 的 tag(显式指定或者不指定), 则 imagePullPolicy 默认为 Always, 但是如果容器指定了其他标记, 则策略默认为 IfNotPresent.</p> <p>当使用非 latest 的 tag 时, 如果对镜像进行更改而不更改 tag, 则需要正确设置 imagePullPolicy. 当然最好使用一个新的 tag 来更新镜像.</p> <p>保持 curl 循环运行的状态下打开另一个终端, 开始启动滚动升级. 你将运行 kubectl rolling-update 命令来执行升级操作. 指定需要替换的 ReplicationController, 以及为新的 ReplicationController 指定一个名称, 并指定你想要替换的新镜像. 下面的代码清单显示了滚动升级的完整命令.</p> <p><strong>代码清单-9.4 使用 kubectl 开始 ReplicationController 的滚动升级</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rolling-update kubia-v1 kubia-v2 <span class="token parameter variable">--image</span><span class="token operator">=</span>luksa/kubia:v2
Created kubia-v2
Scaling up kubia-v2 from <span class="token number">0</span> to <span class="token number">3</span>, scaling down kubia-v1 from <span class="token number">3</span> to <span class="token number">0</span> <span class="token punctuation">(</span>keep <span class="token number">3</span>
     pods available, don't exceed <span class="token number">4</span> pods<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>使用 kubia v2 版本应用来替换运行着 kubia-v1 的 ReplicationController, 将新的复制控制器命名为 kubia-v2, 并使用 luksa/kubia:v2 作为容器镜像.</p> <p>当你运行该命令时, 一个名为 kubia-v2 的新 ReplicationController 会立即被创建. 此时系统的状态如图 9.5 所示.</p> <p><img src="/img/image-20240227234412-8qegqsm.png" alt="image" title="图9.5 开始滚动升级之后的系统状态"></p> <p><strong>新的 ReplicationController 的 pod 模板引用了 luksa/kubia:v2 镜像, 并且其初始期望的副本数被设置为0</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-9.5 滚动升级过程中创建的新的 ReplicationController 描述</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl describe rc kubia<span class="token punctuation">-</span>v2
<span class="token key atrule">Name</span><span class="token punctuation">:</span>       kubia<span class="token punctuation">-</span>v2
<span class="token key atrule">Namespace</span><span class="token punctuation">:</span>  default
<span class="token key atrule">Image(s)</span><span class="token punctuation">:</span>   luksa/kubia<span class="token punctuation">:</span>v2        <span class="token comment"># 新的ReplicationController                                   1</span>
<span class="token key atrule">Selector</span><span class="token punctuation">:</span>   app=kubia<span class="token punctuation">,</span>deployment=757d16a0f02f6a5c387f2b5edb62b155
<span class="token key atrule">Labels</span><span class="token punctuation">:</span>     app=kubia
<span class="token key atrule">Replicas</span><span class="token punctuation">:</span>   0 current / 0 desired   <span class="token comment"># 期望副本数一开始被设置为0</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><blockquote><p>了解滚动升级前 kubectl 所执行的操作</p></blockquote> <p><strong>kubectl 通过复制 kubia-v1 的 ReplicationController 并在其 pod 模板中改变镜像版本</strong>. 如果仔细观察控制器的标签选择器, 会发现它也被做了修改. 它不仅包含一个简单的 app=kubia 标签, 而且还包含一个额外的 deployment 标签, 为了由这个 ReplicationController 管理, pod 必须具备这个标签.</p> <p>你可能已经知道, <strong>需要尽可能避免使用新的和旧的 ReplicationController 来管理同一组 pod</strong>. 但是即使新创建的 pod 添加了除额外的 deployment 标签以外, 还有 app=kubia 标签, 这是否也意味着它们会被第一个 ReplicationController 的选择器选中? 因为它的标签内也含有 app=kubia.</p> <p>其实在滚动升级过程中, 第一个 ReplicationController 的选择器也会被修改:</p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl describe rc kubia<span class="token punctuation">-</span>v1
<span class="token key atrule">Name</span><span class="token punctuation">:</span>       kubia<span class="token punctuation">-</span>v1
<span class="token key atrule">Namespace</span><span class="token punctuation">:</span>  default
<span class="token key atrule">Image(s)</span><span class="token punctuation">:</span>   luksa/kubia<span class="token punctuation">:</span>v1
<span class="token key atrule">Selector</span><span class="token punctuation">:</span>   app=kubia<span class="token punctuation">,</span>deployment=3ddd307978b502a5b975ed4045ae4964<span class="token punctuation">-</span>orig
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>但这是不是意味着第一个控制器现在选择不到 pod 呢? 因为之前由它创建的三个 pod 只包含 app=kubia 标签. 事实并不是这样, 因为在修改 ReplicationController 的选择器之前, kubectl 修改了当前 pod 的标签:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po --show-labels
NAME            READY  STATUS   RESTARTS  AGE  LABELS
kubia-v1-m33mv  <span class="token number">1</span>/1    Running  <span class="token number">0</span>         2m   <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia,deployment<span class="token operator">=</span>3ddd<span class="token punctuation">..</span>.
kubia-v1-nmzw9  <span class="token number">1</span>/1    Running  <span class="token number">0</span>         2m   <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia,deployment<span class="token operator">=</span>3ddd<span class="token punctuation">..</span>.
kubia-v1-cdtey  <span class="token number">1</span>/1    Running  <span class="token number">0</span>         2m   <span class="token assign-left variable">app</span><span class="token operator">=</span>kubia,deployment<span class="token operator">=</span>3ddd<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>如果觉得上述内容描述得太过复杂, 请看图 9.6, 其中显示了 pod, 标签, 两个 Replicationcontroller, 以及它们的 pod 标签选择器.</p> <p>kubectl 在开始伸缩服务前, 都会这么做. 设想手动执行滚动升级, 并且在升级过程中出现了问题, 这可能使 ReplicationController 删除所有正在为生产级别的用户提供服务的 pod!</p> <p><img src="/img/image-20240224124901-5hn82qj.png" alt="image" title="图9.6 滚动升级后新旧 ReplicationController 以及 pod 的详细状态"></p> <blockquote><p>通过伸缩两个 ReplicationController 将旧 pod 替换成新的 pod</p></blockquote> <p>设置完所有这些之后, kubectl 开始替换 pod. 首先将新的 Controller 扩展为 1, 新的 Controller 因此创建第一个 v2 pod, 然后 kubectl 将旧的 ReplicationController 缩小 1. 以下两行代码是 kubectl 输出的:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>Scaling kubia-v2 up to <span class="token number">1</span>
Scaling kubia-v1 down to <span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>由于 Service 针对的只是所有带有 app=kubia 标签的 pod, 所以应该可以看到--每进行几次循环访问, 就将 curl 请求的流量切换到新的 v2 pod</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>This is v2 running <span class="token keyword">in</span> pod kubia-v2-nmzw9     <span class="token comment"># 请求被切换到新版本的pod</span>
This is v1 running <span class="token keyword">in</span> pod kubia-v1-kbtsk
This is v1 running <span class="token keyword">in</span> pod kubia-v1-2321o
This is v2 running <span class="token keyword">in</span> pod kubia-v2-nmzw9     <span class="token comment"># 请求被切换到新版本的pod</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><img src="/img/image-20240224125920-shethsy.png" alt="image" title="图9.7 在升级过程中 Service 将请求同时切换到新旧版本的 pod"></p> <p>随着 kubectl 继续滚动升级, 开始看到越来越多的请求被切换到 v2 pod. 因为在升级过程中, v1 pod 不断被删除, 并被替换为运行新镜像的 pod. 最终, 最初的 ReplicationController 被伸缩到 0, 导致最后一个 v1 pod 被删除, 也意味着服务现在只通过 v2 pod 提供. 此时, kubectl 将删除原始的 ReplicationController 完成升级过程, 如下面的代码清单所示.</p> <p><strong>代码清单-9.6 kubectl 执行滚动升级的最终步骤</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
Scaling kubia-v2 up to <span class="token number">2</span>
Scaling kubia-v1 down to <span class="token number">1</span>
Scaling kubia-v2 up to <span class="token number">3</span>
Scaling kubia-v1 down to <span class="token number">0</span>
Update succeeded. Deleting kubia-v1
replicationcontroller <span class="token string">&quot;kubia-v1&quot;</span> rolling updated to <span class="token string">&quot;kubia-v2&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>现在只剩下 kubia-v2 的 ReplicationController 和三个 v2 pod. 在整个升级过程中, <strong>每次发出的请求都有相应的响应, 通过一次滚动升级, 服务一直保持可用状态</strong>.</p> <h6 id="_9-2-3-为什么kubectl-rolling-update已经过时"><a href="#_9-2-3-为什么kubectl-rolling-update已经过时" class="header-anchor">#</a> 9.2.3 为什么kubectl rolling-update已经过时</h6> <p>在本节的开头, 提到了一种比通过 kubectl rolling-update 更好的方式进行升级. 那这个过程有什么不合理的地方呢?</p> <p>首先, <strong>这个过程会直接修改创建的对象</strong>. 直接更新 pod 和 ReplicationController 的标签并不符合之前创建时的预期. 还有更重要的一点是, <strong>kubectl 只是执行滚动升级过程中所有这些步骤的客户端</strong>.</p> <p>当触发滚动更新时, 可以使用 --v 选项打开详细的日志并能看到这一点:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rolling-update kubia-v1 kubia-v2 <span class="token parameter variable">--image</span><span class="token operator">=</span>luksa/kubia:v2 <span class="token parameter variable">--v</span> <span class="token number">6</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>提示: 使用 --v 6 选项会提高日志级别使得所有 kubectl 发起的到 API 服务器的请求都会被输出.</p> <p>使用这个选项, kubectl 会输出所有发送至 Kubernetes API 服务器的 HTTP 请求. 你会看到一个 PUT 请求:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/api/v1/namespaces/default/replicationcontrollers/kubia-v1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>它是<strong>表示 kubia-v1 ReplicationController 资源的 RESTful URL</strong>. 这些请求减少了 ReplicationController 的副本数, 这<strong>表明伸缩的请求是由 kubectl 客户端执行的, 而不是由 Kubernetes master 执行的</strong>.</p> <p>提示: 使用详细日志模式运行其他 kubectl 命令, 将会看到 kubectl 和 API 服务器之前的更多通信细节.</p> <p><mark><strong>但是为什么由客户端执行升级过程, 而不是服务端执行是不好的呢? 在上述的例子中, 升级过程看起来很顺利, 但是如果在 kubectl 执行升级时失去了网络连接, 升级进程将会中断. pod 和 ReplicationController 最终会处于中间状态</strong></mark>.</p> <p>这样的升级不符合预期的原因还有一个. 在本书中一直强调 Kubernetes 是如何通过不断地收敛达到期望的系统状态的. 这就是 pod 的部署方式, 以及 pod 的伸缩方式. 直接使用期望副本数来伸缩 pod 而不是通过手动地删除一个 pod 或者增加一个 pod.</p> <p>同样, <mark><strong>只需要在 pod 定义中更改所期望的镜像 tag, 并让 Kubernetes 用运行新镜像的 pod 替换旧的 pod. 正是这一点推动了一种称为 Deployment 的新资源的引入, 这种资源正是现在 Kubernetes 中部署应用程序的首选方式</strong></mark>.</p> <h5 id="_9-3-使用deployment声明式地升级应用"><a href="#_9-3-使用deployment声明式地升级应用" class="header-anchor">#</a> 9.3 使用Deployment声明式地升级应用</h5> <p><mark><strong>Deployment 是一种更高阶资源, 用于部署应用程序并以声明的方式升级应用</strong></mark>, 而不是通过 ReplicationController 或 ReplicaSet 进行部署, 它们都被认为是更底层的概念.</p> <p><strong>当创建一个 Deployment 时, ReplicaSet 资源也会随之创建(最终会有更多的资源被创建)</strong> . 在第 4 章中, ReplicaSet 是新一代的 ReplicationController, 并推荐使用它替代 ReplicationController 来复制和管理 pod. <strong>在使用 Deployment 时, 实际的 pod 是由 Deployment 的 Replicaset 创建和管理的, 而不是由 Deployment 直接创建和管理的</strong>(如图 9.8 所示).</p> <p><img src="/img/image-20240224130915-fas3r8d.png" alt="image" title="图9.8 Deployment 由 ReplicaSet 组成, 并由它接管 Deployment 的 pod"></p> <p>你可能想知道为什么要在 ReplicationController 或 ReplicaSet 上引入另一个对象来使整个过程变得更复杂, 因为它们已经足够保证一组 pod 的实例正常运行了. 如 9.2 节中的滚动升级示例所示, 在升级应用程序时, 需要引入一个额外的 ReplicationController, 并协调两个 Controller, 使它们再<strong>根据彼此不断修改</strong>, 而不会造成干扰. <strong>所以需要另一个资源用来协调</strong>. Deployment 资源就是用来负责处理这个问题的(不是 Deployment 资源本身, 而是在 Kubernetes 控制层上运行的控制器进程. 会在第 11 章中再做介绍).</p> <p><strong>使用 Deployment 可以更容易地更新应用程序, 因为可以直接定义单个 Deployment 资源所需达到的状态, 并让 Kubernetes 处理中间的状态</strong>, 接下来将会介绍整个过程.</p> <h6 id="_9-3-1-创建一个deployment"><a href="#_9-3-1-创建一个deployment" class="header-anchor">#</a> 9.3.1 创建一个Deployment</h6> <p>创建 Deployment 与创建 ReplicationController 并没有任何区别. <mark><strong>Deployment 也是由标签选择器, 期望副数和 pod 模板组成的. 此外, 它还包含另一个字段, 指定一个部署策略, 该策略定义在修改 Deployment 资源时应该如何执行更新</strong></mark>.</p> <blockquote><p>创建一个 Deployment Manifest</p></blockquote> <p>下面看一下如何使用本章前面的 kubia-v1 ReplicationController 示例, 并对其稍作修改, 使其描述一个 Deployment, 而不是一个 ReplicationController. 这只需要三个简单的更改, 下面的代码清单显示了修改后的 YAML.</p> <p><strong>代码清单-9.7 Deployment 定义: kubia-deployment-v1.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1beta1     <span class="token comment"># Deployment属于 apps API组, 版本为v1beta1</span>
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment        <span class="token comment"># 资源类型修改为Deployment</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia           <span class="token comment"># Deployment的名称中不再需要包含版本号</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">:</span>v1
        <span class="token key atrule">name</span><span class="token punctuation">:</span> nodejs
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>注意: 你将在 extensions/v1beta1 中找到 Deployment 资源的旧版本, 在 apps/v1beta2 中找到新版本, 它们有不同的必需字段和不同的默认值. 请注意, kubectl explain 显示了旧版本.</p> <p>因为之前的 ReplicationController <strong>只维护和管理了一个特定版本的 pod, 并需要命名为 kubia-v1</strong>. 另一方面, 一个 Deployment 资源高于版本本身. <strong>Deployment 可以同时管理多个版本的 pod, 所以在命名时不需要指定应用的版本号</strong>.</p> <blockquote><p>创建 Deployment 资源</p></blockquote> <p>在创建这个 Deployment 之前, 请确保删除仍在运行的任何 ReplicationController 和 pod, 但是暂时保留 kubia Service. 可以使用 --all 选项来删除所有的 ReplicationController:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete rc <span class="token parameter variable">--all</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>此时已经可以创建一个 Deployment 了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-deployment-v1.yaml <span class="token parameter variable">--record</span>
deployment <span class="token string">&quot;kubia&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 确保在创建时使用了 --record 选项. 这个选项会记录历史版本号, 在之后的操作中非常有用.</p> <blockquote><p>展示 Deployment 滚动过程中的状态</p></blockquote> <p>可以直接<strong>使用 kubectl get deployment 和 kubectl describe deployment 命令来查看 Deployment 的详细信息</strong>, 但是还有另外一个命令, 专门用于查看部署状态:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout status deployment kubia
deployment kubia successfully rolled out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>通过上述命令查看到, Deployment 已经完成了滚动升级, 可以看到三个 pod 副本已经正常创建和运行了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
kubia-1506449474-otnnh   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          14s
kubia-1506449474-vmn7s   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          14s
kubia-1506449474-xis6m   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          14s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><blockquote><p>了解 Deployment 如何创建 Replicaset 以及 pod</p></blockquote> <p>注意这些 pod 的命名, 之前当使用 ReplicationController 创建 pod 时, 它们的名称是由 Controller 的名称加上一个运行时生成的随机字符串(例如 kubia-v1-m33mv)组成的. 现在由 Deployment 创建的三个 pod 名称中均包含一个<strong>额外的数字</strong>. 那是什么呢?</p> <p><strong>这个数字实际上对应 Deployment 和 ReplicaSet 中的 pod 模板的哈希值</strong>. 如前所述, Deployment 不能直接管理 pod. 相反, 它创建了 ReplicaSet 来管理 pod. 所以来看看 Deployment 创建的 ReplicaSet 是什么样子的.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get replicasets
NAME               DESIRED   CURRENT   AGE
kubia-1506449474   <span class="token number">3</span>         <span class="token number">3</span>         10s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>ReplicaSet 的名称中也包含了其 pod 模板的哈希值</strong>. 之后的篇幅也会介绍, <strong>Deployment 会创建多个 ReplicaSet, 用来对应和管理一个版本的 pod 模板. 像这样使用 pod 模板的哈希值, 可以让 Deployment 始终对给定版本的 pod 模板创建相同的(或使用已有的)ReplicaSet</strong>.</p> <blockquote><p>通过 Service 访问 pod</p></blockquote> <p>ReplicaSet 创建了三个副本并成功运行以后, 因为<strong>新的 pod 的标签和 Service 的标签选择器相匹配, 因此可以直接通过之前创建的 Service 来访问它们</strong>.</p> <p>至此可能还没有从根本上解释, 为什么更推荐使用 Deployment 而不是直接使用 ReplicationController. 另一方面看, 创建一个 Deployment 的难度和成本也并没有比 ReplicationController 更高. 后面将针对这个 Deployment 做一些操作, 并从根本上了解 Deployment 的优点和强大之处. 接下来会介绍如何通过 Deployment 资源升级应用, 并对比通过 ReplicationController 升级应用的区别, 你就会明白这一点.</p> <h6 id="_9-3-2-升级deployment"><a href="#_9-3-2-升级deployment" class="header-anchor">#</a> 9.3.2 升级Deployment</h6> <p>前面提到, 当使用 ReplicationController 部署应用时, 必须通过运行 kubectl rolling-update 显式地告诉 Kubernetes 来执行更新, 甚至必须为新的 ReplicationController 指定名称来替换旧的资源. Kubernetes 会将所有原来的 pod 替换为新的 pod, 并在结束后删除原有的 ReplicationController. 在整个过程中必须保持终端处于打开状态, 让 kubectl 完成滚动升级.</p> <p>接下来更新 Deployment 的方式和上述的流程相比, 只需修改 Deployment 资源中定义的 pod 模板, Kubernetes 会自动将实际的系统状态收敛为资源中定义的状态. 类似于将 ReplicationController 或 ReplicaSet 扩容或者缩容, 升级需要做的就是在部署的 pod 模板中修改镜像的 tag,Kubernetes 会收敛系统, 匹配期望的状态.</p> <blockquote><p>不同的 Deployment 升级策略</p></blockquote> <p>实际上, <strong>如何达到新的系统状态的过程是由 Deployment 的升级策略决定的, 默认策略是执行滚动更新(策略名为 RollingUpdate)</strong> . 另一种策略为 <strong>Recreate</strong>, 它会一次性删除所有旧版本的 pod, 然后创建新的 pod, 整个行为类似于修改 ReplicationController 的 pod 模板, 然后删除所有的 pod.</p> <p>Recreate 策略在删除旧的 pod 之后才开始创建新的 pod. 如果应用程序不支持多个版本同时对外提供服务, 需要在启动新版本之前<strong>完全停用旧版本</strong>, 那么需要使用这种策略. 但是使用这种策略的话, 会导致应用程序出现短暂的不可用.</p> <p><strong>RollingUpdate 策略会渐进地删除旧的 pod, 与此同时创建新的 pod, 使应用程序在整个升级过程中都处于可用状态, 并确保其处理请求的能力没有因为升级而有所影响</strong>. 这就是 Deployment 默认使用的升级策略. 升级过程中 pod 数量可以在期望副本数的一定区间内浮动, 并且其上限和下限是可配置的. 如果应用能够支持多个版本同时对外提供服务, 则推荐使用这个策略来升级应用.</p> <blockquote><p>演示如何减慢滚动升级速度</p></blockquote> <p>在接下来的练习中, 将使用 RollingUpdate 策略, 但是需要略微减慢滚动升级的速度, 以便观察升级过程确实是以滚动的方式执行的. 可以通过在 Deployment 上设置 <strong>minReadySeconds</strong> 属性来实现. 将在本章末尾解释这个属性的作用. 现在, 使用 kubectl patch 命令将其设置为 10 秒.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl patch deployment kubia <span class="token parameter variable">-p</span> <span class="token string">'{&quot;spec&quot;: {&quot;minReadySeconds&quot;: 10}}'</span>
<span class="token string">&quot;kubia&quot;</span> patched
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>提示: kubectl patch 对于修改单个或者少量资源属性非常有用, 不需要再通过编辑器编辑.</p> <p><strong>使用 patch 命令更改 Deployment 的自有属性, 并不会导致 pod 的任何更新, 因为 pod 模板并没有被修改</strong>. 更改其他 Deployment 的属性, 比如所需的副本数或部署策略, 也不会触发滚动升级, 现有运行中的 pod 也不会受其影响.</p> <blockquote><p>触发滚动升级</p></blockquote> <p>如果想要跟踪更新过程中应用的运行状况, 需要先在另一个终端中再次运行 curl 循环, 以查看请求的返回情况(需要将 IP 替换为 Service 实际暴露 IP):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token keyword">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">curl</span> http://130.211.109.222<span class="token punctuation">;</span> <span class="token keyword">done</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>要触发滚动升级, 需要将 pod 镜像修改为 luksa/kubia:v2. 和直接编辑 Deployment 资源的 YAML 文件或使用 patch 命令更改镜像有所不同, 将<mark><strong>使用 kubectl set image 命令来更改任何包含容器资源的镜像(ReplicationController, ReplicaSet, Deployment 等)</strong></mark> . 将使用这个命令来修改 Deployment:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">set</span> image deployment kubia <span class="token assign-left variable">nodejs</span><span class="token operator">=</span>luksa/kubia:v2
deployment <span class="token string">&quot;kubia&quot;</span> image updated
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>当执行完这个命令, kubia Deployment 的 pod 模板内的镜像会被更改为 luksa/kubia:v2(从 v1 更改而来), 如图 9.9 所示.</p> <p><img src="/img/image-20240224131705-tfgb6fq.png" alt="image" title="图9.9 为 Deployment 内的 pod 模板指定新的镜像"></p> <blockquote><p>修改 Deployment 或其他资源的不同方式</p></blockquote> <p>在本书中已经了解了几种不同的修改对象方式, 现在把它们列在一起来重新回顾一下.</p> <p><strong>表-9.1 在 Kubernetes 中修改资源</strong></p> <p><img src="/img/image-20240224131753-izifcox.png" alt="image"></p> <p>这些方式在操作 Deployment 资源时效果都是一样的. 它们无非就是<strong>修改 Deployment 的规格定义, 修改后会触发滚动升级过程</strong>.</p> <p>如果循环执行了 curl 命令, 将看到一开始请求只是切换到 v1 pod; 然后越来越多的请求切换到 v2 pod 中, 最后所有的 v1 pod 都被删除, 请求全部切换到 v2.pod. 这和 kubectl 的滚动更新过程非常相似.</p> <blockquote><p>Deployment 的优点</p></blockquote> <p>回顾一下刚才的过程, 通过更改 Deployment 资源中的 pod 模板, 应用程序已经被升级为一个更新的版本——仅仅通过更改一个字段而已!</p> <p>这个升级过程是<strong>由运行在 Kubernetes 上的一个控制器处理和完成的</strong>, 而不再是运行 kubectl rolling-update 命令(它的升级是由 kubectl 客户端执行的). <strong>让 Kubernetes 的控制器接管使得整个升级过程变得更加简单可靠</strong>.</p> <p>注意: 如果 Deployment 中的 pod 模板引用了一个 ConfigMap(或 Secret), 那么更改 ConfigMap 资源本身将不会触发升级操作. 如果真的需要修改应用程序的配置并想触发更新的话, 可以通过创建一个新的 ConfigMap 并修改 pod 模板引用新的 ConfigMap.</p> <p>Deployment 背后完成的整个升级过程和执行 kubectl rolling-update 命令非常相似. <strong>一个新的 ReplicaSet 会被创建然后慢慢扩容, 同时之前版本的 Replicaset 会慢慢缩容至 0</strong>(初始状态和最终状态如图 9.10 所示).</p> <p><img src="/img/image-20240224131910-t9y5ybu.png" alt="image" title="图9.10 滚动升级开始和结束时 Deployment 状态"></p> <p>可以通过下面的命令列出所有新旧 ReplicaSet:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rs
NAME               DESIRED   CURRENT   AGE
kubia-1506449474   <span class="token number">0</span>         <span class="token number">0</span>         24m
kubia-1581357123   <span class="token number">3</span>         <span class="token number">3</span>         23m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>与 ReplicationController 类似, 所有新的 pod 现在都由新的 ReplicaSet 管理. 与以前不同的是, 旧的 ReplicaSet 仍然会<strong>被保留</strong>, 而旧的 ReplicationController 会在<strong>滚动升级过程结束后被删除</strong>. 之后马上会介绍这个被保留的旧 ReplicaSet 的用处.</p> <p>因为并没有直接创建 ReplicaSet, 所以这里的 ReplicaSet 本身并不需要用户去关心和维护. 所有操作都是在 Deployment 资源上完成的, 底层的 ReplicaSet 只是实现的细节. 和处理与维护多个 ReplicationController 相比, 管理单个 Deployment 对象要容易得多.</p> <p>尽管这种差异在滚动升级中可能不太明显, 但是如果<strong>在滚动升级过程中出错或遇到问题</strong>, 就可以明显看出两种方案的差异. 下面来模拟一个错误.</p> <h6 id="_9-3-3-回滚deployment"><a href="#_9-3-3-回滚deployment" class="header-anchor">#</a> 9.3.3 回滚Deployment</h6> <p>现阶段, 应用使用 v2 版本的镜像运行, 接下来会先准备 v3 版本的镜像.</p> <blockquote><p>创建 v3 版本的应用程序</p></blockquote> <p>在 v3 版本中, 将引入一个 bug, 使应用程序只能正确地处理前四个请求. 第五个请求之后的所有请求将返回一个内部服务器错误(HTTP 状态代码 500). 你将通过在处理程序函数的开头添加 if 语句来模拟这个 bug. 下面的代码清单显示了修改后的代码, 所有需要更改的地方都用粗体显示.</p> <p><strong>代码清单-9.8 v3 版本的应用(运行出错版本): v3/app.js</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token keyword">const</span> http <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'http'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> os <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'os'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> requestCount <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">&quot;Kubia server starting...&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> <span class="token function-variable function">handler</span> <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span> response</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">&quot;Received request from &quot;</span> <span class="token operator">+</span> request<span class="token punctuation">.</span>connection<span class="token punctuation">.</span>remoteAddress<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">++</span>requestCount <span class="token operator">&gt;=</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;Some internal error has occurred! This is pod &quot;</span> <span class="token operator">+</span> os<span class="token punctuation">.</span><span class="token function">hostname</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
  response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;This is v3 running in pod &quot;</span> <span class="token operator">+</span> os<span class="token punctuation">.</span><span class="token function">hostname</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> www <span class="token operator">=</span> http<span class="token punctuation">.</span><span class="token function">createServer</span><span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span class="token punctuation">;</span>
www<span class="token punctuation">.</span><span class="token function">listen</span><span class="token punctuation">(</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>如你所见, 在第 5 个请求和所有后续请求中, 返回一个状态码为 500 的错误, 错误消息 &quot;Some internal error has occurred...&quot;.</p> <blockquote><p>部署v3版本</p></blockquote> <p>笔者已经将 v3 版本的镜像推送至 <strong>luksa/kubia:v3</strong>. 可以直接修改 Deployment 中镜像的字段来部署新的版本:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">set</span> image deployment kubia nodejs<span class="token operator">=</span>luksa<span class="token operator">/</span>kubia<span class="token operator">:</span>v3
deployment <span class="token string">&quot;kubia&quot;</span> image updated
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以通过运行 <code>kubectl rollout status</code>​ 来观察整个升级过程:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl rollout status deployment kubia
Waiting <span class="token keyword">for</span> rollout to finish<span class="token operator">:</span> <span class="token number">1</span> out <span class="token keyword">of</span> <span class="token number">3</span> <span class="token keyword">new</span> <span class="token class-name">replicas</span> have been updated<span class="token operator">...</span>
Waiting <span class="token keyword">for</span> rollout to finish<span class="token operator">:</span> <span class="token number">2</span> out <span class="token keyword">of</span> <span class="token number">3</span> <span class="token keyword">new</span> <span class="token class-name">replicas</span> have been updated<span class="token operator">...</span>
Waiting <span class="token keyword">for</span> rollout to finish<span class="token operator">:</span> <span class="token number">1</span> old replicas are pending termination<span class="token operator">...</span>
deployment <span class="token string">&quot;kubia&quot;</span> successfully rolled out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>新的版本已经开始运行. 接下来的代码清单会显示, 在发送几个请求之后, <strong>客户端开始收到服务端返回的错误</strong>.</p> <p><strong>代码清单-9.9 访问出错的 v3版本</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ <span class="token keyword">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword">do</span> curl http<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token number">130.211</span><span class="token number">.109</span><span class="token number">.222</span><span class="token punctuation">;</span> done
This is v3 running <span class="token keyword">in</span> pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>lalmx
This is v3 running <span class="token keyword">in</span> pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>bz35w
This is v3 running <span class="token keyword">in</span> pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>w0voh
<span class="token operator">...</span>
This is v3 running <span class="token keyword">in</span> pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>w0voh
Some internal error has occurred<span class="token operator">!</span> This is pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>bz35w
This is v3 running <span class="token keyword">in</span> pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>w0voh
Some internal error has occurred<span class="token operator">!</span> This is pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>lalmx
This is v3 running <span class="token keyword">in</span> pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>w0voh
Some internal error has occurred<span class="token operator">!</span> This is pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>lalmx
Some internal error has occurred<span class="token operator">!</span> This is pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>bz35w
Some internal error has occurred<span class="token operator">!</span> This is pod kubia<span class="token operator">-</span><span class="token number">1914148340</span><span class="token operator">-</span>w0voh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><blockquote><p>回滚升级</p></blockquote> <p>不能让你的用户感知到升级导致的内部服务器错误, 因此需要快速处理. 在 9.3.6 节中, 你将看到<strong>如何自动停止出错版本的滚动升级</strong>, 但是现在先看一下<strong>如何手动停止</strong>. 比较好的是, Deployment 可以非常容易地<strong>回滚到先前部署的版本</strong>, 它可以让 Kubernetes 取消最后一次部署的 Deployment:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl rollout undo deployment kubia
deployment <span class="token string">&quot;kubia&quot;</span> rolled back
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>Deployment 会被回滚到上一个版本.</p> <p>提示: undo 命令也可以在滚动升级过程中运行, 并直接停止滚动升级. 在升级过程中已创建的 pod 会被删除并被老版本的 pod 替代.</p> <blockquote><p>显示 Deployment 的滚动升级历史</p></blockquote> <p>回滚升级之所以可以这么快地完成, 是因为 <strong>Deployment 始终保持着升级的版本历史记录</strong>. 之后也会看到, 历史版本号会被<strong>保存在 ReplicaSet 中</strong>. 滚动升级成功后, 老版本的 ReplicaSet 也不会被删掉, 这也使得回滚操作可以<strong>回滚到任何一个历史版本</strong>, 而不仅仅是上一个版本. 可以使用 kubectl rollout history 来显示升级的版本:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl rollout history deployment kubia
deployments <span class="token string">&quot;kubia&quot;</span><span class="token operator">:</span>
<span class="token constant">REVISION</span>    <span class="token constant">CHANGE</span><span class="token operator">-</span><span class="token constant">CAUSE</span>
<span class="token number">2</span>           kubectl <span class="token keyword">set</span> image deployment kubia nodejs<span class="token operator">=</span>luksa<span class="token operator">/</span>kubia<span class="token operator">:</span>v2
<span class="token number">3</span>           kubectl <span class="token keyword">set</span> image deployment kubia nodejs<span class="token operator">=</span>luksa<span class="token operator">/</span>kubia<span class="token operator">:</span>v3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>还记得创建 Deployment 时的 --record 参数吗? 如果不给定这个参数, 版本历史中的 CHANGE-CAUSE 这一栏会为空. 这也会使用户很难辨别每次的版本做了哪些修改.</p> <blockquote><p>回滚到一个特定的 Deployment 版本</p></blockquote> <p>通过在 undo 命令中指定一个特定的版本号, 便可以回滚到那个特定的版本. 例如, 如果想回滚到第一个版本, 可以执行下述命令:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl rollout undo deployment kubia <span class="token operator">--</span>to<span class="token operator">-</span>revision<span class="token operator">=</span><span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>还记得第一次修改 Deployment 时留下的 ReplicaSet 吗? 这个 ReplicaSet 便表示 Deployment 的第一次修改版本. 由 Deployment 创建的所有 ReplicaSet 表示完整的修改版本历史, 如图 9.11 所示. <strong>每个 ReplicaSet 都用特定的版本号来保存 Deployment 的完整信息</strong>, 所以不应该手动删除 ReplicaSet. 如果这么做便会丢失 Deployment 的历史版本记录而导致无法回滚.</p> <p><img src="/img/image-20240224132700-8nwxdk0.png" alt="image" title="图9.11 Deployment 的 ReplicaSet 也保存版本历史"></p> <p>旧版本的 ReplicaSet 过多会导致 ReplicaSet 列表过于混乱, 可以通过指定 Deployment 的 <strong>revisionHistoryLimit</strong> 属性来限制历史版本数量. 默认值是 2, 所以正常情况下在版本列表里只有当前版本和上一个版本(以及只保留了当前和上一个 ReplicaSet), 所有再早之前的 ReplicaSet 都会被删除.</p> <p>注意: extensions/v1beta1 版本的 Deployment 的 revisionHistoryLimit 没有值, 在 apps/v1beta2 版本中, 这个默认值是 10.</p> <h6 id="_9-3-4-控制滚动升级速率"><a href="#_9-3-4-控制滚动升级速率" class="header-anchor">#</a> 9.3.4 控制滚动升级速率</h6> <p>当执行 kubectl rollout status 命令来观察升级到 v3 的过程时, 会看到第一个 pod 被新创建, 等到它运行时, 一个旧的 pod 会被删除, 然后又一个新的 pod 被创建, 直到再没有新的 pod 可以更新. 创建新 pod 和删除旧 pod 的方式可以通过<strong>配置滚动更新策略内的两个属性</strong>.</p> <blockquote><p>介绍滚动升级策略的 maxSurge 和 maxUnavailable 属性</p></blockquote> <p>在 Deployment 的滚动升级期间, <strong>有两个属性会决定一次替换多少个 pod: maxSurge 和 maxUnavailable</strong>. 可以通过 Deployment 的 strategy 字段下 rollingUpdate 的子属性来配置, 如下面的代码清单所示.</p> <p><strong>代码清单-9.10 为 rollingUpdate 策略指定参数</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token literal-property property">spec</span><span class="token operator">:</span>
  <span class="token literal-property property">strategy</span><span class="token operator">:</span>
    <span class="token literal-property property">rollingUpdate</span><span class="token operator">:</span>
      <span class="token literal-property property">maxSurge</span><span class="token operator">:</span> <span class="token number">1</span>
      <span class="token literal-property property">maxUnavailable</span><span class="token operator">:</span> <span class="token number">0</span>
    <span class="token literal-property property">type</span><span class="token operator">:</span> RollingUpdate
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这些参数的含义会在表 9.2 中详细解释.</p> <p><strong>表-9.2 控制滚动升级速率的属性</strong></p> <p><img src="/img/image-20240224132818-l8zm6q1.png" alt="image"></p> <p>由于在之前场景中, 设置的期望副本数为 3, 上述的两个属性都设置为 25%, maxSurge 允许最多 pod 数量达到 4, 同时 maxUnavailable 不允许出现任何不可用的 pod(也就是说<strong>三个 pod 必须一直处于可运行状态</strong>), 如图9.12所示.</p> <p><img src="/img/image-20240227234513-385977m.png" alt="image" title="图9.12 Deployment 滚动升级时的三个副本数和默认 maxSurge, maxUnavailable 配置"></p> <blockquote><p>了解 maxUnavailable 属性</p></blockquote> <p>extensions/v1beta1 版本的 Deployment 使用不一样的默认值, maxSurge 和 maxUnavailable 会被设置为1, 而不是 25%. 对于三个副本的情况, maxSurge 还是和之前一样, 但是 maxUnavailable 是不一样的(是 1 而不是 0), 这使得滚动升级的过程稍有不同, 如图 9.13 所示.</p> <p><img src="/img/image-20240227234531-zuvsbpr.png" alt="image" title="图9.13 Deployment 滚动升级时 maxSurge=1且 maxUnavailable=1"></p> <p>在这种情况下, 一个副本处于不可用状态, 如果期望副本数为 3, 则只需要两个副本处于可用状态. 这就是为什么上述滚动升级过程中会立即删除一个 pod 并创建两个新的 pod. 这确保了两个 pod 是可用的并且不超过 pod 允许的最大数量(在这种情况下, 最大数量是 4, 三个加上 maxSurges 的一个. 一旦两个新的 pod 处于可用状态, 剩下的两个旧的 pod 就会被删除了.</p> <p>这里相对有点难以理解, <strong>主要是 maxUnavailable 这个属性, 它表示最大不可用 pod 数量</strong>. 但是如果仔细查看前一个图的第二列, 即使 maxUnavailable 被设置为 1, 可以看到两个 pod 处于不可用的状态.</p> <p>重要的是<mark><strong>要知道 maxUnavailable 是相对于期望副本数而言的</strong></mark>. 如果 replica 的数量设置为 3, maxUnavailable 设置为 1, 则更新过程中必须保持<strong>至少两个(3-1)pod 始终处于可用状态</strong>, 而不可用 pod 数量可以超过一个.</p> <h6 id="_9-3-5-暂停滚动升级"><a href="#_9-3-5-暂停滚动升级" class="header-anchor">#</a> 9.3.5 暂停滚动升级</h6> <p>在经历了 v3 版本应用的糟糕体验之后, 假设现在已经修复了这个错误, 并推送了第四个版本的镜像. 但是你还是有点担心像之前那样将其滚动升级到所有的 pod 上. 你需要的是<strong>在现有的 v2 pod 之外运行一个 v4 pod, 并查看一小部分用户请求的处理情况. 如果一旦确定符合预期, 就可以用新的 pod 替换所有旧的 pod</strong>.</p> <p>可以通过直接运行一个额外的 pod 或通过一个额外的 Deployment, ReplicationController 或 ReplicaSet 来实现上述的需求. 但是通过 Deployment 自身的一个选项, 便可以让部署过程暂停, 方便用户在继续完成滚动升级之前来验证新的版本的行为是否都符合预期.</p> <blockquote><p>暂停滚动升级</p></blockquote> <p>笔者已经事先准备好了 v4 的镜像, 所以可以直接将镜像修改为 <code>luksa/kubia:v4</code>​ 并触发滚动更新, 然后立马(在几秒之内)暂停滚动更新:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">set</span> image deployment kubia nodejs<span class="token operator">=</span>luksa<span class="token operator">/</span>kubia<span class="token operator">:</span>v4
deployment <span class="token string">&quot;kubia&quot;</span> image updated

$ kubectl rollout pause deployment kubia
deployment <span class="token string">&quot;kubia&quot;</span> paused
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p><strong>一个新的 pod 会被创建, 与此同时所有旧的 pod 还在运行</strong>. 一旦新的 pod 成功运行, 服务的一部分请求将被切换到新的 pod. 这样相当于运行了一个<mark><strong>金丝雀版本</strong></mark>. 金丝雀发布是一种可以将应用程序的出错版本和其影响到的用户的风险化为最小的技术. <strong>与其直接向每个用户发布新版本, 不如用新版本替换一个或一小部分的 pod</strong>. 通过这种方式, 在升级的初期只有少数用户会访问新版本. 验证新版本是否正常工作之后, 可以将剩余的 pod 继续升级或者回滚到上一个的版本.</p> <blockquote><p>恢复滚动升级</p></blockquote> <p>在上述例子中, 通过暂停滚动升级过程, 只有一小部分客户端请求会切换到 v4 pod, 而大多数请求依然仍只会切换到 v3 pod. 一旦确认新版本能够正常工作, 就可以恢复滚动升级, 用新版本 pod 替换所有旧版本的 pod:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl rollout resume deployment kubia
deployment <span class="token string">&quot;kubia&quot;</span> resumed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在滚动升级过程中, 想要在一个确切的位置暂停滚动升级目前还无法做到, 以后可能会有一种新的升级策略来自动完成上面的需求. <strong>但目前想要进行金丝雀发布的正确方式是, 使用两个不同的 Deployment 并同时调整它们对应的 pod 数量</strong>.</p> <blockquote><p>使用暂停功能来停止滚动升级</p></blockquote> <p>暂停部署还可以用于阻止更新 Deployment 而自动触发的滚动升级过程, 用户可以对 Deployment 进行多次更改, 并在完成所有更改后才恢复滚动升级. 一旦更改完毕, 则恢复并启动滚动更新过程.</p> <p>注意: 如果部署被暂停, 那么在恢复部署之前, 撤销命令不会撤销它.</p> <h6 id="_9-3-6-阻止出错版本的滚动升级"><a href="#_9-3-6-阻止出错版本的滚动升级" class="header-anchor">#</a> 9.3.6 阻止出错版本的滚动升级</h6> <p>在结束本章之前, 还会讨论 Deployment 资源的另一个属性. 还记得在 9.3.2 节开始时在 Deployment 中设置的 <strong>minReadySeconds 属性</strong>吗? 使用它来<strong>减慢滚动升级速率</strong>, 使用这个参数之后确实执行了滚动更新, 并且没有一次性替换所有的 pod. minReadySeconds 的主要功能是避免部署出错版本的应用, 而不只是单纯地减慢部署的速度.</p> <blockquote><p>了解 minReadySeconds 的用处</p></blockquote> <p><strong>minReadySeconds 属性指定新创建的 pod 至少要成功运行多久之后, 才能将其视为可用</strong>. 在 pod 可用之前, 滚动升级的过程不会继续(还记得 maxUnavailable 属性吗?). 当所有容器的就绪探针返回成功时, pod 就被标记为就绪状态. 如果一个新的 pod 运行出错, 就绪探针返回失败, 如果一个新的 pod 运行出错, 并且在 minReadySeconds 时间内它的就绪探针出现了失败, 那么<strong>新版本的滚动升级将被阻止</strong>.</p> <p><strong>使用这个属性可以通过让 Kubernetes 在 pod 就绪之后继续等待 10 秒, 然后继续执行滚动升级, 来减缓滚动升级的过程</strong>. 通常情况下需要将 minReadySeconds 设置为更高的值, 以确保 pod 在它们真正开始接收实际流量之后可以持续保持就绪状态.</p> <p>当然在将 pod 部署到生产环境之前, <strong>需要在测试和预发布环境中对 pod 进行测试</strong>. 但使用 minReadySeconds 就像一个安全气囊, 保证了即使不小心将 bug 发布到生产环境的情况下, 也不会导致更大规模的问题.</p> <p>使用正确配置的就绪探针和适当的 minReadySeconds 值, Kubernetes 将预先阻止发布部署带有 bug 的 v3 版本. 下面会展示如何实现.</p> <blockquote><p>配置就绪探针来阻止全部 v3 版本的滚动部署</p></blockquote> <p>再一次部署 v3 版本, 但这一次会为 pod 配置正确的就绪探针. 由于当前部署的是 v4 版本, 所以在开始之前再次回滚到 v2 版本, 来模拟假设是第一次升级到 v3. 当然也可以直接从 v4 升级到 v3, 但是后续假设都是先回滚到了 v2 版本.</p> <p>与之前只更新 pod 模板中的镜像不同的是, 还将同时为<strong>容器添加就绪探针</strong>. 之前因为就绪探针一直未被定义, 所以容器和 pod 都处于就绪状态, 即使应用程序本身并没有真正就绪甚至是正在返回错误. Kubernetes 是无法知道应用本身是否出现了故障, 也不会将未就绪信息暴露给客户端.</p> <p>同时更改镜像并添加就绪探针, 则可以使用 <strong>kubectl apply 命令</strong>. 使用下面的 YAML 来更新 Deployment(将它另存为 kubian-deployment-v3-withreadinesscheck.yaml), 如下面的代码清单所示.</p> <p><strong>代码清单-9.11 Deployment 包含一个就绪探针: kubia-deployment-v3with-readinesscheck.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>          <span class="token comment"># 设置minReadySeconds的值为10</span>
  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>
    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>
      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>
      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>       <span class="token comment"># 设置maxUnavailable的值为0来确保升级过程中pod被挨个替换</span>
    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">:</span>v3
        <span class="token key atrule">name</span><span class="token punctuation">:</span> nodejs
        <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>
          <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">1</span>   <span class="token comment"># 定义一个就绪探针并每隔一秒执行一次</span>
          <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>           <span class="token comment"># 就绪探针会执行HTTP GET请求到容器</span>
            <span class="token key atrule">path</span><span class="token punctuation">:</span> /             
            <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>          
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><blockquote><p>使用 kubectl apply 升级 Deployment</p></blockquote> <p>可以使用如下方式直接使用 kubectl apply 来升级 Deployment:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl apply <span class="token parameter variable">-f</span> kubia-deployment-v3-with-readinesscheck.yaml
deployment <span class="token string">&quot;kubia&quot;</span> configured
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>apply 命令可以用 YAML 文件中声明的字段来更新 Deployment. 不仅更新镜像, 而且还添加了就绪探针, 以及在 YAML 中添加或修改的其他声明</strong>. 如果新的 YAML 也包含 replicas 字段, 当它与现有 Deployment 中的数量不一致时, 那么 apply 操作也会对 Deployment 进行扩容.</p> <p>提示: 使用 kubectl apply 更新 Deployment 时如果不期望副本数被更改, 则不用在 YAML 文件中添加 replicas 这个字段.</p> <p>运行 apply 命令会自动开始滚动升级过程, 可以再一次运行 rollout status 命令来查看升级过程:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout status deployment kubia
Waiting <span class="token keyword">for</span> rollout to finish: <span class="token number">1</span> out of <span class="token number">3</span> new replicas have been updated<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>因为升级状态显示一个新的 pod 已经创建, 一小部分流量应该也会切换到这个 pod. 可以通过下面的命令看到:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token keyword">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">curl</span> http://130.211.109.222<span class="token punctuation">;</span> <span class="token keyword">done</span>
This is v2 running <span class="token keyword">in</span> pod kubia-1765119474-jvslk
This is v2 running <span class="token keyword">in</span> pod kubia-1765119474-jvslk
This is v2 running <span class="token keyword">in</span> pod kubia-1765119474-xk5g3
This is v2 running <span class="token keyword">in</span> pod kubia-1765119474-pmb26
This is v2 running <span class="token keyword">in</span> pod kubia-1765119474-pmb26
This is v2 running <span class="token keyword">in</span> pod kubia-1765119474-xk5g3
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>结果显示并没有请求被切换到 v3 pod, 为什么呢? 先列出所有 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
kubia-1163142519-7ws0i   <span class="token number">0</span>/1       Running   <span class="token number">0</span>          30s
kubia-1765119474-jvslk   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          9m
kubia-1765119474-pmb26   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          9m
kubia-1765119474-xk5g3   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          8m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到, <strong>有一个 pod 并没有处于就绪状态</strong>, 这是为什么呢?</p> <blockquote><p>就绪探针如何阻止出错版本的滚动升级</p></blockquote> <p>当新的 pod 启动时, 就绪探针会每隔一秒发起请求(在 pod spec 中, 就绪探针的间隔被设置为 1 秒). 在就绪探针发起第五个请求的时候会出现失败, 因为应用从第五个请求开始一直返回 HTTP 状态码 500.</p> <p>因此, <strong>pod 会从 Service 的 endpoint 中移除</strong>(参见图9.14). 当执行 curl 循环请求服务时, pod 已经被标记为<strong>未就绪</strong>. 这就解释了为什么 curl 发出的请求不会切换到新的 pod. 这正是符合预期的, 因为你不希望客户端流量会切换到一个无法正常工作的 pod.</p> <p><img src="/img/image-20240224133733-gadbs0q.png" alt="image" title="图9.14 Deployment 因为新创建 pod 的就绪探针失败而被阻止"></p> <p>rollout status 命令显示只有一个新副本启动, 之后<strong>滚动升级过程没有再继续下去, 因为新的 pod 一直处于不可用状态</strong>. 即使变为就绪状态之后, 也至少需要保持 10 秒, 才是真正可用的. 在这之前滚动升级过程将不再创建任何新的 pod, 因为当前 maxUnavailable 属性设置为 0, 所以也<strong>不会删除</strong>任何原始的 pod.</p> <p><strong>实际上部署过程自动被阻止是一件好事</strong>. 如果继续用新的 pod 替换旧的 pod, 那么最终服务将处于完全不能工作的状态, 就和当时没有使用就绪探针的情况下滚动升级 v3 版本时出现的结果一样. 但是添加了就绪探针之后, 升级出错程序而对用户造成的影响面不会过大, 对比之前替换所有 pod 的方式, 只有一小部分用户受到了影响.</p> <p>提示: <strong>如果只定义就绪探针没有正确设置 minReadySeconds, 一旦有一次就绪探针调用成功, 便会认为新的 pod 已经处于可用状态. 因此最好适当地设置 minReadySeconds 的值</strong>.</p> <blockquote><p>为滚动升级配置 deadline</p></blockquote> <p>默认情况下, 在 10 分钟内不能完成滚动升级的话, 将被视为失败. 如果运行 kubectl describe deployment 命令, 将会显示一条 ProgressDeadlineExceeded 的记录, 如下面的代码清单所示.</p> <p><strong>代码清单-9.12 使用 kubectl describe 查看 Deployment 的详细情况</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe deploy kubia
Name:                   kubia
<span class="token punctuation">..</span>.
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   False   ProgressDeadlineExceeded 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>判定 Deployment 滚动升级失败的<strong>超时时间</strong>, 可以通过设定 Deployment spec 中的 progressDeadlineSeconds 来指定.</p> <p>注意: extensions/v1beta1 版本不会设置默认的 deadline.</p> <blockquote><p>取消出错版本的滚动升级</p></blockquote> <p>因为滚动升级过程不再继续, 所以只能通过 rollout undo 命令来取消滚动升级:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl rollout undo deployment kubia
deployment <span class="token string">&quot;kubia&quot;</span> rolled back
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 在后续的版本中, <strong>如果达到了 progressDeadlineSeconds 指定的时间, 则滚动升级过程会自动取消</strong>.</p> <h5 id="_9-4-本章小结"><a href="#_9-4-本章小结" class="header-anchor">#</a> 9.4 本章小结</h5> <p>本章展示了如何通过声明的方式在 Kubernetes 中部署和更新应用. 阅读了这一章之后, 你应该知道如何:</p> <ul><li>使用 ReplicationController 管理 pod 并执行滚动升级</li> <li><strong>创建 Deployment, 而不是底层的 ReplicationController 和 ReplicaSet</strong></li> <li><strong>通过更新 Deployment 定义的 pod 模板来更新 pod</strong></li> <li><strong>回滚 Deployment 到上个版本或历史版本列表中的任意一个历史版本</strong></li> <li>中止 Deployment 滚动升级</li> <li><strong>滚动升级时, 在所有 pod 被替换之前, 暂停 Deployment 滚动升级查看单个新版本的实例状况</strong></li> <li><strong>通过 maxSurge 和 maxUnavailable 属性控制滚动升级的速率</strong></li> <li><strong>使用 minReadySeconds 和 就绪探针自动避免错误版本的升级</strong></li></ul> <p>现在你已经知道如何部署和管理从相同的 pod 模板创建的一组 pod, 共享同一个持久性存储. 甚至了解如何以声明的方式升级它们. <strong>但是如果是运行一组 pod 并且每个实例都需要使用自己的持久存储该怎么办</strong>. 这就是接下来这一章的主题.</p> <h4 id="_10-statefulset-部署有状态的多副本应用"><a href="#_10-statefulset-部署有状态的多副本应用" class="header-anchor">#</a> 10.StatefulSet:部署有状态的多副本应用</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li><strong>部署有状态集群应用</strong></li> <li><strong>为每个副本 pod 实例提供独立存储</strong></li> <li><strong>保证 pod 副本有固定的名字和主机名</strong></li> <li><strong>按预期顺序启停 pod 副本</strong></li> <li>通过 DNS SRV 记录查询伙伴节点</li></ul> <p>现在知道了如何<strong>运行一个单实例 pod 和无状态的多副本 pod</strong>, 还有如何通过持久化存储运行一个有状态 pod. 可以运行几个多副本的 web-server pod 实例, 运行一个提供持久化存储的单数据库 pod 实例, 这个持久化存储可以是简单的 pod 卷, 也可以是一个绑定到持久卷上的持久卷声明. <strong>但是是否可以通过 ReplicaSet 来复制数据库 pod 呢</strong>?</p> <h5 id="_10-1-复制有状态pod"><a href="#_10-1-复制有状态pod" class="header-anchor">#</a> 10.1 复制有状态pod</h5> <p><strong>ReplicaSet 通过一个 pod 模板创建多个 pod 副本. 这些副本除了它们的</strong>​<mark><strong>名字和 IP 地址不同</strong></mark>​<strong>外, 没有别的差异. 如果 pod 模板里描述了一个关联到特定持久卷声明的数据卷, 那么 ReplicaSet 的</strong>​<mark><strong>所有副本都将共享这个持久卷声明, 也就是绑定到同持久卷声明, 也就是绑到同一个声明的持久卷</strong></mark>(如下图 10.1 所示).</p> <p><img src="/img/image-20240224134857-seq15ma.png" alt="image" title="图-10.1 ReplicaSet 里的所有 pod 共享相同的持久卷声明和持久卷"></p> <p>因为是在 pod 模板里关联声明的, 又会依据 pod 模板创建多个 pod 副本, 则不能对每个副本都指定独立的持久卷声明. 所以<strong>也不能通过一个 ReplicaSet 来运行一个每个实例都需要独立存储的分布式数据存储服务, 至少通过单个 ReplicaSet 是做不到的</strong>. 老实说, 之前学习到的所有 API 对象都不能提供这样的数据存储服务, 还需要其他的对象.</p> <h6 id="_10-1-1-运行每个实例都有单独存储的多副本"><a href="#_10-1-1-运行每个实例都有单独存储的多副本" class="header-anchor">#</a> 10.1.1 运行每个实例都有单独存储的多副本</h6> <p>那<mark><strong>如何运行一个 pod 的多个副本, 让每个 pod 都有独立的存储卷</strong></mark>呢? ReplicaSet 会依据一个 pod 创建一致的副本, 所以不能通过它们来达到目的, 那你可以使用什么呢?</p> <blockquote><p>手动创建 pod</p></blockquote> <p>可以手动创建多个 pod, 每个 pod 使用一个独立的持久卷声明, 但是因为没有一个 ReplicaSet 在后面对应它们, 所以需要手动管理它们. 当有的 pod 消失后(比如节点故障), 需要手动创建它们. 因此这不是一个好的选择.</p> <blockquote><p>一个 pod 实例对应一个 ReplicaSet</p></blockquote> <p>与直接创建不同, 可以<strong>创建多个 ReplicaSet, 每个 ReplicaSet 的副本数设为 1,</strong>  做到 pod 和 ReplicaSet 的一一对应, 为每个 ReplicaSet 的 pod 模板关联一个专属的持久卷声明(如图 10.2 所示).</p> <p>尽管这种方法能保证在节点故障或者 pod 误删时能自动重新调度创建, 但是与单个 ReplicaSet 相比, 它还是显得比较笨重的. 例如, 在这种情况下要如何伸缩 pod? 扩容的话, 必须重新创建新的 ReplicaSet.</p> <p><img src="/img/image-20240224135031-px2rtct.png" alt="image" title="图10.2 每个 pod 实例对应一个 ReplicaSet"></p> <p>所以说使用多个 ReplicaSet 也不是最好的方案. 那是否可以创建一个 ReplicaSet, 即使在共享一个存储卷的情况下, 让每个 pod 实例都独立保持自己的持久化状态呢?</p> <blockquote><p>使用同一数据卷中的不同目录</p></blockquote> <p>一个比较取巧的做法是: 所有 pod 共享同一数据卷, 但是每个 pod 在数据卷中使用不同的数据目录(如图 10.3 所示).</p> <p><img src="/img/image-20240224135101-0yj4zcq.png" alt="image" title="图10.3 每个 pod 中的应用使用同一数据卷中的不同目录"></p> <p>因为不能在一个 pod 模板中差异化配置 pod 副本, 所以<strong>不能指定一个实例使用哪个特定目录</strong>! 但是可以让每个实例自动选择(或创建)一个别的实例还没有使用的数据目录. 这种方案要求实例之间相互协作, 其正确性很难保证, 同时共享存储也会成为整个应用的性能瓶颈.</p> <h6 id="_10-1-2-每个pod都提供稳定的标识"><a href="#_10-1-2-每个pod都提供稳定的标识" class="header-anchor">#</a> 10.1.2 每个pod都提供稳定的标识</h6> <p>除了上面说的存储需求, 集群应用也会<strong>要求每一个实例拥有生命周期内唯一标识</strong>. pod 可以随时被删掉, 然后被新的 pod 替代. 当一个 ReplicaSet 中的 pod 被替换时, 尽管新的 pod 也可能使用被删掉 pod 数据卷中的数据, 但它却是拥有全新主机名和 IP 的崭新 pod. <strong>在一些应用中, 当启动的实例拥有完全新的网络标识, 但还使用旧实例的数据时, 很可能引起问题</strong>.</p> <p>为什么一些应用<strong>需要维护一个稳定的网络标识</strong>呢? 这个需求在<strong>有状态的分布式应用</strong>中很普遍. 这类应用要求管理者在每个集群成员的配置文件中列出所有其他集群成员和它们的 IP 地址(或主机名). 但是在 Kubernetes 中, 每次重新调度一个 pod, 这个新的 pod 就有一个新的主机名和 IP 地址, 这样就要求当集群中任何一个成员被重新调度后, 整个应用集群都需要重新配置.</p> <blockquote><p>每个 pod 实例配置单独的 Service</p></blockquote> <p>一个比较取巧的做法是: 针对集群中的每个成员实例, 都创建一个独立的 Kubernetes Service 来提供稳定的网络地址. 因为服务 IP 是固定的, 可以在配置文件中指定集群成员对应的服务 IP(而不是 pod IP).</p> <p>这种做法跟之前提到的一种方法类似: <strong>为每个成员创建一个 ReplicaSet, 并配置独立存储</strong>. 把这两种方法结合起来就构成如图 10.4 所示的结构(额外添加一个访问集群所有成员的服务, 因为需要它来服务集群中的客户端).</p> <p><img src="/img/image-20240224135233-l7giypn.png" alt="image" title="图10.4 每个 pod 对应一个 Service 和 ReplicaSet 提供稳定的网络地址, 每个 pod 配置一个独立的数据卷"></p> <p>这种解决方案不仅令人厌恶, 而且它也不是一个完美的解决办法. 每个单独的 pod 没法知道它对应的 Service(所以也无法知道对应的稳定 IP), 所以它们不能在别的 pod 里通过服务 IP 自行注册.</p> <p>幸运的是, Kubernetes 提供了这类需求的完美解决方案. <strong>在 Kubernetes 中运行这类特定需求应用的最简单的办法就是通过 Statefulset</strong>.</p> <h5 id="_10-2-了解statefulset"><a href="#_10-2-了解statefulset" class="header-anchor">#</a> 10.2 了解Statefulset</h5> <p><mark><strong>可以创建一个 Statefulset 资源替代 ReplicaSet 来运行这类 pod. 它是专门定制的一类应用, 这类应用中每一个实例都是不可替代的个体, 都拥有稳定的名字和状态</strong></mark>.</p> <h6 id="_10-2-1-对比statefulset和replicaset"><a href="#_10-2-1-对比statefulset和replicaset" class="header-anchor">#</a> 10.2.1 对比Statefulset和ReplicaSet</h6> <p>要很好地理解 Statefulset 的用途, 最好先与 ReplicaSet 或 ReplicationControllers 对比一下. 首先拿一个通用的类比来解释它们.</p> <blockquote><p>通过宠物与牛的类比来理解有状态</p></blockquote> <p>你可能已经听说过宠物与牛的类比. 如果没有, 先简单介绍一下. 可以把应用看作宠物或牛.</p> <p>注意: Statefulset 最初被称为 PetSet, 这个名字来源于宠物与牛的类比.</p> <p>我们倾向于把<strong>应用看作宠物</strong>, 给每个实例起一个名字, 细心照顾每个实例. 但是也许把它们看成牛更为合适, 并不需要对单独的实例有太多关心. 这样就可以非常方便地替换掉不健康的实例, 就跟农场主替换掉一头生病的牛一样.</p> <p><strong>对于无状态的应用实例来说, 行为非常像农场里的牛</strong>. 一个实例挂掉后并没什么影响, 可以创建一个新实例, 而让用户完全无感知.</p> <p>另一方面, <strong>有状态的应用的一个实例更像一个宠物. 若一只宠物死掉, 不能买到一只完全一样的, 而不让用户感知到</strong>. 若要替换掉这只宠物, 需要找到一只行为举止与之完全一致的宠物. 对应用来说, 意味着新的实例需要拥有跟旧的案例完全一致的状态和标识.</p> <blockquote><p>Statefulset 与 ReplicaSet 或 ReplicationController 的对比</p></blockquote> <p>RelicaSet 或 ReplicationController 管理的 pod 副本比较像牛, 这是因为它们都是<strong>无状态</strong>的, 任何时候它们都可以被一个全新的 pod 替换. 然而<mark><strong>有状态的 pod 需要不同的方法, 当一个有状态的 pod 挂掉后(或者它所在的节点故障), 这个 pod 实例需要在别的节点上重建, 但是新的实例必须与被替换的实例拥有相同的名称, 网络标识和状态. 这就是 StatefulSet 如何管理 pod 的</strong></mark>.</p> <p><strong>Statefulset 保证了 pod 在重新调度后保留它们的标识和状态</strong>. 它让你方便地扩容, 缩容. 与 ReplicaSet 类似, Statefulset 也会指定期望的副本个数, 它决定了在同一时间内运行的宠物的数量. 与 ReplicaSet 类似, pod 也是依据 Statefulset 的 pod 模板创建的(想象一下曲奇饼干模板). 与 ReplicaSet 不同的是, <strong>Statefulset 创建的 pod 副本并不是完全一样的. 每个 pod 都可以拥有一组独立的数据卷(持久化状态)而有所区别. 另外 &quot;宠物&quot; pod 的名字都是规律的(固定的), 而不是每个新 pod 都随机获取一个名字</strong>.</p> <h6 id="_10-2-2-提供稳定的网络标识"><a href="#_10-2-2-提供稳定的网络标识" class="header-anchor">#</a> 10.2.2 提供稳定的网络标识</h6> <p>一个 Statefulset 创建的每个 pod 都有一个<strong>从零开始</strong>的顺序索引, 这个会体现在 pod 的名称和主机名上, 同样还会体现在 pod 对应的固定存储上. 这些 pod 的名称则是可预知的, 因为它是由 Statefulset 的名称加该实例的顺序索引值组成的. 不同于 pod 随机生成一个名称, 这样有规则的 pod 名称是很方便管理的, 如图 10.5 所示.</p> <p><img src="/img/image-20240224140731-qsjbvgi.png" alt="image" title="图10.5 与 ReplicaSet 不同, 由 Statefulset 创建的 pod 拥有规则的名称(和主机名)"></p> <blockquote><p>控制服务介绍</p></blockquote> <p>让 pod 拥有可预知的名称和主机名并不是全部, 与普通的 pod 不一样的是, <strong>有状态的 pod 有时候需要通过其主机名来定位</strong>, 而无状态的 pod 则不需要, 因为每个无状态的 pod 都是一样的, 在需要的时候随便选择一个即可. 但对于有状态的 pod 来说, 因为它们都是彼此不同的(比如拥有不同的状态), 通常希望操作的是其中特定的一个.</p> <p>基于以上原因, <strong>一个 Statefulset 通常要求你创建一个用来记录每个 pod 网络标记的 headless Service. 通过这个 Service, 每个 pod 将拥有独立的 DNS 记录, 这样集群里它的伙伴或者客户端可以通过主机名方便地找到它</strong>. 比如说, 一个属于 default 命名空间, 名为 foo 的控制服务, 它的一个 pod 名称为 A-0, 那么可以通过下面的完整域名来访问它: a-0.foo.default.svc.cluster.local. 而在 ReplicaSet 中这样是行不通的.</p> <p>另外, 也可以<strong>通过 DNS 服务</strong>, 查找域名 foo.default.svc.cluster.local 对应的所有 SRV 记录, 获取一个 Statefulset 中所有 pod 的名称. 将在 10.4 节中介绍 SRV 记录, 解释如何通过它来发现一个 Statefulset 中的所有成员.</p> <blockquote><p>替换消失的宠物</p></blockquote> <p>当一个 Statefulset 管理的一个 pod <strong>实例消失</strong>后(pod 所在节点发生故障, 或有人手动删除 pod), Statefulset 会<strong>保证重启一个新的 pod 实例替换它</strong>, 这与 ReplicaSet 类似. 但与 ReplicaSet 不同的是, <strong>新的 pod 会拥有与之前 pod 完全一致的名称和主机名</strong>(ReplicaSet 和 Statefulset 的差异如图 10.6 所示).</p> <p><img src="/img/image-20240224140924-4pt7ys8.png" alt="image" title="图10.6 Statefulset 使用标识完全一致的新的 pod 替换, ReplicaSet 则是使用一个不相干的新的 pod 替换"></p> <p>如之前了解的那样, <strong>pod 运行在哪个节点上并不重要, 新的 pod 并不一定会调度到相同的节点上. 对于有状态的 pod 来说也是这样, 即使新的 pod 被调度到一个不同的节点, 也同样可以通过主机名来访问</strong>.</p> <blockquote><p>扩缩容 Statefulset</p></blockquote> <p>扩容一个 Statefulset 会使用<strong>下一个还没用到的顺序索引值创建一个新的 pod 实例</strong>. 比如, 要把一个 Statefulset 从两个实例扩容到三个实例, 那么新实例的索引值就会是 2(现有实例使用的索引值为 0 和 1).</p> <p>当缩容一个 Statefulset 时, 比较好的是很明确哪个 pod 将要被删除. 作为对比, ReplicaSet 的缩容操作则不同, 不知道哪个实例会被删除, 也不能指定先删除哪个实例(也许这个功能会在将来实现). 缩容一个 Statefulset 将会<strong>最先删除最高索引值的实例</strong>(如图 10.7 所示), 所以缩容的结果是可预知的.</p> <p><img src="/img/image-20240224140950-roe0jp4.png" alt="image" title="图10.7 缩容一个 Statefulset 将会最先删除最高索引值的实例"></p> <p><mark><strong>因为 Statefulset 缩容任何时候只会操作一个 pod 实例, 所以有状态应用的缩容不会很迅速</strong></mark>. 举例来说, 一个分布式存储应用若同时下线多个节点, 则可能导致其数据丢失. 比如说一个数据项副本数设置为 2 的数据存储应用, 若同时有两个节点下线, 一份数据记录就会丢失, 如果它正好保存在这两个节点上. 若缩容是线性的, 则分布式存储应用就有时间把丢失的副本复制到其他节点, 保证数据不会丢失.</p> <p>基于以上原因, <mark><strong>Statefulset 在有实例不健康的情况下是不允许做缩容操作的</strong></mark>. 若一个实例是不健康的, 而这时再缩容一个实例的话, 也就意味着你实际上同时失去了两个集群成员.</p> <h6 id="_10-2-3-为每个有状态实例提供稳定的专属存储"><a href="#_10-2-3-为每个有状态实例提供稳定的专属存储" class="header-anchor">#</a> 10.2.3 为每个有状态实例提供稳定的专属存储</h6> <p>你已经知道了 Statefulset 如何保证一个有状态的 pod 拥有稳定的标识, 那存储呢? <strong>一个有状态的 pod 需要拥有自己的存储, 即使该有状态的 pod 被重新调度(新的 pod 与之前 pod 的标识完全一致), 新的实例也必须挂载着相同的存储</strong>. 那 Statefulset 是如何做到这一点的呢?</p> <p>很明显, 有状态的 pod 的存储必须是<strong>持久</strong>的, 并且<strong>与 pod 解耦</strong>. 在第 6 章中学习了持久卷和持久卷声明, 通过<strong>在 pod 中关联一个持久卷声明的名称, 就可以为 pod 提供持久化存储</strong>. 因为<mark><strong>持久卷声明与持久卷是一对一的关系, 所以每个 Statefulset 的 pod 都需要关联到不同的持久卷声明, 与独自的持久卷相对应</strong></mark>. 因为所有的 pod 实例都是依据一个相同的 pod 模板创建的, 那它们是如何关联到不同的持久卷是的呢? 并且由谁来创建这些持久卷是呢? 当然你肯定不想手在动创建 Statefulset 之前, 依据 pod 的个数创建相同数量的持久卷量. 当然不用这么做!</p> <blockquote><p>在 pod 模板中添加卷声明模板</p></blockquote> <p><strong>像 Statefulset 创建 pod 一样, Statefulset 也需要创建持久卷声明. 所以一个 Statefulset 可以拥有一个或多个卷声明模板, 这些持久卷声明会在创建 pod 前创建出来, 绑定到一个 pod 实例上</strong>(如图 10.8 所示).</p> <p><img src="/img/image-20240224141012-eg2ults.png" alt="image" title="图10.8 一个 Statefulset 创建 pod 和持久卷声明"></p> <p>声明的持久卷既可以通过 administrator 用户预先创建出来, 也可以如第 6 章所述, 由持久卷的动态供应机制实时创建出来.</p> <blockquote><p>持久卷的创建和删除</p></blockquote> <p>扩容 StatefulSet 增加一个副本数时, 会创建两个或更多的 API 对象(一个 pod 和与之关联的一个或多个持久卷声明). 但是对缩容来说, 则只会删除一个 pod, 而<strong>遗留下之前创建的声明</strong>. 当你知道一个声明被删除会发生什么的话, 你就明白为什么这么做了. 因为当一个声明被删除后, 与之绑定的持久卷就会被回收或删除, 则其上面的数据就会丢失.</p> <p>因为有状态的 pod 是用来运行有状态应用的, 所以其在数据卷上存储的数据非常重要, 在 Statefulset 缩容时删除这个声明将是灾难性的, 特别是对于 Statefulset 来说, 缩容就像减少其 replicas 数值一样简单. 基于这个原因, <strong>当需要释放特定的持久卷时, 需要手动删除对应的持久卷声明</strong>.</p> <blockquote><p>重新挂载持久卷声明到相同 pod 的新实例上</p></blockquote> <p><strong>因为缩容 Statefulset 时会保留持久卷声明, 所以在随后的扩容操作中, 新的 pod 实例会使用绑定在持久卷上的相同声明和其上的数据</strong>(如图 10.9 所示). 当因为误操作而缩容一个 Statefulset 后, 可以做一次扩容来弥补自己的过失, 新的 pod 实例会运行到与之前<strong>完全一致的状态</strong>(名字也是一样的).</p> <p><img src="/img/image-20240224141042-d26xldk.png" alt="image" title="图10.9 Statefulset 缩容时不删除持久卷声明, 扩容时会重新挂载上"></p> <h6 id="_10-2-4-statefulset的保障"><a href="#_10-2-4-statefulset的保障" class="header-anchor">#</a> 10.2.4 Statefulset的保障</h6> <p>如之前描述的, Statefulset 的行为与 ReplicaSet 或 ReplicationController 是不一样的. Statefulset 不仅拥有稳定的标记和独立的存储, 它的 pod 还有其他的一些保障.</p> <blockquote><p>稳定标识和独立存储的影响</p></blockquote> <p>通常来说, 无状态的 pod 是可以替代的, 而有状态的 pod 则不行. 之前已经描述了一个有状态的 pod 总是会被一个完全一致的 pod 替换(两者有相同的名称, 主机名和存储等). 这个替换发生在 Kubernetes 发现旧的 pod 不存在时(例如手动删除这个 pod).</p> <p>那么当 Kubernetes 不能确定一个 pod 的状态时呢? 如果它创建一个完全一致的 pod, 那系统中就会有两个完全一致的 pod 在同时运行. 这两个 pod 会绑定到相同的存储, 所以这两个相同标记的进程会同时写相同的文件. 对于 ReplicaSet 的 pod 来说, 这不是问题, 因为应用本来就是设计为在相同的文件上工作的. 并且我们知道 ReplicaSet 会以一个随机的标识来创建 pod, 所以<strong>不可能存在两个相同标识的进程同时运行</strong>.</p> <blockquote><p>介绍 Statefulset 的 at-most-one 的语义</p></blockquote> <p><strong>Kubernetes 必须保证两个拥有相同标记和绑定相同持久卷声明的有状态的 pod 实例不会同时运行. 一个 Statefulset 必须保证有状态的 pod 实例的 at-most-one 语义</strong>.</p> <p>也就是说一个 Statefulset 必须在准确确认一个 pod 不再运行后, 才会去创建它的替换 pod. 这对如何处理节点故障有很大的影响, 这个会在本章后面详细介绍. 在做这些之前, 需要先创建一个 Statefulset, 看看它是如何工作的.</p> <h5 id="_10-3-使用statefulset"><a href="#_10-3-使用statefulset" class="header-anchor">#</a> 10.3 使用Statefulset</h5> <p>为了恰当地展示 Statefulset 的行为, 将会创建一个<strong>小的集群数据存储</strong>. 没有太多功能, 就像石器时代的一个数据存储.</p> <h6 id="_10-3-1-创建应用和容器镜像"><a href="#_10-3-1-创建应用和容器镜像" class="header-anchor">#</a> 10.3.1 创建应用和容器镜像</h6> <p>这里将使用书中一直使用的 kubia 应用作为基础来扩展它, 达到它的<strong>每个 pod 实例都能用来存储和接收一个数据项</strong>.</p> <p>下面列举了数据存储的关键代码.</p> <p><strong>代码清单-10.1 一个简单的有状态应用: kubia-pet-image/app.js</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token operator">...</span>
<span class="token keyword">const</span> dataFile <span class="token operator">=</span> <span class="token string">&quot;/var/data/kubia.txt&quot;</span><span class="token punctuation">;</span>
<span class="token operator">...</span>
<span class="token keyword">var</span> <span class="token function-variable function">handler</span> <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span> response</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>request<span class="token punctuation">.</span>method <span class="token operator">==</span> <span class="token string">'POST'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">var</span> file <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">createWriteStream</span><span class="token punctuation">(</span>dataFile<span class="token punctuation">)</span><span class="token punctuation">;</span>   # 在<span class="token constant">POST</span>请求中<span class="token punctuation">,</span> 把请求的body存储到一个数据文件              
    file<span class="token punctuation">.</span><span class="token function">on</span><span class="token punctuation">(</span><span class="token string">'open'</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">fd</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                            
      request<span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>file<span class="token punctuation">)</span><span class="token punctuation">;</span>                                      
      console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">&quot;New data has been received and stored.&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   
      response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                 
      response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;Data stored on pod &quot;</span> <span class="token operator">+</span> os<span class="token punctuation">.</span><span class="token function">hostname</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
    <span class="token keyword">var</span> data <span class="token operator">=</span> <span class="token function">fileExists</span><span class="token punctuation">(</span>dataFile<span class="token punctuation">)</span>      # 在<span class="token constant">GET</span>请求中<span class="token punctuation">,</span> 返回主机名和数据文件的内容                      
      <span class="token operator">?</span> fs<span class="token punctuation">.</span><span class="token function">readFileSync</span><span class="token punctuation">(</span>dataFile<span class="token punctuation">,</span> <span class="token string">'utf8'</span><span class="token punctuation">)</span>                      
      <span class="token operator">:</span> <span class="token string">&quot;No data posted yet&quot;</span><span class="token punctuation">;</span>                                  
    response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                   
    response<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token string">&quot;You've hit &quot;</span> <span class="token operator">+</span> os<span class="token punctuation">.</span><span class="token function">hostname</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      
    response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;Data stored on this pod: &quot;</span> <span class="token operator">+</span> data <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> www <span class="token operator">=</span> http<span class="token punctuation">.</span><span class="token function">createServer</span><span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span class="token punctuation">;</span>
www<span class="token punctuation">.</span><span class="token function">listen</span><span class="token punctuation">(</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p>当应用接收到一个 POST 请求时, 它把请求中的 body 数据内容写入 <code>/var/data/kubia.txt</code>​ 文件中. 而在收到 GET 请求时, 它返回主机名和存储数据(文件中的内容). 是不是很简单呢? 这是应用的第一版本. 它还不是一个集群应用, 但它足够让你可以开始工作. 在本章的后面会扩展这个应用.</p> <p>用来构建这个容器镜像的 Dockerfile 文件与之前的一样, 如下面的代码清单所示.</p> <p><strong>代码清单-10.2 有状态应用的 Dockerfile:kubia-pet-image/Dockerfile</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token constant">FROM</span> <span class="token literal-property property">node</span><span class="token operator">:</span><span class="token number">7</span>
<span class="token constant">ADD</span> app<span class="token punctuation">.</span>js <span class="token operator">/</span>app<span class="token punctuation">.</span>js
<span class="token constant">ENTRYPOINT</span> <span class="token punctuation">[</span><span class="token string">&quot;node&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;app.js&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>现在来构建容器镜像, 或者使用笔者上传的镜像: <code>docker.io/luksa/kubia-pet</code>​.</p> <h6 id="_10-3-2-通过statefulset部署应用"><a href="#_10-3-2-通过statefulset部署应用" class="header-anchor">#</a> 10.3.2 通过Statefulset部署应用</h6> <p>为了部署应用, 需要创建两个(或三个)不同类型的对象:</p> <ul><li><strong>存储数据文件的持久卷</strong>(当集群不支持持久卷的动态供应时, 需要手动创建)</li> <li><strong>Statefulset 必需的一个控制 Service</strong></li> <li><strong>Statefulset 本身</strong></li></ul> <p>对于<strong>每一个 pod 实例, Statefulset 都会创建一个绑定到一个持久卷上的持久卷声明</strong>. 如果集群支持动态供应, 就不需要手动创建持久卷(可跳过下一节). 如果不支持的话, 可以按照下一节所述创建它们.</p> <blockquote><p>创建持久化存储卷</p></blockquote> <p>因为你会调度 Statefulset 创建三个副本, 所以这里需要<strong>三个持久卷</strong>. 如果使用 Minikube, 请参考本书代码附件中的 Chapter06/persistentvolumes-hostpath.yaml 来部署持久卷.</p> <p>如果使用谷歌的 Kubernetes 引擎, 需要首先创建实际的 GCE 持久磁盘:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ gcloud compute disks create <span class="token operator">--</span>size<span class="token operator">=</span>1GiB <span class="token operator">--</span>zone<span class="token operator">=</span>europe<span class="token operator">-</span>west1<span class="token operator">-</span>b pv<span class="token operator">-</span>a
$ gcloud compute disks create <span class="token operator">--</span>size<span class="token operator">=</span>1GiB <span class="token operator">--</span>zone<span class="token operator">=</span>europe<span class="token operator">-</span>west1<span class="token operator">-</span>b pv<span class="token operator">-</span>b
$ gcloud compute disks create <span class="token operator">--</span>size<span class="token operator">=</span>1GiB <span class="token operator">--</span>zone<span class="token operator">=</span>europe<span class="token operator">-</span>west1<span class="token operator">-</span>b pv<span class="token operator">-</span>c
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: 保证创建的持久磁盘和运行的节点在同一区域.</p> <p>然后通过 persistent-volumes-gcepd.yaml 文件创建需要的持久卷, 如下面的代码清单所示.</p> <p><strong>代码清单-10.3 三个持久卷: persistent-volumes-gcepd.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> List          <span class="token comment"># 这几行是创建三个持久卷的文件描述                         </span>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">items</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
  <span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume                   
  <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>a      <span class="token comment"># 持久卷的名称为pv-a, pv-b和pv-c</span>
  <span class="token key atrule">spec</span><span class="token punctuation">:</span>
    <span class="token key atrule">capacity</span><span class="token punctuation">:</span>
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Mi   <span class="token comment"># 每个持久卷的大小为1M</span>
    <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> ReadWriteOnce
    <span class="token key atrule">persistentVolumeReclaimPolicy</span><span class="token punctuation">:</span> Recycle   <span class="token comment"># 当卷被声明释放后, 空间会被回收再利用</span>
    <span class="token key atrule">gcePersistentDisk</span><span class="token punctuation">:</span>     <span class="token comment"># 这三行指定这个卷使用GCE持久磁盘和指定的存储策略                 </span>
      <span class="token key atrule">pdName</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>a                        
      <span class="token key atrule">fsType</span><span class="token punctuation">:</span> nfs4                         
<span class="token punctuation">-</span> <span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
  <span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume
  <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>b
 <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>注意: 在上一节通过在同一 YAML 文件中添加三个横杠(---)来区分定义多个资源, 这里使用另外一种方法, <strong>定义一个 List 对象, 然后把各个资源作为 List 对象的各个项目</strong>. 上述两种方法的效果是一样的.</p> <p>通过上诉文件创建了 pv-a, pv-b 和 pv-c 三个持久卷. 它们使用 GCE 持久磁盘和指定的存储策略, 所以它们并不适合没有运行在谷歌 Kubernetes 引擎(Google Kubernetes Engine)或谷歌计算引擎(Google Compute Engine)上的集群. 如果你的集群运行在其他地方, 必须修改持久卷的定义, 使用正确的卷类型, 比如 NFS(网络文件系统)或其他类似的类型.</p> <blockquote><p>创建控制 Service</p></blockquote> <p>如之前所述, 在<strong>部署一个 Statefulset 之前, 需要创建一个用于在有状态的 pod 之间提供网络标识的 headless Service</strong>. 下面的代码显示了 Service 的详细信息.</p> <p><strong>代码清单-10.4 在 Statefulset 中使用的 headless service:kubia-serviceheadless.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia         <span class="token comment"># Service的名称</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">clusterIP</span><span class="token punctuation">:</span> None     <span class="token comment"># Statefulset的控制Service必须是headless模式</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>       
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia        <span class="token comment"># 所有标签为app=kubia的pod都属于这个Service</span>
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>上面指定了 clusterIP 为 None, 这就标记了它是一个 headless Service. 它使得你的 <strong>pod 之间可以彼此发现</strong>(后续会用到这个功能). 创建完这个 Service 之后, 就可以继续往下创建实际的 Statefulset 了.</p> <blockquote><p>创建 Statefulset 详单</p></blockquote> <p>最后可以创建 Statefulset 了, 下面的代码清单显示了其详细信息.</p> <p><strong>代码清单-10.5 Statefulset 详单: kubia-statefulset.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> kubia
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>     <span class="token comment"># Statefulset创建的pod都带有app=kubia标签           </span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia           
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
        <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">-</span>pet
        <span class="token key atrule">ports</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http
          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data       <span class="token comment"># pod 中的容器会把pvc数据卷嵌入指定目录        </span>
          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/data   
  <span class="token key atrule">volumeClaimTemplates</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">metadata</span><span class="token punctuation">:</span>               <span class="token comment"># 创建持久卷声明的模板     </span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> data             
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>                    
      <span class="token key atrule">resources</span><span class="token punctuation">:</span>             
        <span class="token key atrule">requests</span><span class="token punctuation">:</span>            
          <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Mi       
      <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>           
      <span class="token punctuation">-</span> ReadWriteOnce        
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><p>这个 Statefulset 详单与之前创建的 ReplicaSet 和 Deployment 的详单没太多区别, 这里使用的新组件是 volumeClaimTemplates 列表. 其中仅仅定义了一个名为 data 的卷声明, <strong>会依据这个模板为每个 pod 都创建一个持久卷声明</strong>. 如之前在第 6 章中介绍的, pod 通过在其详单中包含一个 PersistentVolumeClaim 卷来关联一个声明. 但在上面的 pod 模板中并没有这样的卷, 这是因为在 Statefulset 创建指定 pod 时, 会自动将 PersistentVolumeClaim 卷添加到 pod 详述中, 然后将这个卷关联到一个声明上.</p> <blockquote><p>创建 Statefulset</p></blockquote> <p>现在就要创建 Statefulset 了:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> kubia-statefulset.yaml
statefulset <span class="token string">&quot;kubia&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在列出你的 pod:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>              <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       ContainerCreating   <span class="token number">0</span>          1s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>有没有发现不同之处? 是否记得一个 ReplicationController 或 ReplicaSet 会同时创建所有的 pod 实例? 你的 Statefulset 配置去创建两个副本, 但是它仅仅创建了单个 pod.</p> <p>不要担心, 这里没有出错. <strong>第二个 pod 会在第一个 pod 运行并且处于就绪状态后创建</strong>. Statefulset 这样的行为是因为: 状态明确的集群应用对同时有两个集群成员启动引起的竞争情况是非常敏感的. <strong>所以依次启动每个成员是比较安全可靠的</strong>. 特定的有状态应用集群在两个或多个集群成员同时启动时引起的竞态条件是非常敏感的, 所以在每个成员完全启动后再启动剩下的会更加安全.</p> <p>再次列出 pod 并查看 pod 的创建过程:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>              <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          8s
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       ContainerCreating   <span class="token number">0</span>          2s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到, 第一个启动的 pod 状态是 running, 第二个 pod 已经创建并在<strong>启动过程中</strong>.</p> <blockquote><p>检查生成的有状态 pod</p></blockquote> <p>现在看一下第一个 pod 的详细参数, 看一下 Statefulset 如何从 pod 模板和持久卷声明模板来构建 pod, 如下面的代码清单所示.</p> <p><strong>代码清单-10.6 Statefulset 创建的有状态 pod</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">-</span>pet
    <span class="token punctuation">...</span>
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/data     <span class="token comment"># 在manifest里指定的存储卷挂载点</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> data                       
    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/run/secrets/kubernetes.io/serviceaccount
      <span class="token key atrule">name</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>token<span class="token punctuation">-</span>r2m41
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token punctuation">...</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data           <span class="token comment"># Statefulset创建的数据卷           </span>
    <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span>             
      <span class="token key atrule">claimName</span><span class="token punctuation">:</span> data<span class="token punctuation">-</span>kubia<span class="token punctuation">-</span><span class="token number">0</span>     <span class="token comment"># 与数据卷相关的声明</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>token<span class="token punctuation">-</span>r2m41
    <span class="token key atrule">secret</span><span class="token punctuation">:</span>
      <span class="token key atrule">secretName</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>token<span class="token punctuation">-</span>r2m41
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>通过持久卷声明模板来创建持久卷声明和 pod 中使用的与持久卷声明相关的数据卷.</p> <blockquote><p>检查生成的持久卷声明</p></blockquote> <p>现在列出生成的持久卷声明来确定它们被创建了:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> pvc
<span class="token constant">NAME</span>           <span class="token constant">STATUS</span>    <span class="token constant">VOLUME</span>    <span class="token constant">CAPACITY</span>   <span class="token constant">ACCESSMODES</span>   <span class="token constant">AGE</span>
data<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token number">0</span>   Bound     pv<span class="token operator">-</span>c      <span class="token number">0</span>                        37s
data<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token number">1</span>   Bound     pv<span class="token operator">-</span>a      <span class="token number">0</span>                        37s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>生成的<strong>持久卷声明的名称由在 volumeClaimTemplate 字段中定义的名称和每个 pod 的名称组成</strong>. 可以检查声明的 YAML 文件来确认它们符合模板的定义.</p> <h6 id="_10-3-3-使用你的pod"><a href="#_10-3-3-使用你的pod" class="header-anchor">#</a> 10.3.3 使用你的pod</h6> <p>现在数据存储集群的节点都已经运行, 可以开始使用它们了. 因为之前创建的 Service 处于 headless 模式, 所以不能通过它来访问你的 pod. 需要<strong>直接连接每个单独的 pod 来访问</strong>(或者创建一个普通的 Service, 但是这样还是不允许访问指定的 pod).</p> <p>前面已经介绍过如何直接访问 pod: 借助另一个 pod, 然后在里面运行 curl 命令或者使用端口转发. 这次来介绍另外一种方法, <strong>通过 API 服务器作为代理</strong>.</p> <blockquote><p>通过 API 服务器与 pod 通信</p></blockquote> <p><strong>API 服务器的一个很有用的功能就是通过代理直接连接到指定的 pod</strong>. 如果想请求当前的 kubia-0 pod, 可以通过如下 URL:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token operator">&lt;</span>apiServerHost<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>port<span class="token operator">&gt;</span>/api/v1/namespaces/default/pods/kubia-0/proxy/<span class="token operator">&lt;</span>path<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>因为 API 服务器是有安全保障的, 所以通过 API 服务器发送请求到 pod 是烦琐的(需要额外在每次请求中添加授权令牌). 幸运的是, 在第 8 章中已经学习了如何使用 kubectl proxy 来与 API 服务器通信, 而不必使用麻烦的授权和 SSL 证书. 再次运行代理如下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl proxy
Starting to serve on <span class="token number">127.0</span>.0.1:8001
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在, 因为要<strong>通过 kubectl 代理来与 API 服务器通信</strong>, 将使用 localhost:8001 来代替实际的 API 服务器主机地址和端口. 你将发送一个如下所示的请求到 kubia-0 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: No data posted yet
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>返回的消息表明请求被正确收到, 并在 kubia-0 pod 的应用中被正确处理.</p> <p>注意: 如果收到一个空的回应, 请确保在 URL 的最后没有忘记输入 <code>/</code>​ 符号(或者用 curl 的-L 选项来允许重定向)</p> <p>因为正在使用代理的方式通过 API 服务器与 pod 通信, <strong>每个请求都会经过两个代理(第一个是 kubectl 代理, 第二个是把请求代理到 pod 的 API 服务器)</strong> . 详细的描述如图 10.10 所示.</p> <p><img src="/img/image-20240224142228-cpw5zh7.png" alt="image" title="图10.10 通过 kubectl 代理和 API 服务器代理来与一个 pod 通信"></p> <p>上面介绍的是发送一个 GET 请求到 pod, 也可以通过 API 服务器发送 POST 请求. 发送 POST 请求使用的代理 URL 与发送 GET 请求一致.</p> <p>当应用收到一个 POST 请求时, 它把请求的主体内容保存到本地一个文件中. 发送一个 POST 请求到 kubia-0 pod 的示例:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> <span class="token parameter variable">-X</span> POST <span class="token parameter variable">-d</span> <span class="token string">&quot;Hey there! This greeting was submitted to kubia-0.&quot;</span> localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>发送的数据现在已经保存到 pod 中, 那检查一下当再次发送一个 GET 请求时, 它<strong>是否返回存储的数据</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there<span class="token operator">!</span> This greeting was submitted to kubia-0.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>挺好的, 到目前为止都工作正常. 现在来看看集群其他节点(kubia-1 pod):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>与期望的一致, <strong>每个节点拥有独自的状态</strong>. 那这些状态是否是持久的呢? 来进一步验证.</p> <blockquote><p>删除一个有状态 pod 来检查重新调度的 pod 是否关联了相同的存储</p></blockquote> <p>下面将会删除 kubia-0 pod, 等待它被重新调度, 然后就可以检查它是否会返回与之前一致的数据:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po kubia-0
pod <span class="token string">&quot;kubia-0&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果列出当前 pod, 可以看到该 pod 正在终止运行:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>        <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Terminating   <span class="token number">0</span>          3m
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running       <span class="token number">0</span>          3m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>当它一旦成功终止, Statefulset 会<strong>重新创建一个具有相同名称的新的 pod</strong>:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>              <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       ContainerCreating   <span class="token number">0</span>          6s
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          4m

$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          9s
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          4m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>请记住, <strong>新的 pod 可能会被调度到集群中的任何一个节点, 并不一定保持与旧的 pod 所在的节点一致. 旧的 pod 的全部标记(名称, 主机名和存储)实际上都会转移到新的 pod 上</strong>(如图 10.11 所示). 如果使用 Minikube, 将看不到这些, 因为它仅仅运行在单个节点上, 但是对于多个节点的集群来说, 可以看到新的 pod 会被调度到与之前 pod 不一样的节点上.</p> <p><img src="/img/image-20240224142620-4sgesnx.png" alt="image" title="图10.11 一个有状态 pod 会被重新调度到新的节点, 但会保留它的名称, 主机名和存储"></p> <p>现在新的 pod 已经运行了, 下面<strong>检查一下它是否拥有与之前的 pod 一样的标记</strong>. pod 的名称是一样的, 那它的主机名和持久化数据呢? 可以通过访问 pod 来确认:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: Hey there<span class="token operator">!</span> This greeting was submitted to kubia-0.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>从 pod 返回的信息表明它的主机名和持久化数据与之前 pod 是完全一致的, 所以可以确认 Statefulset 会使用一个完全一致的 pod 来替换被删除的 pod.</strong></p> <blockquote><p>扩缩容 Statefulset</p></blockquote> <p>缩容一个 Statefulset, 然后在完成后再扩容它, 与删除一个 pod 后让 Statefulset 立马重新创建它的表现是没有区别的. 需要记住的是, 缩容一个 Statefulset 只会删除对应的 pod, 留下卸载后的持久卷声明. 可以尝试缩容一个 Statefulset, 来进行确认.</p> <p>需要明确的关键点是, 缩容/扩容都是<strong>逐步进行</strong>的, 与 Statefulset 最初被创建时会创建各自的 pod 一样. <strong>当缩容超过一个实例的时候, 会首先删除拥有最高索引值的 pod. 只有当这个 pod 被完全终止后, 才会开始删除拥有次高索引值的 pod</strong>.</p> <blockquote><p>通过一个普通的非 headless 的 Service 暴露 Statefulset 的 pod</p></blockquote> <p>在阅读这一章的最后一部分之前, 需要为 pod 添加一个适当的非 headless Service, 这是因为客户端通常不会直接连接 pod, 而是通过一个<strong>服务</strong>.</p> <p>你应该知道了如何创建 Service, 如果不知道的话, 请看下面的代码清单.</p> <p><strong>代码清单-10.7 一个用来访问有状态 pod 的常规 Service:kubia-servicepublic.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>public
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
  <span class="token key atrule">ports</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>因为它不是外部暴露的 Service(它是一个常规的 ClusterIP Service, 不是一个 NodePort 或 LoadBalancer-type Service), 只能在集群内部访问它. 那是否需要一个 pod 来访问它呢? 答案是不需要.</p> <blockquote><p>通过 API 服务器访问集群内部的服务</p></blockquote> <p>不通过额外的 pod 来访问集群内部的服务的话, 与之前使用访问单独 pod 的方法一样, 可以<strong>使用 API 服务器提供的相同代理属性</strong>来访问.</p> <p>代理请求到 Service 的 URL 路径格式如下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/api/v1/namespaces/<span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span>/services/<span class="token operator">&lt;</span>service name<span class="token operator">&gt;</span>/proxy/<span class="token operator">&lt;</span>path<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>因此可以在本地机器上运行 curl 命令, 通过 kubectl 代理来访问服务(之前启动过 kubectl proxy, 现在它应该还在运行着):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>客户端(集群内部)同样可以通过 kubia-public 服务来存储或者读取你的集群中的数据. 当然, 每个请求会随机分配到一个集群节点上, 所以每次都会随机获取一个节点上的数据. 后面我们会改进它.</p> <h5 id="_10-4-在statefulset中发现伙伴节点"><a href="#_10-4-在statefulset中发现伙伴节点" class="header-anchor">#</a> 10.4 在Statefulset中发现伙伴节点</h5> <p>我们仍然需要弄清楚一件很重要的事情. 集群应用中很重要的一个需求是<mark><strong>伙伴节点彼此能发现</strong></mark>--这样才可以找到集群中的其他成员. 一个 Statefulset 中的成员需要很容易地找到其他的所有成员. 当然它可以通过与 API 服务器通信来获取, 但是 Kubernetes 的一个目标是设计功能来帮助应用完全感觉不到 Kubernetes 的存在. 因此<strong>让应用与 API 服务器通信的设计是不允许的</strong>.</p> <p>那如何使得一个 pod 可以不通过 API 与其他伙伴通信呢? 是否有已知的广泛存在的技术来帮助你达到目的呢? 那使用域名系统(DNS)如何? 这依赖于你对 DNS 系统有多熟悉, 你可能理解什么是 A, CNAME 或 MX 记录的用处是什么. DNS 记录里还有其他一些不是那么知名的类型, <strong>SRV 记录</strong>就是其中的一个.</p> <blockquote><p>介绍 SRV 记录</p></blockquote> <p><strong>SRV 记录用来指向提供指定服务的服务器的主机名和端口号</strong>. Kubernetes 通过一个 headless service 创建 SRV 记录来指向 pod 的主机名.</p> <p>可以在一个临时 pod 里运行 DNS 查询工具--dig 命令, 列出有状态 pod 的 SRV 记录. 示例命令如下:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run <span class="token parameter variable">-it</span> srvlookup <span class="token parameter variable">--image</span><span class="token operator">=</span>tutum/dnsutils <span class="token parameter variable">--rm</span>
 <span class="token parameter variable">--restart</span><span class="token operator">=</span>Never -- <span class="token function">dig</span> SRV kubia.default.svc.cluster.local
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>上面的命令运行一个名为 srvlookup 的一次性 pod(--restart=Never), 它会关联控制台(-it)并且在终止后立即删除(--rm). 这个 pod 依据 tutum/dnsutils 镜像启动单独的容器, 然后运行下面的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token function">dig</span> SRV kubia.default.svc.cluster.local
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>下面的代码清单显示了这个命令的输出结果.</p> <p><strong>代码清单-10.8 列出你的 headless Service 的 DNS SRV 记录</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token punctuation">..</span>.
<span class="token punctuation">;</span><span class="token punctuation">;</span> ANSWER SECTION:
k.d.s.c.l. <span class="token number">30</span> IN  SRV     <span class="token number">10</span> <span class="token number">33</span> <span class="token number">0</span> kubia-0.kubia.default.svc.cluster.local.
k.d.s.c.l. <span class="token number">30</span> IN  SRV     <span class="token number">10</span> <span class="token number">33</span> <span class="token number">0</span> kubia-1.kubia.default.svc.cluster.local.

<span class="token punctuation">;</span><span class="token punctuation">;</span> ADDITIONAL SECTION:
kubia-0.kubia.default.svc.cluster.local. <span class="token number">30</span> IN A <span class="token number">172.17</span>.0.4
kubia-1.kubia.default.svc.cluster.local. <span class="token number">30</span> IN A <span class="token number">172.17</span>.0.6
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>注意: 为了让记录可以在一行里显示, 对真实名称做了缩减, 对应 <code>kubia.d.s.c.l.</code>​ 的全称是 kubia.default.svc.cluster.local.</p> <p>上面的 ANSWER SECTION 显示了两条指向后台 headless service 的 SRV 记录. 同时如 ADDITIONAL SECTION 所示, 每个 pod 都拥有独自的一条记录.</p> <p><strong>当一个 pod 要获取一个 Statefulset 里的其他 pod 列表时, 需要做的就是触发一次 SRV DNS 查询</strong>. 例如, 在 Node.js 中查询命令为:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>dns.resolveSrv<span class="token punctuation">(</span><span class="token string">&quot;kubia.default.svc.cluster.local&quot;</span>, callBackFunction<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以在应用中使用上述命令让每个 pod 发现它的伙伴 pod.</p> <p>注意: 返回的 SRV 记录顺序是<strong>随机</strong>的, 因为它们拥有相同的优先级. 所以不要期望总是看到 kubia-0 会排在 kubia-1 前面.</p> <h6 id="_10-4-1-通过dns实现伙伴间彼此发现"><a href="#_10-4-1-通过dns实现伙伴间彼此发现" class="header-anchor">#</a> 10.4.1 通过DNS实现伙伴间彼此发现</h6> <p>原始的数据存储服务还不是集群级别的, 每个数据存储节点都是完全独立于其他节点的--它们彼此之间没有通信. 下一步要做的就是<mark><strong>让它们彼此通信</strong></mark>.</p> <p>客户端通过 kubia-public Service 连接你的数据存储服务, 并且会到达集群里<strong>随机</strong>的一个节点. 集群可以存储多条数据项, 但是<strong>客户端当前却不能看到所有的数据项</strong>. 因为服务把请求随机地送达一个 pod, 所以若客户端想获取所有 pod 的数据, 必须发送很多次请求, 一直到它的请求发送到所有的 pod 为止.</p> <p><strong>可以通过让节点返回所有集群节点数据的方式来改进这个行为</strong>. 为了达到目的, 节点需要能找到它所有的伙伴节点. 可以使用之前学习到的 Statefulset 和 SRV 记录来实现这个功能.</p> <p>可以如下面的代码清单所示修改应用源码(完整的代码在本书的代码附件中, 这里仅展示其中重要的一段).</p> <p><strong>代码清单-10.9 在简单应用中发现伙伴节点: kubia-pet-peers-image/app.js</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token operator">...</span>
<span class="token keyword">const</span> dns <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'dns'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">const</span> dataFile <span class="token operator">=</span> <span class="token string">&quot;/var/data/kubia.txt&quot;</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> serviceName <span class="token operator">=</span> <span class="token string">&quot;kubia.default.svc.cluster.local&quot;</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> port <span class="token operator">=</span> <span class="token number">8080</span><span class="token punctuation">;</span>
<span class="token operator">...</span>

<span class="token keyword">var</span> <span class="token function-variable function">handler</span> <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span> response</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>request<span class="token punctuation">.</span>method <span class="token operator">&lt;</span>mark<span class="token operator">&gt;</span> <span class="token string">'POST'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token operator">...</span>
  <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
    response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>request<span class="token punctuation">.</span>url <span class="token operator">&lt;</span><span class="token operator">/</span>mark<span class="token operator">&gt;</span> <span class="token string">'/data'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
      <span class="token keyword">var</span> data <span class="token operator">=</span> <span class="token function">fileExists</span><span class="token punctuation">(</span>dataFile<span class="token punctuation">)</span>
        <span class="token operator">?</span> fs<span class="token punctuation">.</span><span class="token function">readFileSync</span><span class="token punctuation">(</span>dataFile<span class="token punctuation">,</span> <span class="token string">'utf8'</span><span class="token punctuation">)</span>
        <span class="token operator">:</span> <span class="token string">&quot;No data posted yet&quot;</span><span class="token punctuation">;</span>
      response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
      response<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token string">&quot;You've hit &quot;</span> <span class="token operator">+</span> os<span class="token punctuation">.</span><span class="token function">hostname</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      response<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token string">&quot;Data stored in the cluster:\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      dns<span class="token punctuation">.</span><span class="token function">resolveSrv</span><span class="token punctuation">(</span>serviceName<span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">err<span class="token punctuation">,</span> addresses</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>     
        <span class="token keyword">if</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>
          response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;Could not look up DNS SRV records: &quot;</span> <span class="token operator">+</span> err<span class="token punctuation">)</span><span class="token punctuation">;</span>
          <span class="token keyword">return</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token keyword">var</span> numResponses <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>addresses<span class="token punctuation">.</span>length <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
          response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">&quot;No peers discovered.&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
          addresses<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">item</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                     
            <span class="token keyword">var</span> requestOptions <span class="token operator">=</span> <span class="token punctuation">{</span>
              <span class="token literal-property property">host</span><span class="token operator">:</span> item<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
              <span class="token literal-property property">port</span><span class="token operator">:</span> port<span class="token punctuation">,</span>
              <span class="token literal-property property">path</span><span class="token operator">:</span> <span class="token string">'/data'</span>
            <span class="token punctuation">}</span><span class="token punctuation">;</span>
            <span class="token function">httpGet</span><span class="token punctuation">(</span>requestOptions<span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">returnedData</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>       
              numResponses<span class="token operator">++</span><span class="token punctuation">;</span>
              response<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token string">&quot;- &quot;</span> <span class="token operator">+</span> item<span class="token punctuation">.</span>name <span class="token operator">+</span> <span class="token string">&quot;: &quot;</span> <span class="token operator">+</span> returnedData<span class="token punctuation">)</span><span class="token punctuation">;</span>
              response<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
              <span class="token keyword">if</span> <span class="token punctuation">(</span>numResponses <span class="token operator">==</span> addresses<span class="token punctuation">.</span>length<span class="token punctuation">)</span> <span class="token punctuation">{</span>
                response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
              <span class="token punctuation">}</span>
            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
          <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br></div></div><p>图 10.12 展示了一个 GET 请求到达应用后的处理过程. 首先收到请求的服务器会<strong>触发一次 headless kubia 服务的 SRV 记录查询, 然后发送 GET 请求到服务背后的每一个 pod</strong>(也会发送给自己, 虽说没有必要, 这里只是为了保证代码简单易懂), 然后返回所有节点和它们的数据信息的列表.</p> <p><img src="/img/image-20240224143001-1rhgpub.png" alt="image" title="图10.12 简单的分布式数据存储服务的操作流程"></p> <p>包含最新版本内容的应用对应的容器镜像链接为: docker.io/luksa/kubia-petpeers.</p> <h6 id="_10-4-2-更新statefulset"><a href="#_10-4-2-更新statefulset" class="header-anchor">#</a> 10.4.2 更新Statefulset</h6> <p>现在你的 Statefulset 已经运行起来, 来看一下如何更新它的 pod 模板, 让它使用新的镜像. 同时你也会修改副本数为 3. 通常会<strong>使用 kubectl edit 命令来更新 Statefulset</strong>(另一个选择是 patch 命令).</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl edit statefulset kubia
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>上面的命令会使用默认的编辑器打开 Statefulset 的定义. 在定义中, 修改 spec.replicas 为 3, <strong>修改 spec.template.spec.containers.image 属性指向新的镜像</strong>(使用 luksa/kubia-pet-peers 替换 luksa/kubia-pet). 然后保存文件并退出, Statefulset 就会更新. 之前 Statefulset 有两个副本, 现在应该可以看到一个新的名叫 kubia-2 的副本启动了. 通过下面的代码列出 pod 来确认:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>              <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          25m
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          26m
kubia<span class="token operator">-</span><span class="token number">2</span>   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       ContainerCreating   <span class="token number">0</span>          4s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>新的 pod 实例会使用<strong>新的镜像</strong>运行, 那已经存在的两个副本呢? 通过它们的寿命可以看出它们并没有更新. 这是符合预期的. 因为, 首先 Statefulset 更像 ReplicaSet, 而不是 Deployment, <strong>所以在模板被修改后, 它们不会重启更新. 需要手动删除这些副本, 然后 Statefulset 会依据新的模板重新调度启动它们</strong>.</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">delete</span> po kubia<span class="token operator">-</span><span class="token number">0</span> kubia<span class="token operator">-</span><span class="token number">1</span>
pod <span class="token string">&quot;kubia-0&quot;</span> deleted
pod <span class="token string">&quot;kubia-1&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: 从 Kubernetes 1.7 版本开始, Statefulset 支持与 Deployment 和 DaemonSet 一样的<strong>滚动升级</strong>. 通过 kubectl explain 获取 Statefulset 的 spec.updateStrategy 相关文档来获取更多信息.</p> <h6 id="_10-4-3-尝试集群数据存储"><a href="#_10-4-3-尝试集群数据存储" class="header-anchor">#</a> 10.4.3 尝试集群数据存储</h6> <p>当两个 pod 都启动后, 即可测试数据存储是否按预期一样工作了. 如下面的代码清单所示, 发送一些请求到集群.</p> <p><strong>代码清单-10.10 通过 service 往集群数据存储中写入数据</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ curl <span class="token operator">-</span><span class="token constant">X</span> <span class="token constant">POST</span> <span class="token operator">-</span>d <span class="token string">&quot;The sun is shining&quot;</span> <span class="token literal-property property">localhost</span><span class="token operator">:</span><span class="token number">8001</span><span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>namespaces<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>services<span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token keyword">public</span><span class="token operator">/</span>proxy<span class="token operator">/</span>
Data stored on pod kubia<span class="token operator">-</span><span class="token number">1</span>

$ curl <span class="token operator">-</span><span class="token constant">X</span> <span class="token constant">POST</span> <span class="token operator">-</span>d <span class="token string">&quot;The weather is sweet&quot;</span> <span class="token literal-property property">localhost</span><span class="token operator">:</span><span class="token number">8001</span><span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>namespaces<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>services<span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token keyword">public</span><span class="token operator">/</span>proxy<span class="token operator">/</span>
Data stored on pod kubia<span class="token operator">-</span><span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>现在, 读取存储的数据, 如下面的代码清单所示.</p> <p><strong>代码清单-10.11 从数据存储中读取数据</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ curl localhost<span class="token operator">:</span><span class="token number">8001</span><span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>namespaces<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>services<span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token keyword">public</span><span class="token operator">/</span>proxy<span class="token operator">/</span>
You've hit kubia<span class="token operator">-</span><span class="token number">2</span>
Data stored on each cluster node<span class="token operator">:</span>
<span class="token operator">-</span> kubia<span class="token operator">-</span><span class="token number">0</span><span class="token punctuation">.</span>kubia<span class="token punctuation">.</span>default<span class="token punctuation">.</span>svc<span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>local<span class="token operator">:</span> The weather is sweet
<span class="token operator">-</span> kubia<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">.</span>kubia<span class="token punctuation">.</span>default<span class="token punctuation">.</span>svc<span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>local<span class="token operator">:</span> The sun is shining
<span class="token operator">-</span> kubia<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span>kubia<span class="token punctuation">.</span>default<span class="token punctuation">.</span>svc<span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>local<span class="token operator">:</span> No data posted yet
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>非常棒! <strong>当一个客户端请求到达集群中任意一个节点后, 它会发现它的所有伙伴节点, 然后通过它们收集数据, 然后把收集到的所有数据返回给客户端</strong>. 即使扩容或缩容 Statefulset, 服务于客户端请求的 pod 都会找到所有的伙伴节点.</p> <p>这个应用本身也许没太多用处, 但笔者希望你觉得这是一种有趣的方式, 一个多副本 Statefulset 应用的实例如何发现它的伙伴, 并且随需求做到横向扩展.</p> <h5 id="_10-5-了解statefulset如何处理节点失效"><a href="#_10-5-了解statefulset如何处理节点失效" class="header-anchor">#</a> 10.5 了解Statefulset如何处理节点失效</h5> <p>在 10.2.4 节中, 阐述了 Kubernetes 必须完全保证: <strong>一个有状态 pod 在创建它的代替者之前已经不再运行, 当一个节点突然失效, Kubernetes 并不知道节点或者它上面的 pod 的状态. 它并不知道这些 pod 是否还在运行, 或者它们是否还存在, 甚至是否还能被客户端访问到, 或者仅仅是 Kubelet 停止向主节点上报本节点状态</strong>.</p> <p><strong>因为一个 Statefulset 要保证不会有两个拥有相同标记和存储的 pod 同时运行, 当一个节点似乎失效时, Statefulset 在明确知道一个 pod 不再运行之前, 它不能或者不应该创建一个替换 pod</strong>.</p> <p>只有当集群的管理者告诉它这些信息的时候, 它才能明确知道. 为了做到这一点, 管理者需要删除这个 pod, 或者删除整个节点(这么做会删除所有调度到该节点上的 pod).</p> <p>作为这一章中的最后一个练习, 你会看到<strong>当一个集群节点网络断开后</strong>, Statefulset 和节点上的 pod 都会发生些什么.</p> <h6 id="_10-5-1-模拟一个节点的网络断开"><a href="#_10-5-1-模拟一个节点的网络断开" class="header-anchor">#</a> 10.5.1 模拟一个节点的网络断开</h6> <p>与第 4 章中一致, 可以通过关闭节点的 eth0 网络接口来模拟节点的网络断开. 因为这个例子需要多个节点, 所以不能在 Minikube 上运行, 可以使用谷歌的 Kubernetes 引擎来运行.</p> <blockquote><p>关闭节点的网络适配器</p></blockquote> <p>为了关闭一个节点的网络接口, 需要通过 ssh 登录一个节点:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ gcloud compute ssh gke<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token keyword">default</span><span class="token operator">-</span>pool<span class="token operator">-</span>32a2cac8<span class="token operator">-</span>m0g1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后在节点内部运行如下命令:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ sudo ifconfig eth0 down
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>之后你的 ssh 链接就会中断, 所以需要开启一个新的终端来继续执行.</p> <blockquote><p>通过 Kubernetes 管理节点检查节点的状态</p></blockquote> <p>当这个节点的网络接口关闭以后, 运行在这个节点上的 Kubelet 服务就无法与 Kubernetes API 服务器通信, 无法汇报本节点和上面的 pod 都在正常运行.</p> <p>过了一段时间后, 控制台就会标记该节点状态为 <strong>NotReady</strong>. 如下面的代码清单所示, 当列出节点时可以看到这些.</p> <p><strong>代码清单-10.12 观察到一个失效的节点状态变为 NotReady</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> node
<span class="token constant">NAME</span>                                   <span class="token constant">STATUS</span>     <span class="token constant">AGE</span>       <span class="token constant">VERSION</span>
gke<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token keyword">default</span><span class="token operator">-</span>pool<span class="token operator">-</span>32a2cac8<span class="token operator">-</span>596v   Ready      16m       v1<span class="token punctuation">.</span><span class="token number">6.2</span>
gke<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token keyword">default</span><span class="token operator">-</span>pool<span class="token operator">-</span>32a2cac8<span class="token operator">-</span>m0g1   NotReady   16m       v1<span class="token punctuation">.</span><span class="token number">6.2</span>    # 状态改变了
gke<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token keyword">default</span><span class="token operator">-</span>pool<span class="token operator">-</span>32a2cac8<span class="token operator">-</span>sgl7   Ready      16m       v1<span class="token punctuation">.</span><span class="token number">6.2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>因为控制台不会再收到该节点发送的状态更新, 该<strong>节点上面的所有 pod 状态都会变为 Unknown</strong>. 如下面的代码清单所示, 列举 pod 信息就可以看到.</p> <p><strong>代码清单-10.13 观察到节点变为 NotReady 后, 其上的 pod 状态就会改变</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Unknown   <span class="token number">0</span>          15m
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          14m
kubia<span class="token operator">-</span><span class="token number">2</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          13m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>正如看到的这样, kubia-0 pod 的状态不再已知, 这是因为关闭了这个 pod 之前运行(也许正在运行)的节点的网络接口.</p> <blockquote><p>当一个 pod 状态为 Unknow 时会发生什么</p></blockquote> <p>若该节点过段时间正常连通, 并且重新汇报它上面的 pod 状态, 那这个 pod 就会重新被标记为 Runing. 但如果这个 pod 的未知状态持续几分钟后(这个时间是可以配置的), 这个 pod 就会<strong>自动从节点上驱逐</strong>. 这是由主节点(Kubernetes 的控制组件)处理的. <strong>它通过删除 pod 的资源来把它从节点上驱逐</strong>.</p> <p>当 Kubelet 发现这个 pod 被标记为删除状态后, 它开始终止运行该 pod. 在上面的示例中, Kubelet 已不能与主节点通信(因为断开了这个节点的网络), 这也就意味着这个 pod 会一直运行着.</p> <p>解释一下当前的状况. 通过 kubectl describe 命令查看 kubia-0 pod 的详细信息, 如下面的代码清单所示.</p> <p><strong>代码清单-10.14 显示未知状态的 pod 的详情</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl describe po kubia<span class="token operator">-</span><span class="token number">0</span>
<span class="token literal-property property">Name</span><span class="token operator">:</span>        kubia<span class="token operator">-</span><span class="token number">0</span>
<span class="token literal-property property">Namespace</span><span class="token operator">:</span>   <span class="token keyword">default</span>
<span class="token literal-property property">Node</span><span class="token operator">:</span>        gke<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token keyword">default</span><span class="token operator">-</span>pool<span class="token operator">-</span>32a2cac8<span class="token operator">-</span>m0g1<span class="token operator">/</span><span class="token number">10.132</span><span class="token number">.0</span><span class="token number">.2</span>
<span class="token operator">...</span>
<span class="token literal-property property">Status</span><span class="token operator">:</span>      <span class="token function">Terminating</span> <span class="token punctuation">(</span>expires Tue<span class="token punctuation">,</span> <span class="token number">23</span> May <span class="token number">2017</span> <span class="token number">15</span><span class="token operator">:</span><span class="token number">06</span><span class="token operator">:</span><span class="token number">09</span> <span class="token operator">+</span><span class="token number">0200</span><span class="token punctuation">)</span>
<span class="token literal-property property">Reason</span><span class="token operator">:</span>      NodeLost
<span class="token literal-property property">Message</span><span class="token operator">:</span>     Node gke<span class="token operator">-</span>kubia<span class="token operator">-</span><span class="token keyword">default</span><span class="token operator">-</span>pool<span class="token operator">-</span>32a2cac8<span class="token operator">-</span>m0g1 which was
             running pod kubia<span class="token operator">-</span><span class="token number">0</span> is unresponsive
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>可以看到这个 pod 的状态为 <strong>Terminating</strong>, 原因是 NodeLost. 在信息中说明的是<strong>节点不回应导致的不可达</strong>.</p> <p>注意: 这里展示的是<strong>控制组件</strong>看到的信息. 实际上这个 pod 对应的容器并被没有被终止, 还在正常运行.</p> <h6 id="_10-5-2-手动删除pod"><a href="#_10-5-2-手动删除pod" class="header-anchor">#</a> 10.5.2 手动删除pod</h6> <p>你已经明确这个节点不会再回来, 但是所有处理客户端请求的三个 pod 都必须是正常运行的. 所以需要把 kubia-0 pod <strong>重新调度到一个健康的节点上</strong>. 如之前提到的那样, 需要手动删除整个节点或者这个 pod.</p> <blockquote><p>正常删除 pod</p></blockquote> <p>可以使用一直使用的方式删除该 pod:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">delete</span> po kubia<span class="token operator">-</span><span class="token number">0</span>
pod <span class="token string">&quot;kubia-0&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>是不是所有的都做完了? 删除 pod 后, Statefulset <strong>应该</strong>会立刻创建一个替换的 pod, 这个 pod 会被调度到剩下可用的节点上. 再次列举 pod 信息来确认:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>      <span class="token constant">READY</span>     <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Unknown   <span class="token number">0</span>          15m
kubia<span class="token operator">-</span><span class="token number">1</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          14m
kubia<span class="token operator">-</span><span class="token number">2</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          13m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>非常奇怪, 刚刚删除了这个 pod, kubectl 也返回说它已经被删除. 那为什么这个 pod 还在呢?</p> <p>注意: <strong>列表中的 kubia-0 pod 不是一个有相同名字的新 pod, 在从它的 AGE 列中就可以看出</strong>. 如果它是一个新 pod, 它的 &quot;年龄&quot; 只会是几秒钟.</p> <blockquote><p>为什么 pod 没有被删除</p></blockquote> <p>在删除 pod 之前, 这个 pod 已经被标记为删除. 这是因为控制组件已经删除了它(把它从节点驱逐).</p> <p>如果再次检查一下代码清单 10.14, 可以看出这个 pod 的状态是 <strong>Terminating</strong>. <strong>这个 pod 之前已经被标记为删除, 只要它所在节点上的 Kubelet 通知 API 服务器说这个 pod 的容器已经终止, 那么它就会被清除掉. 但是因为这个节点上的网络断开了, 所以上述情况永远不会发生</strong>.</p> <blockquote><p>强制删除 pod</p></blockquote> <p>现在唯一可以做的是<strong>告诉 API 服务器不用等待 kubelet 来确认这个 pod 已经不再运行, 而是直接删除它</strong>. 可以按照下面所述执行:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">delete</span> po kubia<span class="token operator">-</span><span class="token number">0</span> <span class="token operator">--</span>force <span class="token operator">--</span>grace<span class="token operator">-</span>period <span class="token number">0</span>
<span class="token literal-property property">warning</span><span class="token operator">:</span> Immediate deletion does not wait <span class="token keyword">for</span> confirmation that the running
     resource has been terminated<span class="token punctuation">.</span> The resource may <span class="token keyword">continue</span> to run on the
     cluster indefinitely<span class="token punctuation">.</span>
pod <span class="token string">&quot;kubia-0&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>需要同时使用 <code>--force</code>​ 和 <code>--grace-period 0</code>​ 两个选项. 然后 kubectl 会对你做的事情发出警告信息. 如果再次列举 pod, 就可以看到一个<strong>新的</strong> kubia-0 pod 被创建出来:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>          <span class="token constant">READY</span>     <span class="token constant">STATUS</span>              <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">0</span>       <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       ContainerCreating   <span class="token number">0</span>          8s    # 新的pod创建中
kubia<span class="token operator">-</span><span class="token number">1</span>       <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          20m
kubia<span class="token operator">-</span><span class="token number">2</span>       <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          19m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>警告: 除非确认节点不再运行或者不会再可以访问(永远不会再可以访问), 否则不要强制删除有状态的 pod.</p> <p>在继续操作之前, 可能希望把之前断掉连接的节点恢复正常. 可以通过 GCE web 控制台或在一个终端上执行下面的命令来重启该节点:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ gcloud compute instances reset <span class="token operator">&lt;</span>node name<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h5 id="_10-6-本章小结"><a href="#_10-6-本章小结" class="header-anchor">#</a> 10.6 本章小结</h5> <p>本章描述了如何使用 Statefulset 来部署有状态应用, 具体有如下几点:</p> <ul><li><strong>给副本 pod 配置单独的存储</strong></li> <li><strong>给一个 pod 提供稳定的标识</strong></li> <li>创建一个 Statefulset, 并且配置一个相关的 headless 控制服务</li> <li><strong>扩缩容, 更新一个 Statefulset</strong></li> <li>通过 DNS 发现 Statefulset 的其他成员</li> <li><strong>通过其他成员的主机名与之建立连接</strong></li> <li>强制删除有状态 pod</li></ul> <p>现在已经知道了如何使用主要构件来运行 Kubernetes 和管理应用, 后续会更深入地了解它是如何工作的. 在下一章会学习<strong>如何使用独立的组件来控制 Kubernetes 集群, 并保证应用正常运行</strong>.</p> <h4 id="_11-kubernetes基本原理"><a href="#_11-kubernetes基本原理" class="header-anchor">#</a> 11.Kubernetes基本原理</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>Kubernetes 集群包含哪些组件</li> <li>每个组件的作用以及它们是如何工作的</li> <li>运行的 pod 是如何创建—个部署对象的</li> <li>运行的 pod 是什么</li> <li>pod 之间的网络如何工作</li> <li>Kubernetes 服务如何工作</li> <li>如何保证高可用性</li></ul> <p>本书读至此处, 读者应该已经熟悉 Kubernetes 能提供什么以及做了什么. 不过到目前为止, 笔者有意没有花太多时间具体去阐述它是如何达成这些功能的, 在笔者看来, 在对系统能做什么有较好的理解之前, 钻系统实现细节没有意义. 这就是为什么还没有讨论过 pod 是如何调度的, 以及控制器管理器中的各种控制器如何让部署的资源运行起来. 知道了大多数可以部署到 Kubernetes 的资源, <strong>现在是时候了解下它们是怎么被实现的</strong>了.</p> <h5 id="_11-1-kubernetes架构"><a href="#_11-1-kubernetes架构" class="header-anchor">#</a> 11.1 Kubernetes架构</h5> <p>在研究 Kubernetes 如何实现其功能之前, 先具体<mark><strong>了解下 Kubernetes 集群有哪些组件</strong></mark>. 在第一章中, 可以看到, Kubernetes 集群分为两部分:</p> <ul><li><mark><strong>Kubernetes 控制平面</strong></mark></li> <li><mark><strong>(工作)节点</strong></mark></li></ul> <p>来具体看下这两个部分做了什么, 以及内部运行的内容.</p> <blockquote><p>控制平面的组件</p></blockquote> <p>控制平面负责控制并使得整个集群正常运转. 回顾一下, 控制平面包含如下组件:</p> <ul><li><mark><strong>etcd 分布式持久化存储</strong></mark></li> <li><mark><strong>API 服务器</strong></mark></li> <li><mark><strong>调度器</strong></mark></li> <li><mark><strong>控制器管理器</strong></mark></li></ul> <p>这些组件用来<strong>存储, 管理集群状态</strong>, 但它们不是运行应用的容器.</p> <blockquote><p>工作节点上运行的组件</p></blockquote> <p>运行容器的任务依赖于每个工作节点上运行的组件:</p> <ul><li><strong>Kubelet</strong></li> <li><strong>Kubelet 服务代理(kube-proxy)</strong></li> <li><strong>容器运行时(Docker, rkt 或者其他)</strong></li> <li>附加组件</li></ul> <p>除了控制平面(和运行在节点上的组件, 还要有几个<strong>附加组件</strong>, 这样才能提供所有之前讨论的功能. 包含:</p> <ul><li><strong>Kubernetes DNS 服务器</strong></li> <li><strong>仪表板</strong></li> <li><strong>Ingress 控制器</strong></li> <li><strong>Heapster(容器集群监控)</strong> , 将在第 14 章讨论</li> <li><strong>容器网络接口插件</strong>(本章后面会做讨论)</li></ul> <h6 id="_11-1-1-kubernetes组件的分布式特性"><a href="#_11-1-1-kubernetes组件的分布式特性" class="header-anchor">#</a> 11.1.1 Kubernetes组件的分布式特性</h6> <p>之前提到的组件都是作为<strong>单独进程</strong>运行的. 图 11.1 描述了各个组件及它们之间的依赖关系.</p> <p>若要启用 Kubernetes 提供的所有特性, 需要运行所有的这些组件. 但是有几个组件无须其他组件, 单独运行也能提供非常有用的工作. 接下来会详细查看每一个组件.</p> <p><img src="/img/image-20240224143335-ntm10vg.png" alt="image" title="图11.1 Kubernetes 控制平面以及工作节点的组件"></p> <blockquote><p>检查控制平面组件的状态</p></blockquote> <p>API 服务器对外暴露了一个名为 <strong>ComponentStatus</strong> 的 API 资源, 用来<strong>显示每个控制平面组件的健康状态</strong>. 可以通过 kubectl 列出各个组件以及它们的状态:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> componentstatuses
<span class="token constant">NAME</span>                 <span class="token constant">STATUS</span>    <span class="token constant">MESSAGE</span>              <span class="token constant">ERROR</span>
scheduler            Healthy   ok
controller<span class="token operator">-</span>manager   Healthy   ok
etcd<span class="token operator">-</span><span class="token number">0</span>               Healthy   <span class="token punctuation">{</span><span class="token string-property property">&quot;health&quot;</span><span class="token operator">:</span> <span class="token string">&quot;true&quot;</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><blockquote><p>组件间如何通信</p></blockquote> <p><mark><strong>Kubernetes 系统组件间只能通过 API 服务器通信, 它们之间不会直接通信. API 服务器是和 etcd 通信的唯一组件. 其他组件不会直接和 etcd 通信, 而是通过 API 服务器来修改集群状态.</strong></mark></p> <p><strong>API 服务器和其他组件的连接基本都是由组件发起的</strong>, 如图 11.1 所示. 但是, 当使用 kubectl 获取日志, 使用 kubectl attach 连接到一个运行中的容器或运行 kubectl port-forward 命令时, API 服务器会向 Kubelet 发起连接.</p> <p>注意: kubectl attach 命令和 kubectl exec 命令类似, 区别是: <strong>前者会附属到容器中运行着的主进程上, 而后者是重新运行一个进程</strong>.</p> <blockquote><p>单组件运行多实例</p></blockquote> <p>尽管工作节点上的组件都需要运行在同一个节点上, 控制平面的组件可以被简单地分割在多台服务器上. 为了保证高可用性, <strong>控制平面的每个组件可以有多个实例</strong>. etcd 和 API 服务器的多个实例可以同时并行工作, 但是, <strong>调度器和控制器管理器在给定时间内只能有一个实例起作用, 其他实例处于待命模式</strong>.</p> <blockquote><p>组件是如何运行的</p></blockquote> <p><strong>控制平面的组件以及 kube-proxy 可以直接部署在系统上或者作为 pod 来运行</strong>(如图 11.1 所示). 听到这个你可能比较惊讶, 不过后面讨论 Kubelet 时就都说得通了.</p> <p><strong>Kubelet 是唯一一直作为常规系统组件来运行的组件, 它把其他组件作为 pod 来运行</strong>. 为了将控制平面作为 pod 来运行, Kubelet 被部署在 master 上. 下面的代码清单展示了通过 kubeadm(在附录 B 中阐述)创建的集群里的 kube-system 命名空间里的 pod.</p> <p><strong>代码清单-11.1 作为 pod 运行的 Kubernetes 组件</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>o custom<span class="token operator">-</span>columns<span class="token operator">=</span><span class="token constant">POD</span><span class="token operator">:</span>metadata<span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token constant">NODE</span><span class="token operator">:</span>spec<span class="token punctuation">.</span>nodeName
image<span class="token operator">-</span>placeholder <span class="token operator">--</span>sort<span class="token operator">-</span>by spec<span class="token punctuation">.</span>nodeName <span class="token operator">-</span>n kube<span class="token operator">-</span>system
<span class="token constant">POD</span>                              <span class="token constant">NODE</span>
kube<span class="token operator">-</span>controller<span class="token operator">-</span>manager<span class="token operator">-</span>master   master  # etcd<span class="token punctuation">,</span> <span class="token constant">API</span>服务器<span class="token punctuation">,</span> 调度器<span class="token punctuation">,</span> 控制管理器和<span class="token constant">DNS</span>服务运行在master上  
kube<span class="token operator">-</span>dns<span class="token operator">-</span><span class="token number">2334855451</span><span class="token operator">-</span>37d9k        master  
etcd<span class="token operator">-</span>master                      master  
kube<span class="token operator">-</span>apiserver<span class="token operator">-</span>master            master  
kube<span class="token operator">-</span>scheduler<span class="token operator">-</span>master            master  
kube<span class="token operator">-</span>flannel<span class="token operator">-</span>ds<span class="token operator">-</span>tgj9k            node1   
kube<span class="token operator">-</span>proxy<span class="token operator">-</span>ny3xm                 node1   
kube<span class="token operator">-</span>flannel<span class="token operator">-</span>ds<span class="token operator">-</span>0eek8            node2   
kube<span class="token operator">-</span>proxy<span class="token operator">-</span>sp362                 node2  # 三个节点均运行一个Kube Proxy pod和一个Flannel网络pod   
kube<span class="token operator">-</span>flannel<span class="token operator">-</span>ds<span class="token operator">-</span>r5yf4            node3   
kube<span class="token operator">-</span>proxy<span class="token operator">-</span>og9ac                 node3   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>如代码清单所示, <mark><strong>所有的控制平面组件在主节点上作为 pod 运行</strong></mark>. 这里有三个工作节点, 每一个节点运行 kube-proxy 和一个 Flannel pod, 用来为 pod 提供重叠网络(后面会再讨论 Flannel).</p> <p>提示: 如代码清单所示, 可以通过 -o custom-columns 选项自定义展示的列以及 --sort-by 对资源列表进行排序.</p> <p>现在对每一个组件进行研究, 从控制平面的底层组件--<strong>持久化存储组件</strong>开始.</p> <h6 id="_11-1-2-kubernetes如何使用etcd"><a href="#_11-1-2-kubernetes如何使用etcd" class="header-anchor">#</a> 11.1.2 Kubernetes如何使用etcd</h6> <p>本书创建的所有对象如 pod, ReplicationController, 服务和私密凭据等, 需要以<strong>持久化方式</strong>存储到某个地方, 这样它们的 manifest 在 API 服务器重启和失败的时候才不会丢失. 为此, Kubernetes 使用了 <strong>etcd</strong>. <strong>etcd 是一个响应快, 分布式, 一致的 key-value 存储. 因为它是分布式的, 故可以运行多个 etcd 实例来获取高可用性和更好的性能</strong>.</p> <p><mark><strong>唯一能直接和 etcd 通信的是 Kubernetes 的 API 服务器. 所有其他组件通过 API 服务器间接地读取, 写入数据到 etcd. 这带来一些好处, 其中之一就是增强乐观锁系统, 验证系统的健壮性; 并且, 通过把实际存储机制从其他组件抽离, 未来替换起来也更容易. 值得强调的是, etcd 是 Kubernetes 存储集群状态和元数据的唯一的地方.</strong></mark></p> <blockquote><p>关于乐观并发控制</p></blockquote> <p>乐观并发控制(有时候指乐观锁)是指<strong>一段数据包含一个版本数字</strong>, 而不是锁住该段数据并阻止读写操作. 每当更新数据, 版本数就会增加. 当更新数据时, 就会检查版本值是否在客户端读取数据时间和提交时间之间被增加过. 如果增加过, 那么更新会被拒绝, 客户端必须重新读取新数据, 重新尝试更新.</p> <p>两个客户端尝试更新同一个数据条目, 只有第一个会成功.</p> <p>所有的 Kubernetes 包含一个 <strong>metadata.resourceVersion</strong> 字段, 当更新对象时, 客户端需要返回该值到 API 服务器. <strong>如果版本值与 etcd 中存储的不匹配, API 服务器会拒绝该更新</strong>.</p> <blockquote><p>资源如何存储在 etcd 中</p></blockquote> <p>当笔者撰写此书时, Kubernetes 既可以用 etcd 版本 2 也可以用版本 3. etcd v2 把 key 存储在一个层级键空间中, 这使得<strong>键值对类似文件系统的文件</strong>. etcd 中每个 key 要么是一个目录, 包含其他 key, 要么是一个常规 key, 对应一个值. etcd v3 不支持目录, 但是由于 key 格式保持不变(键可以包含斜杠), 仍然可以认为它们可以被组织为目录. Kubernetes 存储所有数据到 etcd 的 <code>/registry</code>​ 下. 下面的代码清单显示 <code>/registry</code>​ 下存储的一系列 key.</p> <p><strong>代码清单-11.2 etcd 中存储的 Kubernetes 的顶层条目</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ etcdctl ls <span class="token operator">/</span>registry
<span class="token operator">/</span>registry<span class="token operator">/</span>configmaps
<span class="token operator">/</span>registry<span class="token operator">/</span>daemonsets
<span class="token operator">/</span>registry<span class="token operator">/</span>deployments
<span class="token operator">/</span>registry<span class="token operator">/</span>events
<span class="token operator">/</span>registry<span class="token operator">/</span>namespaces
<span class="token operator">/</span>registry<span class="token operator">/</span>pods
<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>你可能会发现, 这些 key 和之前几章中学习到的<strong>资源类型对应</strong>.</p> <p>注意: 如果<strong>使用 etcd v3 的 API, 就无法使用 ls 命令来查看目录的内容</strong>. 但是可以通过 <code>etcdctl get /registry--prefix=true</code>​ 列出所有以给定前缀开始的 key.</p> <p>下面的代码清单显示了 <code>/registry/pods</code>​ 目录的内容.</p> <p><strong>代码清单-11.3 /registry/pods 目录下的 key</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ etcdctl ls <span class="token operator">/</span>registry<span class="token operator">/</span>pods
<span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span><span class="token keyword">default</span>
<span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span>kube<span class="token operator">-</span>system
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>从名称可以看出, 这两个条目对应 default 和 kube-system 命名空间, <strong>意味着 pod 按命名空间存储</strong>. 下面的代码清单显示 <code>/registry/pods/default</code>​ 目录下的条目.</p> <p><strong>代码清单-11.4 default 命名空间中 pod 的 etcd 条目</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ etcdctl ls <span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span><span class="token keyword">default</span>
<span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>xk0vc
<span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>wt6ga
<span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>hp2o5
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>每个条目对应一个单独的 pod. 这些不是目录, 而是键值对. 下面的代码清单展示了其中一条存储的内容.</p> <p><strong>代码清单-11.5 一个 etcd 条目代表一个 pod</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ etcdctl get <span class="token operator">/</span>registry<span class="token operator">/</span>pods<span class="token operator">/</span><span class="token keyword">default</span><span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>wt6ga
<span class="token punctuation">{</span><span class="token string-property property">&quot;kind&quot;</span><span class="token operator">:</span><span class="token string">&quot;Pod&quot;</span><span class="token punctuation">,</span><span class="token string-property property">&quot;apiVersion&quot;</span><span class="token operator">:</span><span class="token string">&quot;v1&quot;</span><span class="token punctuation">,</span><span class="token string-property property">&quot;metadata&quot;</span><span class="token operator">:</span><span class="token punctuation">{</span><span class="token string-property property">&quot;name&quot;</span><span class="token operator">:</span><span class="token string">&quot;kubia-159041347-wt6ga&quot;</span><span class="token punctuation">,</span>
<span class="token string-property property">&quot;generateName&quot;</span><span class="token operator">:</span><span class="token string">&quot;kubia-159041347-&quot;</span><span class="token punctuation">,</span><span class="token string-property property">&quot;namespace&quot;</span><span class="token operator">:</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">,</span><span class="token string-property property">&quot;selfLink&quot;</span><span class="token operator">:</span><span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>你可能发现了, 这就是一个 JSON 格式的 pod 定义. <strong>API 服务器将资源的完整 JSON 形式存储到 etcd 中</strong>. 由于 etcd 的层级键空间, 可以想象成<strong>把资源以 JSON 文件格式存储到文件系统中</strong>. 简单易懂, 对吧?</p> <p>警告: Kubernetes 1.7 之前的版本, 密钥凭据的 JSON 内容也像上面一样存储(没有加密). 如果有人有权限直接访问 etcd, 那么可以获取所有的密钥凭据. 从 1.7 版本开始, 密钥凭据会被加密, 这样存储起来更加安全.</p> <blockquote><p>确保存储对象的一致性和可验证性</p></blockquote> <p>还记得第 1 章中提到的 Kubernetes 所依赖的谷歌的 Borg 和 Omega 系统吗? 和 Kubernetes 类似, Omega 使用一个集中存储模块保存集群状态. 不同之处是, 多个控制平面组件可以直接访问存储模块. 所有这些组件需要确保它们都遵循同一个乐观锁机制, 来保证能正确处理冲突. 只要有一个组件没有完全遵循该机制就可能导致数据不一致.</p> <p>Kubernetes 对此做了改进, 要求<strong>所有控制平面组件只能通过 API 服务器操作存储模块. 使用这种方式更新集群状态总是一致的, 因为 API 服务器实现了乐观锁机制, 如果有错误的话, 也会更少. API 服务器同时确保写入存储的数据总是有效的, 只有授权的客户端才能更改数据</strong>.</p> <blockquote><p>确保 etcd 集群一致性</p></blockquote> <p>为保证高可用性, 常常会运行多个 etcd 实例. <strong>多个 etcd 实例需要保持一致. 这种分布式系统需要对系统的实际状态达成一致.</strong> <mark><strong>etcd 使用 RAFT 一致性算法</strong></mark>​<strong>来保证这一点, 确保在任何时间点, 每个节点的状态要么是大部分节点的当前状态, 要么是之前确认过的状态</strong>.</p> <p>连接到 etcd 集群不同节点的客户端, 得到的要么是当前的实际状态, 要么是之前的状态(在 Kubernetes 中, etcd 的唯一客户端是 API 服务器, 但有可能有多个实例).</p> <p><strong>一致性算法要求集群大部分(法定数量)节点参与才能进行到下一个状态</strong>. 结果就是, 如果集群分裂为两个不互联的节点组, 两个组的状态不可能不一致, 因为要从之前状态变化到新状态, 需要有过半的节点参与状态变更. 如果一个组包含了大部分节点, 那么另外一组只有少量节点成员. 第一个组就可以更改集群状态, 后者则不可以. 当两个组重新恢复连接, 第二个组的节点会更新为第一个组的节点的状态.</p> <p><img src="/img/image-20240227234635-wm0n5it.png" alt="image" title="图11.2 在脑裂场景中, 只有拥有大部分(法定数量)节点的组会接受状态变更"></p> <blockquote><p>为什么 etcd 实例数量应该是奇数</p></blockquote> <p><strong>etcd 通常部署奇数个实例</strong>. 你一定想知道为什么. 来比较有一个实例和有两个实例的情况时. 有两个实例时, 要求另一个实例必须在线, 这样才能符合超过半数的数量要求. 如果有一个宕机, 那么 etcd 集群就不能转换到新状态, 因为没有超过半数. 两个实例的情况比一个实例的情况更糟. 对比单节点宕机, 在有两个实例的情况下, 整个集群挂掉的概率增加了 100%.</p> <p>比较 3 节点和 4 节点也是同样的情况. 3 节点情况下, 一个实例宕机, 但超过半数(2个)的节点仍然运行着. 对于 4 节点情况, 需要 3 个节点才能超过半数(2 个不够). 对于 3 节点和 4 节点, 假设只有一个实例会宕机. 当以 4 节点运行时, 一个节点失败后, 剩余节点宕机的可能性会更大(对比 3 节点集群, 一个节点宕机还剩两个节点的情况).</p> <p>通常, <strong>对于大集群, etcd 集群有 5 个或 7 个节点就足够</strong>了. 可以允许 2-3 个节点宕机, 这对于大多数场景来说足够了.</p> <h6 id="_11-1-3-api服务器做了什么"><a href="#_11-1-3-api服务器做了什么" class="header-anchor">#</a> 11.1.3 API服务器做了什么</h6> <p><mark><strong>Kubernetes API 服务器作为中心组件, 其他组件或者客户端(如 kubectl)都会去调用它. 以 RESTful API 的形式提供了可以查询, 修改集群状态的 CRUD(Create, Read, Update, Delete)接口. 它将状态存储到 etcd 中.</strong></mark></p> <p>API 服务器除了提供一种一致的方式将对象存储到 etcd, 也对这些对象做校验, 这样客户端就无法存入非法的对象了(直接写入存储的话是有可能的). 除了校验, 还会处理乐观锁, 这样对于并发更新的情况, 对对象做更改就不会被其他客户端覆盖.</p> <p>API 服务器的<strong>客户端之一</strong>就是本书一开始就介绍使用的<strong>命令行工具 kubectl</strong>. 举个例子, 当以 JSON 文件创建一个资源, kubectl <strong>通过一个 HTTP POST 请求将文件内容发布到 API 服务器</strong>. 图 11.3 显示了接收到请求后 API 服务器内部发生了什么, 后面会做更详细的介绍.</p> <p><img src="/img/image-20240224143727-6nj1iaf.png" alt="image" title="图11.3 API 服务器的操作"></p> <blockquote><p>通过认证插件认证客户端</p></blockquote> <p>首先, API 服务器<strong>需要认证发送请求的客户端</strong>. 这是通过配置在 API 服务器上的一个或多个<strong>认证插件</strong>来实现的. API 服务器会轮流调用这些插件, 直到有一个能确认是谁发送了该请求. 这是通过检查 HTTP 请求实现的.</p> <p>根据认证方式, 用户信息可以从客户端证书或者第 8 章使用的 HTTP 标头(例如 <strong>Authorization</strong>)获取. 插件抽取客户端的用户名, 用户 ID 和归属组. 这些数据在下一阶段, 认证的时候会用到.</p> <blockquote><p>通过授权插件授权客户端</p></blockquote> <p>除了认证插件, API 服务器还可以配置使用一个或多个<strong>授权插件</strong>. 它们的作用是决定认证的用户是否可以对请求资源执行请求操作. 例如, 当创建 pod 时, API 服务器会轮询所有的授权插件, 来确认该用户是否可以在请求命名空间创建 pod. 一旦插件确认了用户可以执行该操作, API 服务器会继续下一步操作.</p> <blockquote><p>通过准入控制插件验证 AND/OR 修改资源请求</p></blockquote> <p>如果请求尝试创建, 修改或者删除一个资源, 请求需要经过<strong>准入控制插件</strong>的验证. 同理, 服务器会配置多个准入控制插件. 这些插件会因为各种原因修改资源, 可能会初始化资源定义中漏配的字段为默认值甚至重写它们. 插件甚至会去修改并不在请求中的相关资源, 同时也会因为某些原因拒绝一个请求. 资源需要经过所有准入控制插件的验证.</p> <p>注意: 如果请求只是尝试读取数据, 则不会做准入控制的验证.</p> <p>准入控制插件包括:</p> <ul><li>AlwaysPullImages: 重写 pod 的 imagePullPolicy 为 Always, 强制每次部署 pod 时拉取镜像.</li> <li>ServiceAccount: 未明确定义服务账户的使用默认账户.</li> <li>NamespaceLifecycle: 防止在命名空间中创建正在被删除的 pod, 或在不存在的命名空间中创建 pod.</li> <li>ResourceQuota: 保证特定命名空间中的 pod 只能使用该命名空间分配数量的资源, 如 CPU 和内存. 将会在第 14 章深入了解.</li></ul> <p>更多的准入控制插件可以在 https://kubernetes.io/docs/admin/admission-controllers/ 中查看 Kubernetes 文档.</p> <blockquote><p>验证资源以及持久化存储</p></blockquote> <p>请求通过了所有的准入控制插件后, API 服务器会验证存储到 etcd 的对象, 然后返回一个响应给客户端.</p> <h6 id="_11-1-4-api服务器如何通知客户端资源变更"><a href="#_11-1-4-api服务器如何通知客户端资源变更" class="header-anchor">#</a> 11.1.4 API服务器如何通知客户端资源变更</h6> <p>除了前面讨论的, API 服务器没有做其他额外的工作. 例如, <strong>当创建一个 ReplicaSet 资源时, 它不会去创建 pod, 同时它不会去管理服务的端点. 那是控制器管理器的工作</strong>.</p> <p>API 服务器甚至也没有告诉这些控制器去做什么. 它做的就是, <strong>启动这些控制器, 以及其他一些组件来监控已部署资源的变更. 控制平面可以请求订阅资源被创建, 修改或删除的通知. 这使得组件可以在集群元数据变化时候执行任何需要做的任务</strong>.</p> <p><strong>客户端通过创建到 API 服务器的 HTTP 连接来监听变更. 通过此连接, 客户端会接收到监听对象的一系列变更通知. 每当更新对象, 服务器把新版本对象发送至所有监听该对象的客户端</strong>. 图 11.4 显示客户端如何监听 pod 的变更, 以及如何将 pod 的变更存储到 etcd, 然后通知所有监听该 pod 的客户端.</p> <p><img src="/img/image-20240224143801-cl48ik9.png" alt="image" title="图11.4 更新对象时, API 服务器给所有监听者发送更新过的对象"></p> <p>kubectl 工具作为 API 服务器的客户端之一, 也支持监听资源. 例如, 当部署 pod 时, 不需要重复执行 kubectl get pods 来定期查询 pod 列表. 可以使用 -watch 标志, 每当创建, 修改, 删除 pod 时就会通知你, 如下面的代码清单所示.</p> <p><strong>代码清单-11.6 监听创建删除 pod 事件</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> pods <span class="token operator">--</span>watch
<span class="token constant">NAME</span>                    <span class="token constant">READY</span>     <span class="token constant">STATUS</span>              <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Pending             <span class="token number">0</span>          0s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Pending             <span class="token number">0</span>          0s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       ContainerCreating   <span class="token number">0</span>          1s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          3s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running             <span class="token number">0</span>          5s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Terminating         <span class="token number">0</span>          9s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Terminating         <span class="token number">0</span>          17s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Terminating         <span class="token number">0</span>          17s
kubia<span class="token operator">-</span><span class="token number">159041347</span><span class="token operator">-</span>14j3i   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Terminating         <span class="token number">0</span>          17s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>甚至可以让 kubectl 打印出整个监听事件的 YAML 文件, 如下:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> pods <span class="token operator">-</span>o yaml <span class="token operator">--</span>watch
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>监听机制同样也可以用于调度器. 调度器是下一个要着重讲解的控制平面组件.</p> <h6 id="_11-1-5-调度器"><a href="#_11-1-5-调度器" class="header-anchor">#</a> 11.1.5 调度器</h6> <p>前面已经学习过, <strong>通常不会去指定 pod 应该运行在哪个集群节点上, 这项工作交给调度器</strong>. 宏观来看, 调度器的操作比较简单. <mark><strong>就是利用 API 服务器的监听机制等待新创建的 pod, 然后给每个新的, 没有节点集的 pod 分配节点</strong></mark>.</p> <p>调度器不会命令选中的节点(或者节点上运行的 Kubelet)去运行 pod. <strong>调度器做的就是通过 API 服务器更新 pod 的定义</strong>. 然后 API 服务器再去通知 Kubelet(同样, 通过之前描述的监听机制)该 pod 已经被调度过. 当目标节点上的 Kubelet 发现该 pod 被调度到本节点, 它就会创建并且运行 pod 的容器.</p> <p>尽管宏观上调度的过程看起来比较简单, 但实际上为 pod 选择最佳节点的任务并不简单. 当然, 最简单的调度方式是不关心节点上已经运行的 pod, 随机选择一个节点. 另一方面, <strong>调度器可以利用高级技术</strong>, 例如机器学习, 来预测接下来几分钟或几小时哪种类型的 pod 将会被调度, 然后以最大的硬件利用率, 无须重新调度已运行 pod 的方式来调度. <strong>Kubernetes 的默认调度器实现方式处于最简单和最复杂程度之间</strong>.</p> <blockquote><p>默认的调度算法</p></blockquote> <p>选择节点操作可以分解为两部分, 如图 11.5 所示:</p> <ul><li><strong>过滤所有节点, 找出能分配给 pod 的可用节点列表.</strong></li> <li><strong>对可用节点按优先级排序, 找出最优节点</strong>. 如果多个节点都有最高的优先级分数, 那么则循环分配, 确保平均分配给 pod.</li></ul> <p><img src="/img/image-20240227234706-24vji6s.png" alt="image" title="图11.5 调度器为 pod 找到可用节点, 然后选择最优节点"></p> <blockquote><p>查找可用节点</p></blockquote> <p>为了决定哪些节点对 pod 可用, 调度器会给每个节点下发一组配置好的<strong>预测函数</strong>. 这些函数会检查:</p> <ul><li>节点是否能<strong>满足 pod 对硬件资源</strong>的请求. 第 14 章会学习如何定义它们.</li> <li>节点<strong>是否耗尽资源</strong>(是否报告过内存/硬盘压力参数)?</li> <li>pod 是否要求被调度到指定节点(通过名字), 是否是当前节点?</li> <li>节点是否有和 pod 规格定义里的节点选择器一致的标签(如果定义了的话)?</li> <li>如果 pod 要求绑定指定的主机端口(第 13 章中讨论)那么这个节点上的这个端口是否已经被占用?</li> <li>如果 pod 要求有特定类型的卷, 该节点是否能为此 pod 加载此卷, 或者说该节点上是否已经有 pod 在使用该卷了?</li> <li>pod 是否能够容忍节点的污点. 污点以及容忍度在第 16 章讲解.</li> <li>pod 是否定义了节点, pod 的亲缘性以及非亲缘性规则? 如果是, 那么调度节点给该 pod 是否会违反规则? 这个也会在第16 章介绍.</li></ul> <p>所有这些测试都必须通过, <strong>节点才有资格调度给 pod</strong>. 在对每个节点做过这些检查后, 调度器得到节点集的一个子集. 任何这些节点都可以运行 pod, 因为它们都有足够的可用资源, 也确认过满足 pod 定义的所有要求.</p> <blockquote><p>为 pod 选择最佳节点</p></blockquote> <p>尽管所有这些节点都能运行 pod, 其中的一些可能还是优于另外一些. 假设有一个 2 节点集群, 两个节点都可用, 但是其中一个运行 10个 pod, 而另一个, 不知道什么原因, 当前没有运行任何 pod. 本例中, 明显调度器应该选第二个节点.</p> <p>或者说, 如果两个节点是由云平台提供的服务, 那么更好的方式是, pod 调度给第一个节点, 将第二个节点释放回云服务商以节省资金.</p> <blockquote><p>pod 高级调度</p></blockquote> <p>考虑另外一个例子. 假设一个 pod 有多个副本. 理想情况下, 你会期望副本能够分散在尽可能多的节点上, 而不是全部分配到单独一个节点上, 因为该节点的宕机会导致 pod 支持的服务不可用. 但是如果 pod 分散在不同的节点上, 单个节点宕机, 并不会对服务造成什么影响.</p> <p><strong>默认情况下, 归属同一服务和 ReplicaSet 的 pod 会分散在多个节点上</strong>. 但不保证每次都是这样. 不过可以通过定义 pod 的亲缘性, 非亲缘规则强制 pod 分散在集群内或者集中在一起, 相关内容会在第 16 章中介绍.</p> <p>仅通过这两个简单的例子就说明了调度有多复杂, 因为它<strong>依赖于大量的因子</strong>. 因此, 调度器既可以配置成满足特定的需要或者基础设施特性, 也可以整体替换为一个定制的实现. 可以抛开调度器运行一个 Kubernetes, 不过那样的话, 就需要手动实现调度了.</p> <blockquote><p>使用多个调度器</p></blockquote> <p>可以在集群中<strong>运行多个调度器</strong>而非单个. 然后, 对每一个 pod, 可以通过在 pod 特性中设置 schedulerName 属性指定调度器来调度特定的 pod.</p> <p>未设置该属性的 pod 由默认调度器调度, 因此其 schedulerName 被设置为 default-scheduler. 其他设置了该属性的 pod 会被默认调度器忽略掉, 它们要么是手动调用, 要么被监听这类 pod 的调度器调用.</p> <p>可以实现自己的调度器, 部署到集群, 或者可以部署有不同配置项的额外 Kubernetes 调度器实例.</p> <h6 id="_11-1-6-控制器管理器中运行的控制器"><a href="#_11-1-6-控制器管理器中运行的控制器" class="header-anchor">#</a> 11.1.6 控制器管理器中运行的控制器</h6> <p>如前面提到的, <strong>API 服务器只做了存储资源到 etcd 和通知客户端有变更的工作. 调度器则只是给 pod 分配节点, 所以需要有活跃的组件确保系统真实状态朝 API 服务器定义的期望的状态收敛. 这个工作由控制器管理器里的控制器来实现</strong>.</p> <p>单个控制器, 管理器进程当前组合了多个执行不同非冲突任务的控制器. 这些控制器最终会被分解到不同的进程, 如果需要的话, 能够用自定义实现替换它们每一个. 控制器包括</p> <ul><li><strong>Replication 管理器</strong>(ReplicationController 资源的管理器)</li> <li><strong>ReplicaSet, DaemonSet 以及 Job 控制器</strong></li> <li><strong>Deployment 控制器</strong></li> <li><strong>StatefulSet 控制器</strong></li> <li><strong>Node 控制器</strong></li> <li><strong>Service 控制器</strong></li> <li><strong>Endpoints 控制器</strong></li> <li><strong>Namespace 控制器</strong></li> <li><strong>PersistentVolume 控制器</strong></li> <li>其他</li></ul> <p>每个控制器做什么通过名字显而易见. 通过上述列表, 几乎可以知道创建每个资源对应的控制器是什么. <mark><strong>资源描述了集群中应该运行什么, 而控制器就是活跃的 Kubernetes 组件, 去做具体工作部署资源</strong></mark>.</p> <blockquote><p>了解控制器做了什么以及如何做的</p></blockquote> <p>控制器做了许多不同的事情, 但是它们<strong>都通过 API 服务器监听资源(部署, 服务等)变更</strong>, 并且不论是创建新对象还是更新, 删除已有对象, 都对变更执行相应操作. 大多数情况下, 这些操作涵盖了新建其他资源或者更新监听的资源本身(例如, 更新对象的 status).</p> <p><mark><strong>总的来说, 控制器执行一个 &quot;调和&quot; 循环, 将实际状态调整为期望状态(在资源 spec 部分定义), 然后将新的实际状态写入资源的 status 部分. 控制器利用监听机制来订阅变更, 但是由于使用监听机制并不保证控制器不会漏掉时间, 所以仍然需要定期执行重列举操作来确保不会丢掉什么</strong></mark>.</p> <p>控制器之间不会直接通信, 它们甚至不知道其他控制器的存在. 每个控制器都连接到 API 服务器, 通过 11.1.3 节描述的监听机制, 请求订阅该控制器负责的一系列资源的变更.</p> <p>这里概括地了解了每个控制器做了什么, 但是如果想深入了解它们做了什么, 建议直接看源代码.</p> <blockquote><p>浏览控制器源代码的几个要点</p></blockquote> <p>如果你对控制器如何运作感兴趣, 强烈推荐看一遍源代码. 为了更容易上手, 下面有几个小建议:</p> <ul><li>控制器的源代码可以从 https://github.com/kubernetes/kubernetes/blob/master/pkg/controller 获取.</li> <li>每个控制器一般有一个构造器, 内部会创建一个 <strong>Informer</strong>, 其实是个监听器, 每次 API 对象有更新就会被调用. 通常, Informer 会监听特定类型的资源变更事件. 查看构造器可以了解控制器监听的是哪个资源.</li> <li>接下来, 去看 worker() 方法. 其中定义了每次控制器需要工作的时候都会调用 worker() 方法. 实际的函数通常保存在一个叫 syncHandler 或类似的字段里. 该字段也在构造器里初始化, 可以在那里找到被调用函数名. 该函数是所有魔法发生的地方.</li></ul> <blockquote><p>Replication 管理器</p></blockquote> <p><strong>启动 ReplicationController 资源的控制器叫作 Replication 管理器</strong>. 第 4 章介绍过 ReplicationController 是如何工作的, 其实不是 ReplicationController 做了实际的工作, 而是 Replication 管理器. 快速回顾下该控制器做了什么, 这有助于理解其他控制器.</p> <p>在第 4 章中说过, ReplicationController 的操作可以理解为一个<strong>无限循环, 每次循环, 控制器都会查找符合其 pod 选择器定义的 pod 的数量, 并且将该数值和期望的复制集(replica)数量做比较</strong>.</p> <p>既然你知道了 API 服务器可以通过监听机制通知客户端, 那么明显地, <mark><strong>控制器不会每次循环去轮询 pod, 而是通过监听机制订阅可能影响期望的复制集(replica)数量或者符合条件 pod 数量的变更事件(见图 11.6). 任何该类型的变化, 将触发控制器重新检查期望的以及实际的复制集数量, 然后做出相应操作</strong></mark>​.</p> <p>当运行的 pod 实例太少时, ReplicationController 会运行额外的实例, 但它自己实际上不会去运行 pod. 它会<strong>创建新的 pod 清单, 发布到 API 服务器, 让调度器以及 Kubelet 来做调度工作并运行 pod</strong>.</p> <p><img src="/img/image-20240227234729-xagukxe.png" alt="image" title="图11.6 Replication 管理器监听 API 对象变更"></p> <p><mark><strong>Replication 管理器通过 API 服务器操纵 pod API 对象来完成其工作</strong></mark>. 所有控制器就是这样运作的.</p> <blockquote><p>RerlicaSet, DaemonSet 以及 Job 控制器</p></blockquote> <p>ReplicaSet 控制器基本上做了和前面描述的 Replication 管理器一样的事情, 所以这里不再赘述. DaemonSet 以及 Job 控制器比较相似, 从它们各自资源集中定义的 pod 模板创建 pod 资源. 与 Replication 管理器类似, <mark><strong>这些控制器不会运行 pod, 而是将 pod 定义到发布 API 服务器, 让 Kubelet 创建容器并运行</strong></mark>​.</p> <blockquote><p>Deployment 控制器</p></blockquote> <p>Deployment 控制器负责使 deployment 的实际状态与对应 Deployment API 对象的期望状态同步.</p> <p>每次 Deployment 对象修改后(如果修改会影响到部署的 pod), Deployment 控制器都会滚动升级到新的版本. 通过创建一个 ReplicaSet, 然后按照 Deployment 中定义的策略同时伸缩新, 旧 RelicaSet, 直到旧 pod 被新的代替. 并不会直接创建任何 pod.</p> <blockquote><p>StatefulSet 控制器</p></blockquote> <p>StatefulSet 控制器, 类似于 ReplicaSet 控制器以及其他相关控制器, 根据 StatefulSet 资源定义创建, 管理, 删除 pod. 其他的控制器只管理 pod, 而 StatefulSet 控制器会初始化并管理每个 pod 实例的持久卷声明字段.</p> <blockquote><p>Node 控制器</p></blockquote> <p>Node 控制器管理 Node 资源, 描述了集群工作节点. 其中, Node 控制器使节点对象列表与集群中实际运行的机器列表保持同步. 同时监控每个节点的健康状态, 删除不可达节点的 pod.</p> <p>Node 控制器不是唯一对 Node 对象做更改的组件. Kubelet 也可以做更改, 那么显然可以由用户通过 REST API 调用做更改.</p> <blockquote><p>Service 控制器</p></blockquote> <p>在第 5 章, 当讨论服务时, 已经了解了存在不同服务类型. 其中一个是 LoadBalancer 服务, 从基础设施服务请求一个负载均衡器使得服务外部可以用. <strong>Service 控制器就是用来在 LoadBalancer 类型服务被创建或删除时, 从基础设施服务请求, 释放负载均衡器的</strong>.</p> <blockquote><p>Endpoint 控制器</p></blockquote> <p>Service 不会直接连接到 pod, 而是包含一个端点列表(IP 和端口), 列表要么是手动, 要么是根据 Service 定义的 pod 选择器自动创建, 更新. Endpoint 控制器作为活动的组件, 定期根据匹配标签选择器的 pod 的 IP, 端口更新端点列表.</p> <p>如图 11.7 所示, <strong>控制器同时监听了 Service 和 pod. 当 Service 被添加, 修改, 或者 pod 被添加, 修改或删除时, 控制器会选中匹配 Service 的 pod 选择器的 pod, 将其 IP 和端口添加到 Endpoint 资源中</strong>. 请记住, Endpoint 对象是个独立的对象, 所以当需要的时候控制器会创建它. 同样地, 当删除 Service 时, Endpoint 对象也会被删除.</p> <p><img src="/img/image-20240227234752-wjyvd3x.png" alt="image" title="图11.7 Endpoint 控制器监听 Service 和 pod 资源并管理 Endpoint"></p> <blockquote><p>Namespace 控制器</p></blockquote> <p>想起命名空间了吗(第 3 章里讨论过)? 大部分资源归属于某个特定命名空间. 当删除一个 Namespace 资源时, 该命名空间里的所有资源都会被删除. 这就是 <strong>Namespace 控制器做的事情. 当收到删除 Namespace 对象的通知时, 控制器通过 API 服务器删除所有归属该命名空间的资源</strong>.</p> <blockquote><p>PersistentVolume 控制器</p></blockquote> <p>第 6 章学习过持久卷以及持久卷声明. 一旦用户创建了一个持久卷声明, Kubernetes 必须找到一个合适的持久卷同时将其和声明绑定. 这些由<strong>持久卷控制器</strong>实现.</p> <p>对于一个持久卷声明, 控制器为声明查找最佳匹配项, 通过选择匹配声明中的访问模式, 并且声明的容量大于需求的容量的最小持久卷. 实现方式是保存一份有序的持久卷列表, 对于每种访问模式按照容量升序排列, 返回列表的第一个卷.</p> <p>当用户删除持久卷声明时, 会解绑卷, 然后根据卷的回收策略进行回收(原样保留, 删除或清空).</p> <blockquote><p>唤醒控制器</p></blockquote> <p>现在, 总体来说你应该对每个控制器做了什么, 以及是如何工作的有个比较好的感觉了. 再一次强调, <mark><strong>所有这些控制器是通过 API 服务器来操作 API 对象的. 它们不会直接和 Kubelet 通信或者发送任何类型的指令</strong></mark>. 实际上, 它们不知道 Kubelet 的存在. 控制器更新 API 服务器的一个资源后, Kubelet 和 Kubernetes Service Proxy(也不知道控制器的存在)会做它们的工作, 例如启动 pod 容器, 加载网络存储, 或者就服务而言, 创建跨 pod 的负载均衡.</p> <p>控制平面处理了整个系统的一部分操作, 为了完全理解 Kubernetes 集群的内部运作方式, 还需要<strong>理解 Kubelet 和 Kubernetes Service Proxy 做了什么</strong>. 下面将学习这些内容.</p> <h6 id="_11-1-7-kubelet做了什么"><a href="#_11-1-7-kubelet做了什么" class="header-anchor">#</a> 11.1.7 Kubelet做了什么</h6> <p><mark><strong>所有 Kubernetes 控制平面的控制器都运行在主节点上, 而 Kubelet 以及 Service Proxy 都运行在工作节点(实际 pod 容器运行的地方)上</strong></mark>. Kubelet 究竟做了什么事情?</p> <blockquote><p>了解 Kubelet 的工作内容</p></blockquote> <p>简单地说, <strong>Kubelet 就是负责所有运行在工作节点上内容的组件</strong>. 它第一个任务就是在 API 服务器中创建一个 Node 资源来注册该节点. 然后需要持续监控 API 服务器是否把该节点分配给 pod, 然后启动 pod 容器. 具体实现方式是告知配置好的容器运行时(Docker, CoreOS 的 Rkt, 或者其他一些东西)来从特定容器镜像运行容器. <strong>Kubelet 随后持续监控运行的容器, 向 API 服务器报告它们的状态, 事件和资源消耗</strong>.</p> <p><strong>Kubelet 也是运行容器存活探针的组件, 当探针报错时它会重启容器</strong>. 最后一点, 当 pod 从 API 服务器删除时, Kubelet 终止容器, 并通知服务器 pod 已经被终止了.</p> <blockquote><p>抛开 API 服务器运行静态 pod</p></blockquote> <p>尽管 Kubelet 一般会和 API 服务器通信并从中获取 pod 清单, 它也可以基于本地指定目录下的 pod 清单来运行 pod, 如图 11.8 所示. 如本章开头所示, 该特性用于将容器化版本的控制平面组件以 pod 形式运行.</p> <p>不但可以按照原有的方式运行 Kubernetes 系统组件, 也可以将 pod 清单放到 Kubelet 的清单目录中, 让 Kubelet 运行和管理它们.</p> <p><img src="/img/image-20240227234813-sih73st.png" alt="image" title="图11.8 Kubelet 基于 API 服务器/本地文件目录中的 pod 定义运行 pod"></p> <p>也可以同样的方式运行自定义的系统容器, 不过推荐用 DaemonSet 来做这项工作.</p> <h6 id="_11-1-8-kubernetes-service-proxy的作用"><a href="#_11-1-8-kubernetes-service-proxy的作用" class="header-anchor">#</a> 11.1.8 Kubernetes Service Proxy的作用</h6> <p><strong>除了 Kubelet, 每个工作节点还会运行 kube-proxy, 用于确保客户端可以通过 Kubernetes API 连接到你定义的服务</strong>. kube-proxy 确保对服务 IP 和端口的连接最终能到达支持服务(或者其他, 非 pod 服务终端)的某个 pod 处. 如果有多个 pod 支撑一个服务, 那么代理会发挥对 pod 的负载均衡作用.</p> <blockquote><p>为什么被叫作代理</p></blockquote> <p>kube-proxy 最初实现为 userspace 代理. 利用实际的服务器集成接收连接, 同时代理给 pod. 为了拦截发往服务 IP 的连接, <strong>代理配置了 iptables 规则</strong>(iptables 是一个管理 Linux 内核数据包过滤功能的工具), 重定向连接到代理服务器. userspace 代理模式大致如图 11.9 所示.</p> <p><img src="/img/image-20240227234833-ejer82t.png" alt="image" title="图11.9 userspace 代理模式"></p> <p>kube-proxy 之所以叫这个名字是因为它确实就是一个代理器, 不过当前性能更好的实现方式<mark><strong>仅仅通过 iptables 规则重定向数据包到一个随机选择的后端 pod, 而不会传递到一个实际的代理服务器. 这个模式称为 iptables 代理模式</strong></mark>, 如图 11.10 所示.</p> <p><img src="/img/image-20240227234854-vxxktx3.png" alt="image" title="图11.10 iptables 代理模式"></p> <p>两种模式的主要区别是: 数据包是否会传递给 kube-proxy, 是否必须在用户空间处理, 或者数据包只会在内核处理(内核空间). 这对性能有巨大的影响.</p> <p>另外一个小的区别是: userspace 代理模式以轮询模式对连接做负载均衡, 而 iptables 代理模式不会, 它随机选择 pod. 当只有少数客户端使用一个服务时, 可能不会平均分布在 pod 中. 例如, 如果一个服务有两个 pod 支持, 但有 5 个左右的客户端, 如果你看到 4 个连接到 pod A, 而只有一个连接到 pod B, 不必惊讶. 对于客户端数量更多的 pod, 这个问题就不会特别明显.</p> <p>在 11.5 节会学习 iptables 代理模式具体是如何工作的.</p> <h6 id="_11-1-9-介绍kubernetes插件"><a href="#_11-1-9-介绍kubernetes插件" class="header-anchor">#</a> 11.1.9 介绍Kubernetes插件</h6> <p>前面已经讨论了 Kubernetes 集群正常工作所需要的一些核心组件. 但在开头的几章中, 也罗列了一些插件, 它们不是必需的; 这些插件用于启用 Kubernetes 服务的 DNS 查询, 通过单个外部 IP 地址暴露多个 HTTP 服务, Kubernetes web 仪表板等特性.</p> <blockquote><p>如何部署插件</p></blockquote> <p>通过提交 YAML 清单文件到 API 服务器(本书的通用做法), <strong>这些组件会成为插件并作为 pod 部署</strong>. 有些组件是通过 Deployment 资源或者 ReplicationController 资源部署的, 有些是通过 DaemonSet.</p> <p>例如, 写作本书时, 在 Minikube 中, Ingress 控制器和仪表板插件按照 ReplicationController 部署, 如下面的代码清单所示.</p> <p><strong>代码清单-11.7 插件在 Minikube 中 作为 ReplicationController 部署</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> rc <span class="token operator">-</span>n kube<span class="token operator">-</span>system
<span class="token constant">NAME</span>                       <span class="token constant">DESIRED</span>   <span class="token constant">CURRENT</span>   <span class="token constant">READY</span>     <span class="token constant">AGE</span>
<span class="token keyword">default</span><span class="token operator">-</span>http<span class="token operator">-</span>backend       <span class="token number">1</span>         <span class="token number">1</span>         <span class="token number">1</span>         6d
kubernetes<span class="token operator">-</span>dashboard       <span class="token number">1</span>         <span class="token number">1</span>         <span class="token number">1</span>         6d
nginx<span class="token operator">-</span>ingress<span class="token operator">-</span>controller   <span class="token number">1</span>         <span class="token number">1</span>         <span class="token number">1</span>         6d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>DNS 插件作为 Deployment 部署, 如下面的代码清单所示.</p> <p><strong>代码清单-11.8 kube-dns Deployment</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> deploy <span class="token operator">-</span>n kube<span class="token operator">-</span>system
<span class="token constant">NAME</span>       <span class="token constant">DESIRED</span>   <span class="token constant">CURRENT</span>   <span class="token constant">UP</span><span class="token operator">-</span><span class="token constant">TO</span><span class="token operator">-</span><span class="token constant">DATE</span>   <span class="token constant">AVAILABLE</span>   <span class="token constant">AGE</span>
kube<span class="token operator">-</span>dns   <span class="token number">1</span>         <span class="token number">1</span>         <span class="token number">1</span>            <span class="token number">1</span>           6d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>来看看 DNS 和 Ingress 控制器是如何工作的.</p> <blockquote><p>DNS 服务器如何工作</p></blockquote> <p>集群中的<strong>所有 pod 默认配置使用集群内部 DNS 服务器</strong>. 这使得 pod 能够轻松地通过名称查询到服务, 甚至是无头服务 pod 的 IP 地址.</p> <p>DNS 服务 pod 通过 kube-dns 服务对外暴露, 使得该 pod 能够像其他 pod 一样在集群中移动. 服务的 IP 地址在集群每个容器的 <code>/etc/reslv.conf</code>​ 文件的 nameserver 中定义. kube-dns pod 利用 API 服务器的监控机制来订阅 Service 和 Endpoint 的变动, 以及 DNS 记录的变更, 使得其客户端(相对地)总是能够获取到最新的 DNS 信息. 客观地说, 在 Service 和 Endpoint 资源发生变化到 DNS pod 收到订阅通知时间点之间, DNS 记录可能会无效.</p> <blockquote><p>Ingress 控制器如何工作</p></blockquote> <p>和 DNS 插件相比, Ingress 控制器的实现有点不同, 但它们大部分的工作方式相同. <strong>Ingress 控制器运行一个反向代理服务器(例如, 类似 Nginx), 根据集群中定义的 Ingress, Service 以及 Endpoint 资源来配置该控制器. 所以需要订阅这些资源(通过监听机制), 然后每次其中一个发生变化则更新代理服务器的配置</strong>.</p> <p>尽管 Ingress 资源的定义指向一个 Service, <strong>Ingress 控制器会直接将流量转到服务的 pod 而不经过服务 IP</strong>. 当外部客户端通过 Ingress 控制器连接时, 会对客户端 IP 进行保存, 这使得在某些用例中, 控制器比 Service 更受欢迎.</p> <blockquote><p>使用其他插件</p></blockquote> <p>你已经了解了 DNS 服务器和 Ingress 控制器插件同控制器管理器中运行的控制器比较相似, 除了它们不会仅通过 API 服务器监听, 修改资源, 也会接收客户端的连接.</p> <p>其他插件也类似. 它们都需要监听集群状态, 当有变更时执行相应动作. 后面会在剩余的章节中介绍一些其他的插件.</p> <h6 id="_11-1-10-总结概览"><a href="#_11-1-10-总结概览" class="header-anchor">#</a> 11.1.10 总结概览</h6> <p>你已经了解了整个 Kubernetes 系统由相对小的, 完善功能划分的松耦合组件构成. <strong>API 服务器, 调度器, 控制器管理器中运行的控制器, Kubelet 以及 kube-proxy 一起合作来保证实际的状态和你定义的期望状态一致</strong>.</p> <p>例如, 向 API 服务器提交一个 pod 配置会触发 Kubernetes 组件间的协作, 这会导致 pod 的容器运行. 这里的细节将会在接下来的部分详细说明.</p> <h5 id="_11-2-控制器如何协作"><a href="#_11-2-控制器如何协作" class="header-anchor">#</a> 11.2 控制器如何协作</h5> <p>现在已经了解了 Kubernetes 集群包含哪些组件. 为了强化对 Kubernetes 工作方式的理解, 下面看一下<strong>当一个 pod 资源被创建时会发生什么. 因为一般不会直接创建 pod, 所以创建 Deployment 资源作为替代, 然后观察启动 pod 的容器会发生什么</strong>.</p> <h6 id="_11-2-1-了解涉及哪些组件"><a href="#_11-2-1-了解涉及哪些组件" class="header-anchor">#</a> 11.2.1 了解涉及哪些组件</h6> <p>在启动整个流程之前, 控制器, 调度器, Kubelet 就已经通过 API 服务器<strong>监听它们各自资源类型的变化</strong>了. 如图 11.11 所示. 图中描画的每个组件在即将触发的流程中都起到一定的作用. 图表中不包含 etcd, 因为它被隐藏在 API 服务器之后, 可以想象成 API 服务器就是对象存储的地方.</p> <p><img src="/img/image-20240227234919-p4a7xgx.png" alt="image" title="图11.11 Kubernetes 组件通过 API 服务器监听 API 对象"></p> <h6 id="_11-2-2-事件链"><a href="#_11-2-2-事件链" class="header-anchor">#</a> 11.2.2 事件链</h6> <p><strong>准备包含 Deployment 清单的 YAML 文件, 通过 kubetctl 提交到 Kubernetes</strong>. kubectl 通过 HTTP POST 请求发送清单到 Kubernetes API 服务器. API 服务器检查 Deployment 定义, 存储到 etcd, 返回响应给 kubectl. 现在事件链开始被揭示出来, 如图 11.12 所示.</p> <p><img src="/img/image-20240227234940-uwqlwqz.png" alt="image" title="图11.12 Deployment 资源提交到 API 服务器的事件链"></p> <blockquote><p>Deployment 控制器生成 ReplicaSet</p></blockquote> <p>当新创建 Deployment 资源时, 所有通过 API 服务器监听机制<strong>监听 Deployment 列表的客户端马上会收到通知</strong>. 其中有个客户端叫 <strong>Deployment 控制器</strong>, 之前讨论过, 该控制器是一个负责处理部署事务的活动组件.</p> <p>回忆一下第 9 章的内容, <mark><strong>一个 Deployment 由一个或多个 Replicaset 支持, ReplicaSet 后面会创建实际的 pod. 当 Deployment 控制器检查到有一个新的 Deployment 对象时, 会按照 Deploymnet 当前定义创建 ReplicaSet. 这包括通过 Kubernetes API 创建一个新的 ReplicaSet 资源. Deployment 控制器完全不会去处理单个 pod</strong></mark>.</p> <blockquote><p>ReplicaSet 控制器创建 pod 资源</p></blockquote> <p><strong>新创建的 ReplicaSet 由 ReplicaSet 控制器(通过 API 服务器创建, 修改, 删除 ReplicaSet 资源)接收. 控制器会考虑 replica 数量, ReplicaSet 中定义的 pod 选择器, 然后检查是否有足够的满足选择器的 pod.</strong></p> <p><strong>然后控制器会基于 ReplicatSet 的 pod 模板创建 pod 资源(当 Deployment 控制器创建 ReplicaSet 时, 会从 Deployment 复制 pod 模板).</strong></p> <blockquote><p>调度器分配节点给新创建的 pod</p></blockquote> <p>新创建的 pod 目前保存在 etcd 中, 但是它们每个都缺少一个重要的东西--它们还没有任何关联节点. 它们的 nodeName 属性还未被设置. 调度器会监控像这样的 pod, 发现一个, 就会为 pod 选择最佳节点, 并<strong>将节点分配给 pod</strong>. pod 的定义现在就会包含它应该运行在哪个节点.</p> <p>目前, 所有的一切都发生在 Kubernetes <strong>控制平面</strong>中. 参与这个全过程的控制器没有做其他具体的事情, 除了通过 API 服务器更新资源.</p> <blockquote><p>Kubelet 运行 pod 容器</p></blockquote> <p>目前, 工作节点还没做任何事情, pod 容器还没有被启动起来, pod 容器的图片还没有下载.</p> <p>随着 pod 目前分配给了特定的节点, 节点上的 Kubelet 终于可以工作了. <strong>Kubelet 通过 API 服务器监听 pod 变更, 发现有新的 pod 分配到本节点后, 会去检查 pod 定义, 然后命令 Docker 或者任何使用的容器运行时来启动 pod 容器, 容器运行时就会去运行容器</strong>.</p> <h6 id="_11-2-3-观察集群事件"><a href="#_11-2-3-观察集群事件" class="header-anchor">#</a> 11.2.3 观察集群事件</h6> <p><strong>控制平面组件和 Kubelet 执行动作时, 都会发送事件给 API 服务器</strong>. 发送事件是通过<strong>创建事件资源</strong>来实现的, 事件资源和其他的 Kubernetes 资源类似. 每次使用 kubectl describe 来检查资源的时候, 就能看到资源相关的事件, 也可以直接用 kubectl get events 获取事件.</p> <p>可能是个人的感受, 使用 kubectl get 检查事件比较痛苦, 因为不是以合适的时间顺序显示的. 当一个事件发生了多次, 该事件只会被显示一次, 显示首次出现时间, 最后一次出现时间以及发生次数. 幸运的是, 利用 --watch 选项监听事件肉眼看起来更简单, 对于观察集群发生了什么也更有用.</p> <p>下面的代码清单展示了前述过程中发出的事件(由于页面空间有限, 有些列被删掉了, 输出也做了改动).</p> <p><strong>代码清单-11.9 观察控制器发出的事件</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> events <span class="token operator">--</span>watch
    <span class="token constant">NAME</span>             <span class="token constant">KIND</span>         <span class="token constant">REASON</span>              <span class="token constant">SOURCE</span>
<span class="token operator">...</span> kubia            Deployment   ScalingReplicaSet   deployment<span class="token operator">-</span>controller
                     Scaled up replica <span class="token keyword">set</span> kubia<span class="token operator">-</span><span class="token number">193</span> to <span class="token number">3</span>
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span>        ReplicaSet   SuccessfulCreate    replicaset<span class="token operator">-</span>controller
                     Created pod<span class="token operator">:</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>w7ll2
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>tpg6j  Pod          Scheduled           <span class="token keyword">default</span><span class="token operator">-</span>scheduler
                     Successfully assigned kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>tpg6j to node1
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span>        ReplicaSet   SuccessfulCreate    replicaset<span class="token operator">-</span>controller
                     Created pod<span class="token operator">:</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span><span class="token number">39590</span>
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span>        ReplicaSet   SuccessfulCreate    replicaset<span class="token operator">-</span>controller
                     Created pod<span class="token operator">:</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>tpg6j
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span><span class="token number">39590</span>  Pod          Scheduled           <span class="token keyword">default</span><span class="token operator">-</span>scheduler
                     Successfully assigned kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span><span class="token number">39590</span> to node2
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>w7ll2  Pod          Scheduled           <span class="token keyword">default</span><span class="token operator">-</span>scheduler
                     Successfully assigned kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>w7ll2 to node2
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>tpg6j  Pod          Pulled              kubelet<span class="token punctuation">,</span> node1
                     Container image already present on machine
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>tpg6j  Pod          Created             kubelet<span class="token punctuation">,</span> node1
                     Created container <span class="token keyword">with</span> id 13da752
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span><span class="token number">39590</span>  Pod          Pulled              kubelet<span class="token punctuation">,</span> node2
                     Container image already present on machine
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>tpg6j  Pod          Started             kubelet<span class="token punctuation">,</span> node1
                     Started container <span class="token keyword">with</span> id 13da752
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span>w7ll2  Pod          Pulled              kubelet<span class="token punctuation">,</span> node2
                     Container image already present on machine
<span class="token operator">...</span> kubia<span class="token operator">-</span><span class="token number">193</span><span class="token operator">-</span><span class="token number">39590</span>  Pod          Created             kubelet<span class="token punctuation">,</span> node2
                     Created container <span class="token keyword">with</span> id <span class="token number">8850184</span>
<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br></div></div><p>SOURCE 列显示<strong>执行动作的控制器</strong>, NAME 和 KIND 列显示控制器作用的资源. REASON 列以及 MESSAGE 列(显示在每一项的第二行)提供控制器所做的更详细的信息.</p> <h5 id="_11-3-了解运行中的pod是什么"><a href="#_11-3-了解运行中的pod是什么" class="header-anchor">#</a> 11.3 了解运行中的pod是什么</h5> <p>当 pod 运行时, 来仔细看一下<mark><strong>运行的 pod 到底是什么</strong></mark>. 如果 pod <strong>包含单个容器</strong>, 你认为 Kubelet 会只运行单个容器, 还是更多?</p> <p>读本书的过程中, 你已经运行过多个 pod 了. 如果你是个喜欢深究的人, 那么你可能已经看过, 当创建一个 pod 时实际运行的 Docker. 如果没有, 让笔者为你解释.</p> <p>想象你运行<strong>单个容器的 pod</strong>, 假设创建了一个 Nginx pod:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl run nginx <span class="token operator">--</span>image<span class="token operator">=</span>nginx
deployment <span class="token string">&quot;nginx&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>此时, 可以 ssh 到运行 pod 的工作节点, 检查一系列运行的 Docker 容器. 笔者用的是 Minikube, 所以使用 minikube ssh 来 ssh 到单个节点.</p> <p><strong>一旦进入节点内部, 可以通过 docker ps 命令列出所有运行的容器</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-11.10 列出运行的 Docker 容器</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>docker@minikubeVM<span class="token operator">:</span><span class="token operator">~</span>$ docker ps
<span class="token constant">CONTAINER</span> <span class="token constant">ID</span>   <span class="token constant">IMAGE</span>                  <span class="token constant">COMMAND</span>                 <span class="token constant">CREATED</span>
c917a6f3c3f7   nginx                  <span class="token string">&quot;nginx -g 'daemon off&quot;</span>  <span class="token number">4</span> seconds ago
98b8bf797174   gcr<span class="token punctuation">.</span>io<span class="token operator">/</span><span class="token operator">...</span><span class="token operator">/</span>pause<span class="token operator">:</span><span class="token number">3.0</span>   <span class="token string">&quot;/pause&quot;</span>                <span class="token number">7</span> seconds ago
<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>注意: 笔者已经把不相关的信息(包含列和行)从前面的代码清单中删除了, 也删除了所有其他运行的容器. 如果你自己尝试该命令, 注意几秒前创建的两个容器.</p> <p>可以看到 Nginx 容器, 以及一个附加容器. 从 COMMAND 列判断, 附加容器没有做任何事情(容器命令是 &quot;pause&quot;). 如果仔细观察, 会发现容器是在 Nginx 容器前几秒创建的. 它的作用是什么?</p> <p>被暂停(pause)的容器将一个 pod 所有的容器收纳到一起. 还记得<strong>一个 pod 的所有容器是如何共享同一个网络和 Linux 命名空间</strong>的吗? 暂停的容器是一个<strong>基础容器, 它的唯一目的就是保存所有的命名空间</strong>. 所有 pod 的其他用户定义容器<strong>使用 pod 的该基础容器的命名空间</strong>(见图 11.13).</p> <p><img src="/img/image-20240224225616-r91glmo.png" alt="image" title="图11.13 一个双容器 pod 有 3个运行的容器, 共享同一个 Linux 命名空间"></p> <p>实际的应用容器可能会挂掉并重启. 当容器重启, 容器需要处于与之前相同的 Linux 命名空间中. 基础容器(infrastructure container)使这成为可能, 因为它的生命周期和 pod 绑定, 基础容器 pod 被调度直到被删除一直会运行. 如果基础 pod 在这期间被关闭, Kubelet 会重新创建它, 以及 pod 的所有容器.</p> <h5 id="_11-4-跨pod网络通信"><a href="#_11-4-跨pod网络通信" class="header-anchor">#</a> 11.4 跨pod网络通信</h5> <p>现在已经知道<strong>每个 pod 有自己唯一的 IP 地址, 可以通过一个扁平的, 非 NAT 网络和其他 pod 通信</strong>. Kubernetes 是如何做到这一点的? 简单来说, Kubernetes 不负责这块. <strong>网络是由系统管理员或者 Container Network Interface(CNI)插件建立的, 而非 Kubernetes 本身</strong>.</p> <h6 id="_11-4-1-网络应该是什么样的"><a href="#_11-4-1-网络应该是什么样的" class="header-anchor">#</a> 11.4.1 网络应该是什么样的</h6> <p>Kubernetes 并不会要求你使用特定的网络技术, 但是授权 pod(或者更准确地说, 其容器)不论是否运行在同一个工作节点上, 可以互相通信. pod 用于通信的网络必须是: <strong>pod 自己认为的 IP 地址一定和所有其他节点认为该 pod 拥有的 IP 地址一致</strong>.</p> <p>查看图 11.14. 当 pod A 连接(发送网络包)到 pod B 时, <strong>pod B 获取到的源 IP 地址必须和 pod A 自己认为的 IP 地址一致. 其间应该没有网络地址转换(NAT)操作, 即 pod A 发送到 pod B 的包必须保持源和目的地址不变</strong>.</p> <p>这很重要, 保证运行在 pod 内部的应用网络的<strong>简洁性</strong>, 就像运行在<strong>同一个网关机</strong>上一样. pod 没有 NAT 使得运行在其中的应用可以自己注册在其他 pod 中.</p> <p><img src="/img/image-20240224225647-txia6gu.png" alt="image" title="图11.14 Kubernetes 规定 pod 必须通过非 NAT 网络进行连接"></p> <p>例如, 有客户端 pod X 和 pod Y, 为所有通过它们注册的 pod 提供通知服务. pod X 连接到 pod Y 并且告诉 pod Y, &quot;你好, 我是 pod X,IP 地址为 1.2.3.4, 请把更新发送到这个 IP 地址&quot;. 提供服务的 pod 可以通过收到的 IP 地址连接第一个 pod.</p> <p>pod 到节点及节点到 pod 通信也应用了<strong>无 NAT 通信</strong>. 但是当 pod 和 internet 上的服务通信时, pod 发送包的源 IP 不需要改变, 因为 pod 的 IP 是私有的. 向外发送包的源 IP 地址会被改成主机工作节点的 IP 地址.</p> <p>构建一个像样的 Kubernetes 集群包含按照这些要求建立网络. 有不同的方法和技术来建立, 在给定场景中它们都有其优点和缺点. 因此, 这里不会深入探究特定的技术, 会阐述跨 pod 网络通用的工作原理.</p> <h6 id="_11-4-2-深入了解网络工作原理"><a href="#_11-4-2-深入了解网络工作原理" class="header-anchor">#</a> 11.4.2 深入了解网络工作原理</h6> <p>在 11.3 节看到创建了 pod 的 IP 地址以及网络命名空间, 由<strong>基础设施容器(暂停容器)来保存这些信息</strong>, 然后 pod 容器就可以使用网络命名空间了. pod 网络接口就是生成在基础设施容器的一些东西. 下面看一下接口是如何被创建的, 以及如何连接到其他 pod 的接口, 如图 11.15 所示.</p> <p><img src="/img/image-20240224225712-03ogp76.png" alt="image" title="图11.15 同一节点上 pod 通过虚拟 Ethernet 接口对连接到同一个桥接"></p> <blockquote><p>同节点 pod 通信</p></blockquote> <p><strong>基础设施容器启动之前, 会为容器创建一个虚拟 Ethernet 接口对(一个 veth pair), 其中一个对的接口保留在主机的命名空间中(在节点上运行 ifconfig 命令时可以看到 vethXXX 的条目), 而其他的对被移入容器网络命名空间, 并重命名为 eth0. 两个虚拟接口就像管道的两端(或者说像 Ethernet 电缆连接的两个网络设备)——从一端进入, 另一端出来, 等等</strong>.</p> <p><strong>主机网络命名空间的接口会绑定到容器运行时配置使用的网络桥接上. 从网桥的地址段中取 IP 地址赋值给容器内的 eth0 接口. 应用的任何运行在容器内部的程序都会发送数据到 eth0 网络接口(在容器命名空间中的那一个), 数据从主机命名空间的另一个 veth 接口出来, 然后发送给网桥. 这意味着任何连接到网桥的网络接口都可以接收该数据</strong>.</p> <p>如果 pod A 发送网络包到 pod B, 报文首先会经过 pod A 的 veth 对到网桥然后经过 pod B 的 veth 对. <mark><strong>所有节点上的容器都会连接到同一个网桥, 意味着它们都能够互相通信</strong></mark>. 但是要让运行在不同节点上的容器之间能够通信, <strong>这些节点的网桥需要以某种方式连接起来</strong>.</p> <blockquote><p>不同节点上的 pod 通信</p></blockquote> <p><strong>有多种连接不同节点上的网桥的方式</strong>. 可以通过 <strong>overlay 或 underlay 网络</strong>, 或者常规的三层路由, 这会在后面看到.</p> <p><strong>跨整个集群的 pod 的 IP 地址必须是唯一的, 所以跨节点的网桥必须使用非重叠地址段, 防止不同节点上的 pod 拿到同一个 IP</strong>. 如图 11.16 所示的例子, 节点 A 上的网桥使用 10.1.1.0/24 IP 段, 节点 B 上的网桥使用 10.1.2.0/24 IP 段, 确保没有 IP 地址冲突的可能性.</p> <p>图 11.16 显示了通过三层网络支持跨两个节点 pod 通信, 节点的<strong>物理网络接口也需要连接到网桥</strong>. 节点 A 的路由表需要被配置成图中所示, 这样所有目的地为 10.1.2.0/24 的报文会被路由到节点 B, 同时节点 B 的路由表需要被配置成图中所示, 这样发送到 10.1.1.0/24 的包会被发送到节点 A.</p> <p><img src="/img/image-20240224225745-6ppzz1l.png" alt="image" title="图11.16 为了让不同节点上的 pod 能够通信, 网桥需要以某种方式连接"></p> <p>按照该配置, <mark><strong>当报文从一个节点上容器发送到其他节点上的容器, 报文先通过 veth pair, 通过网桥到节点物理适配器, 然后通过网线传到其他节点的物理适配器, 再通过其他节点的网桥, 最终经过 veth pair 到达目标容器</strong></mark>.</p> <p>仅当节点连接到相同网关, 之间没有任何路由时上述方案有效. 否则路由器会扔包, 因为它们所涉及的 pod IP 是私有的. 当然, 也可以配置路由使其在节点间能够路由报文, 但是随着节点数量增加, 配置会变得更困难, 也更容易出错. 因此, 使用 SDN(软件定义网络)技术可以简化问题, SDN 可以让节点忽略底层网络拓扑, 无论多复杂, 结果就像连接到同一个网关上. 从 pod 发出的报文会被封装, 通过网络发送给运行其他 pod 的网络, 然后被解封装, 以原始格式传递给 pod.</p> <h6 id="_11-4-3-引入容器网络接口"><a href="#_11-4-3-引入容器网络接口" class="header-anchor">#</a> 11.4.3 引入容器网络接口</h6> <p>为了让连接容器到网络更加方便, 启动一个项目<strong>容器网络接口(CNI)</strong> . CNI 允许 Kubernetes 可配置使用任何 CNI 插件. 这些插件包含:</p> <ul><li>Calico</li> <li>Flannel</li> <li>Romana</li> <li>Weave Net</li> <li>其他</li></ul> <p>安装一个网络插件并不难, 只需要<strong>部署一个包含 DaemonSet 以及其他支持资源的 YAML</strong>. 每个插件项目首页都会提供这样一个 YAML 文件. 如你所想, DaemonSet 用于往所有集群节点部署一个网络代理, 然后会绑定 CNI 接口到节点. 但是, 注意 Kubetlet 需要用 --network-plugin=cni 命令启动才能使用 CNI.</p> <h5 id="_11-5-服务是如何实现的"><a href="#_11-5-服务是如何实现的" class="header-anchor">#</a> 11.5 服务是如何实现的</h5> <p>在第 5 章中学习过 Service, <strong>Service 允许长时间对外暴露一系列 pod, 稳定的 IP 地址以及端口</strong>. 为了聚焦 Service 的目的以及它们如何被使用, 当时并没有深入探究其工作原理. 但要真正理解服务, 并更好地了解当事情的行为与预期不一致时应该从哪着手, 就需要<mark><strong>了解服务的实现原理</strong></mark>.</p> <h6 id="_11-5-1-引入kube-proxy"><a href="#_11-5-1-引入kube-proxy" class="header-anchor">#</a> 11.5.1 引入kube-proxy</h6> <p><strong>和 Service 相关的任何事情都由每个节点上运行的 kube-proxy 进程处理</strong>. 开始的时候, kube-proxy 确实是一个 proxy, 等待连接, 对每个进来的连接, 连接到一个 pod. 这称为 userspace(用户空间)代理模式. 后来, <mark><strong>性能更好的 iptables 代理模式取代了它. iptables 代理模式目前是默认的模式</strong></mark>, 如果有需要也仍然可以配置 Kubernetes 使用旧模式.</p> <p>在继续之前, 先快速回顾一下 Service 的几个知识点, 对理解下面几段有帮助.</p> <p>之前了解过, <strong>每个 Service 有其自己稳定的 IP 地址和端口. 客户端(通常为 pod)通过连接该 IP 和端口使用服务. IP 地址是虚拟的, 没有被分配给任何网络接口, 当数据包离开节点时也不会列为数据包的源或目的 IP 地址. Service 的一个关键细节是, 它们包含一个 IP, 端口对(或者针对多端口 Service 有多个 IP, 端口对), 所以服务 IP 本身并不代表任何东西. 这就是为什么不能够 ping 通它们</strong>.</p> <h6 id="_11-5-2-kube-proxy如何使用iptables"><a href="#_11-5-2-kube-proxy如何使用iptables" class="header-anchor">#</a> 11.5.2 kube-proxy如何使用iptables</h6> <p><strong>当在 API 服务器中创建一个服务时, 虚拟 IP 地址立刻就会分配给它</strong>. 之后很短时间内, API 服务器会通知所有运行在工作节点上的 kube-proxy 客户端<strong>有一个新服务已经被创建</strong>了. 然后, <strong>每个 kube-proxy 都会让该服务在自己的运行节点上可寻址. 原理是通过建立一些 iptables 规则, 确保每个目的地为服务的 IP/端口对 的数据包被解析, 目的地址被修改, 这样数据包就会被重定向到支持服务的一个 pod</strong>.</p> <p>除了监控 API 对 Service 的更改, kube-proxy 也监控对 Endpoint 对象的更改. 在第 5 章讨论过, 下面回顾一下, 因为你基本上不会去手动创建它们, 所以比较容易忘记它们的存在. <strong>Endpoint 对象保存所有支持服务的 pod 的 IP/端口对</strong>(一个 IP/端口对也可以指向除 pod 之外的其他对象). 这就是为什么 kube-proxy 必须监听所有 Endpoint 对象. 毕竟 Endpoint 对象在每次新创建或删除支持 pod 时都会发生变更, 当 pod 的就绪状态发生变化或者 pod 的标签发生变化, 就会落入或超出服务的范畴.</p> <p>现在来了解一下 kube-proxy 如何让客户端能够通过 Service 连接到这些 pod, 如图 11.17 所示.</p> <p><img src="/img/image-20240224225846-19key1i.png" alt="image" title="图11.17 发送到服务虚拟 IP/端口对的网络包会被修改, 重定向到一个随机选择的后端 pod"></p> <p>图中描述 kube-proxy 做了什么, 以及数据包如何通过客户端 pod 发送到支持服务的一个 pod 上. 下面检查一下<strong>当通过客户端 pod(图中的 pod A)发送数据包时发生了什么</strong>.</p> <p><strong>包目的地初始设置为服务的 IP 和端口</strong>(在本例中, Service 是在 172.30.0.1:80). 发送到网络之前, 节点 A 的内核会根据<strong>配置在该节点上的 iptables 规则处理数据包</strong>.</p> <p>内核会检查数据包<strong>是否匹配任何这些 iptables 规则</strong>. 其中有个规则规定如果有任何数据包的目的地 IP 等于 172.30.0.1, 目的地端口等于 80, 那么<strong>数据包的目的地 IP 和端口应该被替换为随机选中的 pod 的 IP 和端口</strong>.</p> <p>本例中的数据包满足规则, 故而它的 <strong>IP/端口被改变</strong>了. 在本例中, pod B2 被随机选中了, 所有数据包的目的地 IP <strong>变更</strong>为 10.1.2.1(pod B2 的 IP), 端口改为 8080(Service 中定义的目标端口). 就<strong>好像是客户端 pod 直接发送数据包给 pod B 而不是通过 Service</strong>.</p> <p>实际上可能比描述的要更复杂一点儿, 但是上述内容是需要理解的<strong>最重要</strong>的内容.</p> <h5 id="_11-6-运行高可用集群"><a href="#_11-6-运行高可用集群" class="header-anchor">#</a> 11.6 运行高可用集群</h5> <p>在 Kubernetes 上运行应用的一个理由就是, 保证运行不被中断, 或者说尽量少地人工介入基础设施导致的宕机. 为了能够不中断地运行服务, 不仅应用要一直运行, <strong>Kubernetes 控制平面的组件也要不间断运行</strong>. 接下来就了解一下<mark><strong>达到高可用性需要做到什么</strong></mark>.</p> <h6 id="_11-6-1-让应用变得高可用"><a href="#_11-6-1-让应用变得高可用" class="header-anchor">#</a> 11.6.1 让应用变得高可用</h6> <p>当在 Kubernetes 运行应用时, 有不同的<strong>控制器来保证应用的平滑运行</strong>, 即使节点宕机也能够保持特定的规模. 为了保证应用的高可用性, <strong>只需通过 Deployment 资源运行应用, 配置合适数量的复制集, 其他的交给 Kubernetes 处理</strong>.</p> <blockquote><p>运行多实例来减少宕机可能性</p></blockquote> <p>这需要应用可以<strong>水平扩展</strong>, 不过即使不可以, 仍然可以使用 Deployment, 将复制集数量设为 1. 如果复制集不可用, 会快速替换为一个新的, 尽管不会同时发生. 让所有相关控制器都发现有节点宕机, 创建新的 pod 复制集, 启动 pod 容器可能需要一些时间. 不可避免中间会有小段宕机时间.</p> <blockquote><p>对不能水平扩展的应用使用领导选举机制</p></blockquote> <p>为了避免宕机, 需要在运行一个活跃的应用的同时再运行一个附加的非活跃复制集, 通过一个快速起效租约或者领导选举机制来确保只有一个是有效的. 以防你不熟悉领导者选举算法, 提一下, 它是一种分布式环境中多应用实例对谁是领导者达成一致的方式. 例如, 领导者要么是唯一执行任务的那个, 其他所有节点都在等待该领导者宕机, 然后自己变成领导者; 或者是都是活跃的, 但是领导者是唯一能够执行写操作的, 而其他的只能读数据. 这样能保证两个实例不会做同一个任务, 否则会因为竞争条件导致不可预测的系统行为.</p> <p><strong>该机制自身不需要集成到应用中, 可以使用一个 sidecar 容器来执行所有的领导选举操作, 通知主容器什么时候它应该活跃起来</strong>. 一个 Kubernetes 中领导选举的例子: https://github.com/kubernetes/contrib/tree/master/election.</p> <p>保证应用高可用相对简单, 因为 Kubernetes 几乎替你完成所有事情. 但是<mark><strong>假如 Kubernetes 自身宕机了呢? 如果是运行 Kubernetes 控制平面组件的服务器挂了呢? 这些组件是如何做到高可用的呢</strong></mark>?</p> <h6 id="_11-6-2-让kubernetes控制平面变得高可用"><a href="#_11-6-2-让kubernetes控制平面变得高可用" class="header-anchor">#</a> 11.6.2 让Kubernetes控制平面变得高可用</h6> <p>本章一开始学习了 Kubernetes 控制平面的一些组件. <strong>为了使得 Kubernetes 高可用, 需要运行多个主节点, 即运行下述组件的多个实例</strong>:</p> <ul><li>etcd 分布式数据存储, 所有 API 对象存于此处</li> <li>API 服务器</li> <li>控制器管理器, 所有控制器运行的进程</li> <li>调度器</li></ul> <p>不需要深入了解如何安装和运行这些组件的细节. 下面看一下如何让这些组件高可用. 图 11.18 显示了一个高可用集群的概览.</p> <p><img src="/img/image-20240227235013-mtrzubs.png" alt="image" title="图11.18 三节点高可用集群"></p> <blockquote><p>运行 etcd 集群</p></blockquote> <p>因为 etcd 被设计为一个<strong>分布式系统</strong>, 其核心特性之一就是可以运行多个 etcd 实例, 所以它做到高可用并非难事. 要做的就是将其<strong>运行在合适数量的机器上</strong>(3 个, 5 个或者 7 个, 如章节刚开始所述), 使得它们能够互相感知. 实现方式通过在每个实例的配置中包含其他实例的列表. 例如, 当启动一个实例时, 指定其他 etcd 实例可达的 IP 和端口.</p> <p><strong>etcd 会跨实例复制数据</strong>, 所以三节点中其中一个宕机并不会影响处理读写操作. 为了增加错误容忍度不仅仅支持一台机器宕机, 需要运行 5个或者 7个 etcd 节点, 这样集群可以分别容忍 2 个或者 3 个节点宕机. 拥有超过 7 个实例基本上没有必要, 并且会影响性能.</p> <blockquote><p>运行多实例 API 服务器</p></blockquote> <p>保证 API 服务器高可用甚至更简单, <mark><strong>因为 API 服务器是(几乎全部)无状态的</strong></mark>(所有数据存储在 etcd 中, API 服务器不做缓存), 你需要多少就能运行多少 API 服务器, 它们直接不需要感知对方存在. <strong>通常, 一个 API 服务器会和每个 etcd 实例搭配. 这样做, etcd 实例之前就不需要任何负载均衡器, 因为每个 API 服务器只和本地 etcd 实例通信</strong>.</p> <p>而 API 服务器确实需要一个<strong>负载均衡器</strong>, 这样客户端(kubectl, 也有可能是控制器管理器, 调度器以及所有 Kubelet)总是只连接到健康的 API 服务器实例.</p> <blockquote><p>确保控制器和调度器的高可用性</p></blockquote> <p>对比 API 服务器可以同时运行多个复制集, 运行控制器管理器或者调度器的多实例情况就没那么简单了. 因为控制器和调度器都会积极地监听集群状态, 发生变更时做出相应操作, 可能未来还会修改集群状态(例如, 当 ReplicaSet 上期望的复制集数量增加 1 时, ReplicaSet 控制器会额外创建一个 pod), <strong>多实例运行这些组件会导致它们执行同一个操作, 会导致产生竞争状态, 从而造成非预期影响</strong>(如前例提到的, 创建了两个新 pod 而非一个).</p> <p><mark><strong>由于这个原因, 当运行这些组件的多个实例时, 给定时间内只有一个实例有效. 幸运的是, 这些工作组件自己都做了(由 --leader-elect 选项控制, 默认为 true). 只有当成为选定的领导者时, 组件才可能有效. 只有领导者才会执行实际的工作, 而其他实例都处于待命状态, 等待当前领导者宕机. 当领导者宕机, 剩余实例会选举新的领导者, 接管工作. 这种机制确保不会出现同一时间有两个有效组件做同样的工作(见图 11.19).</strong></mark></p> <p>控制器管理器和调度器可以和 API 服务器, etcd 搭配运行, 或者也可以运行在不同的机器上. 当搭配运行时, 可以直接跟本地 API 服务器通信; 否则就是通过负载均衡器连接到 API 服务器.</p> <p><img src="/img/image-20240227235035-5wb2xbx.png" alt="image" title="图11.19 只有一个控制器管理器和一个调度器有效; 其他的待机"></p> <blockquote><p>控制平面组件使用的领导选举机制</p></blockquote> <p>我发现最有趣的是: 选举领导时这些组件不需要互相通信. 领导选举机制的实现方式是在 API 服务器中创建一个资源, 而且甚至不是什么特殊种类的资源--Endpoint 资源就可以拿来用于达到目的(滥用更贴切一点).</p> <p>使用 Endpoint 对象来完成该工作没有什么特别之处. 使用 Endpoint 对象的原因是只要没有同名 Service 存在, 就没有副作用. 也可以使用任何其他资源(事实上, 领导选举机制不就会使用 ConfigMap 来替代 Endpoint).</p> <p>你一定对资源如何被应用于该目的感兴趣. 这里以调度器为例. 所有调度器实例都会尝试创建(之后更新)一个 Endpoint 资源, 称为 kube-scheduler. 可以在 kube-system 命名空间中找到它, 如下面的代码清单所示.</p> <p><strong>代码清单-11.11 用于领导选举的 kube-scheduler Endpoint 资源</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> endpoints kube<span class="token operator">-</span>scheduler <span class="token operator">-</span>n kube<span class="token operator">-</span>system <span class="token operator">-</span>o yaml
<span class="token literal-property property">apiVersion</span><span class="token operator">:</span> v1
<span class="token literal-property property">kind</span><span class="token operator">:</span> Endpoints
<span class="token literal-property property">metadata</span><span class="token operator">:</span>
  <span class="token literal-property property">annotations</span><span class="token operator">:</span>
    control<span class="token operator">-</span>plane<span class="token punctuation">.</span>alpha<span class="token punctuation">.</span>kubernetes<span class="token punctuation">.</span>io<span class="token operator">/</span>leader<span class="token operator">:</span> '<span class="token punctuation">{</span><span class="token string-property property">&quot;holderIdentity&quot;</span><span class="token operator">:</span>
      image<span class="token operator">-</span>placeholder <span class="token string">&quot;minikube&quot;</span><span class="token punctuation">,</span><span class="token string-property property">&quot;leaseDurationSeconds&quot;</span><span class="token operator">:</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token string-property property">&quot;acquireTime&quot;</span><span class="token operator">:</span>
      image<span class="token operator">-</span>placeholder <span class="token string">&quot;2017-05-27T18:54:53Z&quot;</span><span class="token punctuation">,</span><span class="token string-property property">&quot;renewTime&quot;</span><span class="token operator">:</span><span class="token string">&quot;2017-05-28T13:07:49Z&quot;</span><span class="token punctuation">,</span>
      image<span class="token operator">-</span>placeholder <span class="token string">&quot;leaderTransitions&quot;</span><span class="token operator">:</span><span class="token number">0</span><span class="token punctuation">}</span>'
  <span class="token literal-property property">creationTimestamp</span><span class="token operator">:</span> <span class="token number">2017</span><span class="token operator">-</span><span class="token number">05</span><span class="token operator">-</span>27T18<span class="token operator">:</span><span class="token number">54</span><span class="token operator">:</span>53Z
  <span class="token literal-property property">name</span><span class="token operator">:</span> kube<span class="token operator">-</span>scheduler
  <span class="token literal-property property">namespace</span><span class="token operator">:</span> kube<span class="token operator">-</span>system
  <span class="token literal-property property">resourceVersion</span><span class="token operator">:</span> <span class="token string">&quot;654059&quot;</span>
  <span class="token literal-property property">selfLink</span><span class="token operator">:</span> <span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>namespaces<span class="token operator">/</span>kube<span class="token operator">-</span>system<span class="token operator">/</span>endpoints<span class="token operator">/</span>kube<span class="token operator">-</span>scheduler
  <span class="token literal-property property">uid</span><span class="token operator">:</span> f847bd14<span class="token operator">-</span>430d<span class="token operator">-</span><span class="token number">11e7</span><span class="token operator">-</span><span class="token number">9720</span><span class="token operator">-</span>080027f8fa4e
<span class="token literal-property property">subsets</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>control-plane.alpha.kubernetes.io/leader 注释是比较重要的部分. 如你所见, 其中包含了一个叫作 <strong>holderIdentity</strong> 的字段, 包含了当前领导者的名字. 第一个成功将姓名填入该字段的实例成为领导者. 实例之间会竞争, 但是最终只有一个胜出.</p> <p>还记得之前讨论过的乐观并发概念吗? 乐观并发保证如果有多个实例尝试写名字到资源, 只有一个会成功. 根据是否写成功, 每个实例就知道自己是否是领导者.</p> <p>一旦成为领导者, 必须顶起更新资源(默认每 2 秒), 这样所有其他的实例就知道它是否还存活. 当领导者宕机, 其他实例会发现资源有一阵没被更新了, 就会尝试将自己的名字写到资源中尝试成为领导者. 简单吧, 对吧?</p> <h5 id="_11-7-本章小结"><a href="#_11-7-本章小结" class="header-anchor">#</a> 11.7 本章小结</h5> <p>期望这么有趣的一章能够增加你对 Kubernetes 内部机制的理解. 本章讲述了:</p> <ul><li><p><strong>Kubernetes 由哪些组件构成, 以及每个组件的责任是什么</strong></p> <p>‍</p></li> <li><p><strong>API 服务器, 调度器, 运行在控制器管理器中的各种控制器, 以及 Kubelet 是如何协同工作让 pod 运行起来的</strong></p></li> <li><p><strong>基础设施容器是如何将同一个 pod 的容器联系在一起的</strong></p></li> <li><p><strong>相同节点上的 pod 如何通过网桥通信? 不同节点上的网桥是如何连接的? 运行在不同的节点上的 pod 是如何通信的?</strong></p></li> <li><p><strong>如何通过配置各个节点上 iptables 规则, 让 Kube-proxy 在同一服务中跨 pod 发挥负载均衡功能的.</strong></p></li> <li><p><strong>控制平面每个组件的多个实例是如何运行来保证集群的高可用性的</strong></p></li></ul> <p>接下来一起了解一下如何确保 API 服务器的安全性, 乃至整个集群的安全性.</p> <h4 id="_12-kubernetes-api服务器的安全防护"><a href="#_12-kubernetes-api服务器的安全防护" class="header-anchor">#</a> 12.Kubernetes API服务器的安全防护</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>了解认证机制</li> <li>SerViceAccounts 是什么及使用的原因</li> <li>了解基于角色(RBAC)的权限控制插件</li> <li>使用角色和角色绑定</li> <li>使用集群角色和集群角色绑定</li> <li>了解默认角色及其绑定</li></ul> <p>在第 8 章里学习了运行在 pod 中的应用如何与 API 服务器交互来查看和控制部署在集群中的资源状态. 在上述过程中, <strong>使用挂载进 pod 中的 ServiceAccount token 和 API 服务器完成认证</strong>. 在本章会学习到 <strong>ServiceAccount 是什么, 以及如何配置它们的权限和在集群中用到的其他产品的权限</strong>.</p> <h5 id="_12-1-了解认证机制"><a href="#_12-1-了解认证机制" class="header-anchor">#</a> 12.1 了解认证机制</h5> <p>在前面的章节讲到 API 服务器可以<strong>配置一到多个认证的插件(授权插件同样也可以)</strong> . API 服务器接收到的请求会经过一个认证插件的列表, 列表中的每个插件都可以检查这个请求和尝试确定谁在发送这个请求. 列表中的第一个插件可以提取请求中客户端的用户名, 用户 ID 和组信息, 并返回给 API 服务器. API 服务器会停止调用剩余的认证插件并继续进入授权阶段.</p> <p>目前有几个认证插件是直接可用的. 它们<mark><strong>使用下列方法获取客户端的身份认证</strong></mark>:</p> <ul><li><strong>客户端证书</strong></li> <li><strong>传入在 HTTP 头中的认证 token</strong></li> <li><strong>基础的 HTTP 认证</strong></li> <li>其他</li></ul> <p>可以启动 API 服务器时, 通过命令行选项可以<strong>开启认证插件</strong>.</p> <h6 id="_12-1-1-用户和组"><a href="#_12-1-1-用户和组" class="header-anchor">#</a> 12.1.1 用户和组</h6> <p>认证插件会返回已经认证过<strong>用户的用户名和组(多个组)</strong> . Kubernetes 不会在任何地方存储这些信息, 这些信息被用来验证用户是否被授权执行某个操作.</p> <blockquote><p>了解用户</p></blockquote> <p>Kubernetes 区分了两种连接到 API 服务器的客户端.</p> <ul><li><strong>真实的人(用户)</strong></li> <li><strong>pod(更准确地说是运行在 pod 中的应用)</strong></li></ul> <p>这两种类型的客户端都使用了上述的认证插件进行认证. 用户应该被管理在外部系统中, 例如单点登录系统(SSO), 但是 <strong>pod 使用一种称为 service accounts 的机制, 该机制被创建和存储在集群中作为 ServiceAccount 资源</strong>. 相反, 没有资源代表用户账户, 这也就意味着不能通过 API 服务器来创建, 更新或删除用户.</p> <p>这里不会详细讨论如何管理用户, 但是会具体地探讨 <strong>ServiceAccount</strong>, 因为它们对于运行中的 pod 很重要. 关于如何配置集群来供用户身份认证的更多信息, 集群管理员应该参考 http://kubernetes.io/docs/admin 中的 Kubernetes 集群管理员指南.</p> <blockquote><p>了解组</p></blockquote> <p>正常用户和 ServiceAccount 都可以属于一个或多个组. 前面已经讲过认证插件会连同用户名和用户 ID 返回组. 组可以一次给多个用户赋予权限, 而不是必须单独给用户赋予权限.</p> <p>由插件返回的组仅仅是表示组名称的字符串, 但是<strong>系统内置的组</strong>会有一些特殊的含义.</p> <ul><li>​<code>system:unauthenticated</code>​ 组: 用于所有认证插件都不会认证客户端身份的请求.</li> <li>​<code>system:authenticated</code>​ 组: 会自动分配给一个成功通过认证的用户.</li> <li>​<code>system:serviceaccounts</code>​ 组: 包含所有在系统中的 ServiceAccount.</li> <li>​<code>system:serviceaccounts:&lt;namespace&gt;</code>​ 组: 包含了所有在特定命名空间中的 ServiceAccount.</li></ul> <h6 id="_12-1-2-serviceaccount介绍"><a href="#_12-1-2-serviceaccount介绍" class="header-anchor">#</a> 12.1.2 ServiceAccount介绍</h6> <p>接下来更详细地探讨 ServiceAccount. 前面已经了解 API 服务器要求客户端在服务器上执行操作之前对自己进行身份认证, 并且已经了解了 pod 是怎么通过发送 <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>​ 文件内容来进行<strong>身份认证</strong>的. <strong>这个文件通过加密卷挂载进每个容器的文件系统中</strong>.</p> <p>但是那个文件具体表示了什么呢? <mark><strong>每个 pod 都与一个 ServiceAccount 相关联, 它代表了运行在 pod 中应用程序的身份证明. token 文件持有 ServiceAccount 的认证 token. 应用程序使用这个 token 连接 API 服务器时, 身份认证插件会对 ServiceAccount 进行身份认证, 并将 ServiceAccount 的用户名传回 API 服务器内部</strong></mark>. ServiceAccount 用户名的格式像下面这样:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>system:serviceaccount:<span class="token operator">&lt;</span>namespace<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>service account name<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>API 服务器将这个用户名传给已配置好的授权插件, 这决定该应用程序所尝试执行的操作是否被 ServiceAccount 允许执行.</p> <p><mark><strong>ServiceAccount 只不过是一种运行在 pod 中的应用程序和 API 服务器身份认证的一种方式</strong></mark>. 如前所述, 应用程序通过<strong>在请求中传递 ServiceAccount token 来实现</strong>这一点.</p> <blockquote><p>了解 ServiceAccount 资源</p></blockquote> <p>ServiceAccount 就像 Pod, Secret, ConfigMap 等一样都是<strong>资源</strong>, 它们作用在单独的命名空间, <strong>为每个命名空间自动创建一个默认的 ServiceAccount(你的 pod 会一直使用)</strong> .</p> <p>可以像其他资源那样查看 ServiceAccount 列表:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> sa
<span class="token constant">NAME</span>      <span class="token constant">SECRETS</span>   <span class="token constant">AGE</span>
<span class="token keyword">default</span>   <span class="token number">1</span>         1d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>注意: serviceaccount 的缩写是 sa.</p> <p>如你所见, 当前命名空间只包含 <strong>default ServiceAccount</strong>, 其他额外的 ServiceAccount 可以在需要时添加. <strong>每个 pod 都与一个 ServiceAccount 相关联, 但是多个 pod 可以使用同一个 ServiceAccount</strong>. 通过图 12.1 可以了解, <strong>pod 只能使用同一个命名空间中的 ServiceAccount</strong>.</p> <p><img src="/img/image-20240224234934-oc6g010.png" alt="image" title="图12.1 每个 pod 会分配一个在这个 pod 命名空间中的单一 ServiceAccount"></p> <blockquote><p>ServiceAccount 如何和授权进行绑定</p></blockquote> <p><strong>在 pod 的 manifest 配置文件中, 可以用指定账户名称的方式将一个 ServiceAccount 赋值给一个 pod. 如果不显式地指定 ServiceAccount 的账户名称, pod 会使用在这个命名空间中的默认 ServiceAccount.</strong></p> <p><mark><strong>可以通过将不同的 ServiceAccount 赋值给 pod 来控制每个 pod 可以访问的资源. 当 API 服务器接收到一个带有认证 token 的请求时, 服务器会用这个 token 来验证发送请求的客户端所关联的 ServiceAccount 是否允许执行请求的操作</strong></mark>. API 服务器通过管理员配置好的系统级别认证插件来获取这些信息. 其中一个现成的授权插件是基于角色控制的插件(RBAC), 这个插件会在本章后续进行讨论. 从 Kubernetes 1.6 版本开始, RBAC 插件是绝大多数集群应该使用的授权插件.</p> <h6 id="_12-1-3-创建serviceaccount"><a href="#_12-1-3-创建serviceaccount" class="header-anchor">#</a> 12.1.3 创建ServiceAccount</h6> <p>前面已经讲过<strong>每个命名空间都拥有一个默认的 ServiceAccount</strong>, 也可以在需要时创建额外的 ServiceAccount. 但是为什么应该费力去创建新的 ServiceAccount 而不是对所有的 pod 都使用默认的 ServiceAccount?</p> <p>显而易见的原因是<strong>集群安全性</strong>. 不需要读取任何集群元数据的 pod 应该运行在一个受限制的账户下, 这个账户不允许它们检索或修改部署在集群中的任何资源. 需要检索资源元数据的 pod 应该运行在只允许读取这些对象元数据的 ServiceAccount 下. 反之, 需要修改这些对象的 pod 应该在它们自己的 ServiceAccount 下运行, 这些 ServiceAccount 允许修改 API 对象.</p> <p>下面了解一下<mark><strong>如何创建其他的 ServiceAccount, 它们如何与密钥进行关联, 以及如何将它们分配给 pod</strong></mark>.</p> <blockquote><p>创建 ServiceAccount</p></blockquote> <p>得益于 <code>kubectl create serviceaccount</code>​ 命令, 创建 ServiceAccount 非常容易. 下面新创建一个名为 foo 的 ServiceAccount:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl create serviceaccount foo
serviceaccount <span class="token string">&quot;foo&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>然后如下面的代码清单所示的那样, 可以使用 describe 命令来查看 ServiceAccount.</p> <p><strong>代码清单-12.1 使用 kubectl describe 命令查看 ServiceAccount</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe sa foo
Name:               foo
Namespace:          default
Labels:             <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>

Image pull secrets: <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>   <span class="token comment"># 这些会被自动地添加到使用这个ServiceAccount的所有pod中</span>
Mountable secrets:  foo-token-qzq7j  <span class="token comment"># 如果强制使用可挂载的秘钥, 那么使用这个ServiceAccount的pod只能挂载这些秘钥</span>
Tokens:             foo-token-qzq7j  <span class="token comment"># 认证token, 第一个token挂载在容器内</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>可以看到, 已经创建了<strong>自定义的 token 密钥, 并将它和 ServiceAccount 相关联</strong>. 如果通过 kubectl describe secret foo-token-qzq7j 查看密钥里面的数据, 如下面的代码清单所示, 可以发现它包含了和默认的 ServiceAccount 相同的条目(<mark><strong>CA 证书, 命名空间和 token</strong></mark>), 当然这两个 token 本身显然是不相同的.</p> <p><strong>代码清单-12.2 查看自定义的 ServiceAccount 密钥</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe secret foo-token-qzq7j
<span class="token punctuation">..</span>.
ca.crt:         <span class="token number">1066</span> bytes
namespace:      <span class="token number">7</span> bytes
token:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>注意: 你可能已经了解过 JSON Web Token(JWT). <strong>ServiceAccount 中使用的身份认证 token 就是 JWT token</strong>.</p> <blockquote><p>了解 ServiceAccount 上的可挂载密钥</p></blockquote> <p>通过使用 kubectl describe 命令查看 ServiceAccount 时, token 会显示在可挂载密钥列表中. 下面来解释一下这个列表代表什么. 在第 7 章中, 已经学会了<strong>如何创建密钥并且把它们挂载进一个 pod 里</strong>. 在默认情况下, pod 可以挂载任何它需要的密钥. 但是<strong>可以通过对 ServiceAccount 进行配置, 让 pod 只允许挂载 ServiceAccount 中列出的可挂载密钥</strong>. 为了开启这个功能, ServiceAccount 必须包含以下注解: <code>kubernetes.io/enforce-mountable-secrets=&quot;true&quot;</code>​.</p> <p>如果 ServiceAccount 被加上了这个注解, 任何使用这个 ServiceAccount 的 pod 只能挂载进 ServiceAccount 的可挂载密钥, 这些 pod 就不能使用其他的密钥.</p> <blockquote><p>了解 ServiceAccount 的镜像拉取密钥</p></blockquote> <p>ServiceAccount 也可以包含镜像拉取密钥的 list. 这个 list 曾经在第 7 章中查看过; 如果你已经不记得了, 镜像拉取密钥持有从私有镜像仓库拉取容器镜像的凭证.</p> <p>下面的代码清单中显示了 ServiceAccount 定义的一个例子, 它包含了在第 7 章中创建的镜像拉取密钥.</p> <p><strong>代码清单-12.3 带有镜像拉取密钥的 ServiceAccount:sa-image-pull-secrets.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>service<span class="token punctuation">-</span>account
<span class="token key atrule">imagePullSecrets</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>dockerhub<span class="token punctuation">-</span>secret
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>ServiceAccount 的镜像拉取密钥和它的可挂载密钥表现有些轻微不同. 和可挂载密钥不同的是, ServiceAccount 中的镜像拉取密钥不是用来确定一个 pod 可以使用哪些镜像拉取密钥的. 添加到 ServiceAccount 中的镜像拉取密钥会自动添加到所有使用这个 ServiceAccount 的 pod 中. 向 ServiceAccount 中添加镜像拉取密钥可以不必对每个 pod 都单独进行镜像拉取密钥的添加操作.</p> <h6 id="_12-1-4-将serviceaccount分配给pod"><a href="#_12-1-4-将serviceaccount分配给pod" class="header-anchor">#</a> 12.1.4 将ServiceAccount分配给pod</h6> <p>在创建另外的 ServiceAccount 之后, 需要将它们<strong>赋值给 pod</strong>. 通过<mark><strong>在 pod 定义文件中的 spec.serviceAccountName 字段上设置 ServiceAccount 的名称即可进行分配</strong></mark>.</p> <p>注意: <strong>pod 的 ServiceAccount 必须在 pod 创建时进行设置, 后续不能被修改</strong>.</p> <blockquote><p>创建使用自定义 ServiceAccount 的 pod</p></blockquote> <p>在第 8 章中部署了一个运行在基于 tutum/curl 镜像的容器上的 pad, 并在其旁边放置了一个 ambassador 容器. 可以使用它来查看 API 服务器的 REST 接口. <strong>这个 ambassador 容器会运行 kubectl proxy 进程, 这个进程会使用 pod 的 ServiceAccount 的 token 和 API 服务器进行身份认证</strong>.</p> <p>现在可以修改 pod, 让 pod 使用刚才创建的 foo ServiceAccount. 接下来的代码清单展示了这个 pod 的定义.</p> <p><strong>代码清单-12.4 使用一个非默认 ServiceAccount 的 pod:curl-custom-sa.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> curl<span class="token punctuation">-</span>custom<span class="token punctuation">-</span>sa
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> foo    <span class="token comment"># 这个pod使用新创建的foo ServiceAccount而不是默认的ServiceAccount</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> tutum/curl
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;9999999&quot;</span><span class="token punctuation">]</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ambassador
    <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubectl<span class="token punctuation">-</span>proxy<span class="token punctuation">:</span>1.6.2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>为了确认自定义的 ServiceAccount token <strong>已经挂载进这两个容器</strong>中, 如下面的代码清单所示的那样, 可以打印出这个 token 的内容.</p> <p><strong>代码清单-12.5 查看挂载进 pod 容器内的 token</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl exec <span class="token operator">-</span>it curl<span class="token operator">-</span>custom<span class="token operator">-</span>sa <span class="token operator">-</span>c main cat <span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run<span class="token operator">/</span>secrets<span class="token operator">/</span>kubernetes<span class="token punctuation">.</span>io<span class="token operator">/</span>serviceaccount<span class="token operator">/</span>token
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>通过对比代码清单 12.5 和 12.2 中 token 的字符串, 你会发现这个 token 来自 foo ServiceAccount 的 token.</p> <blockquote><p>使用自定义的 ServiceAccount token 和 API 服务器进行通信</p></blockquote> <p>下面看看<strong>是否可以使用这个 token 和 API 服务器进行通信</strong>. 前面提到过, ambassador 容器在使用这个 token 和服务器进行通信, 因此可以通过 ambassador 来测试这个 token, 这个 ambassador 监听在 localhost:8001 上, 如下面的代码清单所示.</p> <p><strong>代码清单-12.6 使用自定义的 ServiceAccount 和 API 服务器进行通信</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl exec <span class="token operator">-</span>it curl<span class="token operator">-</span>custom<span class="token operator">-</span>sa <span class="token operator">-</span>c main curl localhost<span class="token operator">:</span><span class="token number">8001</span><span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>pods
<span class="token punctuation">{</span>
  <span class="token string-property property">&quot;kind&quot;</span><span class="token operator">:</span> <span class="token string">&quot;PodList&quot;</span><span class="token punctuation">,</span>
  <span class="token string-property property">&quot;apiVersion&quot;</span><span class="token operator">:</span> <span class="token string">&quot;v1&quot;</span><span class="token punctuation">,</span>
  <span class="token string-property property">&quot;metadata&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
    <span class="token string-property property">&quot;selfLink&quot;</span><span class="token operator">:</span> <span class="token string">&quot;/api/v1/pods&quot;</span><span class="token punctuation">,</span>
    <span class="token string-property property">&quot;resourceVersion&quot;</span><span class="token operator">:</span> <span class="token string">&quot;433895&quot;</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string-property property">&quot;items&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
  <span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>可以看到从服务器得到了正确的响应, 也就意味着自定义的 ServiceAccount 可以允许列出 pod. 这可能是因为集群没有使用 RBAC 授权插件, 或者按照第 8 章所讲的那样, 你给了所有的 ServiceAccount 全部的<strong>权限</strong>.</p> <p><strong>如果集群没有使用合适的授权, 创建和使用额外的 ServiceAccount 并没有多大意义, 因为即使默认的 ServiceAccount 也允许执行任何操作</strong>. 在这种情况下, 使用 ServiceAccount 的唯一原因就是前面讲过的加强可挂载密钥, 或者通过 ServiceAccount 提供镜像拉取密钥.</p> <p>如果<strong>使用 RBAC 授权插件, 创建额外的 ServiceAccount 实际上是必要的</strong>, 这会在后面讨论 RBAC 授权插件的使用.</p> <h5 id="_12-2-通过基于角色的权限控制加强集群安全"><a href="#_12-2-通过基于角色的权限控制加强集群安全" class="header-anchor">#</a> 12.2 通过基于角色的权限控制加强集群安全</h5> <p>从 Kubernetes 1.6.0 版本开始, 集群安全性显著提高. 在早期版本中, 如果设法从集群中的一个 pod 获得了身份认证 token, 就可以使用这个 token 在集群中执行任何想要的操作. 通过这种方式可以获取 token, 并用这个 token 在不安全的 Kubernetes 集群中运行恶意的 pod.</p> <p>但是在 Kubernetes 1.8.0 版本中, <strong>RBAC 授权插件升级为 GA(通用可用性)</strong> , 并且在很多集群上默认开启(例如, 通过 kubadm 部署的集群, 如附录 B 所述). <strong>RBAC 会阻止未授权的用户查看和修改集群状态. 除非你授予默认的 ServiceAccount 额外的特权, 否则默认的 ServiceAccount 不允许查看集群状态, 更不用说以任何方式去修改集群状态</strong>. 要编写和 Kubernetes API 服务器通信的 APP(如第 8 章所讲的那样), 需要了解如何通过 RBAC 具体的<strong>资源管理授权</strong>.</p> <p>注意: 除了 RBAC 插件, Kubernetes 也包含其他的授权插件, 比如基于属性的访问控制插件(ABAC), WebHook 插件和自定义插件实现. 但是, <strong>RBAC 插件是标准</strong>的.</p> <h6 id="_12-2-1-rbac授权插件"><a href="#_12-2-1-rbac授权插件" class="header-anchor">#</a> 12.2.1 RBAC授权插件</h6> <p><strong>Kubernetes API 服务器可以配置</strong>​<mark><strong>使用一个授权插件来检查是否允许用户请求的动作执行</strong></mark>. 因为 API 服务器对外暴露了 REST 接口, 用户可以通过向服务器发送 HTTP 请求来执行动作, 通过在请求中包含<strong>认证凭证</strong>来进行认证(认证 token, 用户名和密码或者客户端证书).</p> <blockquote><p>了解动作</p></blockquote> <p>但是有什么动作? 如你所了解的, REST 客户端发送 GET, POST, PUT, DELETE 和其他类型的 HTTP 请求到特定的 URL 路径上, 这些路径表示特定的 <strong>REST 资源</strong>. 在 Kubernetes 中, 这些资源是 Pod, Service, Secret, 等等. 以下是 Kubernetes 请求动作的一些例子:</p> <ul><li><strong>获取 pod</strong></li> <li><strong>创建服务</strong></li> <li>更新密钥</li></ul> <p>这些示例中的动词(get, create, update)映射到客户端请求的 HTTP 方法(GET, POST, PUT)上(完整的映射如表12.1所示). 名词(Pod, Service, Secret)显然是映射到 Kubernetes 上的资源.</p> <p>例如 RBAC 这样的授权插件运行在 API 服务器中, 它会<strong>决定一个客户端是否允许在请求的资源上执行请求的动词</strong>.</p> <p><strong>表-12.1 认证动词和 HTTP 方法之间的映射关系</strong></p> <p><img src="/img/image-20240224235429-x7jile6.png" alt="image"></p> <p>注意: 额外的动词 use 用于 PodSecurityPolicy 资源, 会在下一章进行解释.</p> <p>除了可以对全部资源类型应用安全权限, RBAC 规则还可以应用于特定的资源实例(例如, 一个名为 myservice 的服务), 并且后面会看到权限也可以应用于 non-resource(非资源)URL 路径, 因为并不是 API 服务器对外暴露的每个路径都映射到一个资源(例如 /api 路径本身或服务器健康信息在的路径 /healthz).</p> <blockquote><p>了解 RBAC 插件</p></blockquote> <p>顾名思义, <strong>RBAC 授权插件将用户角色作为决定用户能否执行操作的关键因素</strong>. 主体(可以是一个人, 一个 ServiceAccount, 或者一组用户或 ServiceAccount)和一个或多个角色相关联, <strong>每个角色被允许在特定的资源上执行特定的动词</strong>.</p> <p>如果一个用户有多个角色, 他们可以做任何他们的角色允许他们做的事情. 如果用户的角色都没有包含对应的权限, 例如, 更新密钥, 那么 API 服务器会阻止用户等密钥执行 PUT 或 PATCH 请求.</p> <p>通过 RBAC 插件管理授权是简单的, 这一切都是通过<strong>创建四种 RBAC 特定的 Kubernetes 资源</strong>来完成的, 下面会学习这个过程.</p> <h6 id="_12-2-2-rbac资源"><a href="#_12-2-2-rbac资源" class="header-anchor">#</a> 12.2.2 RBAC资源</h6> <p><strong>RBAC 授权规则是通过四种资源来进行配置</strong>的, 它们可以分为两个组:</p> <ul><li><strong>Role(角色)和 ClusterRole(集群角色), 它们指定了在资源上可以执行哪些动词</strong>.</li> <li><strong>RoleBinding(角色绑定)和 ClusterRoleBinding(集群角色绑定), 它们将上述角色绑定到特定的用户, 组或 ServiceAccounts 上</strong>.</li></ul> <p>角色定义了可以做什么操作, 而绑定定义了谁可以做这些操作(如图 12.2 所示).</p> <p><img src="/img/image-20240227235130-xnryj6y.png" alt="image" title="图12.2 Role 授予权限, 同时 RoleBinding 将 Role 绑定到主体上"></p> <p><strong>角色和集群角色, 或者角色绑定和集群角色绑定之间的区别在于角色和角色绑定是命名空间的资源, 而集群角色和集群角色绑定是集群级别的资源(不是命名空间的)</strong> , 如图 12.3 所示.</p> <p>从图中可以看到, 多个角色绑定可以存在于单个命名空间中(对于角色也是如此). 同样地, 可以创建多个集群绑定和集群角色. 图中显示的另外一件事情是, 尽管角色绑定是在命名空间下的, 但它们也可以引用不在命名空间下的集群角色.</p> <p><img src="/img/image-20240227235150-z1prrtg.png" alt="image" title="图12.3 Role 和 RoleBinding 都在命名空间中, ClusterRole 和 ClusterRoleBinding 不在命名空间中"></p> <p>学习这四种资源及其影响的最好方法就是在实践中尝试. 可以现在就开始尝试.</p> <blockquote><p>开始练习</p></blockquote> <p>在研究 RBAC 资源是怎样通过 API 服务器影响你可以执行什么操作之前, 需要确定 RABC 在集群中已经开启. 首先, 确保使用的 Kubernetes 在 1.6 版本以上, 并且 RBAC 插件是<strong>唯一的配置生效的授权插件</strong>. 可以同时并行启用多个插件, 如果其中一个插件允许执行某个操作, 那么这个操作就会被允许.</p> <p>注意: 如果正在使用 Minikube, 可能还需要在启动 Minikube 时使用 <code>--extra-config=apiserver.Authorization.Mode=RBAC</code>​ 选项来启用 RBAC.</p> <p>如果按照第 8 章中讲述的如何禁用 RBAC 的指令进行操作, 现在通过运行下面的命令可以重新启用 RBAC:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">delete</span> clusterrolebinding permissive<span class="token operator">-</span>binding
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>为了尝试 RBAC, 就像第 8 章中所做的那样, 可以运行一个 pod, 通过它尝试和 API 服务器进行通信. 但是这次, <strong>要在不同的命名空间运行两个 pod, 用来观察每个命名空间表现出来的安全性</strong>.</p> <p>在第 8 章的示例中, 运行了两个容器演示一个容器中的应用如何使用另一个容器来和 API 进行服务器通信. 这次要运行一个容器(基于 kubectl-proxy 的镜像), 并且直接在容器中使用 kubectl exec 运行 curl 命令. 代理会负责验证和 HTTPS, 因此可以关注 API 服务器的安全性授权.</p> <blockquote><p>创建命名空间和运行 pod</p></blockquote> <p>创建一个在命名空间 <strong>foo</strong> 中的 pod 和另一个在命名空间 <strong>bar</strong> 中的 pod, 如下面的代码清单所示.</p> <p><strong>代码清单-12.7 在不同的命名空间中运行测试 pod</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 创建命名空间foo</span>
$ kubectl create ns foo
namespace <span class="token string">&quot;foo&quot;</span> created
$ kubectl run <span class="token builtin class-name">test</span> <span class="token parameter variable">--image</span><span class="token operator">=</span>luksa/kubectl-proxy <span class="token parameter variable">-n</span> foo
deployment <span class="token string">&quot;test&quot;</span> created

<span class="token comment"># 创建命名空间bar</span>
$ kubectl create ns bar
namespace <span class="token string">&quot;bar&quot;</span> created
$ kubectl run <span class="token builtin class-name">test</span> <span class="token parameter variable">--image</span><span class="token operator">=</span>luksa/kubectl-proxy <span class="token parameter variable">-n</span> bar
deployment <span class="token string">&quot;test&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>现在打开两个命令行终端, 并使用 kubectl exec 在两个 pod 中分别(每个终端对应一个 pod 的 shell)运行一个 shell. 例如, 要在命名空间 foo 中运行 pod 中的 shell, 首先要<strong>获得 pod 的名称</strong>:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>n foo
<span class="token constant">NAME</span>                   <span class="token constant">READY</span>     <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
test<span class="token operator">-</span><span class="token number">145485760</span><span class="token operator">-</span>ttq36   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>然后<strong>在 kubectl exec 命令中使用这个名称</strong>:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl exec <span class="token operator">-</span>it test<span class="token operator">-</span><span class="token number">145485760</span><span class="token operator">-</span>ttq36 <span class="token operator">-</span>n foo sh
<span class="token operator">/</span> #
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>对于在 bar 命名空间中的 pod, 在另外一个命令行终端执行同样的操作.</p> <blockquote><p>列出 pod 中的服务</p></blockquote> <p>为了验证 RBAC 是否已经开启并且<strong>阻止 pod 读取集群状态</strong>, 可以使用 curl 命令来列出 foo 命名空间中的服务:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token operator">/</span> # curl localhost<span class="token operator">:</span><span class="token number">8001</span><span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>namespaces<span class="token operator">/</span>foo<span class="token operator">/</span>services
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list services <span class="token keyword">in</span> the namespace <span class="token string">&quot;foo&quot;</span><span class="token punctuation">.</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>你正在连接到 localhost:8001, 这是 kubectl proxy 进程监听的地址(如第 8 章所述). 这个进程接收到请求并将其发送到 API 服务器, 同时以 foo 命名空间中默认的 ServiceAccount 进行身份认证(从 API 服务器的响应中可以明显看出这一点).</p> <p>API 服务器响应表明 ServiceAccount 不允许列出 foo 命名空间中的服务, 即使 pod 就运行在同一个命名空间中. 可以看到 RBAC 插件已经起作用了. ServiceAccount 的默认权限不允许它列出或修改任何资源. 下面来学习<strong>如何让 ServiceAccount 做到这一点</strong>. 首先, 需要<strong>创建一个 Role 资源</strong>.</p> <h6 id="_12-2-3-使用role和rolebinding"><a href="#_12-2-3-使用role和rolebinding" class="header-anchor">#</a> 12.2.3 使用Role和RoleBinding</h6> <p><strong>Role 资源定义了哪些操作可以在哪些资源上执行(或者如前面讲过的, 哪种类型的 HTTP 请求可以在哪些 RESTful 资源上执行)</strong> . 下面的代码清单定义了一个 Role, 它允许用户获取并列出 foo 命名空间中的服务.</p> <p><strong>代码清单-12.8 一个 Role 的定义文件: service-reader.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Role          <span class="token comment"># 资源类型为Role</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> foo    <span class="token comment"># Role所在的命名空间(如果没有填写命名空间则使用当前的命名空间)</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> service<span class="token punctuation">-</span>reader
<span class="token key atrule">rules</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;&quot;</span><span class="token punctuation">]</span>    <span class="token comment"># Service是核心apiGroup的资源, 所以没有apiGroup名, 就是&quot;&quot;</span>
  <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;get&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;list&quot;</span><span class="token punctuation">]</span>   <span class="token comment"># 获取独立的Service(通过名字)并且列出所有允许的服务</span>
  <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;services&quot;</span><span class="token punctuation">]</span>  <span class="token comment"># 这条规则和服务有关, 必须使用复数的名字</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>警告: 在指定资源时必须使用<strong>复数</strong>的形式.</p> <p><strong>这个 Role 资源会在 foo 命名空间中创建出来</strong>. 在第 8 章中, 已经了解到<strong>每个资源类型属于一个 API 组, 在资源清单(manifest)的 apiVersion 字段中指定 API 组(以及版本)</strong> . 在角色定义中, 需要为定义包含的每个规则涉及的资源指定 apiGroup. 如果你允许访问属于不同 API 组的资源, 可以使用多种规则.</p> <p>注意: 在本例中, 你允许访问所有服务资源, 但是也可以通过额外的 resourceNames 字段指定服务实例的名称来限制对服务实例的访问.</p> <p>图 12.4 中显示了角色, 以及它的动词和资源, 还有它的命名空间.</p> <p><img src="/img/image-20240227235211-hwg31xo.png" alt="image" title="图12.4 service-reader Role 允许获取和列出在 foo 命名空间中的服务"></p> <blockquote><p>创建角色</p></blockquote> <p>现在在 foo 命名空间中<strong>创建先前讲的角色</strong>:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl create <span class="token operator">-</span>f service<span class="token operator">-</span>reader<span class="token punctuation">.</span>yaml <span class="token operator">-</span>n foo
role <span class="token string">&quot;service-reader&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: -n 选项是 --namespace 的缩写.</p> <p>可以使用特殊的 <code>kubectl create role</code>​ 命令创建 service-reader 角色, 而不是通过 YAML 文件来创建. 下面使用这个方法来创建 bar 命名空间中的角色:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl create role service<span class="token operator">-</span>reader <span class="token operator">--</span>verb<span class="token operator">=</span>get <span class="token operator">--</span>verb<span class="token operator">=</span>list
 <span class="token operator">--</span>resource<span class="token operator">=</span>services <span class="token operator">-</span>n bar
role <span class="token string">&quot;service-reader&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这两个角色会允许你<strong>在两个 pod(分别在 foo 和 bar 命名空间中运行)中列出 foo 和 bar 命名空间中的服务</strong>. 但是创建了两个角色还不够(可以执行 curl 命令来再次检查), 需要将每个角色绑定到各自命名空间中的 ServiceAccount 上.</p> <blockquote><p>绑定角色到 ServiceAccount</p></blockquote> <p><strong>角色定义了哪些操作可以执行, 但没有指定谁可以执行这些操作. 要做到这一点, 必须将角色绑定一个到主体, 它可以是一个 user(用户), 一个 ServiceAccount 或一个组(用户或 ServiceAccount 的组)</strong> .</p> <p>通过<strong>创建一个 RoleBinding 资源来实现将角色绑定到主体</strong>. 运行以下命令, 可以将角色绑定到 default ServiceAccount:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl create rolebinding test <span class="token operator">--</span>role<span class="token operator">=</span>service<span class="token operator">-</span>reader <span class="token operator">--</span>serviceaccount<span class="token operator">=</span>foo<span class="token operator">:</span><span class="token keyword">default</span> <span class="token operator">-</span>n foo
rolebinding <span class="token string">&quot;test&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>命令应该是不言自明的. 这里创建一个 RoleBinding 资源, 它<strong>将 servicereader 角色绑定到命名空间 foo 中的 default ServiceAccount 上</strong>. 这个 RoleBinding 资源会被创建在命名空间 foo 中. RoleBinding 资源和 ServiceAccount 和角色的引用如图 12.5 所示.</p> <p>注意: 如果要绑定一个角色到一个 user(用户)而不是 ServiceAccount 上, 使用 --user 作为参数来指定用户名. 如果要绑定角色到组, 可以使用 --group 参数.</p> <p><img src="/img/image-20240227235243-t67ico2.png" alt="image" title="图12.5 test RoleBinding 将 default ServiceAccount 和 service-reader Role 绑定"></p> <p>下面的代码清单显示了创建的 RoleBinding 的 YAML 格式.</p> <p><strong>代码清单-12.9 一个 RoleBinding 引用一个 Role</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rolebinding <span class="token builtin class-name">test</span> <span class="token parameter variable">-n</span> foo <span class="token parameter variable">-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: <span class="token builtin class-name">test</span>
  namespace: foo
  <span class="token punctuation">..</span>.
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role         <span class="token comment"># 这个RoleBinding引用了service-reader Role     </span>
  name: service-reader            
subjects:
- kind: ServiceAccount      <span class="token comment"># 并且将它绑定到foo命名空间中的default ServiceAccount上         </span>
  name: default                    
  namespace: foo                   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>如你所见, RoleBinding 始终<strong>引用单个角色</strong>(从 roleRef 属性中可以看出), 但是可以将角色绑定到多个主体(例如, 一个或多个 ServiceAccount 和任意数量的用户或组)上. 因为这个 RoleBinding 将角色绑定到一个 ServiceAccount 上, 这个 ServiceAccount 运行在 foo 命名空间中的 pod 上, 所以现在<strong>可以</strong>列出来自 pod 中的服务.</p> <p><strong>代码清单-12.10 从 API 服务器中获取服务</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/namespaces/foo/services</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;ServiceList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token string">&quot;metadata&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;selfLink&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;/api/v1/namespaces/foo/services&quot;</span>,
    <span class="token string">&quot;resourceVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;24906&quot;</span>
  <span class="token punctuation">}</span>,
  <span class="token string">&quot;items&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token comment"># item的列表是空的, 因为没有服务存在</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><blockquote><p>在角色绑定中使用其他命名空间的 ServiceAccount</p></blockquote> <p>bar 命名空间中的 pod 不能列出自己命名空间中的服务, 显然也不能列出 foo 命名空间中的服务. 但是可以修改在 foo 命名空间中的 RoleBinding 并添加另一个 pod 的 ServiceAccount, 即使这个 ServiceAccount 在另一个不同的命名空间中. 运行下面的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl edit rolebinding <span class="token builtin class-name">test</span> <span class="token parameter variable">-n</span> foo
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>然后对于列出的 subjects 增加下面几行, 如下面的代码清单所示.</p> <p><strong>代码清单-12.11 从另一个命名空间中引用 ServiceAccount</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">subjects</span><span class="token punctuation">:</span>
<span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount
  <span class="token key atrule">name</span><span class="token punctuation">:</span> default    <span class="token comment"># 引用来着bar命名空间中的default ServiceAccount    </span>
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> bar       
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>现在就可以从运行在 bar 命名空间里的 pod 中列出 foo 命名空间中的服务. 运行和代码清单 12.10 中相同的命令, 但是在另一个终端执行, 这个终端运行另一个 pod 的 shell.</p> <p>在探讨 ClusterRole 和 ClusterRoleBinding 之前, 先总结当前拥有的 RBAC 资源. <strong>在 foo 命名空间中有一个 RoleBinding, 它引用 service-reader 角色(也在 foo 命名空间中), 并且绑定 foo 和 bar 命名空间中的 default ServiceAccount</strong>, 如图 12.6 所示.</p> <p><img src="/img/image-20240228091715-3z4sv7h.png" alt="image" title="图12.6 RoleBinding 将来自不同命名空间中的 ServiceAccount 绑定到同一个 Role"></p> <h6 id="_12-2-4-使用clusterrole和clusterrolebinding"><a href="#_12-2-4-使用clusterrole和clusterrolebinding" class="header-anchor">#</a> 12.2.4 使用ClusterRole和ClusterRoleBinding</h6> <p><strong>Role 和 RoleBinding 都是命名空间的资源, 这意味着它们属于和应用在一个单一的命名空间资源上</strong>. 但是可以看到, RoleBinding 可以引用来自其他命名空间中的 ServiceAccount.</p> <p>除了这些命名空间里的资源, 还<strong>存在两个集群级别的 RBAC 资源: ClusterRole 和 ClusterRoleBinding, 它们不在命名空间里</strong>. 来看看为什么需要它们.</p> <p>一个常规的角色只允许访问和角色在同一命名空间中的资源. 如果希望<strong>允许跨不同命名空间访问资源, 就必须要在每个命名空间中创建一个 Role 和 RoleBinding</strong>. 如果想将这种行为扩展到所有的命名空间(集群管理员可能需要), 需要在<strong>每个命名空间中创建相同的 Role 和 RoleBinding. 当创建一个新的命名空间时, 必须记住也要在新的命名空间中创建这两个资源</strong>.</p> <p>但是, <strong>一些特定的资源完全不在命名空间中</strong>(包括 Node, PersistentVolume, Namespace, 等等). 前面也提到过 API 服务器对外暴露了一些不表示资源的 URL 路径(例如/healthz). <strong>常规角色不能对这些资源或非资源型的 URL 进行授权, 但是 ClusterRole 可以</strong>.</p> <p><mark><strong>ClusterRole 是一种集群级资源, 它允许访问没有命名空间的资源和非资源型的 URL, 或者作为单个命名空间内部绑定的公共角色, 从而避免必须在每个命名空间中重新定义相同的角色</strong></mark>.</p> <blockquote><p>允许访问集群级别的资源</p></blockquote> <p>已经提到过, 可以使用 ClusterRole 来允许集群级别的资源访问. 下面来了解一下如何允许 pod 列出集群中的 PersistentVolume. 首先, 需要创建一个<strong>叫作 pvreader 的 ClusterRole</strong>:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrole pv-reader <span class="token parameter variable">--verb</span><span class="token operator">=</span>get,list <span class="token parameter variable">--resource</span><span class="token operator">=</span>persistentvolumes
clusterrole <span class="token string">&quot;pv-reader&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个 ClusterRole 的 YAML 格式内容展示在下面的代码清单中.</p> <p><strong>代码清单-12.12 一个 ClusterRole 的定义</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get clusterrole pv-reader <span class="token parameter variable">-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:             <span class="token comment"># ClusterRole不在命名空间内, 所以没有命名空间字段                            </span>
  name: pv-reader     <span class="token comment"># ClusterRole的名称                          </span>
  resourceVersion: <span class="token string">&quot;39932&quot;</span>                
  selfLink: <span class="token punctuation">..</span>.                           
  uid: e9ac1099-30e2-11e7-955c-080027e6b159   
rules:
- apiGroups:      <span class="token comment"># 在这个例子中, 这些规则完全和正常的Role一样                    </span>
  - <span class="token string">&quot;&quot;</span>                                    
  resources:                              
  - persistentvolumes                     
  verbs:                                  
  - get                                   
  - list                                  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>在将这个 ClusterRole 绑定到 pod 的 ServiceAccount 之前, 请验证 pod 是否可以列出 PersistentVolume. 在第一个命令行终端上运行下面的命令, 这个终端正在运行一个 shell, 这个 shell 在 foo 命名空间下的 pod 内:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/persistentvolumes</span>
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list persistentvolumes at the cluster scope.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 这个 URL 没有包含命名空间, 因为 PersistentVolume 不在命名空间里.</p> <p>和预期的一样, 默认 ServiceAccount <strong>不能列出</strong> PersistentVolume, 需要将 ClusterRole 绑定到 ServiceAccount 来允许它这样做. ClusterRole 可以通过常规的 RoleBinding(角色绑定)来和主体绑定, 因此要创建一个 RoleBinding:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create rolebinding pv-test <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>pv-reader <span class="token parameter variable">--serviceaccount</span><span class="token operator">=</span>foo:default <span class="token parameter variable">-n</span> foo
rolebinding <span class="token string">&quot;pv-test&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在能列出 PersistentVolume 了吗?</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/persistentvolumes</span>
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list persistentvolumes at the cluster scope.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>呃, 这就奇怪了. 让我们在下面的代码清单中检查 RoleBinding 的 YAML 内容. 你能说出它有什么问题吗?</p> <p><strong>代码清单-12.13 一个 RoleBinding 引用一个 ClusterRole</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get rolebindings pv-test <span class="token parameter variable">-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pv-test
  namespace: foo
  <span class="token punctuation">..</span>.
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole      <span class="token comment"># 这个绑定引用了pv-reader ClusterRole</span>
  name: pv-reader                      
subjects:
- kind: ServiceAccount   <span class="token comment"># 这个绑定的主体是在foo命名空间中的默认ServiceAccount</span>
  name: default                        
  namespace: foo                       
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>这个 YAML 内容看起来相当正确. 它正在引用正确的 ClusterRole 和正确的 ServiceAccount, 如图 12.7 所示. <strong>那么有什么问题呢</strong>?</p> <p><img src="/img/image-20240228091742-4i68njq.png" alt="image" title="图12.7 RoleBinding 引用了一个 ClusterRole, 不会授予集群级别的资源的权限"></p> <p><strong>尽管可以创建一个 RoleBinding 并在想开启命名空间资源的访问时引用一个 ClusterRole, 但是不能对集群级别(没有命名空间的)资源使用相同的方法. 必须始终使用 ClusterRoleBinding 来对集群级别的资源进行授权访问</strong>.</p> <p>幸运的是, 创建一个 ClusterRoleBinding 和创建一个 RoleBinding 并没有什么太大区别, 但是首先要清理和删除 RoleBinding:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete rolebinding pv-test
rolebinding <span class="token string">&quot;pv-test&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>接下来创建 ClusterRoleBinding:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrolebinding pv-test <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>pv-reader <span class="token parameter variable">--serviceaccount</span><span class="token operator">=</span>foo:default
clusterrolebinding <span class="token string">&quot;pv-test&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如你所见, 在命令中<strong>使用 clusterrolebinding 替换了 rolebinding, 并且不(需要)指定命名空间.</strong>  图 12.8 显示了现在有的资源.</p> <p><img src="/img/image-20240228091813-x3r39si.png" alt="image" title="图12.8 ClusterRoleBinding 和 ClusterRole 必须一起使用授予集群级别的资源的权限"></p> <p>来看一下, 现在是否可以列出 PersistentVolume:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/persistentvolumes</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PersistentVolumeList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以了! 这表明<strong>在授予集群级别的资源访问权限时, 必须使用一个 ClusterRole 和一个 ClusterRoleBinding</strong>.</p> <p>提示: 记住一个 RoleBinding 不能授予集群级别的资源访问权限, 即使它引用了一个 ClusterRoleBinding.</p> <blockquote><p>允许访问非资源型的 URL</p></blockquote> <p>前面已经提过, API 服务器也会<strong>对外暴露非资源型的 URL</strong>. 访问这些 URL 也必须要显式地授予权限; 否则, API 服务器会拒绝客户端的请求. 通常, 这个会通过 <code>system:discovery ClusterRole</code>​ 和相同命名的 ClusterRoleBinding 来自动完成, 它出现在其他预定义的 ClusterRoles 和 ClusterRoleBindings 中(将将在 12.2.5 节讨论它们).</p> <p>下面查看一下下面的代码清单中的 <code>system:discovery ClusterRole</code>​.</p> <p><strong>代码清单-12.14 默认 system:discovery ClusterRole</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get clusterrole system:discovery <span class="token parameter variable">-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:discovery
  <span class="token punctuation">..</span>.
rules:
- nonResourceURLs:  <span class="token comment"># 这条规则指向了非资源型的URL而不是资源</span>
  - /api            
  - /api/*          
  - /apis           
  - /apis/*         
  - /healthz        
  - /swaggerapi     
  - /swaggerapi/*   
  - /version        
  verbs:             <span class="token comment"># 对于这些URL只有HTTP GET方法是被允许的</span>
  - get             
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>可以发现, ClusterRole <strong>引用的是 URL 路径而不是资源</strong>(使用的是非资源 URL 字段而不是资源字段). verbs 字段只允许在这些 URL 上使用 GET HTTP 方法.</p> <p>注意: 对于非资源型 URL, 使用普通的 HTTP 动词, 如 post, put 和 patch, 而不是 create 或 update. 动词需要使用小写的形式指定.</p> <p>和集群级别的资源一样, <strong>非资源型的 URL ClusterRole 必须与 ClusterRoleBinding 结合使用</strong>. 把它们和 RoleBinding 绑定不会有任何效果. system:discovery ClusterRole 有一个与之对应的 system:discovery ClusterRoleBinding, 所以下面通过下面的代码清单来看看它们里面有什么.</p> <p><strong>代码清单-12.15 默认的 system:discovery ClusterRoleBinding</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get clusterrolebinding system:discovery <span class="token parameter variable">-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:discovery
  <span class="token punctuation">..</span>.
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole      <span class="token comment"># ClusterRoleBinding引用了system:discovery ClusterRole</span>
  name: system:discovery                  
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group            <span class="token comment"># 它将ClusterRole绑定到所有认证过和没有认证过的用户上</span>
  name: system:authenticated              
- apiGroup: rbac.authorization.k8s.io
  kind: Group                             
  name: system:unauthenticated            
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>YAML 内容显示 ClusterRoleBinding 正如预期的那样指向 system:discovery ClusterRole. 它绑定到了两个组, 分别是 <code>system:authenticated</code>​ 和 <code>system:unauthenticated</code>​, 这使得它和所有用户绑定在一起. 这意味着每个人都绝对可以访问列在 ClusterRole 中的 URL.</p> <p>注意: 组位于身份认证插件的域中. API 服务器接收到一个请求时, 它会调用身份认证插件来获取用户所属组的列表, 之后授权中会使用这些组的信息.</p> <p>可以通过在一个 pod 内和本地机器访问 /api URL 路径来进行确认(通过 kubectl proxy, 意味着你会使用 pod 的 ServiceAccount 来进行身份认证), 访问 URL 时不要指定任何认证的 token(这会让你成为一个未认证的用户).</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">curl</span> https://<span class="token variable"><span class="token variable">$(</span>minikube <span class="token function">ip</span><span class="token variable">)</span></span>:8443/api <span class="token parameter variable">-k</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;APIVersions&quot;</span>,
  <span class="token string">&quot;versions&quot;</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
  <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>现在已经使用 ClusterRole 和 ClusterRoleBinding 授权访问集群级别的资源和非资源型的 URL. 下面来了解一下 <strong>ClusterRole 怎么和命名空间中的 RoleBinding 一起来授权访问 RoleBinding 的命名空间中的资源</strong>.</p> <blockquote><p>使用 ClusterRole 来授权访问指定命名空间中的资源</p></blockquote> <p>ClusterRole 不是必须一直和集群级别的 ClusterRoleBinding 捆绑使用. 它们也可以和常规的有命名空间的 RoleBinding 进行捆绑. 我们已经研究过预先定义好的 ClusterRole, 所以下面来了解另一个名为 view 的 ClusterRole, 如下面的代码清单所示.</p> <p><strong>代码清单-12.16 默认的 view ClusterRole</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get clusterrole view <span class="token parameter variable">-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: view
  <span class="token punctuation">..</span>.
rules:
- apiGroups:
  - <span class="token string">&quot;&quot;</span>
  resources:     <span class="token comment"># 这条规则应用于这些资源上(注意: 它们都是命名空间的资源)</span>
  - configmaps                     
  - endpoints                      
  - persistentvolumeclaims         
  - pods                           
  - replicationcontrollers         
  - replicationcontrollers/scale   
  - serviceaccounts                
  - services                       
  verbs:          <span class="token comment"># 如ClusterRole名称表述的那样, 它只允许读操作, 不能对列出的资源进行写操作</span>
  - get                            
  - list                          
  - <span class="token function">watch</span>                          
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><p>这个 ClusterRole 有很多规则. 只有第一条会展示在这个代码清单中. 这个规则允许 get, list 和 watch 资源, 这些资源就像 ConfigMap, Endpoint, PersistentVolumeClaim, 等等. 这些资源是有命名空间的, 即使现在正在了解的是一个 ClusterRole(它不是一个常规的, 有命名空间的角色). 这个 ClusterRole 到底做了什么?</p> <p>这<strong>取决于它是和 ClusterRoleBinding 还是和 RoleBinding 绑定(可以和其中的一个进行绑定)</strong> . 如果你创建了一个 ClusterRoleBinding 并在它里面引用了 ClusterRole, 在绑定中列出的主体可以在<strong>所有命名空间</strong>中查看指定的资源. 相反, 如果创建的是一个 RoleBinding, 那么在绑定中列出的主体只能查看<strong>在 RoleBinding 命名空间</strong>中的资源. 现在可以尝试使用这两个选项.</p> <p>可以看到这两种方式如何影响测试 pod 列出 pod 的能力. 首先来看看这些绑定生效之前会发生什么:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/pods</span>
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list pods at the cluster scope./ <span class="token comment">#</span>
/ <span class="token comment"># curl localhost:8001/api/v1/namespaces/foo/pods</span>
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list pods <span class="token keyword">in</span> the namespace <span class="token string">&quot;foo&quot;</span><span class="token builtin class-name">.</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>通过使用第一个命令, 可以试着列出所有命名空间的 pod. 使用第二个命令, 可以试着列出在 foo 命名空间中的 pod. 服务器<strong>都不允许</strong>你执行这些操作.</p> <p>现在来看看<strong>创建一个 ClusterRoleBinding 并且把它绑定到 pod 的 ServiceAccount</strong> 上时发生了什么:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrolebinding view-test <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>view <span class="token parameter variable">--serviceaccount</span><span class="token operator">=</span>foo:default
clusterrolebinding <span class="token string">&quot;view-test&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在这个 pod 能列出在 foo 命名空间中的 pod 了吗?</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/namespaces/foo/pods</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PodList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以了! 因为你创建了一个 ClusterRoleBinding, 并且它<strong>应用在所有的命名空间</strong>上. 通过它命名空间 foo 中的 pod 也可以列出 bar 命名空间中的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/namespaces/bar/pods</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PodList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这个 pod 现在允许列出一个不同的命名空间中的 pod. 它也可以使用 <code>/api/v1/pods</code>​ URL 路径来检索所有命名空间中的 pod.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ <span class="token comment"># curl localhost:8001/api/v1/pods</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PodList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>正如预期的那样, <strong>这个 pod 可以获取集群中所有 pod 的列表</strong>. 总之, <strong>将 ClusterRoleBinding 和 ClusterRole 结合指向命名空间的资源, 允许 pod 访问任何命名空间中的资源</strong>, 如图 12.9 所示.</p> <p><img src="/img/image-20240228091845-jrdupzx.png" alt="image" title="图12.9 ClusterRoleBinding 和 ClusterRole 授予跨所有命名空间的资源权限"></p> <p>现在, 来看看如果<strong>用一个 RoleBinding 替换 ClusterRoleBinding</strong> 会发生什么. 首先, 删除 ClusterRoleBinding:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete clusterrolebinding view-test
clusterrolebinding <span class="token string">&quot;view-test&quot;</span> deleted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>接下来创建一个 RoleBinding 作为替代. 因为 RoleBinding 使用了<strong>命名空间</strong>, 所以需要指定你希望 RoleBinding 创建在里面的命名空间. 在 foo 命名空间中创建 RoleBinding:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create rolebinding view-test <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>view <span class="token parameter variable">--serviceaccount</span><span class="token operator">=</span>foo:default <span class="token parameter variable">-n</span> foo
rolebinding <span class="token string">&quot;view-test&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在在 foo 命名空间中有一个 RoleBinding, 它将在同一个命名空间中的 default ServiceAccount 绑定到 view ClusterRole. 现在你的 pod 可以访问什么资源?</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 可以列出 foo 命名空间中的 pod</span>
/ <span class="token comment"># curl localhost:8001/api/v1/namespaces/foo/pods</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;kind&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;PodList&quot;</span>,
  <span class="token string">&quot;apiVersion&quot;</span><span class="token builtin class-name">:</span> <span class="token string">&quot;v1&quot;</span>,
  <span class="token punctuation">..</span>.

<span class="token comment"># 不能列出 bar 命名空间中的 pod</span>
/ <span class="token comment"># curl localhost:8001/api/v1/namespaces/bar/pods</span>
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list pods <span class="token keyword">in</span> the namespace <span class="token string">&quot;bar&quot;</span><span class="token builtin class-name">.</span>

/ <span class="token comment"># curl localhost:8001/api/v1/pods</span>
User <span class="token string">&quot;system:serviceaccount:foo:default&quot;</span> cannot list pods at the cluster scope.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>可以看到, 这个 pod 可以列出 foo 命名空间中的 pod, 但并不是其他特定的命名空间或者所有的命名空间都能做到, 如图 12.10 所示.</p> <p><img src="/img/image-20240228091909-sfht8lj.png" alt="image" title="图12.10 指向一个 ClusterRole 的 RoleBinding 只授权获取在 RoleBinding 命名空间中的资源"></p> <blockquote><p>总结 Role, ClusterRole, Rolebinding 和 ClusterRoleBinding 的组合</p></blockquote> <p>前面已经介绍了许多不同的组合, 可能很难记住何时去使用对应的每个组合. 来看看如果对所有组合按每个特定的用例进行分类会不会对记忆有帮助, 参考表 12.2.</p> <p><strong>表-12.2 何时使用具体的 role 和 binding 的组合</strong></p> <p><img src="/img/image-20240225103726-ouh3fvd.png" alt="image"></p> <p>希望现在这四个 RBAC 资源之间的关系更加清晰了. 不要担心, 如果你仍然觉得自己还没有掌握这些. 在接下来的章节中, 随着讨论预先配置好的 ClusterRole 和 ClusterRoleBinding, 这些内容会变得更加清晰.</p> <h6 id="_12-2-5-了解默认的clusterrole和clusterrolebinding"><a href="#_12-2-5-了解默认的clusterrole和clusterrolebinding" class="header-anchor">#</a> 12.2.5 了解默认的ClusterRole和ClusterRoleBinding</h6> <p><strong>Kubernetes 提供了一组默认的 ClusterRole 和 ClusterRoleBinding, 每次 API 服务器启动时都会更新它们</strong>. 这保证了在你错误地删除角色和绑定, 或者 Kubernetes 的新版本使用了不同的集群角色和绑定配置时, 所有的默认角色和绑定都会被重新创建.</p> <p>可以在下面的代码清单中看到默认的集群角色和绑定.</p> <p><strong>代码清单-12.17 列出所有 ClusterRoleBinding 和 ClusterRole</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> clusterrolebindings
<span class="token constant">NAME</span>                                           <span class="token constant">AGE</span>
cluster<span class="token operator">-</span>admin                                  1d
<span class="token literal-property property">system</span><span class="token operator">:</span>basic<span class="token operator">-</span>user                              1d
<span class="token literal-property property">system</span><span class="token operator">:</span>controller<span class="token operator">:</span>attachdetach<span class="token operator">-</span>controller      1d
<span class="token operator">...</span>
<span class="token literal-property property">system</span><span class="token operator">:</span>controller<span class="token operator">:</span>ttl<span class="token operator">-</span>controller               1d
<span class="token literal-property property">system</span><span class="token operator">:</span>discovery                               1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>controller<span class="token operator">-</span>manager                 1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>dns                                1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>scheduler                          1d
<span class="token literal-property property">system</span><span class="token operator">:</span>node                                    1d
<span class="token literal-property property">system</span><span class="token operator">:</span>node<span class="token operator">-</span>proxier                            1d

$ kubectl <span class="token keyword">get</span> clusterroles
<span class="token constant">NAME</span>                                           <span class="token constant">AGE</span>
admin                                          1d
cluster<span class="token operator">-</span>admin                                  1d
edit                                           1d
<span class="token literal-property property">system</span><span class="token operator">:</span>auth<span class="token operator">-</span>delegator                          1d
<span class="token literal-property property">system</span><span class="token operator">:</span>basic<span class="token operator">-</span>user                              1d
<span class="token literal-property property">system</span><span class="token operator">:</span>controller<span class="token operator">:</span>attachdetach<span class="token operator">-</span>controller      1d
<span class="token operator">...</span>
<span class="token literal-property property">system</span><span class="token operator">:</span>controller<span class="token operator">:</span>ttl<span class="token operator">-</span>controller               1d
<span class="token literal-property property">system</span><span class="token operator">:</span>discovery                               1d
<span class="token literal-property property">system</span><span class="token operator">:</span>heapster                                1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>aggregator                         1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>controller<span class="token operator">-</span>manager                 1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>dns                                1d
<span class="token literal-property property">system</span><span class="token operator">:</span>kube<span class="token operator">-</span>scheduler                          1d
<span class="token literal-property property">system</span><span class="token operator">:</span>node                                    1d
<span class="token literal-property property">system</span><span class="token operator">:</span>node<span class="token operator">-</span>bootstrapper                       1d
<span class="token literal-property property">system</span><span class="token operator">:</span>node<span class="token operator">-</span>problem<span class="token operator">-</span>detector                   1d
<span class="token literal-property property">system</span><span class="token operator">:</span>node<span class="token operator">-</span>proxier                            1d
<span class="token literal-property property">system</span><span class="token operator">:</span>persistent<span class="token operator">-</span>volume<span class="token operator">-</span>provisioner           1d
view                                           1d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><p><strong>view, edit, admin 和 cluster-admin ClusterRole 是最重要的角色, 它们应该绑定到用户定义 pod 中的 ServiceAccount 上.</strong></p> <blockquote><p>用 view ClusterRole 允许对资源的只读访问</p></blockquote> <p>在前面的例子中, 已经使用了默认的 view ClusterRole. <strong>它允许读取一个命名空间中的大多数资源, 除了 Role, RoleBinding 和 Secret</strong>. 你可能会想为什么 Secrets 不能被读取? 因为 Secrets 中的某一个可能包含一个认证 token, 它比定义在 view ClusterRole 中的资源有更大的权限, 并且允许用户伪装成不同的用户来获取额外的权限(权限扩散).</p> <blockquote><p>用 edit ClusterRole 允许对资源的修改</p></blockquote> <p>接下来是 <strong>edit ClusterRole, 它允许你修改一个命名空间中的资源, 同时允许读取和修改 Secret</strong>. 但是, 它也不允许查看或修改 Role 和 RoleBinding, 这是为了防止权限扩散.</p> <blockquote><p>用 admin ClusterRole 赋予一个命名空间全部的控制权</p></blockquote> <p>一个命名空间中的<strong>资源的完全控制权是由 admin ClusterRole 赋予的</strong>. 有这个 ClusterRole 的主体可以读取和修改命名空间中的任何资源, 除了 ResourceQuota(会在第 14 章中了解它是什么)和命名空间资源本身. edit 和 admin ClusterRole 之间的主要区别是能否在命名空间中查看和修改 Role 和 RoleBinding.</p> <p>注意: 为了防止权限扩散, API 服务器只允许用户在已经拥有一个角色中列出的所有权限(以及相同范围内的所有权限)的情况下, 创建和更新这个角色.</p> <blockquote><p>用 cluster-admin ClusterRole 得到完全的控制</p></blockquote> <p>通过将 cluster-admin ClusterRole 赋给主体, <strong>主体可以获得 Kubernetes 集群完全控制的权限</strong>. 正如前面了解的那样, admin ClusterRole 不允许用户修改命名空间的 ResourceQuota 对象或者命名空间资源本身. 如果想允许用户这样做, 需要创建一个指向 cluster-admin ClusterRole 的 RoleBinding. 这使得 RoleBinding 中包含的用户能够完全控制创建 RoleBinding 所在命名空间上的所有方面.</p> <p>如果留心观察, 可能已经知道如何授予用户一个集群中所有命名空间的完全控制权. 就是通过在 ClusterRoleBinding 而不是 RoleBinding 中引用 clusteradmin ClusterRole.</p> <blockquote><p>了解其他默认的 ClusterRole</p></blockquote> <p>默认的 ClusterRole 列表包含了大量其他的 ClusterRole, 它们以 <code>system:</code>​ 为前缀. 这些角色用于各种 Kubernetes 组件中. 在它们之中, 可以找到如 <code>system:kube-scheduler</code>​ 之类的角色, 它明显是给调度器使用的, <code>system:node</code>​ 是给 Kubelets 组件使用的, 等等.</p> <p>虽然 Controller Manager 作为一个独立的 pod 来运行, 但是在其中运行的每个控制器都可以使用单独的 ClusterRole 和 ClusterRoleBinding(它们以 <code>system: Controller:</code>​ 为前缀).</p> <p>这些系统的每个 ClusterRole 都有一个匹配的 ClusterRoleBinding, 它会绑定到系统组件用来身份认证的用户上. 例如, <code>system:kube-scheduler ClusterRoleBinding</code>​ 将名称相同的 ClusterRole 分配给 <code>system:kube-scheduler</code>​ 用户, 它是调度器作为身份认证的用户名.</p> <h6 id="_12-2-6-理性地授予授权权限"><a href="#_12-2-6-理性地授予授权权限" class="header-anchor">#</a> 12.2.6 理性地授予授权权限</h6> <p>在默认情况下, 命名空间中的默认 ServiceAccount 除了未经身份验证的用户没有其他权限(你可能记得前面的示例之一, <code>system:discovery ClusterRole</code>​ 和相关联的绑定允许任何人对一些非资源型的 URL 发送 GET 请求). 因此, 在默认情况下, pod 甚至不能查看集群状态. 应该授予它们适当的权限来做这些操作.</p> <p>显然, 将所有的 ServiceAccounts 赋予 cluster-admin ClusterRole 是一个坏主意. <mark><strong>和安全问题一样, 最好只给每个人提供他们工作所需要的权限, 一个单独权限也不能多(最小权限原则)</strong></mark> .</p> <blockquote><p>为每个 pod 创建特定的 ServiceAccount</p></blockquote> <p>一个好的想法是为每一个 pod(或一组 pod 的副本)创建一个特定 ServiceAccount, 并且把它和一个定制的 Role(或 ClusterRole)通过 RoleBinding 联系起来(不是 ClusterRoleBinding, 因为这样做会给其他命名空间的 pod 对资源的访问权限, 这可能不是你想要的).</p> <p>如果一个 pod(应用程序在它内部运行)只需要读取 pod, 而其他的 pod 也需要修改它们, 然后创建两个不同的 ServiceAccount, 并且让这些 pod 通过指定在 pod spec 中的 serviceAccountName 属性来进行使用, 和你在本章的第一部分了解的那样. 不要将这两个 pod 所需的所有必要权限添加到命名空间中的默认 ServiceAccount 上.</p> <blockquote><p>假设你的应用会被入侵</p></blockquote> <p>你的目标是减少入侵者获得集群控制的可能性. 现在复杂的应用程序会包含很多的漏洞, 你应该期望不必要的用户最终获得 ServiceAccount 的身份认证 token. 因此你应该<strong>始终限制 ServiceAccount, 以防止它们造成任何实际的伤害</strong>.</p> <h5 id="_12-3-本章小结"><a href="#_12-3-本章小结" class="header-anchor">#</a> 12.3 本章小结</h5> <p>本章介绍了如何对 Kubernetes API 服务器进行安全防护的基础知识. 你已经了解了下面这几点:</p> <ul><li><strong>API 服务器的客户端有真实的用户和 pod 中运行的应用.</strong></li> <li><strong>pod 中的应用与一个 ServiceAccount 关联.</strong></li> <li>用户和 ServiceAccount 都与组进行关联.</li> <li><strong>在默认情况下, pod 运行在每个命名空间自动创建的默认 ServiceAccount 下.</strong></li> <li>额外的 ServiceAccount 可以手动创建, 并且和一个 pod 关联.</li> <li><strong>ServiceAccount 通过配置可以允许只挂载给定 pod 中受限的 Secret 列表.</strong></li> <li>一个 ServiceAccount 也可以用来给 pod 添加镜像拉取密钥, 因此你就不需要在每个 pod 里指定密钥了.</li> <li><strong>Role 和 ClusterRole 定义了可以在哪些资源上执行什么操作.</strong></li> <li><strong>RoleBinding 和 ClusterRoleBinding 将 Role 和 ClusterRole 绑定给用户, 组和 ServiceAccount.</strong></li> <li><strong>每个集群都有默认的 ClusterRole 和 ClusterRoleBinding.</strong></li></ul> <p>在下一章中会学习如何保护集群节点不受 pod 影响, 以及如何通过网络安全防护将 pod 隔离开.</p> <h4 id="_13-保障集群内节点和网络安全"><a href="#_13-保障集群内节点和网络安全" class="header-anchor">#</a> 13.保障集群内节点和网络安全</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>在 pod 中使用宿主节点的默认 LinuX 命名空间</li> <li>以不同用户身份运行容器</li> <li>运行特权容器</li> <li>添加或禁用容器的内核功能</li> <li>定义限制 pod 行为的安全策略</li> <li>保障 pod 的网络安全</li></ul> <p>在第 12 章中, 谈到了如何保障 API 服务器的安全. 如果攻击者获得了访问 API 服务器的权限, 他们可以通过在容器镜像中打包自己的代码并在 pod 中运行来做任何事. 但是这样做真的能够造成损害吗? 容器不是与同一宿主节点上的其他容器隔离开来的吗?</p> <p>并不一定. 本章将会介绍<mark><strong>如何允许 pod 访问所在宿主节点的资源. 本章还会介绍如何配置集群, 使得用户不能通过 pod 在集群中为所欲为. 本章的最后将会介绍如何保障 pod 间通信的网络的安全</strong></mark>.</p> <h5 id="_13-1-在pod中使用宿主节点的linux命名空间"><a href="#_13-1-在pod中使用宿主节点的linux命名空间" class="header-anchor">#</a> 13.1 在pod中使用宿主节点的Linux命名空间</h5> <p><strong>pod 中的容器通常在分开的 Linux 命名空间中运行. 这些命名空间将容器中的进程与其他容器中, 或者宿主机默认命名空间中的进程隔离开来.</strong></p> <p>例如, <strong>每一个 pod 有自己的 IP 和端口空间, 这是因为它拥有自己的网络命名空间. 类似地, 每一个 pod 拥有自己的进程树, 因为它有自己的 PID 命名空间. 同样地, pod 拥有自己的 IPC 命名空间, 仅允许同一 pod 内的进程通过进程间通信(Inter Process Communication, 简称 IPC)机制进行交流</strong>.</p> <h6 id="_13-1-1-在pod中使用宿主节点的网络命名空间"><a href="#_13-1-1-在pod中使用宿主节点的网络命名空间" class="header-anchor">#</a> 13.1.1 在pod中使用宿主节点的网络命名空间</h6> <p><strong>部分 pod(特别是系统 pod)需要在宿主节点的默认命名空间中运行, 以允许它们看到和操作节点级别的资源和设备</strong>. 例如, 某个 pod 可能需要使用宿主节点上的网络适配器, 而不是自己的虚拟网络设备. 这可以通过<strong>将 pod spec 中的 hostNetwork 设置为 true 实现</strong>.</p> <p>如图 13.1 所示, 在这种情况下, 这个 pod 可以<strong>使用宿主节点的网络接口</strong>, 而不是拥有自己独立的网络. 这意味着<strong>这个 pod 没有自己的 IP 地址; 如果这个 pod 中的某一进程绑定了某个端口, 那么该进程将被绑定到宿主节点的端口上</strong>.</p> <p><img src="/img/image-20240228091948-pfvj9ou.png" alt="image" title="图13.1 一个配置了 hostNetwork:true 的 pod 使用宿主节点的网络接口, 而不是它自己的"></p> <p>可以尝试运行一个这样的 pod. 以下的代码清单展示了此种 pod 的一个例子.</p> <p><strong>代码清单-13.1 一个使用宿主节点默认的网络命名空间的 pod:pod-with-hostnetwork.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>with<span class="token punctuation">-</span>host<span class="token punctuation">-</span>network
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token comment"># 配置使用宿主节点的网络命名空间</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>在运行了这个 pod 之后, 可以用如下的命令来<strong>验证</strong>它确实使用了宿主节点的网络命名空间(例如, 它可以看到宿主节点上所有的网络接口).</p> <p><strong>代码清单-13.2 使用宿主机网络命名空间的 pod 网络</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl exec pod<span class="token operator">-</span><span class="token keyword">with</span><span class="token operator">-</span>host<span class="token operator">-</span>network ifconfig
docker0   Link encap<span class="token operator">:</span>Ethernet  HWaddr <span class="token number">02</span><span class="token operator">:</span><span class="token number">42</span><span class="token operator">:</span><span class="token number">14</span><span class="token operator">:</span><span class="token number">08</span><span class="token operator">:</span><span class="token number">23</span><span class="token operator">:</span><span class="token number">47</span>
          inet addr<span class="token operator">:</span><span class="token number">172.17</span><span class="token number">.0</span><span class="token number">.1</span>  <span class="token literal-property property">Bcast</span><span class="token operator">:</span><span class="token number">0.0</span><span class="token number">.0</span><span class="token number">.0</span>  <span class="token literal-property property">Mask</span><span class="token operator">:</span><span class="token number">255.255</span><span class="token number">.0</span><span class="token number">.0</span>
          <span class="token operator">...</span>

eth0      Link encap<span class="token operator">:</span>Ethernet  HWaddr <span class="token number">08</span><span class="token operator">:</span><span class="token number">00</span><span class="token operator">:</span><span class="token number">27</span><span class="token operator">:</span><span class="token constant">F8</span><span class="token operator">:</span><span class="token constant">FA</span><span class="token operator">:</span>4E
          inet addr<span class="token operator">:</span><span class="token number">10.0</span><span class="token number">.2</span><span class="token number">.15</span>  <span class="token literal-property property">Bcast</span><span class="token operator">:</span><span class="token number">10.0</span><span class="token number">.2</span><span class="token number">.255</span>  <span class="token literal-property property">Mask</span><span class="token operator">:</span><span class="token number">255.255</span><span class="token number">.255</span><span class="token number">.0</span>
          <span class="token operator">...</span>

lo        Link encap<span class="token operator">:</span>Local Loopback
          inet addr<span class="token operator">:</span><span class="token number">127.0</span><span class="token number">.0</span><span class="token number">.1</span>  <span class="token literal-property property">Mask</span><span class="token operator">:</span><span class="token number">255.0</span><span class="token number">.0</span><span class="token number">.0</span>
          <span class="token operator">...</span>

veth1178d4f Link encap<span class="token operator">:</span>Ethernet  HWaddr 1E<span class="token operator">:</span><span class="token number">03</span><span class="token operator">:</span>8D<span class="token operator">:</span><span class="token constant">D6</span><span class="token operator">:</span><span class="token constant">E1</span><span class="token operator">:</span>2C
          inet6 addr<span class="token operator">:</span> fe80<span class="token operator">:</span><span class="token operator">:</span>1c03<span class="token operator">:</span>8dff<span class="token operator">:</span>fed6<span class="token operator">:</span>e12c<span class="token operator">/</span><span class="token number">64</span> <span class="token literal-property property">Scope</span><span class="token operator">:</span>Link
          <span class="token constant">UP</span> <span class="token constant">BROADCAST</span> <span class="token constant">RUNNING</span> <span class="token constant">MULTICAST</span>  <span class="token constant">MTU</span><span class="token operator">:</span><span class="token number">1500</span>  <span class="token literal-property property">Metric</span><span class="token operator">:</span><span class="token number">1</span>
<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>Kubernetes 控制平面组件通过 pod 部署时(例如, 像附录 B 中那样使用 kubeadm 部署 Kubernetes 集群), <strong>这些 pod 都会使用 hostNetwork 选项</strong>, 让它们的行为与不在 pod 中运行时相同.</p> <h6 id="_13-1-2-绑定宿主节点上的端口而不使用宿主节点的网络命名空间"><a href="#_13-1-2-绑定宿主节点上的端口而不使用宿主节点的网络命名空间" class="header-anchor">#</a> 13.1.2 绑定宿主节点上的端口而不使用宿主节点的网络命名空间</h6> <p>一个与此有关的功能可以<strong>让 pod 在拥有自己的网络命名空间的同时, 将端口绑定到宿主节点的端口上</strong>. 这可以通过<strong>配置 pod 的 spec.containers.ports 字段中某个容器某一端口的 hostPort 属性</strong>来实现.</p> <p>不要混淆使用 hostPort 的 pod 和通过 NodePort 服务暴露的 pod. 如图 13.2 所示, 它们是不同的.</p> <p>在图中首先注意到的是, 对于一个使用 hostPort 的 pod, <strong>到达宿主节点的端口的连接会被直接转发到 pod 的对应端口上</strong>; 然而在 NodePort 服务中, 到达宿主节点的端口的连接将被转发到随机选取的 pod 上(这个 pod 可能在其他节点上). 另外一个区别是, 对于使用 hostPort 的 pod, 仅有运行了这类 pod 的节点会绑定对应的端口; 而 NodePort 类型的服务会在所有的节点上绑定端口, 即使这个节点上没有运行对应的 pod(如图中所示的节点 3).</p> <p><img src="/img/image-20240228092015-qgmoici.png" alt="image" title="图13.2 使用 hostPort 的 pod 和通过 NodePort 服务暴露的 pod 的区别"></p> <p>很重要的一点是, <strong>如果一个 pod 绑定了宿主节点上的一个特定端口, 每个宿主节点只能调度一个这样的 pod 实例, 因为两个进程不能绑定宿主机上的同一个端口</strong>. 调度器在调度 pod 时会考虑这一点, 所以它不会把两个这样的 pod 调度到同一个节点上, 如图 13.3 所示. 如果要在 3 个节点上部署 4 个这样的 pod 副本, 只有 3 个副本能够成功部署(剩余 1 个 pod 保持 Pending 状态).</p> <p><img src="/img/image-20240228092044-4xi8xmm.png" alt="image" title="图13.3 如果使用了 hostport, 一个宿主节点只能调度一个副本"></p> <p>如何在 pod 的 YAML 定义文件中定义 hostPort 选项. 以下代码清单展示了一个运行 kubia pod, 并将该 pod <strong>绑定到宿主机的 9000 端口</strong>的 YAML 描述文件.</p> <p><strong>代码清单-13.3 将 pod 中的一个端口绑定到宿主节点默认网络命名空间的端口: kubia-hostport.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>hostport
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>     <span class="token comment"># 该容器可以通过pod IP的8080端口访问</span>
      <span class="token key atrule">hostPort</span><span class="token punctuation">:</span> <span class="token number">9000</span>          <span class="token comment"># 它也可以通过所在节点的9000端口访问</span>
      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>创建这个 pod 之后, 可以通过它所在节点的 9000 端口访问这个 pod. 有多个宿主节点时, 并不能通过其他宿主节点的同一端口访问该 pod.</p> <p>hostPort 功能最初是用于暴露通过 DeamonSet 部署在每个节点上的系统服务的. 最初这个功能也用于保证一个 pod 的两个副本不被调度到同一节点上, 但是现在有更好的方法来实现这一需求. 这种方法将在第 16 章中介绍.</p> <h6 id="_13-1-3-使用宿主节点的pid与ipc命名空间"><a href="#_13-1-3-使用宿主节点的pid与ipc命名空间" class="header-anchor">#</a> 13.1.3 使用宿主节点的PID与IPC命名空间</h6> <p><strong>pod spec 中的 hostPID 和 hostIPC 选项与 hostNetwork 相似. 当它们被设置为 true 时, pod 中的容器会使用宿主节点的 PID 和 IPC 命名空间, 分别允许它们看到宿主机上的全部进程, 或通过 IPC 机制与它们通信</strong>. 以下代码清单是一个使用 hostPID 和 hostIPC 的 pod 的例子.</p> <p><strong>代码清单-13.4 使用宿主节点的 PID 和 IPC 命名空间: pod-with-host-pidand-ipc.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>with<span class="token punctuation">-</span>host<span class="token punctuation">-</span>pid<span class="token punctuation">-</span>and<span class="token punctuation">-</span>ipc
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hostPID</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token comment"># 希望这个pod使用宿主节点的PID命名空间</span>
  <span class="token key atrule">hostIPC</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token comment"># 希望pod使用宿主节点的IPC命名空间</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>pod 中通常只能看到<strong>自己内部</strong>的进程, 但在这个 pod 的容器中<strong>列出进程, 可以看到宿主机上的所有进程</strong>, 而不仅仅是容器内的进程, 就如同以下的代码清单所示.</p> <p><strong>代码清单-13.5 配置 hostPID:true 的 pod 内可见的进程列表</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> pod-with-host-pid-and-ipc <span class="token function">ps</span> aux
PID   <span class="token environment constant">USER</span>     TIME   COMMAND
    <span class="token number">1</span> root       <span class="token number">0</span>:01 /usr/lib/systemd/systemd --switched-root <span class="token parameter variable">--system</span> <span class="token punctuation">..</span>.
    <span class="token number">2</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>kthreadd<span class="token punctuation">]</span>
    <span class="token number">3</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>ksoftirqd/0<span class="token punctuation">]</span>
    <span class="token number">5</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>kworker/0:0H<span class="token punctuation">]</span>
    <span class="token number">6</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>kworker/u2:0<span class="token punctuation">]</span>
    <span class="token number">7</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>migration/0<span class="token punctuation">]</span>
    <span class="token number">8</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>rcu_bh<span class="token punctuation">]</span>
    <span class="token number">9</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>rcu_sched<span class="token punctuation">]</span>
   <span class="token number">10</span> root       <span class="token number">0</span>:00 <span class="token punctuation">[</span>watchdog/0<span class="token punctuation">]</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p><strong>将 hostIPC 设置为 true, pod 中的进程就可以通过进程间通信机制与宿主机上的其他所有进程进行通信</strong>.</p> <h5 id="_13-2-配置节点的安全上下文"><a href="#_13-2-配置节点的安全上下文" class="header-anchor">#</a> 13.2 配置节点的安全上下文</h5> <p>除了让 pod 使用宿主节点的 Linux 命名空间, 还可以在 pod 或其所属容器的描述中通过 <strong>security-Context 选项配置其他与安全性相关的特性</strong>. 这个选项可以运用于整个 pod, 或者每个 pod 中单独的容器.</p> <blockquote><p>了解安全上下文中可以配置的内容</p></blockquote> <p>配置安全上下文可以允许你做很多事:</p> <ul><li><strong>指定容器中运行进程的用户(用户 ID)</strong> .</li> <li><strong>阻止容器使用 root 用户运行</strong>(容器的默认运行用户通常在其镜像中指定, 所以可能需要阻止容器以 root 用户运行).</li> <li>使用<strong>特权模式</strong>运行容器, 使其对宿主节点的内核具有完全的访问权限.</li> <li>与以上相反, 通过添加或禁用内核功能, 配置细粒度的内核访问权限.</li> <li>设置 SELinux(Security Enhaced Linux, 安全增强型 Linux)选项, 加强对容器的限制.</li> <li>阻止进程写入容器的根文件系统.</li></ul> <p>以下内容将开始探索这些功能的细节.</p> <blockquote><p>运行 pod 而不配置安全上下文</p></blockquote> <p>首先, 运行一个<strong>没有任何安全上下文配置</strong>的 pod(不指定任何安全上下文选项), 与配置了安全上下文的 pod 形成对照:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run pod-with-defaults <span class="token parameter variable">--image</span> alpine <span class="token parameter variable">--restart</span> Never -- /bin/sleep <span class="token number">999999</span>
pod <span class="token string">&quot;pod-with-defaults&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>来看一看这个容器中的<strong>用户 ID 和组 ID</strong>, 以及它所属的用户组. 这可以通过在容器中运行 id 命令查看.</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> pod-with-defaults <span class="token function">id</span>
<span class="token assign-left variable">uid</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span> <span class="token assign-left variable">gid</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span> <span class="token assign-left variable">groups</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span>, <span class="token number">1</span><span class="token punctuation">(</span>bin<span class="token punctuation">)</span>, <span class="token number">2</span><span class="token punctuation">(</span>daemon<span class="token punctuation">)</span>, <span class="token number">3</span><span class="token punctuation">(</span>sys<span class="token punctuation">)</span>, <span class="token number">4</span><span class="token punctuation">(</span>adm<span class="token punctuation">)</span>, <span class="token number">6</span><span class="token punctuation">(</span>disk<span class="token punctuation">)</span>, <span class="token number">10</span><span class="token punctuation">(</span>wheel<span class="token punctuation">)</span>, <span class="token number">11</span><span class="token punctuation">(</span>floppy<span class="token punctuation">)</span>, <span class="token number">20</span><span class="token punctuation">(</span>dialout<span class="token punctuation">)</span>, <span class="token number">26</span><span class="token punctuation">(</span>tape<span class="token punctuation">)</span>, <span class="token number">27</span><span class="token punctuation">(</span>video<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个容器在用户 ID(uid)为 0 的用户, 即 root, 用户组 ID(gid)为 0 (同样是 root)的用户组下运行. 它同样还属于一些其他的用户组.</p> <p>注意: 容器运行时使用的用户在镜像中指定. 在 Dockerfile 中, 这是通过使用 USER 命令实现的. 如果该命令被省略, 容器将使用 root 用户运行.</p> <p>现在来<strong>运行一个使用特定用户运行容器的 pod</strong>.</p> <h6 id="_13-2-1-使用指定用户运行容器"><a href="#_13-2-1-使用指定用户运行容器" class="header-anchor">#</a> 13.2.1 使用指定用户运行容器</h6> <p>为了使用一个与镜像中不同的用户 ID 来运行 pod, 需要设置该 pod 的 <strong>securityContext.runAsUser</strong> 选项. 可以通过以下代码清单来运行一个使用 guest 用户运行的容器, 该用户在 alpine 镜像中的用户 ID 为 405.</p> <p><strong>代码清单-13.6 使用特定用户运行容器: pod-as-user-guest.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>as<span class="token punctuation">-</span>user<span class="token punctuation">-</span>guest
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>
      <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">405</span>    <span class="token comment"># 需要指明一个用户ID, 而不是用户名(id 405对应guest用户)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>现在可以像之前一样在 pod 中运行 id 命令, 查看 runAsUser 选项的效果:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> pod-as-user-guest <span class="token function">id</span>
<span class="token assign-left variable">uid</span><span class="token operator">=</span><span class="token number">405</span><span class="token punctuation">(</span>guest<span class="token punctuation">)</span> <span class="token assign-left variable">gid</span><span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">(</span>users<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>与要求的一样, 该容器在 guest 用户下运行.</p> <h6 id="_13-2-2-阻止容器以root用户运行"><a href="#_13-2-2-阻止容器以root用户运行" class="header-anchor">#</a> 13.2.2 阻止容器以root用户运行</h6> <p>如果不关心容器是哪个用户运行的, 只是<strong>希望阻止以 root 用户运行</strong>呢?</p> <p>假设有一个已经部署好的 pod, 它使用一个在 Dockerfile 中使用 USER daemon 命令制作的镜像, 使其在 daemon 用户下运行. 如果攻击者获取了访问镜像仓库的权限, 并上传了一个标签完全相同, 在 root 用户下运行的镜像, 会发生什么? 当 Kubernetes 的调度器运行该 pod 的新实例时, kubelet 会下载攻击者的镜像, 并运行该镜像中的任何代码.</p> <p>虽然容器与宿主节点基本上是隔离的, <strong>使用 root 用户运行容器中的进程仍然是一种不好的实践</strong>. 例如, 当宿主节点上的一个目录被挂载到容器中时, 如果这个容器中的进程使用了 root 用户运行, 它就拥有该目录的完整访问权限; 如果用非 root 用户运行, 则没有完整权限.</p> <p>为了防止以上的攻击场景发生, 可以进行<strong>配置 pod 中的容器以非 root 用户运行</strong>, 如以下的代码清单所示.</p> <p><strong>代码清单-13.7 阻止容器使用 root 用户运行: pod-run-as-non-root.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>run<span class="token punctuation">-</span>as<span class="token punctuation">-</span>non<span class="token punctuation">-</span>root
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>   <span class="token comment"># 这个容器只允许以非root用户运行</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>    
      <span class="token key atrule">runAsNonRoot</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>部署这个 pod 之后, 它会被成功调度, 但是不允许运行:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po pod-run-as-non-root
NAME                 READY  STATUS
pod-run-as-non-root  <span class="token number">0</span>/1    container has runAsNonRoot and image will run as root
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>现在, 即使攻击者篡改了镜像, 他们也无法做出进一步的破坏.</p> <h6 id="_13-2-3-使用特权模式运行pod"><a href="#_13-2-3-使用特权模式运行pod" class="header-anchor">#</a> 13.2.3 使用特权模式运行pod</h6> <p>有时 pod 需要做它们的宿主节点上能够做的任何事, 例如操作被保护的系统设备, 或使用其他在通常容器中不能使用的内核功能.</p> <p>这种 pod 的一个样例就是 kube-proxy pod, 该 pod 需要像第 11 章中描述的那样, 修改宿主机的 iptables 规则来让 Kubernetes 中的服务规则生效. 当按照附录 B, 使用 kubeadm 部署集群时, 会看到每个节点上都运行了 kube-proxy pod, 并且可以查看 YAML 描述文件中所有使用到的特殊特性.</p> <p><mark><strong>为获取宿主机内核的完整权限, 该 pod 需要在特权模式下运行</strong></mark>. 这可以通过<strong>将容器的 securityContext 中的 privileged 设置为 true 实现</strong>. 可以通过以下代码清单中的 YAML 文件创建一个特权模式的 pod.</p> <p><strong>代码清单-13.8 一个带有特权容器的 pod:pod-privileged.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>privileged
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>
      <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token comment"># 配置这个容器在特权模式下运行</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>部署这个 pod, 然后与之前部署的非特权模式的 pod 做对比.</p> <p>熟悉 Linux 的读者会知道 Linux 中有一个叫作 /dev 的特殊目录, 该目录包含系统中所有设备对应的设备文件. 这些文件不是磁盘上的常规文件, 而是用于与设备通信的特殊文件. 通过列出 /dev 目录下文件的方式查看先前部署的非特权模式容器(名为 pod-with-defaults 的 pod)中的设备, 如以下代码清单所示.</p> <p><strong>代码清单-13.9 非特权 pod 可用的设备列表</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-with-defaults <span class="token function">ls</span> /dev
core             null             stderr           urandom
fd               ptmx             stdin            zero
full             pts              stdout
fuse             random           termination-log
mqueue           shm              <span class="token function">tty</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这个相当短的列表已经列出了全部的设备, 将这个列表与下面的列表比较. 下面的列表列出了在特权 pod 中能看到的特权设备.</p> <p><strong>代码清单-13.10 特权 pod 可用的设备列表</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-privileged <span class="token function">ls</span> /dev
autofs              snd                 tty46
bsg                 sr0                 tty47
btrfs-control       stderr              tty48
core                stdin               tty49
cpu                 stdout              tty5
cpu_dma_latency     termination-log     tty50
fd                  <span class="token function">tty</span>                 tty51
full                tty0                tty52
fuse                tty1                tty53
hpet                tty10               tty54
hwrng               tty11               tty55
<span class="token punctuation">..</span>.                 <span class="token punctuation">..</span>.                 <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>由于完整的设备列表过长, 以上没有完整列出所有的设备, 但这已经足以证明这个设备列表远远长于之前的列表. 事实上, <strong>特权模式的 pod 可以看到宿主节点上的所有设备. 这意味着它可以自由使用任何设备</strong>.</p> <p>举个例子, 如果要在树莓派上运行一个 pod, 用这个 pod 来控制相连的 LED, 那么必须使用特权模式运行这个 pod.</p> <h6 id="_13-2-4-为容器单独添加内核功能"><a href="#_13-2-4-为容器单独添加内核功能" class="header-anchor">#</a> 13.2.4 为容器单独添加内核功能</h6> <p>上一节中已经介绍了一种给予容器无限力量的方法. 过去, 传统的 UNIX 实现只区分特权和非特权进程, 但是经过多年的发展, Linux 已经可以通过内核功能支持更细粒度的权限系统.</p> <p>相比于让容器运行在特权模式下以给予其无限的权限, 一个更加安全的做法是<strong>只给予它使用真正需要的内核功能的权限</strong>. Kubernetes 允许为特定的容器添加内核功能, 或禁用部分内核功能, 以允许对容器进行更加精细的权限控制, 限制攻击者潜在侵入的影响.</p> <p>例如, 一个容器通常不允许修改系统时间(硬件时钟的时间). 可以通过在 pod-with-defaults pod 中修改设定时间来验证:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-with-defaults -- <span class="token function">date</span> +%T <span class="token parameter variable">-s</span> <span class="token string">&quot;12:00:00&quot;</span>
date: can't <span class="token builtin class-name">set</span> date: Operation not permitted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果需要允许容器修改系统时间, 可以<strong>在容器的 capbilities 里 add 一项名为 CAP_SYS_TIME 的功能</strong>, 如以下代码清单所示.</p> <p><strong>代码清单-13.11 添加 CAP_SYS_TIME 功能: pod-add-settime-capability.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>add<span class="token punctuation">-</span>settime<span class="token punctuation">-</span>capability
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>      <span class="token comment"># 在securityContext中添加或禁用内核功能</span>
      <span class="token key atrule">capabilities</span><span class="token punctuation">:</span>                  
        <span class="token key atrule">add</span><span class="token punctuation">:</span>              <span class="token comment"># 咋这里添加了SYS_TIME功能</span>
        <span class="token punctuation">-</span> SYS_TIME                   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>注意: Linux 内核功能的名称通常以 <code>CAP_</code>​ 开头. 但在 pod spec 中指定内核功能时, 必须省略 <code>CAP_</code>​ 前缀.</p> <p>在新的容器中运行同样的命令, 可以成功修改系统时间:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-add-settime-capability -- <span class="token function">date</span> +%T <span class="token parameter variable">-s</span> <span class="token string">&quot;12:00:00&quot;</span>
<span class="token number">12</span>:00:00

$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-add-settime-capability -- <span class="token function">date</span>
Sun May  <span class="token number">7</span> <span class="token number">12</span>:00:03 UTC <span class="token number">2017</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>警告: 自行尝试时, 请注意这样可能导致节点不可用. 在 Minikube 中, 尽管系统时间成功被网络时间协议(Network Time Protocol,NTP)重置, 仍然不得不重启节点以调度新的 pod.</p> <p>可以通过在运行该 pod 的节点上查看时间来确认系统时间已经被成功修改. 笔者使用的是 Minikube, 仅有一个节点, 可以通过如下命令查看时间:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube <span class="token function">ssh</span> <span class="token function">date</span>
Sun May  <span class="token number">7</span> <span class="token number">12</span>:00:07 UTC <span class="token number">2017</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>添加内核功能远比通过设置 privileged:true 更好, 诚然这样需要使用者了解各种内核功能.</p> <p>提示: 可以在 Linux 手册中查阅 Linux 内核功能列表.</p> <h6 id="_13-2-5-在容器中禁用内核功能"><a href="#_13-2-5-在容器中禁用内核功能" class="header-anchor">#</a> 13.2.5 在容器中禁用内核功能</h6> <p>你已经了解到如何给容器添加内核功能, 另一方面也可以<strong>禁用容器中的内核功能</strong>. 例如, 默认情况下容器拥有 <strong>CAP_CHOWN</strong> 权限, 允许进程修改文件系统中文件的所有者.</p> <p>在以下示例中可以看到, 可以在 pod-with-defaults 中将 /tmp 目录的所有者改为 guest 用户:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> pod-with-defaults <span class="token function">chown</span> guest /tmp
$ kubectl <span class="token builtin class-name">exec</span> pod-with-defaults -- <span class="token function">ls</span> <span class="token parameter variable">-la</span> / <span class="token operator">|</span> <span class="token function">grep</span> tmp
drwxrwxrwt    <span class="token number">2</span> guest    root             <span class="token number">6</span> May <span class="token number">25</span> <span class="token number">15</span>:18 tmp
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>为了阻止容器的此种行为, 需要如以下代码清单所示, <strong>在容器的 securityContext.capabilities.drop 列表中加入此项, 以禁用这个修改文件所有者的内核功能</strong>.</p> <p><strong>代码清单-13.12 禁用容器中的内核功能: pod-drop-chown-capability.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>drop<span class="token punctuation">-</span>chown<span class="token punctuation">-</span>capability
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>
      <span class="token key atrule">capabilities</span><span class="token punctuation">:</span>
        <span class="token key atrule">drop</span><span class="token punctuation">:</span>      <span class="token comment"># 在这里禁止了容器修改文件的所有者</span>
        <span class="token punctuation">-</span> CHOWN        
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>禁用 CHOWN 内核功能后, 不允许在这个 pod 中修改文件所有者:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> pod-drop-chown-capability <span class="token function">chown</span> guest /tmp
chown: /tmp: Operation not permitted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这里已经对容器安全上下文的大部分选项研究完毕. 下面再介绍一个选项.</p> <h6 id="_13-2-6-阻止对容器根文件系统的写入"><a href="#_13-2-6-阻止对容器根文件系统的写入" class="header-anchor">#</a> 13.2.6 阻止对容器根文件系统的写入</h6> <p>因为安全原因, 可能需要<strong>阻止容器中的进程对容器的根文件系统进行写入, 仅允许它们写入挂载的存储卷</strong>.</p> <p>假如你在运行一个有隐藏漏洞, 可以允许攻击者写入文件系统的 PHP 应用. 这些 PHP 文件在构建时放入容器的镜像中, 并且在容器的根文件系统中提供服务. 由于漏洞的存在, 攻击者可以修改这些文件, 在其中注入恶意代码.</p> <p>这一类攻击可以通过阻止容器写入自己的根文件系统(应用的可执行代码的通常储存位置)来防止. 可以如以下代码清单所示, 将<strong>容器的 securityContext.readOnlyRootFilesystem 设置为 true 来实现</strong>.</p> <p><strong>代码清单-13.13 根文件系统只读的容器: pod-with-readonly-filesystem.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>with<span class="token punctuation">-</span>readonly<span class="token punctuation">-</span>filesystem
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>                      <span class="token comment"># 这个容器的根文件系统不允许写入</span>
      <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>                         <span class="token comment"># 但是向/volume写入是允许的, 因为这个目录挂载了一个存储卷</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>volume                 
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /volume              
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>                 
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>volume
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>这个 pod 中的容器虽然以 root 用户运行, 拥有 <code>/</code>​ 目录的写权限, 但在该目录下写入一个文件会失败:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-with-readonly-filesystem <span class="token function">touch</span> /new-file
touch: /new-file: Read-only <span class="token function">file</span> system
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>另一方面, <strong>对挂载的卷的写入是允许</strong>的:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-with-readonly-filesystem <span class="token function">touch</span> /volume/newfile
$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-with-readonly-filesystem -- <span class="token function">ls</span> <span class="token parameter variable">-la</span> /volume/newfile
-rw-r--r--    <span class="token number">1</span> root     root       <span class="token number">0</span> May  <span class="token number">7</span> <span class="token number">19</span>:11 /mountedVolume/newfile
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>如以上例子所示, 如果容器的根文件系统是只读的, 就很可能需要为应用会写入的每一个目录(如日志, 磁盘缓存等)挂载存储卷.</p> <p>提示: 为了增强安全性, <strong>请将在生产环境运行的容器的 readOnlyRootFilesystem 选项设置为 true</strong>.</p> <blockquote><p>设置 pod 级别的安全上下文</p></blockquote> <p>以上的例子都是对单独的容器设置安全上下文. 这些选项中的一部分也可以从 pod 级别设定(通过 pod.spec.securityContext 属性). 它们会作为 pod 中每一个容器的默认安全上下文, 但是会被容器级别的安全上下文覆盖. 下面将会介绍 pod 级别安全上下文独有的内容.</p> <h6 id="_13-2-7-容器使用不同用户运行时共享存储卷"><a href="#_13-2-7-容器使用不同用户运行时共享存储卷" class="header-anchor">#</a> 13.2.7 容器使用不同用户运行时共享存储卷</h6> <p>第 6 章中已经介绍了如何使用存储卷在 pod 的不同容器中共享数据. 可以顺利地在一个容器中写入数据, 在另一个容器中读出这些数据.</p> <p>但这只是因为两个容器都以 root 用户运行, 对存储卷中的所有文件拥有全部权限. 现在假设使用前面介绍的 runAsUser 选项. 你可能需要<strong>在一个 pod 中用两个不同的用户运行两个容器</strong>(可能是两个第三方的容器, 都以它们自己的特定用户运行进程). 如果这样的两个容器通过存储卷共享文件, 它们不一定能够读取或写入另一个容器的文件.</p> <p>因此, Kubernetes 允许为 pod 中所有容器指定 supplemental 组, 以允许它们无论以哪个用户 ID 运行都可以共享文件. 这可以通过以下两个属性设置:</p> <ul><li>fsGroup</li> <li>supplementalGroups</li></ul> <p>解释它们的效果的最好方法是使用例子说明, 下面来看一下如何在 pod 中使用它们, 以及它们效果. 以下代码清单描述了一个拥有两个共享同一存储卷的容器的 pod.</p> <p><strong>代码清单-13.14 fsGroup 和 supplementalGroups: pod-with-sharedvolume-fsgroup.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>with<span class="token punctuation">-</span>shared<span class="token punctuation">-</span>volume<span class="token punctuation">-</span>fsgroup
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>           <span class="token comment"># fsGroup和supplementalGroups在pod级别的安全上下文中定义</span>
    <span class="token key atrule">fsGroup</span><span class="token punctuation">:</span> <span class="token number">555</span>                 
    <span class="token key atrule">supplementalGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">666</span><span class="token punctuation">,</span> <span class="token number">777</span><span class="token punctuation">]</span>   
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> first
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>                     <span class="token comment"># 第一个容器使用的用户ID为1111</span>
      <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">1111</span>            
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>                        <span class="token comment"># 两个容器使用同一存储卷</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shared<span class="token punctuation">-</span>volume        
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /volume
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> second
    <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;/bin/sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;999999&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>                     <span class="token comment"># 第二个容器使用的用户ID为2222</span>
      <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">2222</span>            
    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>                        <span class="token comment"># 两个容器使用同一存储卷</span>
    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shared<span class="token punctuation">-</span>volume        
      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /volume
      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>                       
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> shared<span class="token punctuation">-</span>volume          
    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><p>创建这个 pod 之后, 进入第一个容器查看它的用户 ID 和组 ID:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-with-shared-volume-fsgroup <span class="token parameter variable">-c</span> first <span class="token function">sh</span>
/ $ <span class="token function">id</span>
<span class="token assign-left variable">uid</span><span class="token operator">=</span><span class="token number">1111</span> <span class="token assign-left variable">gid</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span> <span class="token assign-left variable">groups</span><span class="token operator">=</span><span class="token number">555,666</span>,777
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>id 命令显示, 这个 pod 运行在 ID 为 1111 的用户下, 它的用户组为 0(root), 但用户组 555, 666, 777 也关联到了该用户下.</p> <p>在 pod 的定义中, 将 fsGroup 设置成了 555, 因此, 存储卷属于用户组 ID 为 555 的用户组:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ $ <span class="token function">ls</span> <span class="token parameter variable">-l</span> / <span class="token operator">|</span> <span class="token function">grep</span> volume
drwxrwsrwx    <span class="token number">2</span> root     <span class="token number">555</span>              <span class="token number">6</span> May <span class="token number">29</span> <span class="token number">12</span>:23 volume
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>该容器在这个存储卷所在目录中创建的文件, 所属的用户 ID 为 1111(即该容器运行时使用的用户 ID), 所属的用户组 ID 为 555:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ $ <span class="token builtin class-name">echo</span> foo <span class="token operator">&gt;</span> /volume/foo
/ $ <span class="token function">ls</span> <span class="token parameter variable">-l</span> /volume
total <span class="token number">4</span>
-rw-r--r--    <span class="token number">1</span> <span class="token number">1111</span>     <span class="token number">555</span>              <span class="token number">4</span> May <span class="token number">29</span> <span class="token number">12</span>:25 foo
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这个文件的所属用户情况与通常设置下的新建文件不同. 在通常情况下, 某一用户新创建文件所属的用户组 ID, 与该用户的所属用户组 ID 相同, 在这种情下是 0. 在这个容器的根文件系统中创建一个文件, 可以验证这一点:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/ $ <span class="token builtin class-name">echo</span> foo <span class="token operator">&gt;</span> /tmp/foo
/ $ <span class="token function">ls</span> <span class="token parameter variable">-l</span> /tmp
total <span class="token number">4</span>
-rw-r--r--    <span class="token number">1</span> <span class="token number">1111</span>     root             <span class="token number">4</span> May <span class="token number">29</span> <span class="token number">12</span>:41 foo
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>如你所见, <strong>安全上下文中的 fsGroup 属性当进程在存储卷中创建文件时起作用, 而 supplementalGroups 属性定义了某个用户所关联的额外的用户组</strong>.</p> <p>这一节对配置容器安全上下文的介绍到此结束. 接下来看一下<strong>集群管理员对用户的限制</strong>.</p> <h5 id="_13-3-限制pod使用安全相关的特性"><a href="#_13-3-限制pod使用安全相关的特性" class="header-anchor">#</a> 13.3 限制pod使用安全相关的特性</h5> <p>以上章节中的例子已经介绍了<strong>如何在部署 pod 时在任一宿主节点上做任何想做的事</strong>. 比如, 部署一个特权模式的 pod. 很明显, 需要有一种机制阻止用户使用其中的部分功能. <strong>集群管理人员可以通过创建 PodSecurityPolicy 资源来限制对以上提到的安全相关的特性的使用</strong>.</p> <h6 id="_13-3-1-podsecuritypolicy资源介绍"><a href="#_13-3-1-podsecuritypolicy资源介绍" class="header-anchor">#</a> 13.3.1 PodSecurityPolicy资源介绍</h6> <p><strong>PodSecurityPolicy 是一种集群级别(无命名空间)的资源, 它定义了用户能否在 pod 中使用各种安全相关的特性. 维护 PodSecurityPolicy 资源中配置策略的工作由集成在 API 服务器中的 PodSecurityPolicy 准入控制插件完成</strong>(第11章中介绍了准入控制插件).</p> <p>注意: 你的集群中不一定启用了 PodSecurityPolicy 准入控制插件. 在运行以下样例时, 请确保它已被启用.</p> <p>当有人向 API 服务器发送 pod 资源时, PodSecurityPolicy <strong>准入控制插件</strong>会将这个 pod 与已经配置的 PodSecurityPolicy 进行校验. 如果这个 pod 符合集群中已有安全策略, 它会被接收并存入 etcd; 否则它会立即被拒绝. 这个插件也会根据安全策略中配置的默认值对 pod 进行修改.</p> <blockquote><p>在 Minikube 中启用 RBAC 和 PodSecurityPolicy 准入控制</p></blockquote> <p>笔者使用 Minikube v0.19.0 来运行以下样例. 这个版本没有启用 RBAC 和 PodSecurityPolicy 准入控制插件, 这些在以下的部分练习中是必需的. 其中一个练习需要以不同用户认证, 因此还需要开启 basic authenticate 插件, 其中用户的信息在一个文件中定义.</p> <p>为了在 Minikube 中启用这些插件, 需要运行如下命令(或类似的命令, 这取决于使用的版本):</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube start --extra-config apiserver.Authentication.PasswordFile.
 <span class="token assign-left variable">BasicAuthFile</span><span class="token operator">=</span>/etc/kubernetes/passwd --extra-config<span class="token operator">=</span>apiserver.
 <span class="token assign-left variable">Authorization.Mode</span><span class="token operator">=</span>RBAC --extra-config<span class="token operator">=</span>apiserver.GenericServerRun
 <span class="token assign-left variable">Options.AdmissionControl</span><span class="token operator">=</span>NamespaceLifecycle,LimitRanger,Service
 Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,
 DefaultTolerationSeconds,PodSecurityPolicy
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这个 API 服务器需要创建在以上命令中制定的口令文件才能开始运行. 以下命令可以创建这个文件:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">cat</span> <span class="token operator">&lt;&lt;</span><span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">|</span> minikube <span class="token function">ssh</span> <span class="token function">sudo</span> <span class="token function">tee</span> /etc/kubernetes/passwd</span>
password,alice,1000,basic-user
password,bob,2000,privileged-user
EOF</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以在本书的代码存档的 Chapter13/minikube-with-rbac-and-psp-enabled.sh 中找到运行以上命令的 shell 脚本.</p> <blockquote><p>了解 PodSecurityPolicy 可以做的事</p></blockquote> <p>一个 PodSecurityPolicy 资源可以定义以下事项:</p> <ul><li><strong>是否允许 pod 使用宿主节点的 PID, IPC, 网络命名空间</strong></li> <li><strong>pod 允许绑定的宿主节点端口</strong></li> <li>容器运行时允许使用的用户 ID</li> <li><strong>是否允许拥有特权模式容器的 pod</strong></li> <li><strong>允许添加哪些内核功能, 默认添加哪些内核功能, 总是禁用哪些内核功能</strong></li> <li>允许容器使用哪些 SELinux 选项</li> <li><strong>容器是否允许使用可写的根文件系统</strong></li> <li>允许容器在哪些文件系统组下运行</li> <li><strong>允许 pod 使用哪些类型的存储卷</strong></li></ul> <p>你应该已经熟悉了以上列表除最后一项外的内容. 最后一项也应当比较清楚.</p> <blockquote><p>检视一个 PodSecurityPolicy 样例</p></blockquote> <p>以下代码清单展示了一个 PodSecurityPolicy 的样例. <strong>它阻止了 pod 使用宿主节点的 PID, IPC, 网络命名空间, 运行特权模式的容器, 以及绑定大多数宿主节点的端口</strong>(除 11000～11000 和 13000～14000 范围内的端口). 它没有限制容器运行时使用的用户, 用户组和 SELinux 选项.</p> <p><strong>代码清单-13.15 一个 PodSecurityPolicy 的样例: pod-security-policy.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PodSecurityPolicy   <span class="token comment"># 资源类型为PodSecurityPolicy</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> default
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hostIPC</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>          <span class="token comment"># 容器不允许使用宿主节点的IPD, PID和网络命名空间   </span>
  <span class="token key atrule">hostPID</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>         
  <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>     
  <span class="token key atrule">hostPorts</span><span class="token punctuation">:</span>              <span class="token comment"># 容器只能绑定宿主节点的10000-10001端口或者13000-14000端口</span>
  <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">10000</span>           
    <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">11000</span>          
  <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">13000</span>           
    <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">14000</span>           
  <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>        <span class="token comment"># 容器不能在特权模式下运行</span>
  <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>   <span class="token comment"># 容器强制使用只读的根文件系统</span>
  <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span>                     <span class="token comment"># 容器可以以任意用户和用户组运行</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny       
  <span class="token key atrule">fsGroup</span><span class="token punctuation">:</span>               
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny       
  <span class="token key atrule">supplementalGroups</span><span class="token punctuation">:</span>    
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny       
  <span class="token key atrule">seLinux</span><span class="token punctuation">:</span>                       <span class="token comment"># 它们也可以使用任何SELinux选项</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny       
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>                       <span class="token comment"># pod可以使用所有类型的存储卷</span>
  <span class="token punctuation">-</span> <span class="token string">'*'</span>                  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br></div></div><p>以上样例的大部分选项是不言自明的, 特别是当你已经阅读了本章前几节的内容时. <strong>这个 PodSecurityPolicy 在集群中创建成功之后, API 服务器将不再允许之前样例中的特权 pod</strong>. 例如</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> pod-privileged.yaml
Error from server <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span>: error when creating <span class="token string">&quot;pod-privileged.yaml&quot;</span><span class="token builtin class-name">:</span>
pods <span class="token string">&quot;pod-privileged&quot;</span> is forbidden: unable to validate against any pod
security policy: <span class="token punctuation">[</span>spec.containers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>.securityContext.privileged: Invalid
value: true: Privileged containers are not allowed<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>类似地, 集群中不能再部署使用宿主节点的 PID, IPC, 网络命名空间的 pod 了. 同样, 因为以上策略中的 readOnlyRootFilesystem 选项已设置为 true, 容器的根文件系统将变为只读(容器只能写入挂载的存储卷).</p> <h6 id="_13-3-2-了解runasuser-fsgroup和supplementalgroup策略"><a href="#_13-3-2-了解runasuser-fsgroup和supplementalgroup策略" class="header-anchor">#</a> 13.3.2 了解runAsUser,fsGroup和supplementalGroup策略</h6> <p>前面的例子中的策略没有对容器运行时可以使用的用户和用户组施加任何限制, 因为它们在 runAsUser, fsGroup, supplementalGroups 等字段中使用了 runAsAny 规则. 如果需要限制容器可以使用的用户和用户组 ID, 可以将规则改为 MustRunAs, 并指定允许使用的 ID 范围.</p> <blockquote><p>使用 MustRunAs 规则</p></blockquote> <p>来看以下的例子. 为了只允许容器以用户 ID 2 的身份运行并限制默认的文件系统组和增补组 ID 在 2-10 或 20-30 的范围(包含临界值)内, 需要<strong>在 PodSecurityPolicy 资源</strong>中加入如以下代码清单所示片段.</p> <p><strong>代码清单-13.16 指定容器运行时必须使用的用户和用户组 ID:psp-must-runas.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">runAsUser</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> MustRunAs
    <span class="token key atrule">ranges</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">2</span>      <span class="token comment"># 添加一个max=min的range来制定一个特定ID          </span>
      <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">2</span>              
  <span class="token key atrule">fsGroup</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> MustRunAs
    <span class="token key atrule">ranges</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">2</span>      <span class="token comment"># 支持指定多个取件-这里组ID可以在2-10或20-30之间</span>
      <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">10</span>             
    <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">20</span>             
      <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">30</span>             
  <span class="token key atrule">supplementalGroups</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> MustRunAs
    <span class="token key atrule">ranges</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">2</span>       <span class="token comment"># 支持指定多个取件-这里组ID可以在2-10或20-30之间</span>
      <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">10</span>             
    <span class="token punctuation">-</span> <span class="token key atrule">min</span><span class="token punctuation">:</span> <span class="token number">20</span>             
      <span class="token key atrule">max</span><span class="token punctuation">:</span> <span class="token number">30</span>             
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>如果 pod spec 试图将其中的任一字段设置为该范围之外的值, 这个 pod 将不会被 API 服务器接收. 可以通过删除之前的 PodSecurityContextPolicy, 并通过 pspmust-run-as.yaml 文件创建一个新的来实践这一点.</p> <p>注意: 修改策略对已经存在的 pod 无效, 因为 PodSecurityPolicy 资源仅在创建和升级 pod 时起作用.</p> <blockquote><p>部署 runAsUser 在指定范围之外的 pod</p></blockquote> <p>如果尝试使用之前的 pod-as-user-guest.yaml 文件部署一个 pod, 其中指定了容器运行的用户 ID 为 405, API 服务器会拒绝这个 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> pod-as-user-guest.yaml
Error from server <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span>: error when creating <span class="token string">&quot;pod-as-user-guest.yaml&quot;</span>
<span class="token builtin class-name">:</span> pods <span class="token string">&quot;pod-as-user-guest&quot;</span> is forbidden: unable to validate against any pod
security policy: <span class="token punctuation">[</span>securityContext.runAsUser: Invalid value: <span class="token number">405</span>: <span class="token environment constant">UID</span> on
container main does not match required range.  Found <span class="token number">405</span>, allowed: <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token number">2</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这个是显然的. 但是如果部署 pod 时没有指定 runAsUser 属性, 但用户 ID 被注入到镜像的情况下(在 Dockerfile 中使用 USER 命令), 会发生什么?</p> <blockquote><p>部署镜像中用户 ID 在指定范围之外的 pod</p></blockquote> <p>笔者创建了一个不同版本的 Node.js 镜像, 在全书的例子中使用. 这个镜像被配置为使用用户 ID 为 5 的用户运行. 该镜像使用的 Dockerfile 如以下代码清单所示.</p> <p><strong>代码清单-13.17 包含 USER 指令的 Dockerfile:kubia-run-as-user-5/Dockerfile</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FROM node:7
ADD app.js /app.js
<span class="token environment constant">USER</span> <span class="token number">5</span>       <span class="token comment"># 使用这个镜像运行的容器会在ID为5的用户下运行</span>
ENTRYPOINT <span class="token punctuation">[</span><span class="token string">&quot;node&quot;</span>, <span class="token string">&quot;app.js&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>笔者将这个镜像命名为 <code>uksa/kubia-run-as-user-5</code>​, 上传到 DockerHub. 如果使用这个镜像创建 pod, API 服务器不会拒绝:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run run-as-5 <span class="token parameter variable">--image</span> luksa/kubia-run-as-user-5 <span class="token parameter variable">--restart</span> Never
pod <span class="token string">&quot;run-as-5&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>与之前不同, API 服务器接收了这个 pod, kubelet 也运行了这个容器. 接下来查看这个容器使用的用户 ID 和用户组 ID:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> run-as-5 -- <span class="token function">id</span>
<span class="token assign-left variable">uid</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">(</span>bin<span class="token punctuation">)</span> <span class="token assign-left variable">gid</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">(</span>bin<span class="token punctuation">)</span> <span class="token assign-left variable">groups</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">(</span>bin<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>可以看到, 这个容器运行时使用的用户 ID 为 2, 就是<strong>在 PodSecurityPolicy 中指定的 ID</strong>. PodSecurityPolicy 可以将硬编码覆盖到镜像中的用户 ID.</p> <blockquote><p>在 runAsUser 字段中使用 mustRunAsNonRoot 规则</p></blockquote> <p>runAsUser 字段中还可以使用另一种规则: mustRunAsNonRoot. 正如其名, 它将阻止用户部署以 root 用户运行的容器. 在此种情况下, spec 容器中必须指定 runAsUser 字段, 并且不能为 0(0为 root 用户的 ID), 或者容器的镜像本身指定了用一个非 0 的用户 ID 运行. 这种做法的好处已经在之前介绍过.</p> <h6 id="_13-3-3-配置允许-默认添加-禁止使用的内核功能"><a href="#_13-3-3-配置允许-默认添加-禁止使用的内核功能" class="header-anchor">#</a> 13.3.3 配置允许,默认添加,禁止使用的内核功能</h6> <p>如你所知, <strong>容器可以运行在特权模式下, 也可以通过对每个容器添加或禁用 Linux 内核功能来定义更细粒度的权限配置</strong>. 以下三个字段会影响容器可以使用的内核功能:</p> <ul><li><strong>allowedCapabilities</strong></li> <li><strong>defaultAddCapabilities</strong></li> <li><strong>requiredDropCapabilities</strong></li></ul> <p>下面先来看一个例子, 然后讨论这三个字段各自的行为. 以下代码清单展示了一个定义了这三个字段的 PodSecurityPolicy 资源.</p> <p><strong>代码清单-13.18 在 PodSecurityPolicy 资源中指定内核功能: psp-capabilities.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PodSecurityPolicy          <span class="token comment"># 资源类型为PodSecurityPolicy</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">allowedCapabilities</span><span class="token punctuation">:</span>           <span class="token comment"># 允许容器添加SYS_TIME功能</span>
  <span class="token punctuation">-</span> SYS_TIME              
  <span class="token key atrule">defaultAddCapabilities</span><span class="token punctuation">:</span>        <span class="token comment"># 为每个容器自动添加CHOWN功能</span>
  <span class="token punctuation">-</span> CHOWN                 
  <span class="token key atrule">requiredDropCapabilities</span><span class="token punctuation">:</span>      <span class="token comment"># 要求容器禁用SYS_ADMIN和SYS_MODULE功能 </span>
  <span class="token punctuation">-</span> SYS_ADMIN             
  <span class="token punctuation">-</span> SYS_MODULE            
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>注意: SYS_ADMIN 功能允许使用一系列的管理操作; SYS_MODULE 功能允许加载或卸载 Linux 内核模块.</p> <blockquote><p>指定容器中可以添加的内核功能</p></blockquote> <p>allowedCapabilities 字段用于指定 spec 容器的 securityContext.capabilities 中<strong>可以添加哪些内核功能</strong>. 之前的一个例子中, 容器内添加了 SYS_TIME 内核功能. 如果启用了 PodSecurityPolicy 访问控制插件, pod 中不能添加以上内核功能, 除非在 PodSecurityPolicy 中指明允许添加, 如代码清单 13.18 所示.</p> <blockquote><p>为所有容器添加内核功能</p></blockquote> <p>defaultAddCapabilities 字段中<strong>列出的所有内核功能将被添加到每个已部署的 pod 的每个容器中</strong>. 如果用户不希望某个容器拥有这些功能, 必须在容器的 spec 中显式地禁用它们.</p> <p>代码清单 13.18 中的例子自动在每个容器中添加 CAP_CHOWN 功能, 因此容器中的进程允许修改容器中文件的所有者(例如, 使用 chown 命令).</p> <blockquote><p>禁用容器中的内核功能</p></blockquote> <p>这个例子中的最后一个字段是 requiredDropCapabilities. 笔者承认, 这个名字对我来说在最初看到时有点奇怪, 但它并没有那么复杂. 在这个字段中<strong>列出的内核功能会在所有容器中被禁用</strong>(PodSecurityPolicy 访问控制插件会在所有容器的 securityContext.capabilities.drop 字段中加入这些功能).</p> <p>如果用户试图在创建的 pod 中显式加入 requiredDropCapabilities 字段中的内核功能, 这个 pod 会被拒绝:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> pod-add-sysadmin-capability.yaml
Error from server <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span>: error when creating <span class="token string">&quot;pod-add-sysadmin-
capability.yaml&quot;</span><span class="token builtin class-name">:</span> pods <span class="token string">&quot;pod-add-sysadmin-capability&quot;</span> is forbidden: unable
to validate against any pod security policy: <span class="token punctuation">[</span>capabilities.add: Invalid
value: <span class="token string">&quot;SYS_ADMIN&quot;</span><span class="token builtin class-name">:</span> capability may not be added<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h6 id="_13-3-4-限制pod可以使用的存储卷类型"><a href="#_13-3-4-限制pod可以使用的存储卷类型" class="header-anchor">#</a> 13.3.4 限制pod可以使用的存储卷类型</h6> <p>最后一项 PodSecurityPolicy 资源可以做到的是<strong>定义用户可以在 pod 中使用哪些类型的存储卷</strong>. 在最低限度上, 一个 PodSecurityPolicy 需要允许 pod 使用以下类型的存储卷: <strong>emptyDir, configMap, secret, downwardAPI, persistentVolumeClaim</strong>. PodSecurityPolicy 资源中的相关部分如以下代码清单所示.</p> <p><strong>代码清单-13.19 仅允许特定类型存储卷的 PodSecurityPolicy 片段: pspvolumes.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">kind</span><span class="token punctuation">:</span> PodSecurityPolicy
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> emptyDir
  <span class="token punctuation">-</span> configMap
  <span class="token punctuation">-</span> secret
  <span class="token punctuation">-</span> downwardAPI
  <span class="token punctuation">-</span> persistentVolumeClaim
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>如果有多个 PodSecurityPolicy 资源, pod 可以使用 PodSecurityPolicy 中允许使用的任何一个存储卷类型(实际生效的是所有 volume 列表的并集).</p> <h6 id="_13-3-5-对不同的用户与组分配不同的podsecuritypolicy"><a href="#_13-3-5-对不同的用户与组分配不同的podsecuritypolicy" class="header-anchor">#</a> 13.3.5 对不同的用户与组分配不同的PodSecurityPolicy</h6> <p>前面已经提到, PodSecurityPolicy 是集群级别的资源, 这意味着它不能存储和应用在某一特定的命名空间上. 这是否意味着它总是会应用在所有的命名空间上呢? 不是的, 因为这样会使得它们相当难以应用. 毕竟, 系统 pod 经常需要允许做一些常规 pod 不应当做的事情.</p> <p>对不同用户分配不同 PodSecurityPolicy 是通过前一章中描述的 RBAC 机制实现的. 这个方法是, 创建需要的 PodSecurityPolicy 资源, 然后创建 ClusterRole 资源并通过名称将它们指向不同的策略, 以此使 PodSecurityPolicy 资源中的策略对不同的用户或组生效. 通过 ClusterRoleBinding 资源将特定的用户或组绑定到 ClusterRole 上, 当 PodSecurityPolicy 访问控制插件需要决定是否接纳一个 pod 时, 它只会考虑创建 pod 的用户可以访问到的 PodSecurityPolicy 中的策略.</p> <p>可以在下面的练习中看到如何做到这些. 首先, 创建另一个 <strong>PodSecurityPolicy</strong>.</p> <blockquote><p>创建一个允许部署特权容器的 PodSecurityPolicy</p></blockquote> <p>首先, 要创建一个特殊的 PodSecurityPolicy, 允许用户创建拥有特权容器的 pod. 以下代码清单展示了该 PodSecurityPolicy 的定义.</p> <p><strong>代码清单-13.20 特权用户使用的 PodSecurityPolicy: psp-privileged.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> PodSecurityPolicy     <span class="token comment"># 资源类型为PodSecurityPolicy</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> privileged          <span class="token comment"># 它的名字为privileged</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>          <span class="token comment"># 它允许创建特权容器</span>
  <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny
  <span class="token key atrule">fsGroup</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny
  <span class="token key atrule">supplementalGroups</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny
  <span class="token key atrule">seLinux</span><span class="token punctuation">:</span>
    <span class="token key atrule">rule</span><span class="token punctuation">:</span> RunAsAny
  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token string">'*'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>在向 API 服务器 post 这个 PodSecurityPolicy 之后, 集群中有两个策略:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get psp
NAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    <span class="token punctuation">..</span>.
default      <span class="token boolean">false</span>   <span class="token punctuation">[</span><span class="token punctuation">]</span>     RunAsAny   RunAsAny    RunAsAny   <span class="token punctuation">..</span>.
privileged   <span class="token boolean">true</span>    <span class="token punctuation">[</span><span class="token punctuation">]</span>     RunAsAny   RunAsAny    RunAsAny   <span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>注意: psp 是 PodSecurityPolicy 的简写.</p> <p>正如 PRIV 列中所示, default 策略禁止运行特权容器, 然而 privileged 策略是允许的. 因为现在是以 cluster-admin 身份登录的, 所以可以看到所有的策略. 部署 pod 时, 如果任一策略允许使用 pod 中使用到的特性, API 服务器就会接收这个 pod.</p> <p>现在考虑另外两个使用该集群的用户: Alice 和 Bob. 你希望 Alice 只能部署受限制的(非特权)pod, 允许 Bob 部署特权 pod. 可以通过让 Alice 只能使用 default PodSecurityPolicy, 而 Bob 可以使用以上两个 PodSecurityPolicy 来做到.</p> <blockquote><p>使用 RBAC 将不同的 PodSecurityPolicy 分配给不同用户</p></blockquote> <p>在上一章中, <strong>已经使用了 RBAC 机制来给用户授予特定类型的资源的访问权限, 但 RBAC 机制也可以通过使用引用其名字来授予对特定资源实例的访问权限</strong>. 这就是为了让不同用户使用不同 PodSecurityPolicy 的方法.</p> <p>首先需要<strong>创建两个 ClusterRole, 分别允许使用其中一个策略</strong>. 将第一个 ClusterRole 命名为 psp-default 并允许其使用 default PodSecurityPolicy 资源. 可以使用 kubectl create clusterrole 来操作:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrole psp-default <span class="token parameter variable">--verb</span><span class="token operator">=</span>use <span class="token parameter variable">--resource</span><span class="token operator">=</span>podsecuritypolicies --resource-name<span class="token operator">=</span>default
clusterrole <span class="token string">&quot;psp-default&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 使用的动词是 use, 而非 get, list, watch 或类似的动词.</p> <p>如你所见, 通过 --resource-name 选项引用了一个 PodSecurityPolicy 资源的特定实例. 现在, 创建另一个名为 psp-Privileged ClusterRole, 指向 privileged 策略:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrole psp-privileged <span class="token parameter variable">--verb</span><span class="token operator">=</span>use <span class="token parameter variable">--resource</span><span class="token operator">=</span>podsecuritypolicies --resource-name<span class="token operator">=</span>privileged
clusterrole <span class="token string">&quot;psp-privileged&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在需要<strong>把这两个策略绑定到用户上</strong>. 在之前的章节提到过, 为了绑定一个 ClusterRole 资源以授予对<strong>集群级别</strong>资源(PodSecurityPolicy 资源就是集群级别的资源)的访问权限, 需要使用 <strong>ClusterRoleBinding</strong> 资源而非(有命名空间的)RoleBinding.</p> <p>要将 <code>psp-default ClusterRole</code>​ 绑定到所有已认证用户上, 而非只有 Alice. 这是必需的, 否则没有用户可以创建 pod, 因为 PodSecurityPolicy 访问控制插件会因为没有找到任何策略而拒绝创建 pod. 所有已认证用户都属于 <code>system:authenticated</code>​ 组, 因此需要将该 ClusterRole 绑定到这个组:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrolebinding psp-all-users <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>psp-default <span class="token parameter variable">--group</span><span class="token operator">=</span>system:authenticated
clusterrolebinding <span class="token string">&quot;psp-all-users&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>接着, 需要将 <code>psp-privileged ClusterRole</code>​ 绑定到用户 Bob:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create clusterrolebinding psp-bob <span class="token parameter variable">--clusterrole</span><span class="token operator">=</span>psp-privileged <span class="token parameter variable">--user</span><span class="token operator">=</span>bob
clusterrolebinding <span class="token string">&quot;psp-bob&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>作为一个已认证用户, Alice 现在拥有使用 default PodSecurityPolicy 的权限, 然而 Bob 拥有使用 default 和 privileged PodSecurityPolicy 的权限. Alice 不能创建特权 pod, 而 Bob 可以. 接下来看看是否确实如此.</p> <blockquote><p>为 kubectl 创建不同用户</p></blockquote> <p>如何以 Alice 或 Bob 的身份通过认证, 而非现在已经认证的用户? 本书的附录 A 说明了如何在多个集群和多个上下文中使用 kubectl. 上下文中包含用来与集群交互的用户凭据. 若需要了解更多的相关知识, 请查阅附录 A. 这里只展示允许你以 Alice 或 Bob 的身份使用 kubectl 的命令.</p> <p>首先, 需要使用如下命令, 用 kubectl 的 config 子命令创建两个新用户:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl config set-credentials alice <span class="token parameter variable">--username</span><span class="token operator">=</span>alice <span class="token parameter variable">--password</span><span class="token operator">=</span>password
User <span class="token string">&quot;alice&quot;</span> set.

$ kubectl config set-credentials bob <span class="token parameter variable">--username</span><span class="token operator">=</span>bob <span class="token parameter variable">--password</span><span class="token operator">=</span>password
User <span class="token string">&quot;bob&quot;</span> set.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这些命令的行为应当很明显. 因为使用了用户名和密码作为凭据, kubectl 将对这两个用户使用基础 HTTP 认证进行认证(其他的认证方法包括 token, 客户端证书等).</p> <blockquote><p>使用不同用户创建 pod</p></blockquote> <p>现在, 可以尝试以 Alice 的身份认证, 并创建一个特权 pod. 可以通过 --user 选项向 kubectl 传达你使用的用户凭据:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token parameter variable">--user</span> alice create <span class="token parameter variable">-f</span> pod-privileged.yaml
Error from server <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span>: error when creating <span class="token string">&quot;pod-privileged.yaml&quot;</span><span class="token builtin class-name">:</span>
     pods <span class="token string">&quot;pod-privileged&quot;</span> is forbidden: unable to validate against any pod
     security policy: <span class="token punctuation">[</span>spec.containers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>.securityContext.privileged: Invalid
     value: true: Privileged containers are not allowed<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>与预期相同, API 服务器不允许 Alice 创建特权 pod. 现在来看一下 Bob 是否允许:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token parameter variable">--user</span> bob create <span class="token parameter variable">-f</span> pod-privileged.yaml
pod <span class="token string">&quot;pod-privileged&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>到这里就可以了. 这里<strong>成功地使用了 RBAC, 让访问控制插件对不同用户使用不同的 PodSecurityPolicy 资源</strong>.</p> <h5 id="_13-4-隔离pod的网络"><a href="#_13-4-隔离pod的网络" class="header-anchor">#</a> 13.4 隔离pod的网络</h5> <p>到此为止, 本章中已经检视了很多与安全相关的配置选项, 作用在 pod 和 pod 中的容器上. 在本章的剩余部分中, 来看一下<mark><strong>如何通过限制 pod 可以与其他哪些 pod 通信, 来确保 pod 之间的网络安全</strong></mark>.</p> <p><strong>是否可以进行这些配置取决于集群中使用的容器网络插件. 如果网络插件支持, 可以通过 NetworkPolicy 资源配置网络隔离.</strong></p> <p>一个 NetworkPolicy 会应用在<strong>匹配它的标签选择器的 pod 上, 指明这些允许访问这些 pod 的源地址, 或这些 pod 可以访问的目标地址</strong>. 这些分别由入向(ingress)和出向(egress)规则指定. 这两种规则都可以匹配由标签选择器选出的 pod, 或者一个 namespace 中的所有 pod, 或者通过无类别域间路由(Classless Inter-Domain Routing,CIDR)指定的 IP 地址段. 注意: 这里的入向规则与第 5 章中的 Ingress 资源无关.</p> <p>下面将介绍这两种规则及全部三种匹配选项.</p> <h6 id="_13-4-1-在一个命名空间中启用网络隔离"><a href="#_13-4-1-在一个命名空间中启用网络隔离" class="header-anchor">#</a> 13.4.1 在一个命名空间中启用网络隔离</h6> <p>在默认情况下, <strong>某一命名空间中的 pod 可以被任意来源访问</strong>. 首先, 需要改变这个设定. 需要创建一个 default-deny NetworkPolicy, 它会阻止任何客户端访问中的 pod. 这个 NetworkPolicy 的定义如以下代码清单所示.</p> <p><strong>代码清单-13.21 default-deny NetworkPolicy 定义: network-policydefault-deny.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> NetworkPolicy    <span class="token comment"># 资源类型为NetworkPolicy</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>deny
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>   <span class="token comment"># 空的标签选择器匹配命令空间中的所有pod</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>在任何一个特定的命名空间中创建该 NetworkPolicy 之后, <strong>任何客户端都不能访问该命名空间中的 pod</strong>.</p> <p>注意: 集群中的 CNI 插件或其他网络方案需要支持 NetworkPolicy, 否则 NetworkPolicy 将不会影响 pod 之间的可达性.</p> <h6 id="_13-4-2-允许同一命名空间中的部分pod访问一个服务端pod"><a href="#_13-4-2-允许同一命名空间中的部分pod访问一个服务端pod" class="header-anchor">#</a> 13.4.2 允许同一命名空间中的部分pod访问一个服务端pod</h6> <p>为了允许同一命名空间中的客户端 pod 访问该命名空间的 pod, 需要<strong>指明哪些 pod 可以访问</strong>. 接下来, 通过例子来探究如何做到这些.</p> <p>假设在 foo namespace 中有一个 pod 运行 PostgreSQL 数据库, 以及一个使用该数据库的网页服务器 pod, 其他 pod 也在这个命名空间中运行, 然而你不允许它们连接数据库. 为了保障网络安全, 需要在数据库 pod 所在的<strong>命名空间</strong>中创建一个如以下代码清单所示的 NetworkPolicy 资源.</p> <p><strong>代码清单-13.22 为 Postgres pod 使用的 NetworkPolicy:network-policypostgres.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> NetworkPolicy    <span class="token comment"># 资源类型为NetworkPolicy</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> postgres<span class="token punctuation">-</span>netpolicy
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>         <span class="token comment"># 这个策略确保了对具有app=database标签的pod的访问安全性</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>               
      <span class="token key atrule">app</span><span class="token punctuation">:</span> database            
  <span class="token key atrule">ingress</span><span class="token punctuation">:</span>             <span class="token comment"># 它只允许来自具有app=webserver标签的pod的访问</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>                     
    <span class="token punctuation">-</span> <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>             
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>           
          <span class="token key atrule">app</span><span class="token punctuation">:</span> webserver       
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>             <span class="token comment"># 允许对这个端口的访问</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">5432</span>              
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>例子中的 NetworkPolicy 允许<strong>具有 app=webserver 标签的 pod 访问具有 app=database 的 pod 的访问</strong>, 并且仅限访问 5432 端口, 如图 13.4 所示.</p> <p><img src="/img/image-20240225165123-wvlpws0.png" alt="image" title="图13.4 一个仅允许部分 pod 访问其他特定 pod 的特定端口的 NetworkPolicy"></p> <p>客户端 pod 通常通过 Service 而非直接访问 pod 来访问服务端 pod, 但这对结果没有改变. NetworkPolicy 在通过 Service 访问时仍然会被执行.</p> <h6 id="_13-4-3-在不同kubernetes命名空间之间进行网络隔离"><a href="#_13-4-3-在不同kubernetes命名空间之间进行网络隔离" class="header-anchor">#</a> 13.4.3 在不同Kubernetes命名空间之间进行网络隔离</h6> <p>现在来看有另一个<strong>多个租户使用同一 Kubernetes 集群</strong>的例子. 每个租户有<strong>多个命名空间</strong>, 每个命名空间中有一个标签指明它们属于哪个租户. 例如, 有一个租户 Manning, 它的所有命名空间中都有标签 <strong>tenant:manning</strong>. 其中的一个命名空间中运行了一个微服务 Shopping Cart, 它需要允许同一租户下所有命名空间的所有 pod 访问. 显然, 其他租户禁止访问这个微服务.</p> <p>为了保障该微服务安全, 可以创建如下的 NetworkPolicy 资源, 如以下代码清单所示.</p> <p><strong>代码清单-13.23 为 shopping cart 微服务中的 pod 使用的 NetworkPolicy:network-policy-cart.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> NetworkPolicy
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> shoppingcart<span class="token punctuation">-</span>netpolicy
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>         <span class="token comment"># 该策略应用具有app=shopping-cart标签的pod</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>             
      <span class="token key atrule">app</span><span class="token punctuation">:</span> shopping<span class="token punctuation">-</span>cart     
  <span class="token key atrule">ingress</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span>    <span class="token comment"># 只有在具有tenant=manning标签的命名空间中运行的pod可以访问该微服务</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>         
          <span class="token key atrule">tenant</span><span class="token punctuation">:</span> manning    
    <span class="token key atrule">ports</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>以上 NetworkPolicy 保证了<strong>只有具有 tenant=manning 标签的命名空间中运行的 pod 可以访问 Shopping Cart 微服务</strong>, 如图 13.5 所示.</p> <p>如果 shopping cart 服务的提供者需要允许其他租户(可能是他们的合作公司)访问该服务, 他们可以创建一个新的 NetworkPolicy 资源, 或者在之前的 Networkpolicy 中添加一条入向规则.</p> <p><img src="/img/image-20240225165307-a9loy5w.png" alt="image" title="图13.5 仅允许匹配 namespaceSelector 的命名空间中的 pod 访问特定 pod 的 NetworkPolicy"></p> <p>注意: 在多租户的 Kubernetes 集群中, 通常租户不能为他们的命名空间添加标签(或注释). 否则, 他们可以规避基于 namespaceSelector 的入向规则.</p> <h6 id="_13-4-4-使用cidr隔离网络"><a href="#_13-4-4-使用cidr隔离网络" class="header-anchor">#</a> 13.4.4 使用CIDR隔离网络</h6> <p>除了通过在 pod 选择器或命名空间选择器定义哪些 pod 可以访问 NetworkPolicy 资源中指定的目标 pod, 还可以<strong>通过 CIDR 表示法指定一个 IP 段</strong>. 例如, 为了允许 IP 在 192.168.1.1 到 192.168.1.255 范围内的客户端访问之前提到的 shoppingcart 的 pod, 可以在<strong>入向规则</strong>中加入如以下代码清单所示的代码.</p> <p><strong>代码清单-13.24 在入向规则中指明 IP 段: network-policy-cidr.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">ingress</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">from</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> <span class="token key atrule">ipBlock</span><span class="token punctuation">:</span>    <span class="token comment"># 这条入向规则来自192.168.1.0/24 IP段的客户端的流量</span>
        <span class="token key atrule">cidr</span><span class="token punctuation">:</span> 192.168.1.0/24   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h6 id="_13-4-5-限制pod的对外访问流量"><a href="#_13-4-5-限制pod的对外访问流量" class="header-anchor">#</a> 13.4.5 限制pod的对外访问流量</h6> <p>在之前的所有例子中, 已经通过入向规则限制了进入 pod 的访问流量. 然而, 也可以通过<strong>出向规则</strong>限制 pod 的对外访问流量. 以下代码清单展示了一个例子.</p> <p><strong>代码清单-13.25 在 NetworkPolicy 中使用出向规则: network-policy-egress.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>         <span class="token comment"># 这个策略应用与包含app=webserver标签的pod   </span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>          
      <span class="token key atrule">app</span><span class="token punctuation">:</span> webserver      
  <span class="token key atrule">egress</span><span class="token punctuation">:</span>              <span class="token comment"># 限制pod的出网流量</span>
  <span class="token punctuation">-</span> <span class="token key atrule">to</span><span class="token punctuation">:</span>                   
    <span class="token punctuation">-</span> <span class="token key atrule">podSelector</span><span class="token punctuation">:</span>     <span class="token comment"># webserver的pod只能与有app=webserver标签的pod通信</span>
        <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      
          <span class="token key atrule">app</span><span class="token punctuation">:</span> database   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>以上的 NetworkPolicy 仅允许具有标签 app=webserver 的 pod 访问具有标签 app=database 的 pod, 除此之外不能访问任何地址(不论是其他 pod, 还是任何其他的 IP, 无论在集群内部还是外部).</p> <h5 id="_13-5-本章小结"><a href="#_13-5-本章小结" class="header-anchor">#</a> 13.5 本章小结</h5> <p>在本章了解了如何保障集群中宿主节点和 pod 的安全, 不被其他 pod 攻击. 你学习了:</p> <ul><li>pod 可以使用宿主节点的 Linux 命名空间, 而不是它们自己的.</li> <li>容器可以运行在与镜像中不同的用户或用户组下.</li> <li>容器可以在特权模式下运行, 允许它们访问通常情况下不对 pod 暴露的设备.</li> <li>容器可以在只读模式下运行, 阻止进程写入容器的根文件系统(只允许写入挂载的存储卷).</li> <li>集群级别的 PodSecurityPolicy 资源可以用来防止用户创建可能危及宿主节点的 pod.</li> <li>PodSecurityPolicy 资源可以通过 RBAC 中的 ClusterRole 和 ClusterRoleBinding 与特定用户关联.</li> <li>NetworkPolicy 资源可以限制 pod 的入向或出向网络流量.</li></ul> <p>在下一章将会了解<strong>如何限制 pod 可以使用的计算资源, 以及如何配置 pod 的服务质量控制(Quality of Service,QoS)</strong> .</p> <h4 id="_14-计算资源管理"><a href="#_14-计算资源管理" class="header-anchor">#</a> 14.计算资源管理</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li><strong>为容器申请 CPU, 内存以及其他计算资源</strong></li> <li><strong>配置 CPU, 内存的硬限制</strong></li> <li><strong>理解 pod 的 QoS 机制</strong></li> <li><strong>为命名空间中每个 pod 配置默认, 最大, 最小资源限制</strong></li> <li><strong>为命名空间配置可用资源总数</strong></li></ul> <p>到目前为止, 在创建 pod 时并不关心它们使用 CPU 和内存资源的最大值. 不过在本章会了解到, <strong>为一个 pod 配置资源的预期使用量和最大使用量是 pod 定义中的重要组成部分</strong>. 通过设置这两组参数, 可以确保 pod 公平地使用 Kubernetes 集群资源, 同时也影响着整个集群 pod 的调度方式.</p> <h5 id="_14-1-为pod中的容器申请资源"><a href="#_14-1-为pod中的容器申请资源" class="header-anchor">#</a> 14.1 为pod中的容器申请资源</h5> <p><mark><strong>在创建一个 pod 时, 可以指定容器对 CPU 和内存的资源请求量(即 requests), 以及资源限制量(即 limits). 它们并不在 pod 里定义, 而是针对每个容器单独指定. pod 对资源的请求量和限制量是它所包含的所有容器的请求量和限制量之和.</strong></mark></p> <h6 id="_14-1-1-创建包含资源requests的pod"><a href="#_14-1-1-创建包含资源requests的pod" class="header-anchor">#</a> 14.1.1 创建包含资源requests的pod</h6> <p>下面看一个示例 pod 的定义, 它只有一个容器, 这里为其指定了 CPU 和内存的资源请求量, 如下代码清单所示.</p> <p><strong>代码清单-14.1 定义了资源 requests 的 pod:requests-pod.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> requests<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;dd&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;if=/dev/zero&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;of=/dev/null&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> main         <span class="token comment"># 为主容器指定了资源请求量</span>
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>          
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>         
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 200m      <span class="token comment"># 容器申请200毫核(即一个CPU核心时间的1/5)</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 10Mi   <span class="token comment"># 容器申请10M内存</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>在 pod manifest 中, 声明了一个容器需要 1/5 核(200 毫核)的 CPU 才能正常运行. 换句话说, 五个同样的 pod(或容器)可以足够快地运行在一个 CPU 核上.</p> <p><strong>当不指定 CPU requests 时, 表示并不关心系统为容器内的进程分配了多少 CPU 时间. 在最坏情况下进程可能根本分不到 CPU 时间(当其他进程对 CPU 需求量很大时会发生)</strong> . 这对一些时间不敏感, 低优先级的 batch jobs 没有问题, 但对于处理用户请求的容器这样配置显然不太合适.</p> <p>在 pod spec 里, 同时为容器申请了 10 MB 的内存, 说明期望容器内的<strong>进程最大消耗 10 MB</strong> 的 RAM. 它们可能实际占用较小, 但在正常情况下并不希望它们占用超过这个值. 在本章后面将会看到如果超过会发生什么.</p> <p>现在运行 pod. 当 pod 启动时, 可以通过<strong>在容器中运行 top 命令快速查看进程的 CPU 使用</strong>, 如下面的代码清单所示.</p> <p><strong>代码清单-14.2 查看容器内 CPU 和内存的使用情况</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> requests-pod <span class="token function">top</span>
Mem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached
CPU:  <span class="token number">9.1</span>% usr <span class="token number">42.1</span>% sys  <span class="token number">0.0</span>% nic <span class="token number">48.4</span>% idle  <span class="token number">0.0</span>% io  <span class="token number">0.0</span>% irq  <span class="token number">0.2</span>% sirq
Load average: <span class="token number">0.79</span> <span class="token number">0.52</span> <span class="token number">0.29</span> <span class="token number">2</span>/481 <span class="token number">10</span>
  PID  <span class="token environment constant">PPID</span> <span class="token environment constant">USER</span>     STAT   VSZ %VSZ CPU %CPU COMMAND
    <span class="token number">1</span>     <span class="token number">0</span> root     R     <span class="token number">1192</span>  <span class="token number">0.0</span>   <span class="token number">1</span> <span class="token number">50.2</span> <span class="token function">dd</span> <span class="token keyword">if</span> /dev/zero of /dev/null
    <span class="token number">7</span>     <span class="token number">0</span> root     R     <span class="token number">1200</span>  <span class="token number">0.0</span>   <span class="token number">0</span>  <span class="token number">0.0</span> <span class="token function">top</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>在容器内执行 dd 命令会消耗尽可能多的 CPU, 但因为它是单线程运行所以最多只能跑满一个核, 而 Minikube VM 拥有两个核, 这就是为什么 top 命令显示进程只占用了 50% CPU 的原因.</p> <p>对于两核来说, 50% 显然就是指一个核, 说明容器实际使用量超过了在 pod 定义中申请的 200 毫核. 这是符合预期的, 因为 requests 不会限制容器可以使用的 CPU 数量. 需要指定 CPU 限制实现这一点, 稍后会进行尝试, 不过首先看看在 pod 中指定资源 requests 对 pod 调度的影响.</p> <h6 id="_14-1-2-资源requests如何影响调度"><a href="#_14-1-2-资源requests如何影响调度" class="header-anchor">#</a> 14.1.2 资源requests如何影响调度</h6> <p>通过设置资源 requests 就<strong>指定了 pod 对资源需求的最小值.</strong>  调度器在将 pod 调度到节点的过程中会用到该信息. 每个节点可分配给 pod 的 CPU 和内存数量都是一定的. 调度器在调度时只考虑那些未分配资源量满足 pod 需求量的节点. 如果节点的未分配资源量小于 pod 需求量, 这时节点没有能力提供 pod 对资源需求的最小量, 因此 Kubernetes 不会将该 pod 调度到这个节点.</p> <blockquote><p>调度器如何判断一个 pod 是否适合调度到某个节点</p></blockquote> <p>这里比较重要而且会令人觉得意外的是, <mark><strong>调度器在调度时并不关注各类资源在当前时刻的实际使用量, 而只关心节点上部署的所有 pod 的资源申请量之和. 尽管现有 pods 的资源实际使用量可能小于它的申请量, 但如果使用基于实际资源消耗量的调度算法将打破系统为这些已部署成功的 pods 提供足够资源的保证</strong></mark>.</p> <p>从图 14.1 中可见, 节点上部署了三个 pod. 它们共申请了节点 80% 的 CPU 和 60% 的内存资源. 图右下方的 pod D 将无法调度到这个节点上, 因为它 25% 的 CPU requests <strong>大于节点未分配</strong>的 20%CPU. 而实际上, 这与当前三个 pods 仅使用 70% 的 CPU 没有什么关系.</p> <p><img src="/img/image-20240225165830-jupk6fo.png" alt="image" title="图14.1 调度器只关注资源 requests, 并不关注实际使用量"></p> <blockquote><p>调度器如何利用 pod requests 为其选择最佳节点</p></blockquote> <p>你也许还记得, 在第 11 章中调度器首先会<strong>对节点列表进行过滤, 排除那些不满足需求的节点, 然后根据预先配置的优先级函数对其余节点进行排序</strong>. 其中有两个基于资源请求量的优先级排序函数: LeastRequestedPriority 和 MostRequestedPriority. <strong>前者优先将 pod 调度到请求量少的节点上(也就是拥有更多未分配资源的节点), 而后者相反, 优先调度到请求量多的节点(拥有更少未分配资源的节点)</strong> . 但是, 正如刚刚解释的, 它们都<mark><strong>只考虑资源请求量, 而不关注实际使用资源量</strong></mark>.</p> <p>调度器只能配置一种优先级函数. 你可能在想为什么有人会使用 MostRequestedPriority 函数. 毕竟如果你有一组节点, 通常会使其负载平均分布, 但是在随时可以增加或删除节点的云基础设施上运行时并非如此. 配置调度器使用 MostRequestedPriority 函数, 可以在为每个 pod 提供足量 <strong>CPU/内存</strong>资源的同时, 确保 Kubernetes 使用尽可能少的节点. 通过使 pod 紧凑地编排, 一些节点可以保持空闲并可随时从集群中移除. 由于通常会按照单个节点付费, 这样便可以节省一笔开销.</p> <blockquote><p>查看节点资源总量</p></blockquote> <p>来看看调度器的行为. 下面将部署另一个资源请求量是之前 4 倍的 pod. 但在这之前, 先看看什么是<strong>节点资源总量</strong>. 因为调度器需要知道每个节点拥有多少 CPU 和 内存资源, Kubelet 会向 API 服务器报告相关数据, 并通过节点资源对外提供访问, 可以使用 kubectl describe 命令进行查看, 如以下代码清单所示.</p> <p><strong>代码清单-14.3 节点的资源总量和可分配资源</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe nodes
Name:       minikube
<span class="token punctuation">..</span>.
Capacity:                         <span class="token comment"># 节点的资源总量</span>
  cpu:           <span class="token number">2</span>            
  memory:        2048484Ki    
  pods:          <span class="token number">110</span>          
Allocatable:                      <span class="token comment"># 可分配给pod的资源量</span>
  cpu:           <span class="token number">2</span>            
  memory:        1946084Ki    
  pods:          <span class="token number">110</span>          
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>命令的输出展示了节点可用资源相关的两组数量: <strong>节点资源总量和可分配资源量</strong>. 资源总量代表节点所有的资源总和, 包括那些可能对 pod 不可用的资源. 有些资源会为 Kubernetes 或者系统组件预留. 调度器的决策仅仅基于可分配资源量.</p> <p>单节点的 minikube 集群运行于 2 核的 VM 之上, 同时从上面的例子中可以看到节点没有预留资源, 全部 CPU 都可以分配给 pod. 因此, 调度器再调度另一个申请了 800 毫核的 pod 是没有问题的.</p> <p>下面运行这个 pod. 可以使用示例代码中的 YAML 文件, 或者简单地执行以下命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run requests-pod-2 <span class="token parameter variable">--image</span><span class="token operator">=</span>busybox <span class="token parameter variable">--restart</span> Never <span class="token parameter variable">--requests</span><span class="token operator">=</span><span class="token string">'cpu=800m,memory=20Mi'</span> -- <span class="token function">dd</span> <span class="token assign-left variable">if</span><span class="token operator">=</span>/dev/zero <span class="token assign-left variable">of</span><span class="token operator">=</span>/dev/null
pod <span class="token string">&quot;requests-pod-2&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>来看看它是否被成功调度:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po requests<span class="token operator">-</span>pod<span class="token operator">-</span><span class="token number">2</span>
<span class="token constant">NAME</span>             <span class="token constant">READY</span>     <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
requests<span class="token operator">-</span>pod<span class="token operator">-</span><span class="token number">2</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>       Running   <span class="token number">0</span>          3m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>没问题, 这个 pod 已经被成功调度而且开始运行了.</p> <blockquote><p>创建一个不适合任何节点的 pod</p></blockquote> <p>现在部署了两个 pod, 共申请了 1000 毫核 CPU. 所以应该<strong>还剩下 1 核</strong>可供其他 pod 使用, 是吧? 因此再部署一个资源申请量为 1 核的 pod. 使用与前面类似的命令:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl run requests<span class="token operator">-</span>pod<span class="token operator">-</span><span class="token number">3</span> <span class="token operator">--</span>image<span class="token operator">=</span>busybox <span class="token operator">--</span>restart Never <span class="token operator">--</span>requests<span class="token operator">=</span><span class="token string">'cpu=1,memory=20Mi'</span> <span class="token operator">--</span> dd <span class="token keyword">if</span><span class="token operator">=</span><span class="token operator">/</span>dev<span class="token operator">/</span>zero <span class="token keyword">of</span><span class="token operator">=</span><span class="token operator">/</span>dev<span class="token operator">/</span><span class="token keyword">null</span>
pod <span class="token string">&quot;requests-pod-2&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>注意: 这次指定 CPU 请求量为 1 核(cpu=1)而不是 1000 毫核(cpu=1000m).</p> <p>到目前为止一切顺利, pod 被 API 服务器接收(你一定记得前面章节提到当 pod 不合法时 API 服务器会拒绝该 pod 的创建请求)</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po requests<span class="token operator">-</span>pod<span class="token operator">-</span><span class="token number">3</span>
<span class="token constant">NAME</span>             <span class="token constant">READY</span>     <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
requests<span class="token operator">-</span>pod<span class="token operator">-</span><span class="token number">3</span>   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       Pending   <span class="token number">0</span>          4m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>尽管等了好一会, pod 依然<strong>卡在 Pending 状态</strong>, 可以通过 describe 命令查看一下出现这种情况的详细原因, 如以下代码清单所示.</p> <p><strong>代码清单-14.4 使用 kubectl describe pod 查看为什么 pod 卡在 Pending 状态</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe po requests-pod-3
Name:       requests-pod-3
Namespace:  default
Node:       /           <span class="token comment"># 没有与该pod关联的节点</span>
<span class="token punctuation">..</span>.
Conditions:
  Type           Status
  PodScheduled   False      <span class="token comment"># pod没有调度成功</span>
<span class="token punctuation">..</span>.
Events:
<span class="token punctuation">..</span>. Warning  FailedScheduling    No nodes are available      <span class="token comment"># CPU资源不足导致调度失败</span>
                                 that match all of the   
                                 following predicates::  
                                 Insufficient cpu <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>.     
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>从命令的输出可以看出单节点集群没有足够的 CPU, pod 不适合任何节点因此没有被成功调度. 但是为什么呢? 这里三个 pod 的 CPU requests 总和是 2000 毫核也就是 2 核, 这个节点正好可以提供, <strong>是哪里出了问题呢</strong>?</p> <blockquote><p>查明 pod 没有被调度成功的原因</p></blockquote> <p>可以通过检查节点资源找出为什么 pod 没有成功调度. 下面再次执行 kubectl describe node 命令并仔细地检查输出.</p> <p><strong>代码清单-14.5 使用 kubectl describe node 检查节点已分配资源</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl describe node
<span class="token literal-property property">Name</span><span class="token operator">:</span>                   minikube
<span class="token operator">...</span>
Non<span class="token operator">-</span>terminated Pods<span class="token operator">:</span>    <span class="token punctuation">(</span><span class="token number">7</span> <span class="token keyword">in</span> total<span class="token punctuation">)</span>
  Namespace    Name            <span class="token constant">CPU</span> Requ<span class="token punctuation">.</span>   <span class="token constant">CPU</span> Lim<span class="token punctuation">.</span>  Mem Req<span class="token punctuation">.</span>    Mem Lim<span class="token punctuation">.</span>
  <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span>    <span class="token operator">--</span><span class="token operator">--</span>            <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>  <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>  <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span>   <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>
  <span class="token keyword">default</span>      requests<span class="token operator">-</span>pod    <span class="token number">200</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token operator">%</span><span class="token punctuation">)</span>  <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">10</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>   <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>
  <span class="token keyword">default</span>      requests<span class="token operator">-</span>pod<span class="token operator">-</span><span class="token number">2</span>  <span class="token number">800</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">40</span><span class="token operator">%</span><span class="token punctuation">)</span>  <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">20</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">%</span><span class="token punctuation">)</span>   <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>
  kube<span class="token operator">-</span>system  dflt<span class="token operator">-</span>http<span class="token operator">-</span>b<span class="token operator">...</span>  <span class="token number">10</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">10</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>  <span class="token number">20</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">%</span><span class="token punctuation">)</span>   <span class="token number">20</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">%</span><span class="token punctuation">)</span>
  kube<span class="token operator">-</span>system  kube<span class="token operator">-</span>addon<span class="token operator">-</span><span class="token operator">...</span>  <span class="token number">5</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>     <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">50</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">%</span><span class="token punctuation">)</span>   <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>
  kube<span class="token operator">-</span>system  kube<span class="token operator">-</span>dns<span class="token operator">-</span><span class="token number">26.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token number">260</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">13</span><span class="token operator">%</span><span class="token punctuation">)</span>  <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">110</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token operator">%</span><span class="token punctuation">)</span>  <span class="token number">170</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token operator">%</span><span class="token punctuation">)</span>
  kube<span class="token operator">-</span>system  kubernetes<span class="token operator">-</span><span class="token operator">...</span>  <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>      <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>      <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>
  kube<span class="token operator">-</span>system  nginx<span class="token operator">-</span>ingre<span class="token operator">...</span>  <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>      <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>    <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>      <span class="token number">0</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>
Allocated resources<span class="token operator">:</span>
  <span class="token punctuation">(</span>Total limits may be over <span class="token number">100</span> percent<span class="token punctuation">,</span> i<span class="token punctuation">.</span>e<span class="token punctuation">.</span><span class="token punctuation">,</span> overcommitted<span class="token punctuation">.</span><span class="token punctuation">)</span>
  <span class="token constant">CPU</span> Requests  <span class="token constant">CPU</span> Limits      Memory Requests Memory Limits
  <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>  <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>      <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span> <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span>
  <span class="token number">1275</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">63</span><span class="token operator">%</span><span class="token punctuation">)</span>   <span class="token number">10</span><span class="token function">m</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">%</span><span class="token punctuation">)</span>        <span class="token number">210</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">11</span><span class="token operator">%</span><span class="token punctuation">)</span>     <span class="token number">190</span><span class="token function">Mi</span> <span class="token punctuation">(</span><span class="token number">9</span><span class="token operator">%</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>在列表的左下角可以看到<strong>共有 1275 毫核已经被运行的 pod 申请</strong>, 比先前部署的两个 pod 申请量多了 275 毫核. 看来有些东西吃掉了额外的 CPU 资源.</p> <p>可以在上面的列表中找到罪魁祸首. 在 kube-system 命名空间内<strong>有三个 pod 明确申请了 CPU</strong>. 这些 pod 加上自己创建的两个 pod, 只剩下 725 毫核可用. 第三个 pod 需要 1000 毫核, 调度器不会将其调度到这个节点上, 因为这将导致<mark><strong>节点超卖</strong></mark>.</p> <blockquote><p>释放资源让 pod 正常调度</p></blockquote> <p>只有当节点资源释放后(比如删除之前两个 pod 中的一个)pod 才会调度上来. 如果删除第二个 pod, 调度器将获取到删除通知(通过第 11 章介绍的监控机制), 并在第二个 pod 成功终止后立即调度第三个 pod. 这在下面的代码清单中可以看到.</p> <p><strong>代码清单-14.6 删除另一个 pod 后看到 pod 已正常调度</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code><span class="token comment"># 删除第二个pod</span>
$ kubectl delete po requests-pod-2
pod <span class="token string">&quot;requests-pod-2&quot;</span> deleted

$ kubectl get po
NAME             READY     STATUS        RESTARTS   AGE
requests-pod     <span class="token number">1</span>/1       Running       <span class="token number">0</span>          2h
requests-pod-2   <span class="token number">1</span>/1       Terminating   <span class="token number">0</span>          1h
requests-pod-3   <span class="token number">0</span>/1       Pending       <span class="token number">0</span>          1h

<span class="token comment"># 之前Pending的Pod3现在变成Running了</span>
$ kubectl get po
NAME             READY     STATUS    RESTARTS   AGE
requests-pod     <span class="token number">1</span>/1       Running   <span class="token number">0</span>          2h
requests-pod-3   <span class="token number">1</span>/1       Running   <span class="token number">0</span>          1h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>在以上所有例子中, 也指定了内存申请量, 不过它并没有对调度产生影响, 因为节点拥有足够多的内存来容纳所有 pod 的需求. 调度器处理 CPU 和内存 requests 的方式没有什么不同, 但与内存 requests 相反的是, pod 的 CPU requests 在其运行时也扮演着一个角色, 后面将在下文中了解这一点.</p> <h6 id="_14-1-3-cpu-requests如何影响cpu时间分配"><a href="#_14-1-3-cpu-requests如何影响cpu时间分配" class="header-anchor">#</a> 14.1.3 CPU requests如何影响CPU时间分配</h6> <p>现在有两个 pod 运行在集群中(暂且不理会那些系统 pod, 因为它们大部分时间都是空闲的). 一个请求了 200 毫核, 另一个是前者的 5 倍. 在本章开始时说到 K<strong>ubernetes 会将 requests 资源和 limits 资源区别对待</strong>. 现在还没有定义任何 limits, 因此每个 pod 分别可以消耗多少 CPU 并没有做任何限制. 那么假设每个 pod 内的进程都<strong>尽情消耗 CPU 时间, 每个 pod 最终能分到多少 CPU 时间呢</strong>?</p> <p><mark><strong>CPU requests 不仅仅在调度时起作用, 它还决定着剩余(未使用)的 CPU 时间如何在 pod 之间分配</strong></mark>. 正如图 14.2 描绘的那样, 因为第一个 pod 请求了 200 毫核, 另一个请求了 1000 毫核, 所以未使用的 CPU 将按照 1:5 的比例来划分给这两个 pod. 如果两个 pod 都全力使用 CPU, 第一个 pod 将获得 16.7% 的 CPU 时间, 另一个将获得 83.3% 的 CPU 时间.</p> <p><img src="/img/image-20240225170238-0dac3kh.png" alt="image" title="图14.2 未使用的 CPU 时间按照 CPU requests 在容器之间分配"></p> <p>另一方面, 如果一个容器能够跑满 CPU, 而另一个容器在该时段处于空闲状态, 那么前者将可以使用整个 CPU 时间(当然会减掉第二个容器消耗的少量时间). 毕竟当没有其他人使用时提高整个 CPU 的利用率也是有意义的, 对吧? 当然, 第二个容器需要 CPU 时间的时候就会获取到, 同时第一个容器会被限制回来.</p> <h6 id="_14-1-4-定义和申请自定义资源"><a href="#_14-1-4-定义和申请自定义资源" class="header-anchor">#</a> 14.1.4 定义和申请自定义资源</h6> <p><strong>Kubernetes 允许用户为节点添加属于自己的自定义资源, 同时支持在 pod 资源 requests 里申请这种资源</strong>. 因为目前是一个 alpha 特性, 所以不打算描述其细节, 而只会简短地介绍一下.</p> <p>首先, 需要通过将自定义资源加入节点 API 对象的 capacity 属性让 Kubernetes 知道它的存在. 这可以通过执行 HTTP 的 PATCH 请求来完成. 资源名称可以是不以 kubernetes.io 域名开头的任意值, 例如 <code>example.org/myresource</code>​, 数量必须是整数(例如不能设为 100m, 因为 0.1 不是整数; 但是可以设置为 1000m, 2000m, 或者简单地设为 1 和 2). 这个值将自动从 capacity 字段复制到 allocatable 字段.</p> <p>然后, 创建 pod 时, 只要简单地在容器 spec 的 resources.requests 字段下, 或者像之前例子那样使用带--requests 参数的 kubectl run 命令来指定自定义资源名称和申请量, 调度器就可以确保这个 pod 只能部署到满足自定义资源申请量的节点, 同时每个已部署的 pod 会减少节点的这类可分配资源数量.</p> <p>一个自定义资源的例子就是节点上可用的 GPU 单元数量. 如果 pod 需要使用 GPU, 只要简单指定其 requests, 调度器就会保证这个 pod 只能调度到至少拥有一个未分配 GPU 单元的节点上.</p> <h5 id="_14-2-限制容器的可用资源"><a href="#_14-2-限制容器的可用资源" class="header-anchor">#</a> 14.2 限制容器的可用资源</h5> <p><strong>设置 pod 的容器资源申请量保证了每个容器能够获得它所需要资源的最小量</strong>. 现在再看看硬币的另一面-<strong>容器可以消耗资源的最大量</strong>.</p> <h6 id="_14-2-1-设置容器可使用资源量的硬限制"><a href="#_14-2-1-设置容器可使用资源量的硬限制" class="header-anchor">#</a> 14.2.1 设置容器可使用资源量的硬限制</h6> <p>之前看到当其他进程处于空闲状态时容器可以被允许使用所有 CPU 资源. 但是你可能想防止一些容器使用超过指定数量的 CPU, 而且经常会希望限制容器的可消耗内存数量.</p> <p><strong>CPU 是一种可压缩资源</strong>, 意味着可以在不对容器内运行的进程产生不利影响的同时, 对其使用量进行限制. 而<strong>内存是一种不可压缩资源</strong>. 一旦系统为进程分配了一块内存, 这块内存在进程主动释放之前将无法被回收. 这就是为什么需要<strong>限制容器的最大内存分配量</strong>的根本原因.</p> <p>如果不对内存进行限制, 工作节点上的容器(或者 pod)可能会吃掉所有可用内存, 会对该节点上所有其他 pod 和任何新调度上来的 pod(记住新调度的 pod 是基于内存的申请量而不是实际使用量的)造成影响. 单个故障 pod 或恶意 pod 几乎可以导致整个节点不可用.</p> <blockquote><p>创建一个带有资源 limits 的 pod</p></blockquote> <p>为了防止这种情况发生, Kubernetes <strong>允许用户为每个容器指定资源 limits(与设置资源 requests 几乎相同)</strong> . 以下代码清单展示了一个包含资源 limits 的 pod 描述文件示例.</p> <p><strong>代码清单-14.7 一个包含 CPU 和内存硬限制的 pod:limited-pod.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> limited<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;dd&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;if=/dev/zero&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;of=/dev/null&quot;</span><span class="token punctuation">]</span>
    <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>             <span class="token comment"># 为容器指定资源limits</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>          
        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token number">1</span>             <span class="token comment"># 允许最大使用1核CPU</span>
        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 20Mi       <span class="token comment"># 允许最大使用20M内存</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>这个 pod 的容器<strong>包含了 CPU 和内存资源 limits 配置</strong>. 容器内的进程不允许消耗超过 1 核 CPU 和 20 MB 内存.</p> <p>注意: 因为没<strong>有指定资源 requests, 它将被设置为与资源 limits 相同的值</strong>.</p> <blockquote><p>可超卖的 limits</p></blockquote> <p>与资源 requests 不同的是, <strong>资源 limits 不受节点可分配资源量的约束</strong>. 所有 limits 的总和<mark><strong>允许超过</strong></mark>节点资源总量的 100%(见图 14.3). 换句话说, <mark><strong>资源 limits 可以超卖. 如果节点资源使用量超过 100%, 一些容器将被杀掉, 这是一个很重要的结果</strong></mark>.</p> <p><img src="/img/image-20240225170418-1zvv4bo.png" alt="image" title="图14.3 节点上所有 pod 的资源 limits 之和可以超过节点资源总量的 100%"></p> <p>在 14.3 节, 可以将看到 Kubernetes 如何决定杀掉哪些容器, 不过只要单个容器尝试使用比自己指定的 limits 更多的资源时也可能会被杀掉. 接下来会了解更多.</p> <h6 id="_14-2-2-超过limits"><a href="#_14-2-2-超过limits" class="header-anchor">#</a> 14.2.2 超过limits</h6> <p>当容器内运行的进程<strong>尝试使用比限额更多的资源时会发生什么</strong>呢?</p> <p>前面已经了解了 CPU 是可压缩资源, 当进程不等待 IO 操作时消耗所有的 CPU 时间是非常常见的. 正如所知道的, 对一个进程的 CPU 使用率可以进行限制, 因此当为一个容器设置 CPU 限额时, 该进程只会分不到比限额更多的 CPU 而已.</p> <p>而内存却有所不同. <strong>当进程尝试申请分配比限额更多的内存时会被杀掉(会说这个容器被 OOMKilled 了, OOM 是 Out Of Memory 的缩写). 如果 pod 的重启策略为 Always 或 OnFailure, 进程将会立即重启, 因此用户可能根本察觉不到它被杀掉. 但是如果它继续超限并被杀死, Kubernetes 会再次尝试重启, 并开始增加下次重启的间隔时间. 这种情况下用户会看到 pod 处于 CrashLoopBackOff 状</strong>态:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po
<span class="token constant">NAME</span>        <span class="token constant">READY</span>     <span class="token constant">STATUS</span>             <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>
memoryhog   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>       CrashLoopBackOff   <span class="token number">3</span>          1m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>CrashLoopBackOff 状态表示 Kubelet 还没有放弃, 它意味着在每次崩溃之后, Kubelet 就会增加下次重启之前的<strong>间隔时间</strong>. 第一次崩溃之后, Kubelet 立即重启容器, 如果容器再次崩溃, Kubelet 会等待 10 秒钟后再重启. 随着不断崩溃, 延迟时间也会按照 20, 40, 80, 160 秒以几何倍数增长, 最终收敛在 300 秒. 一旦间隔时间达到 300 秒, Kubelet 将以 5 分钟为间隔时间对容器进行无限重启, 直到容器正常运行或被删除.</p> <p>要定位容器 crash 的原因, 可以通过查看 pod 日志以及 kubectl describe pod 命令:</p> <p><strong>代码清单-14.8 通过 kubectl describe pod 查看容器终止的原因</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code>$ kubectl describe pod
<span class="token key atrule">Name</span><span class="token punctuation">:</span>       memoryhog
<span class="token punctuation">...</span>
<span class="token key atrule">Containers</span><span class="token punctuation">:</span>
  <span class="token key atrule">main</span><span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
    <span class="token key atrule">State</span><span class="token punctuation">:</span>          Terminated         <span class="token comment"># 当前容器因为OOM被杀死            </span>
      <span class="token key atrule">Reason</span><span class="token punctuation">:</span>       OOMKilled          <span class="token comment"># 终止原因是OOMKilled     </span>
      <span class="token key atrule">Exit Code</span><span class="token punctuation">:</span>    <span class="token number">137</span>
      <span class="token key atrule">Started</span><span class="token punctuation">:</span>      Tue<span class="token punctuation">,</span> 27 Dec 2016 14<span class="token punctuation">:</span>55<span class="token punctuation">:</span>53 +0100
      <span class="token key atrule">Finished</span><span class="token punctuation">:</span>     Tue<span class="token punctuation">,</span> 27 Dec 2016 14<span class="token punctuation">:</span>55<span class="token punctuation">:</span>58 +0100
    <span class="token key atrule">Last State</span><span class="token punctuation">:</span>     Terminated          <span class="token comment"># 上一个容器同样因为OOM被杀死</span>
      <span class="token key atrule">Reason</span><span class="token punctuation">:</span>       OOMKilled           <span class="token comment"># 终止原因是OOMKilled          </span>
      <span class="token key atrule">Exit Code</span><span class="token punctuation">:</span>    <span class="token number">137</span>
      <span class="token key atrule">Started</span><span class="token punctuation">:</span>      Tue<span class="token punctuation">,</span> 27 Dec 2016 14<span class="token punctuation">:</span>55<span class="token punctuation">:</span>37 +0100
      <span class="token key atrule">Finished</span><span class="token punctuation">:</span>     Tue<span class="token punctuation">,</span> 27 Dec 2016 14<span class="token punctuation">:</span>55<span class="token punctuation">:</span>50 +0100
    <span class="token key atrule">Ready</span><span class="token punctuation">:</span>          <span class="token boolean important">False</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>OOMKilled 状态告诉我们容器因为内存不足而被系统杀掉了. 上例中, 容器实际上已经超过了内存限额而被立即杀死. 因此, 如果你不希望容器被杀掉, 重要的一点就是<strong>不要将内存 limits 设置得很低</strong>. 而容器有时即使没有超限也依然会被 OOMKilled. 这个将在 14.3.2 节说明原因, 下面来讨论一下在大多数用户<strong>首次指定 limits 时需要注意的地方</strong>.</p> <h6 id="_14-2-3-容器中的应用如何看待limits"><a href="#_14-2-3-容器中的应用如何看待limits" class="header-anchor">#</a> 14.2.3 容器中的应用如何看待limits</h6> <p>如果你还没有部署代码清单 14.7 中的 pod, 请先部署:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> limited-pod.yaml
pod <span class="token string">&quot;limited-pod&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在在容器内运行 top 命令, 如本章开始时那样. 命令的输出显示在下面的代码清单中.</p> <p><strong>代码清单-14.9 在有 CPU/内存限制的容器内运行 top 命令</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> limited-pod <span class="token function">top</span>
Mem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached
CPU: <span class="token number">10.0</span>% usr <span class="token number">40.0</span>% sys  <span class="token number">0.0</span>% nic <span class="token number">50.0</span>% idle  <span class="token number">0.0</span>% io  <span class="token number">0.0</span>% irq  <span class="token number">0.0</span>% sirq
Load average: <span class="token number">0.17</span> <span class="token number">1.19</span> <span class="token number">2.47</span> <span class="token number">4</span>/503 <span class="token number">10</span>
  PID  <span class="token environment constant">PPID</span> <span class="token environment constant">USER</span>     STAT   VSZ %VSZ CPU %CPU COMMAND
    <span class="token number">1</span>     <span class="token number">0</span> root     R     <span class="token number">1192</span>  <span class="token number">0.0</span>   <span class="token number">1</span> <span class="token number">49.9</span> <span class="token function">dd</span> <span class="token keyword">if</span> /dev/zero of /dev/null
    <span class="token number">5</span>     <span class="token number">0</span> root     R     <span class="token number">1196</span>  <span class="token number">0.0</span>   <span class="token number">0</span>  <span class="token number">0.0</span> <span class="token function">top</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>首先提醒一下, 这个 pod 的 CPU 限额是 500 毫核, 内存限额是 20 MiB. 现在仔细审视一下 top 命令的输出. 看看有什么奇怪之处吗?</p> <p><strong>查看 used 和 free 内存量, 这些数值远超出为容器设置的 20 MiB 限额</strong>. 同样地, 这里设置了 CPU 限额为 1  核, 即使使用的 dd 命令通常会消耗所有可用的 CPU 资源, 但主进程似乎只用到了 50%. 所以究竟发生了什么呢?</p> <blockquote><p>在容器内看到的始终是节点的内存, 而不是容器本身的内存</p></blockquote> <p>即使为容器设置了最大可用内存的限额, <mark><strong>top 命令显示的是运行该容器的节点的内存数量, 而容器无法感知到此限制!!!</strong></mark>  这对任何通过查看系统剩余可用内存数量, 并用这些信息来决定自己该使用多少内存的应用来说具有非常不利的影响.</p> <p>对于 Java 程序来说这是个很大的问题, 尤其是不使用 -Xmx 选项指定虚拟机的最大堆大小时, JVM 会将其设置为主机总物理内存的百分值. 在 Kubernetes 开发集群运行 Java 容器化应用(比如在笔记本电脑上运行)时, 因为内存 limits 和笔记本电脑总内存差距不是很大, 这个问题还不太明显. 但是如果 pod 部署在拥有更大物理内存的生产系统中, JVM 将迅速超过预先配置的内存限额, 然后被 OOM 杀死.</p> <p>也许你觉得可以简单地设置 -Xmx 选项就可以解决这个问题, 那么你就错了, 很遗憾. -Xmx 选项仅仅限制了堆大小, 并不管其他 off-heap 内存. 好在新版本的 Java 会考虑到容器 limits 以缓解这个问题.</p> <blockquote><p>容器内同样可以看到节点所有的 CPU 核</p></blockquote> <p>与内存完全一样, 无论有没有配置 CPU limits, <mark><strong>容器内也会看到节点所有的 CPU</strong></mark>. 将 CPU 限额配置为 1, 并不会神奇地只为容器暴露一个核. <strong>CPU limits 做的只是限制容器使用的 CPU 时间</strong>.</p> <p><strong>因此如果一个拥有 1 核 CPU 限额的容器运行在 64 核 CPU 上, 只能获得 1/64 的全部 CPU 时间. 而且即使限额设置为 1 核, 容器进程也不会只运行在一个核上, 不同时刻, 代码还是会在多个核上执行</strong>.</p> <p>上面的描述没什么问题, 对吧? 虽然一般情况下如此, 但在一些情况下却是灾难. 比如一些程序通过<strong>查询系统 CPU 核数来决定启动工作线程的数量</strong>. 同样在开发环境的笔记本电脑上运行良好, 但是部署在拥有更多数量 CPU 的节点上, 程序将快速启动大量线程, 所有线程都会争夺(可能极其)有限的 CPU 时间. 同时每个线程通常都需要额外的内存资源, 导致应用的内存用量急剧增加.</p> <p>不要依赖应用程序从系统获取的 CPU 数量, 你可能需要<strong>使用 Downward API 将 CPU 限额传递至容器并使用这个值</strong>. 也可以<strong>通过 cgroup 系统直接获取配置的 CPU 限制</strong>, 请查看下面的文件:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>/sys/fs/cgroup/cpu/cpu.cfs_quota_us
/sys/fs/cgroup/cpu/cpu.cfs_period_us
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h5 id="_14-3-了解pod-qos等级"><a href="#_14-3-了解pod-qos等级" class="header-anchor">#</a> 14.3 了解pod QoS等级</h5> <p>前面已经提到资源 limits 可以超卖, 换句话说, <strong>一个节点不一定能提供所有 pod 所指定的资源 limits 之和那么多的资源量</strong>.</p> <p>假设有两个 pod, pod A 使用了节点内存的 90%, pod B 突然需要比之前更多的内存, 这时节点无法提供足量内存, <mark><strong>哪个容器将被杀掉呢</strong></mark>? 应该是 pod B 吗? 因为节点无法满足它的内存请求. 或者应该是 pod A 吗? 这样释放的内存就可以提供给 pod B 了.</p> <p>显然, 这要分情况讨论. <strong>Kubernetes 无法自己做出正确决策, 因此就需要一种方式, 通过这种方式可以指定哪种 pod 在该场景中优先级更高. Kubernetes 将 pod 划分为 3 种 QoS 等级</strong>:</p> <ul><li>BestEffort(优先级最低)</li> <li>Burstable</li> <li><strong>Guaranteed</strong>(优先级最高)</li></ul> <h6 id="_14-3-1-定义pod的qos等级"><a href="#_14-3-1-定义pod的qos等级" class="header-anchor">#</a> 14.3.1 定义pod的QoS等级</h6> <p>你也许希望这个等级会通过一个独立的字段分配, 但并非如此. <mark><strong>QoS 等级来源于 pod 所包含的容器的资源 requests 和 limits 的配置</strong></mark>​. 下面介绍分配 QoS 等级的方法.</p> <blockquote><p>为 pod 分配 BestEffort 等级</p></blockquote> <p><strong>最低优先级的 QoS 等级是 BestEffort. 会分配给那些没有(为任何容器)设置任何 requests 和 limits 的 pod</strong>. 前面章节创建的 pod 都是这个等级. 在这个等级运行的容器<strong>没有任何资源保证</strong>. 在最坏情况下, 它们分不到任何 CPU 时间, 同时在需要为其他 pod 释放内存时, 这些容器<strong>会第一批被杀死</strong>. 不过因为 BestEffort pod 没有配置内存 limits, 当有充足的可用内存时, 这些容器可以使用任意多的内存.</p> <blockquote><p>为 pod 分配 Guaranteed 等级</p></blockquote> <p><strong>与 Burstable 相对的是 Guaranteed 等级, 会分配给那些所有资源 request 和 limits 相等的 pod</strong>. 对于一个 Guaranteed 级别的 pod, 有以下几个条件:</p> <ul><li><strong>CPU 和内存都要设置 requests 和 limits</strong></li> <li>每个容器都需要设置资源量</li> <li><strong>它们</strong>​<mark><strong>必须相等</strong></mark>​ <strong>(每个容器的每种资源的 requests 和 limits 必须相等)</strong></li></ul> <p>因为如果容器的资源 requests 没有显式设置, 默认与 limits 相同, 所以只设置所有资源(pod 内每个容器的每种资源)的限制量就可以使 pod 的 QoS 等级为 Guaranteed. 这些 pod 的容器可以使用它所申请的等额资源, 但是无法消耗更多的资源(因为它们的 limits 和 requests 相等).</p> <blockquote><p>为 pod 分配 Burstable 等级</p></blockquote> <p><strong>Burstable QoS 等级介于 BestEffort 和 Guaranteed 之间. 其他所有的 pod 都属于这个等级</strong>. 包括容器的 requests 和 limits 不相同的单容器 pod, 至少有一个容器只定义了 requests 但没有定义 limits 的 pod, 以及一个容器的 requests 和 limits 相等, 但是另一个容器不指定 requests 或 limits 的 pod. Burstable pod 可以获得它们所申请的等额资源, 并可以使用额外的资源(不超过 limits).</p> <blockquote><p>requests 和 limits 之间的关系如何定义 QoS 等级</p></blockquote> <p>图 14.4 列举了 3个 QoS 等级和它们与 requests 和 limits 之间的关系.</p> <p><img src="/img/image-20240225193217-e3i27l3.png" alt="image" title="图14.4 资源的 requests, limits 和 QoS 等级"></p> <p>考虑一个 pod 应该属于哪个 QoS 等级足以令人脑袋快速运转, 因为它涉及多个容器, 多种资源, 以及 requests 和 limits 之间所有可能的关系. 如果一开始从容器级别考虑 QoS(尽管它并不是容器的属性, 而是 pod 的属性), 然后从容器 QoS 推导出 pod QoS, 这样可能更容易理解.</p> <blockquote><p>明白容器的 QOS 等级</p></blockquote> <p><strong>表 14.1 显示了基于资源 requests 和 limits 如何为单个容器定义 QoS 等级</strong>. 对于单容器 pod, 容器的 QoS 等级也适用于 pod.</p> <p><strong>表-14.1 基于资源请求量和限制量的单容器 pod 的 QoS 等级</strong></p> <p><img src="/img/image-20240225193325-wg4op2a.png" alt="image"></p> <p>注意: 如果设置了 requests 而没有设置 limits, 参考表中 requests 小于 limits 那一行. 如果设置了 limits, requests 默认与 limits 相等, 因此参考 request 等于 limits 那一行.</p> <blockquote><p>了解多容器 pod 的 QoS 等级</p></blockquote> <p><strong>对于多容器 pod, 如果所有的容器的 QoS 等级相同, 那么这个等级就是 pod 的 QoS 等级. 如果至少有一个容器的 QoS 等级与其他不同, 无论这个容器是什么等级, 这个 pod 的 QoS 等级都是 Burstable 等级</strong>. 表 14.2 展示了 pod 的 QoS 等级与其中两个容器的 QoS 等级之间的对应关系. 多容器 pod 可以对此进行简单扩展.</p> <p><strong>表-14.2 由容器的 QoS 等级推导出 pod 的 QoS 等级</strong></p> <table><thead><tr><th style="text-align:center;">容器1的QoS等级</th> <th style="text-align:center;">容器2的QoS等级</th> <th style="text-align:center;">pod的QoS等级</th></tr></thead> <tbody><tr><td style="text-align:center;">BestEffort</td> <td style="text-align:center;">BestEffort</td> <td style="text-align:center;">BestEffort</td></tr> <tr><td style="text-align:center;">BestEffort</td> <td style="text-align:center;">Burstable</td> <td style="text-align:center;">Burstable</td></tr> <tr><td style="text-align:center;">BestEffort</td> <td style="text-align:center;">Guaranteed</td> <td style="text-align:center;">Burstable</td></tr> <tr><td style="text-align:center;">Burstable</td> <td style="text-align:center;">Burstable</td> <td style="text-align:center;">Burstable</td></tr> <tr><td style="text-align:center;">Burstable</td> <td style="text-align:center;">Guaranteed</td> <td style="text-align:center;">Burstable</td></tr> <tr><td style="text-align:center;">Guaranteed</td> <td style="text-align:center;">Guaranteed</td> <td style="text-align:center;">Guaranteed</td></tr></tbody></table> <p>注意: <strong>运行 kubectl describe pod 以及通过 pod 的 YAML/JSON 描述的 status.qosClass 字段都可以查看 pod 的 QoS 等级</strong>.</p> <p>前面解释了如何划分 QoS 等级, 但是依然需要<strong>了解在一个超卖的系统中如何确定哪个容器先被杀掉</strong>.</p> <h6 id="_14-3-2-内存不足时哪个进程会被杀死"><a href="#_14-3-2-内存不足时哪个进程会被杀死" class="header-anchor">#</a> 14.3.2 内存不足时哪个进程会被杀死</h6> <p>在一个超卖的系统, <strong>QoS 等级决定着哪个容器第一个被杀掉, 这样释放出的资源可以提供给高优先级的 pod 使用</strong>. <mark><strong>BestEffort 等级的 pod 首先被杀掉, 其次是 Burstable pod, 最后是 Guaranteed pod. Guaranteed pod 只有在系统进程需要内存时才会被杀掉</strong></mark>.</p> <blockquote><p>了解 QoS 等级的优先顺序</p></blockquote> <p>请看图 14.5 中的例子. 假设<strong>两个单容器的 pod</strong>, 第一个属于 BestEffort QoS 等级, 第二个属于 Burstable 等级. 当节点的全部内存已经用完, 还有进程尝试申请更多的内存时, 系统必须杀死其中一个进程(甚至包括尝试申请额外内存的进程)以兑现内存分配请求. 这种情况下, BestEffort 等级运行的进程会在 Burstable 等级的进程之前被系统杀掉.</p> <p><img src="/img/image-20240228092414-2ttiy56.png" alt="image" title="图14.5 哪个 pod 会第一个被杀掉"></p> <p>显然, BestEffort pod 的进程会在 Guaranteed pod 的进程之前被杀掉. 同样地, Burstable pod 的进程也先于 Guaranteed pod 的进程被杀掉. 但如果只有两个 Burstable pod 会发生什么呢? 很明显需要选择一个优先于另一个的进程.</p> <blockquote><p>如何处理相同 QoS 等级的容器</p></blockquote> <p><strong>每个运行中的进程都有一个称为 OutOfMemory(OOM)分数的值. 系统通过比较所有运行进程的 OOM 分数来选择要杀掉的进程. 当需要释放内存时, 分数最高的进程将被杀死</strong>.</p> <p>OOM 分数由两个参数计算得出: <strong>进程已消耗内存占可用内存的百分比, 与一个基于 pod QoS 等级和容器内存申请量固定的 OOM 分数调节因子</strong>. 对于两个属于 Burstable 等级的单容器的 pod, 系统会杀掉内存实际使用量占内存申请量比例更高的 pod. 这就是图 14.5 中使用了内存申请量 90% 的 pod B 在 pod C(只使用了 70%)之前被杀掉的原因, 尽管 pod C 比 pod B 使用了更多兆字节的内存.</p> <p>这说明不仅要注意 requets 和 limits 之间的关系, 还要<strong>留心 requests 和预期实际消耗内存</strong>之间的关系.</p> <h5 id="_14-4-为命名空间中的pod设置默认的requests和limits"><a href="#_14-4-为命名空间中的pod设置默认的requests和limits" class="header-anchor">#</a> 14.4 为命名空间中的pod设置默认的requests和limits</h5> <p>前面已经了解到如何为单个容器设置资源 requests 和 limits. 如果不做限制, 这个容器将处于其他所有设置了 requests 和 limits 的容器的控制之下. 换句话说, <strong>为每个容器设置 requests 和 limits 是一个很好的实践</strong>.</p> <h6 id="_14-4-1-limitrange资源简介"><a href="#_14-4-1-limitrange资源简介" class="header-anchor">#</a> 14.4.1 LimitRange资源简介</h6> <p>用户可以通过<strong>创建一个 LimitRange 资源来避免必须配置每个容器</strong>. LimitRange 资源不仅允许用户(为每个命名空间)指定能给容器配置的每种资源的最小和最大限额, 还支持在没有显式指定资源 requests 时为容器设置默认值, 如图 14.6 所示.</p> <p><img src="/img/image-20240228092504-fszgfgs.png" alt="image" title="图14.6 LimitRange 用于 pod 的资源校验和设置默认值"></p> <p><strong>LimitRange 资源被 LimitRanger 准入控制插件所使用</strong>(第 11 章介绍过这种插件). <strong>API 服务器接收到带有 pod 描述信息的 POST 请求时, LimitRanger 插件对 pod spec 进行校验. 如果校验失败, 将直接拒绝</strong>. 因此, LimitRange 对象的一个广泛应用场景就是<strong>阻止</strong>用户创建大于单个节点资源量的 pod. 如果没有 LimitRange, API 服务器将欣然接收 pod 创建请求, 但永远无法调度成功.</p> <p><strong>LimitRange 资源中的 limits 应用于同一个命名空间中每个独立的 pod, 容器, 或者其他类型的对象. 它并不会限制这个命名空间中所有 pod 可用资源的总量, 总量是通过 ResourceQuota 对象指定的</strong>, 这将在 14.5 节中进行说明.</p> <h6 id="_14-4-2-limitrange对象的创建"><a href="#_14-4-2-limitrange对象的创建" class="header-anchor">#</a> 14.4.2 LimitRange对象的创建</h6> <p>先看一下 LimitRange 的全貌, 然后单独解释每个属性的作用. 下面的代码清单展示了一个 LimitRange 资源的完整定义.</p> <p><strong>代码清单-14.10 LimitRange 资源: limits.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> LimitRange     <span class="token comment"># 资源类型为LimitRange</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> example
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">limits</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Pod        <span class="token comment"># 指定整个pod的资源limits</span>
    <span class="token key atrule">min</span><span class="token punctuation">:</span>             <span class="token comment"># pod中所有容器的CPU和内存的请求量之和的最小值</span>
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 50m                   
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 5Mi                
    <span class="token key atrule">max</span><span class="token punctuation">:</span>             <span class="token comment"># pod中所有容器的CPU和内存的请求量之和的最大值                3</span>
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token number">1</span>                     
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi                
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Container  <span class="token comment"># 指定容器的资源限制</span>
    <span class="token key atrule">defaultRequest</span><span class="token punctuation">:</span>  <span class="token comment"># 容器没有指定CPU或内存请求量时设置的默认值</span>
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m                  
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 10Mi               
    <span class="token key atrule">default</span><span class="token punctuation">:</span>         <span class="token comment"># 没有指定limits时设置的默认值</span>
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 200m                  
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 100Mi              
    <span class="token key atrule">min</span><span class="token punctuation">:</span>             <span class="token comment"># 容器的CPU和内存的资源requests和limits的最小值和最大值</span>
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 50m                   
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 5Mi                
    <span class="token key atrule">max</span><span class="token punctuation">:</span>                         
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token number">1</span>                     
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi                
    <span class="token key atrule">maxLimitRequestRatio</span><span class="token punctuation">:</span>    <span class="token comment"># 各种资源requests和limits的最大比值</span>
      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token number">4</span>                     
      <span class="token key atrule">memory</span><span class="token punctuation">:</span> <span class="token number">10</span>                 
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> PersistentVolumeClaim    <span class="token comment"># LimitRange还可以指定请求PVC存储容量的最小值和最大值</span>
    <span class="token key atrule">min</span><span class="token punctuation">:</span>                         
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi               
    <span class="token key atrule">max</span><span class="token punctuation">:</span>                         
      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 10Gi                
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br></div></div><p>正如在上面例子中看到的, <strong>整个 pod 资源限制的最小值和最大值是可以配置的. 它应用于 pod 内所有容器的 requests 和 limits 之和</strong>.</p> <p>在更低一层的容器级别, 用户不仅可以设置最小值和最大值, 还可以<strong>为没有显式指定的容器设置资源 requests(defaultRequest)和 limits(default)的默认值</strong>.</p> <p>除了最小值, 最大值和默认值, 用户甚至可以<strong>设置 limits 和 requests 的最大比例</strong>. 上面示例中设置了 maxLimitRequestRatio 为 4, 表示容器的 CPU limits 不能超过 CPU requests 的 4 倍. 因此, 对于一个申请了 200 毫核的容器, 如果它的 CPU 限额设置为 801 毫核或者更大就无法创建. 而对于内存, 这个比例设为了 10.</p> <p>在第 6 章介绍了 PersistentVolumeClaim(PVC), 正如在 pod 中为容器声明 CPU 和内存一样, 用户也可以声明指定大小的持久化存储.</p> <p>这个例子只使用一个 LimitRange 对象, 其中包含了对所有资源的限制, 而如果希望按照类型进行组织, 也可以将其<strong>分割为多个对象</strong>(例如一个用于 pod 限制, 一个用于容器限制, 一个用于 PVC 限制). <strong>多个 LimitRange 对象的限制会在校验 pod 或 PVC 合法性时进行合并</strong>.</p> <p><strong>由于 LimitRange 对象中配置的校验(和默认值)信息在 API 服务器接收到新的 pod 或 PVC 创建请求时执行, 如果之后修改了限制, 已经存在的 pod 和 PVC 将不会再次进行校验, 新的限制只会应用于之后创建的 pod 和 PVC</strong>.</p> <h6 id="_14-4-3-强制进行限制"><a href="#_14-4-3-强制进行限制" class="header-anchor">#</a> 14.4.3 强制进行限制</h6> <p>在设置了限制的情况下, 下面尝试创建一个 CPU 申请量大于 LimitRange 允许值的 pod. 可以在代码库中找到 pod 的 YAML. 下面的代码清单仅展示与本节讨论相关的部分.</p> <p><strong>代码清单-14.11 一个 CPU requests 超过限制的 pod:limits-pod-too-big.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">resources</span><span class="token punctuation">:</span>
  <span class="token key atrule">requests</span><span class="token punctuation">:</span>
    <span class="token key atrule">cpu</span><span class="token punctuation">:</span> <span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这个 pod 的容器需要 2 核 CPU, 大于之前 LimitRange 中设置的最大值. 创建 pod 时会返回以下结果:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> limits-pod-too-big.yaml
Error from server <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span>: error when creating <span class="token string">&quot;limits-pod-too-big.yaml&quot;</span><span class="token builtin class-name">:</span> pods <span class="token string">&quot;too-big&quot;</span> is forbidden: <span class="token punctuation">[</span>
  maximum cpu usage per Pod is <span class="token number">1</span>, but request is <span class="token number">2</span>.,
  maximum cpu usage per Container is <span class="token number">1</span>, but request is <span class="token number">2</span>.<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>为了看起来更清晰, 这里对输出结果稍微做了修改. 服务器返回错误信息的好处是它列出了这个 pod 被拒绝的所有原因, 而不仅仅是第一个错误. 正如从结果看到的, pod 被拒绝的原因有两个: 为容器申请了 2 核的 CPU, 但是容器的最大 CPU 请求量限制为 1 核, 在 Pod 级别也是同样的原因, pod 整体可以请求 2 核的 CPU, 但是允许申请的最大值是 1 核(如果这是一个多容器 pod, 即使每个单独容器的请求量少于最大 CPU 请求量, 所有容器请求量的总和仍然需要少于 2 核才能符合最大 CPU 请求量的限制).</p> <h6 id="_14-4-4-应用资源requests和limits的默认值"><a href="#_14-4-4-应用资源requests和limits的默认值" class="header-anchor">#</a> 14.4.4 应用资源requests和limits的默认值</h6> <p>现在再看看如果<strong>不指定资源 requests 和 limits</strong>, Kubernetes 如何为其设置默认值, 再次部署第 3 章中名叫 kubia-manual 的 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl create <span class="token parameter variable">-f</span> <span class="token punctuation">..</span>/Chapter03/kubia-manual.yaml
pod <span class="token string">&quot;kubia-manual&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在设置 LimitRange 对象之前, 所有 pod 创建后都不包含资源 requests 或 limits, 但现在通过 describe 确认一下刚刚创建的 kubia-manual pod:</p> <p><strong>代码清单-14.12 检查自动应用于 pod 的 limits</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe po kubia-manual
Name:           kubia-manual
<span class="token punctuation">..</span>.
Containers:
  kubia:
    Limits:
      cpu:      200m
      memory:   100Mi
    Requests:
      cpu:      100m
      memory:   10Mi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>容器的 requests 和 limits 与在 LimitRange 对象中设置的一致. <strong>如果在另一个命名空间中指定不同的 LimitRange, 那么这个命名空间中创建的 pod 就会拥有不同的 requests 和 limits</strong>. 这样管理员就可以为每个命名空间的 pod 配置资源的默认值, 最小值和最大值. 如果使用命名空间来区分不同团队, 或是区分开发, 测试, 交付准备, 以及生产环境的 pod 都运行在相同的 Kubernetes 集群中, 那么在每个命名空间中定义不同的 LimitRange 就可以确保只在特定的命名空间中可以创建资源需求大的 pod, 而在另一些命名空间中只能创建资源需求小的 pod.</p> <p>但需要记住的是, <strong>LimitRange 中配置的 limits 只能应用于单独的 pod 或容器</strong>. 用户仍然可以创建大量的 pod 吃掉集群所有可用资源. LimitRange 并不能防止这个问题, 而相反, 后面介绍的 ResourceQuota 对象可以做到这点.</p> <h5 id="_14-5-限制命名空间中的可用资源总量"><a href="#_14-5-限制命名空间中的可用资源总量" class="header-anchor">#</a> 14.5 限制命名空间中的可用资源总量</h5> <p><strong>LimitRange 只应用于单独的 pod, 而我们同时也需要一种手段可以限制命名空间中的可用资源总量. 这通过创建一个 ResourceQuota 对象来实现</strong>.</p> <h6 id="_14-5-1-resourcequota资源介绍"><a href="#_14-5-1-resourcequota资源介绍" class="header-anchor">#</a> 14.5.1 ResourceQuota资源介绍</h6> <p>在第 10 章中讨论了几种<strong>运行在 API 服务器中, 可以判断一个 pod 是否允许创建的接纳控制插件</strong>. 在上一节, 讲到 LimitRanger 插件会强制执行 LimitRange 资源中配置的策略. 类似地, <strong>ResourceQuota 的接纳控制插件会检查将要创建的 pod 是否会引起总资源量超出 ResourceQuota. 如果那样, 创建请求会被拒绝. 因为资源配额在 pod 创建时进行检查, 所以 ResourceQuota 对象仅仅作用于在其后创建的 pod, 这并不影响已经存在的 pod</strong>.</p> <p>**资源配额限制了一个命名空间中 pod 和 PVC 存储最多可以使用的资源总量. 同时也可以限制用户允许在该命名空间中创建 pod, PVC, 以及其他 API 对象的数量. ** 因为到目前为止处理最多的资源是 CPU 和内存, 下面就来看看如何为这两种资源指定配额.</p> <blockquote><p>为 CPU 和内存创建 ResourceQuota</p></blockquote> <p>限制命名空间中所有 pod 允许使用的 CPU 和内存<strong>总量</strong>可以通过<strong>创建 ResourceQuota 对象</strong>来实现, 请看下面的代码清单.</p> <p><strong>代码清单-14.13 CPU 和内存的 ResourceQuota 资源: memory: quota-cpumemory.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ResourceQuota    <span class="token comment"># 资源类型为ResourceQuota</span>
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> cpu<span class="token punctuation">-</span>and<span class="token punctuation">-</span>mem
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hard</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests.cpu</span><span class="token punctuation">:</span> 400m
    <span class="token key atrule">requests.memory</span><span class="token punctuation">:</span> 200Mi
    <span class="token key atrule">limits.cpu</span><span class="token punctuation">:</span> 600m
    <span class="token key atrule">limits.memory</span><span class="token punctuation">:</span> 500Mi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这里<strong>为 CPU 和内存分别定义了 requests 和 limits 总量</strong>, 而不是简单地为每种资源只定义一个总量. 你可能会注意到与 LimitRange 对比, 结构有一些不同. 这里所有资源的 requests 和 limits 都定义在一个字段下.</p> <p>这个 ResourceQuota 设置了命名空间中所有 pod 最多可申请的 CPU 数量为 400 毫核, limits 最大总量为600 毫核. 对于内存, 设置所有 requests 最大总量为 200MiB, limits 为 500MiB.</p> <p>与 LimitRange 一样, <strong>ResourceQuota 对象应用于它所创建的那个命名空间</strong>, 但不同的是, 后者可以限制所有 pod 资源 requests 和 limits 的总量, 而不是每个单独的 pod 或者容器, 如图 14.7 所示.</p> <p><img src="/img/image-20240225220537-kaylm32.png" alt="image" title="图14.7 LimitRange 应用于单独的 pod;ResourceQuota 应用于命名空间中所有的 pod"></p> <blockquote><p>查看配额和配额使用情况</p></blockquote> <p>将 ResourceQuota 对象提交至 API 服务器之后, 可以执行 kubectl describe 命令查看当前配额已经使用了多少, 如以下代码清单所示.</p> <p><strong>代码清单-14.14 使用 kubectl describe quota 查看配额</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl describe quota
<span class="token literal-property property">Name</span><span class="token operator">:</span>           cpu<span class="token operator">-</span>and<span class="token operator">-</span>mem
<span class="token literal-property property">Namespace</span><span class="token operator">:</span>      <span class="token keyword">default</span>
Resource        Used   Hard
<span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>        <span class="token operator">--</span><span class="token operator">--</span>   <span class="token operator">--</span><span class="token operator">--</span>
limits<span class="token punctuation">.</span>cpu      200m   600m
limits<span class="token punctuation">.</span>memory   100Mi  500Mi
requests<span class="token punctuation">.</span>cpu    100m   400m
requests<span class="token punctuation">.</span>memory 10Mi   200Mi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>因为只运行了 kubia-manual pod, 所以 Used 列与这个 pod 的 requests 和 limits 相等. 如果再运行其他 pod, 它们的 requests 和 limits 值会增加至已使用量中.</p> <blockquote><p>与 ResourceQuota 同时创建 LimitRange</p></blockquote> <p>需要注意的一点是, <strong>创建 ResourceQuota 时往往还需要随之创建一个 LimitRange 对象</strong>. 在上一节已经配置了 LimitRange, 但是假设没有配置, kubia-manual pod 将无法成功创建, 因为它没有指定任何资源 requests 和 limits. 看一下这种情况会发生什么:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl create <span class="token operator">-</span>f <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span>Chapter03<span class="token operator">/</span>kubia<span class="token operator">-</span>manual<span class="token punctuation">.</span>yaml
Error from <span class="token function">server</span> <span class="token punctuation">(</span>Forbidden<span class="token punctuation">)</span><span class="token operator">:</span> error when creating &quot;<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span>Chapter03<span class="token operator">/</span>kubia<span class="token operator">-</span>
     manual<span class="token punctuation">.</span>yaml<span class="token string">&quot;: pods &quot;</span>kubia<span class="token operator">-</span>manual&quot; is forbidden<span class="token operator">:</span> failed quota<span class="token operator">:</span> cpu<span class="token operator">-</span>and<span class="token operator">-</span>
     <span class="token literal-property property">mem</span><span class="token operator">:</span> must specify limits<span class="token punctuation">.</span>cpu<span class="token punctuation">,</span>limits<span class="token punctuation">.</span>memory<span class="token punctuation">,</span>requests<span class="token punctuation">.</span>cpu<span class="token punctuation">,</span>requests<span class="token punctuation">.</span>memory
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><strong>因此, 当特定资源(CPU 或内存)配置了(requests 或 limits)配额, 在 pod 中必须为这些资源(分别)指定 requests 或 limits, 否则 API 服务器不会接收该 pod 的创建请求.</strong></p> <h6 id="_14-5-2-为持久化存储指定配额"><a href="#_14-5-2-为持久化存储指定配额" class="header-anchor">#</a> 14.5.2 为持久化存储指定配额</h6> <p>ResourceQuota 对象同样可以限制<strong>某个命名空间中最多可以声明的持久化存储总量</strong>, 如以下代码清单所示.</p> <p><strong>代码清单-14.15 为存储配置 ResourceQuota: quota-storage.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ResourceQuota
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> storage
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hard</span><span class="token punctuation">:</span>
    <span class="token key atrule">requests.storage</span><span class="token punctuation">:</span> 500Gi                <span class="token comment"># 可声明存储总量</span>
    <span class="token key atrule">ssd.storageclass.storage.k8s.io/requests.storage</span><span class="token punctuation">:</span> 300Gi   <span class="token comment"># storageclass ssd的可申请的存储量</span>
    <span class="token key atrule">standard.storageclass.storage.k8s.io/requests.storage</span><span class="token punctuation">:</span> 1Ti
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>在这个例子中, Namespace 中所有可申请的 PVC 总量被限制为 500GiB(通过配额对象中的 requests.storage). 如果你记得第 6 章, PVC 可以申请一个特定 StorageClass, 动态提供的 PV(PersistentVolume). 这就是为什么 Kubernetes 同样允许单独为每个 StorageClass 提供定义存储配额的原因. 上面的示例限制了可声明的 SSD 存储(以 ssd 命名的 StorageClass)的总量为 300GiB. 低性能的 HDD 存储(StorageClass standrad)限制为 1TiB.</p> <h6 id="_14-5-3-限制可创建对象的个数"><a href="#_14-5-3-限制可创建对象的个数" class="header-anchor">#</a> 14.5.3 限制可创建对象的个数</h6> <p><strong>资源配额同样可以限制单个命名空间中的 pod, ReplicationController, Service 以及其他对象的个数</strong>. 集群管理员可以根据比如付费计划限制用户能够创建的对象个数, 同时也可以用来限制公网 IP 或者 Service 可使用的节点端口个数.</p> <p>下面的代码清单展示了一个限制对象个数的 ResourceQuota 定义:</p> <p><strong>代码清单-14.16 一个限制了资源最大个数的 ResourceQuota:quota-objectcount.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ResourceQuota
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> objects
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">hard</span><span class="token punctuation">:</span>
    <span class="token key atrule">pods</span><span class="token punctuation">:</span> <span class="token number">10</span>                     <span class="token comment"># 限制命名空间可以创建的资源总量, 比如10个pod, 5个Service</span>
    <span class="token key atrule">replicationcontrollers</span><span class="token punctuation">:</span> <span class="token number">5</span>                                 
    <span class="token key atrule">secrets</span><span class="token punctuation">:</span> <span class="token number">10</span>                                               
    <span class="token key atrule">configmaps</span><span class="token punctuation">:</span> <span class="token number">10</span>                                            
    <span class="token key atrule">persistentvolumeclaims</span><span class="token punctuation">:</span> <span class="token number">4</span>                                 
    <span class="token key atrule">services</span><span class="token punctuation">:</span> <span class="token number">5</span>                                               
    <span class="token key atrule">services.loadbalancers</span><span class="token punctuation">:</span> <span class="token number">1</span>                                 
    <span class="token key atrule">services.nodeports</span><span class="token punctuation">:</span> <span class="token number">2</span>                                     
    <span class="token key atrule">ssd.storageclass.storage.k8s.io/persistentvolumeclaims</span><span class="token punctuation">:</span> <span class="token number">2</span>   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>上面的例子允许用户在一个命名空间中最多创建 10 个 pod, 无论是手动创建还是通过 ReplicationController, ReplicaSet, DaemonSet 或者 Job 创建的. 同时限制了 ReplicationController 最大个数为 5, Service 最大个数为 5, 其中 LoadBalancer 类型最多 1 个, NotPort 类型最多 2 个. 与通过指定每个 StorageClass 来限制存储资源的申请总量类似, PVC 的个数同样可以按照 StorageClass 来限制.</p> <p>对象个数配额目前可以为以下对象配置:</p> <ul><li><strong>pod</strong></li> <li><strong>ReplicationController</strong></li> <li><strong>Secret</strong></li> <li><strong>ConfigMap</strong></li> <li><strong>Persistent Volume Claim</strong></li> <li><strong>Service</strong>(通用), 以及两种特定类型的 Service, 比如 LoadBalancer Service(services.loadbalancers)和 NodePort Service(services.nodeports)</li></ul> <p>最后, 甚至可以为 ResourceQuota 对象本身设置对象个数配额. 其他对象的个数, 比如 ReplicaSet, Job, Deployment, Ingress 等暂时不能限制(不过在本书出版后可能有所改变, 因此请参考最新文档获取更多信息).</p> <h6 id="_14-5-4-为特定的pod状态或者qos等级指定配额"><a href="#_14-5-4-为特定的pod状态或者qos等级指定配额" class="header-anchor">#</a> 14.5.4 为特定的pod状态或者QoS等级指定配额</h6> <p>目前为止创建的 <strong>Quota 应用于所有的 pod</strong>, 不管 pod 的当前状态和 QoS 等级如何. 但是 Quota 可以被一组 quota scopes 限制. 目前配额作用范围共有4 种: <strong>BestEffort, NotBestEffort, Termination 和 NotTerminating</strong>.</p> <p>BestEffort 和 NotBestEffort 范围决定配额是否应用于 BestEffort QoS 等级或者其他两种等级(Burstable 和 Guaranteed)的 pod.</p> <p>其他两个范围(Terminating 和 NotTerminating)的名称或许有些误导作用, 实际上并不应用于处在(或不处在)停止过程中的 pod. 目前尚未讨论过这个问题, 但可以为每个 pod 指定被标记为 Failed, 然后真正停止之前还可以运行多长时间. 这是通过在 pod spec 中配置 activeDeadlineSeconds 来实现的. 该属性定义了一个 pod 从开始尝试停止的时间到其被标记为 Failed 然后真正停止之前, 允许其在节点上继续运行的秒数. Terminating 配额作用范围应用于这些配置了 activeDeadlineSeconds 的 pod, 而 NotTerminating 应用于那些没有指定该配置的 pod.</p> <p><strong>创建 ResourceQuota 时, 可以为其指定作用范围. 目标 pod 必须与配额中配置的所有范围相匹配</strong>. 另外, 配额的范围也决定着配额可以限制的内容. BestEffort 范围只允许限制 pod 个数, 而其他 3 种范围除了 pod 个数, 还可以限制 CPU/内存的 requests 和 limits.</p> <p>例如, 如果只想将配额应用于 BestEffort, NotTerminating 的 pod, 可以创建如以下代码清单所示的 ResourceQuota 对象.</p> <p><strong>代码列表-14.17 为 BestEffort/NotTerminating pod 设置 ResourceQuota:quota-scoped.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ResourceQuota
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> besteffort<span class="token punctuation">-</span>notterminating<span class="token punctuation">-</span>pods
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">scopes</span><span class="token punctuation">:</span>                 <span class="token comment"># 这个quota只会应用与拥有BestEffort Qos, 以及没有设置有效期的pod上</span>
  <span class="token punctuation">-</span> BestEffort        
  <span class="token punctuation">-</span> NotTerminating    
  <span class="token key atrule">hard</span><span class="token punctuation">:</span>
    <span class="token key atrule">pods</span><span class="token punctuation">:</span> <span class="token number">4</span>               <span class="token comment"># 这样的pod只允许存在4个</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>这个配额允许最多创建 4个 属于 BestEffort QoS 等级, 并没有设置 active deadline 的 pod. 如果配额针对的是 NotBestEffort pod, 便可以指定 requests.cpu,requests.memory,limits.cpu 和 limits.memory.</p> <h5 id="_14-6-监控pod的资源使用量"><a href="#_14-6-监控pod的资源使用量" class="header-anchor">#</a> 14.6 监控pod的资源使用量</h5> <p>设置合适的资源 requests 和 limits 对充分利用 Kubernetes 集群资源来说十分重要. 如果 requests 设置得太高, 集群节点利用率就会比较低, 这样就白白浪费了金钱. 如果设置得太低, 应用就会处于 CPU 饥饿状态, 甚至很容易被 OOM Killer 杀死. <mark><strong>所以如何才能找到 requests 和 limits 的最佳配置呢</strong></mark>?</p> <p>可以通过对容器<strong>在期望负载下的资源实际使用率进行监控来找到这个最佳配置</strong>. 当然一旦应用暴露于公网, 都应该保持监控并且在需要时对其资源的 requests 和 limits 进行调节.</p> <h6 id="_14-6-1-收集与获取实际资源使用情况"><a href="#_14-6-1-收集与获取实际资源使用情况" class="header-anchor">#</a> 14.6.1 收集与获取实际资源使用情况</h6> <p>那么如何监控一个在 Kubernetes 中运行的应用呢? 幸运的是, Kubelet 自身就包含了一个<strong>名为 cAdvisor 的 agent, 它会收集整个节点和节点上运行的所有单独容器的资源消耗情况. 集中统计整个集群的监控信息需要运行一个叫作 Heapster 的附加组件</strong>.</p> <p>Heapster 以 pod 的方式运行在某个节点上, 它通过<strong>普通的 Kubernetes Service 暴露服务</strong>, 使外部可以通过一个稳定的 IP 地址访问. 它从集群中所有的 cAdvisor 收集数据, 然后通过一个单独的地址暴露. 图 14.8 展示了来源于 pod 的监控指标数据, 经过 cAdvisor 最终到达 Heapster 的数据流.</p> <p><img src="/img/image-20240225221242-krt4woq.png" alt="image" title="图14.8 进入 Heapster 的监控指标数据流"></p> <p>图中的箭头表示<strong>监控数据流动</strong>的方向, 它并不代表组件之间用来获取数据的连接关系. pod(或者 pod 中运行的容器)感知不到 cAdvisor 的存在, cAdvisor 也感知不到 Heapster 的存在. Heapster 主动请求所有的 cAdvisor, 同时 cAdvisor 无须通过与 pod 容器内进程通信就可以收集到容器和节点的资源使用数据.</p> <blockquote><p>启用 Heapster</p></blockquote> <p>如果集群运行在 Google Container Engine 上, Heapster 默认已经启用. 如果使用的是 Minikube, 它可以作为插件使用, 通过以下命令开启:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ minikube addons <span class="token builtin class-name">enable</span> heapster
heapster was successfully enabled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果要在其他类型的 Kubernetes 集群中手动运行 Heapster, 可以参考 https://github.com/kubernetes/heapster 中的介绍.</p> <p>启用了 Heapster 之后, 可能需要等待几分钟时间收集足够的指标, 才能看到集群资源的使用统计.</p> <blockquote><p>显示集群节点的 CPU 和内存使用量</p></blockquote> <p>在集群中<strong>运行 Heapster 可以通过 kubectl top 命令获得节点和单个 pod 的资源用量</strong>. 要看节点使用了多少 CPU 和内存, 可以执行以下代码清单所示的命令.</p> <p><strong>代码清单-14.18 节点实际 CPU 和内存使用量</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token function">top</span> <span class="token function">node</span>
NAME       CPU<span class="token punctuation">(</span>cores<span class="token punctuation">)</span>   CPU%      MEMORY<span class="token punctuation">(</span>bytes<span class="token punctuation">)</span>   MEMORY%
minikube   170m         <span class="token number">8</span>%        556Mi           <span class="token number">27</span>%
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>它显示了节点上运行的<strong>所有 pod 当前的 CPU 和内存的实际使用量</strong>, 与 kubectl describe node 命令不同的是, 后者仅仅显示节点 CPU 和内存的 requests 和 limits, 而不是实际运行时的使用数据.</p> <blockquote><p>显示单独 pod 的 CPU 和内存使用量</p></blockquote> <p>要查看每个 pod 使用了多少资源, 可以使用 kubectl top pod 命令, 如以下代码清单所示.</p> <p><strong>代码清单-14.19 pod CPU 和内存的实际使用量</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl top pod <span class="token operator">--</span>all<span class="token operator">-</span>namespaces
<span class="token constant">NAMESPACE</span>      <span class="token constant">NAME</span>                             <span class="token constant">CPU</span><span class="token punctuation">(</span>cores<span class="token punctuation">)</span>   <span class="token constant">MEMORY</span><span class="token punctuation">(</span>bytes<span class="token punctuation">)</span>
kube<span class="token operator">-</span>system    influxdb<span class="token operator">-</span>grafana<span class="token operator">-</span>2r2w9           1m           32Mi
kube<span class="token operator">-</span>system    heapster<span class="token operator">-</span>40j6d                   0m           18Mi
<span class="token keyword">default</span>        kubia<span class="token operator">-</span><span class="token number">3773182134</span><span class="token operator">-</span>63bmb           0m           9Mi
kube<span class="token operator">-</span>system    kube<span class="token operator">-</span>dns<span class="token operator">-</span>v20<span class="token operator">-</span>z0hq6               1m           11Mi
kube<span class="token operator">-</span>system    kubernetes<span class="token operator">-</span>dashboard<span class="token operator">-</span>r53mc       0m           14Mi
kube<span class="token operator">-</span>system    kube<span class="token operator">-</span>addon<span class="token operator">-</span>manager<span class="token operator">-</span>minikube      7m           33Mi
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>这两个命令的输出都相当简单, 因此也许不需要过多解释, 然而有一点需要提醒的是, 有时 top pod 命令会拒绝输出任何指标而输出以下错误:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl top pod
<span class="token constant">W0312</span> <span class="token number">22</span><span class="token operator">:</span><span class="token number">12</span><span class="token operator">:</span><span class="token number">58.021885</span>   <span class="token number">15126</span> top_pod<span class="token punctuation">.</span>go<span class="token operator">:</span><span class="token number">186</span><span class="token punctuation">]</span> Metrics not available <span class="token keyword">for</span> pod <span class="token keyword">default</span><span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token number">3773182134</span><span class="token operator">-</span>63bmb<span class="token punctuation">,</span> <span class="token literal-property property">age</span><span class="token operator">:</span> 1h24m19<span class="token punctuation">.</span>021873823s
<span class="token literal-property property">error</span><span class="token operator">:</span> Metrics not available <span class="token keyword">for</span> pod <span class="token keyword">default</span><span class="token operator">/</span>kubia<span class="token operator">-</span><span class="token number">3773182134</span><span class="token operator">-</span>63bmb<span class="token punctuation">,</span> <span class="token literal-property property">age</span><span class="token operator">:</span> 1h24m19<span class="token punctuation">.</span>021873823s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>如果看到这个错误, 先不要急于寻找出错的原因, 放松一下, 然后等待一会儿重新执行--可能需要好几分钟, 不过最终会看到指标的. 因为 kubectl top 命令从 Heapster 中获取指标, 它将几分钟的数据汇总起来, 所以通常不会立即暴露.</p> <p>提示: 要查看容器而不是 pod 的资源使用情况, 可以使用 --container 选项.</p> <h6 id="_14-6-2-保存并分析历史资源的使用统计信息"><a href="#_14-6-2-保存并分析历史资源的使用统计信息" class="header-anchor">#</a> 14.6.2 保存并分析历史资源的使用统计信息</h6> <p>top 命令仅仅展示了<strong>当前的</strong>资源使用量, 它并不会显示比如从一小时, 一天或者一周前到现在 pod 的 CPU 和内存使用了多少. 事实上, cAdvisor 和 Heapster 都<strong>只保存一个很短时间窗的资源使用量数据</strong>. 如果需要分析一段时间的 pod 的资源使用情况, 必须使用额外的工具. 如果使用 Google Container Engine, 可以通过 Google Cloud Monitoring 来对集群进行监控, 但是如果是本地 Kubernetes 集群(通过 Minikube 或其他方式创建), 人们往往<mark><strong>使用 InfiuxDB 来存储统计数据, 然后使用 Grafana 对数据进行可视化和分析</strong></mark>.</p> <blockquote><p>InfiuxDB 和 Grafana 介绍</p></blockquote> <p><strong>InfiuxDB 是一个用于存储应用指标, 以及其他监控数据的开源的时序数据库</strong>. Grafana 是一个拥有着华丽的 web 控制台的数据分析和可视化套件, 同样也是开源的, 它允许用户对 InfiuxDB 中存储的数据进行可视化, 同时发现应用程序的资源使用行为是如何随时间变化的.</p> <blockquote><p>在集群中运行 InfiuxDB 和 Grafana</p></blockquote> <p>InfiuxDB 和 Grafana <strong>都可以以 pod 运行</strong>, 部署简单方便. 所有需要的部署文件可以在 Heapster Git 仓库中获取: http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/infiuxdb.</p> <p>如果使用 Minikube 就无须手动部署, 因为启用 Heapster 插件时便会随之部署 Heapster.</p> <blockquote><p>使用 Grafana 分析资源使用量</p></blockquote> <p>要发现 pod 对每种资源的需求是如何随时间变化的, 打开 Grafana web 控制台, 开始浏览一些预先定义的面板. 通常可以通过 kubectl cluster -info 命令找到 Grafana web 控制台的 URL.</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl cluster<span class="token operator">-</span>info
<span class="token operator">...</span>
monitoring<span class="token operator">-</span>grafana is running at
     <span class="token literal-property property">https</span><span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token number">192.168</span><span class="token number">.99</span><span class="token number">.100</span><span class="token operator">:</span><span class="token number">8443</span><span class="token operator">/</span>api<span class="token operator">/</span>v1<span class="token operator">/</span>proxy<span class="token operator">/</span>namespaces<span class="token operator">/</span>kube<span class="token operator">-</span>
     system<span class="token operator">/</span>services<span class="token operator">/</span>monitoring<span class="token operator">-</span>grafana
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>使用 minikube 时, Grafana 的 web 控制台通过 NodePort Service 暴露, 因此使用以下命令在浏览器中将其打开:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ minikube service monitoring<span class="token operator">-</span>grafana <span class="token operator">-</span>n kube<span class="token operator">-</span>system
Opening kubernetes service kube<span class="token operator">-</span>system<span class="token operator">/</span>monitoring<span class="token operator">-</span>grafana <span class="token keyword">in</span> <span class="token keyword">default</span>
     browser<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这将打开一个新的浏览器窗口或标签页, 显示 Grafana 的主页面. 在右边可以看到一个包含两个入口的面板列表:</p> <ul><li>Cluster</li> <li>pod</li></ul> <p>打开 Cluster 面板, 可以看到节点的资源使用统计. 在这里将看到一些展示着集群整体使用量, 单节点使用量, 以及 CPU, 内存, 网络和文件系统单独使用量的图表. 这些图表不仅展示了资源的实际使用量, 也展示了这些资源的 requests 和 limits.</p> <blockquote><p>利用图表中展示的信息</p></blockquote> <p>通过查看图表, 可以快速看到是否需要提高之前为 pod 设置的资源 requests 或 limits 值, 或者是否需要降低配置以允许更多的 pod 可以调度到节点上. 来看一个示例, 图 14.10 展示了一个 pod 的 CPU 和内存图表.</p> <p><img src="/img/image-20240225221719-g10sgoy.png" alt="image" title="图14.10 pod 的 CPU 和内存使用图表"></p> <p>在顶部图表的最右侧, <strong>可以看到 pod 实际 CPU 使用量要比 pod 定义中指定的申请量更多. 尽管如果节点上只运行这一个 pod 时没有什么问题, 但需要注意的是, pod 只能保证获取到与其请求量相等的资源. 这个 pod 也许现在运行良好, 但是当其他 pod 部署在相同节点并开始大量使用 CPU 时, 这个 pod 的 CPU 时间将被限制. 因此, 为了保证这个 pod 可以在任何时候都能使用足够的 CPU, 需要提升容器的 CPU 资源请求量</strong>.</p> <p>图表的底部显示了 pod 的内存使用量和请求量. 这个情况恰恰相反, pod 使用的内存数量远远低于 pod spec 中的请求量. 这部分内存会始终为这个 pod 保留而且对其他 pod 来说也不能使用. 因此这些未使用的内存被浪费了. 应该降低 pod 的内存请求量, 使其他 pod 可以使用这个节点上空闲的内存.</p> <h5 id="_14-7-本章小结"><a href="#_14-7-本章小结" class="header-anchor">#</a> 14.7 本章小结</h5> <p>本章讲述了为了确保一切顺利运行, 需要考虑 pod 的<strong>资源使用情况</strong>, 同时为 pod 同时配置资源 requests 和 limits. 本章的主要内容是:</p> <ul><li><strong>指定资源 requests, 帮助 Kubernetes 在集群内对 pod 进行调度</strong></li> <li><strong>指定资源 limits, 防止一个 pod 抢占其他 pod 的资源</strong></li> <li><strong>空闲的 CPU 时间根据容器的 CPU requests 来分配</strong></li> <li><strong>如果容器使用过量的 CPU, 系统不会杀死这个容器, 但如果使用过量的内存会被杀死</strong></li> <li><strong>在一个 overcommited 的系统, 容器同样可以被杀死以释放内存给更重要的 pod, 这基于 pod 的 QoS 等级和实际内存用量</strong></li> <li>可以通过 LimtRange 对象为单个 pod 的资源 requests 和 limits 定义最小值, 最大值和默认值</li> <li><strong>可以通过 ResourceQuota 对象限制一个命名空间中所有 pod 的可用资源数量</strong></li> <li>要知道如何为 pod 设置合适的资源 requests 和 limits, 需要对一段足够长时间内 pod 资源的使用情况进行监控</li></ul> <p>在下一章将了解 Kubernetes 如何<strong>使用这些指标对 pod 进行自动扩缩容</strong>.</p> <h4 id="_15-自动横向伸缩pod与集群节点"><a href="#_15-自动横向伸缩pod与集群节点" class="header-anchor">#</a> 15.自动横向伸缩pod与集群节点</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>基于 CPU 使用率配置 pod 的自动横向伸缩</li> <li>基于自定义度量配置 pod 的自动横向伸缩</li> <li>了解为何 pod 纵向伸缩暂时无法实现</li> <li>了解集群节点的自动横向伸缩</li></ul> <p>可以通过调高 ReplicationController, ReplicaSet, Deployment 等可伸缩资源的 replicas 字段, 来手动实现 pod 中应用的横向扩容. 也可以通过增加 pod 容器的资源请求和限制来纵向扩容 pod(尽管目前该操作只能在 pod 创建时, 而非运行时进行). 虽然如果你能预先知道负载何时会飙升, 或者如果负载的变化是较长时间内逐渐发生的, 手动扩容也是可以接受的, 但指望靠人工干预来处理突发而不可预测的流量增长, 仍然不够理想.</p> <p>幸运的是, Kubernetes 可以监控你的 pod, <strong>并在检测到 CPU 使用率或其他度量增长时自动对它们扩容</strong>. 如果 Kubernetes 运行在云端基础架构之上, 它甚至能在现有节点无法承载更多 pod 之时自动新建更多节点. 本章将会解释<strong>如何让 Kubernetes 进行 pod 与节点级别的自动伸缩</strong>.</p> <p>Kubernetes 的自动伸缩特性在 1.6 与 1.7 版本之间经历了一次重写, 因此注意网上关于此方面的内容有可能已经过时了.</p> <h5 id="_15-1-pod的横向自动伸缩"><a href="#_15-1-pod的横向自动伸缩" class="header-anchor">#</a> 15.1 pod的横向自动伸缩</h5> <p><strong>横向 pod 自动伸缩是指由控制器管理的 pod 副本数量的自动伸缩. 它由 Horizontal 控制器执行, 通过创建一个 HorizontalpodAutoscaler(HPA)资源来启用和配置 Horizontal 控制器. 该控制器周期性检查 pod 度量, 计算满足 HPA 资源所配置的目标数值所需的副本数量, 进而调整目标资源(如 Deployment, ReplicaSet, ReplicationController, StatefulSet 等)的 replicas 字段.</strong></p> <h6 id="_15-1-1-了解自动伸缩过程"><a href="#_15-1-1-了解自动伸缩过程" class="header-anchor">#</a> 15.1.1 了解自动伸缩过程</h6> <p>自动伸缩的过程可以分为三个步骤:</p> <ul><li>获取被伸缩资源对象所管理的所有 pod 度量.</li> <li>计算使度量数值<strong>到达(或接近)所指定目标数值所需的 pod 数量</strong>.</li> <li><strong>更新被伸缩资源的 replicas 字段</strong>.</li></ul> <p>下面就来看看这三个步骤.</p> <blockquote><p>获取 pod 度量</p></blockquote> <p>Autoscaler 本身并不负责采集 pod 度量数据, 而是从另外的来源获取. 正如上一章提到的, pod 与节点度量数据是由运行在每个节点的 kubelet 之上, 名为 cAdvisor 的 agent 采集的; 这些数据将由集群级的组件 Heapster 聚合. <strong>HPA 控制器向 Heapster 发起 REST 调用来获取所有 pod 度量数据</strong>. 图 15.1 展示了度量数据的流动情况(注意所有连接都是按照箭头反方向发起的).</p> <p><img src="/img/image-20240225223248-msxawub.png" alt="image" title="图15.1 度量数据从 pod 到 HPA 的流动"></p> <p>这样的数据流意味着<strong>在集群中必须运行 Heapster 才能实现自动伸缩</strong>. 如果在使用 Minikube 并且在前面的章节中都跟着操作了, Heapster 应该已经在你的集群中启用了.</p> <p>尽管并不需要直接查询 Heapster, 如果你感兴趣, 可以在 kube-system 命名空间中找到 Heapster 的 pod 和 Service.</p> <blockquote><p>关于 Autoscaler 采集度量数据方式的改变</p></blockquote> <p>在 Kubernetes 1.6 版本之前, HPA 直接从 Heapster 采集度量. 在 1.8 版本中, 如果用 <code>--horizontal-pod-autoscaler-use-rest-clients=true</code>​ 参数启动 ControllerManager, <strong>Autoscaler 就能通过聚合版的资源度量 API 拉取度量了</strong>. 该行为从 1.9 版本开始将变为默认.</p> <p>核心 API 服务器本身并不会向外界暴露度量数据. 从 1.7 版本开始, Kubernetes 允许注册多个 API 服务器并使它们对外呈现为单个 API 服务器. 这允许 Kubernetes 通过这些底层 API 服务器之一来对外暴露度量数据. 后面将在最后一章讲述 API 服务器聚合的内容.</p> <p>集群管理员负责选择集群中使用何种度量采集器. 通常需要一层简单的转换组件将度量数据以正确的格式暴露在正确的 API 路径下.</p> <blockquote><p>计算所需的 pod 数量</p></blockquote> <p>一旦 Autoscaler 获得了它所调整的资源(Deployment, ReplicaSet, ReplicationController 或 StatefulSet)所辖 pod 的全部度量, 它便可以利用这些度量计算出所需的副本数量. 它需要计算出一个合适的副本数量, 以使所有副本上度量的平均值尽量接近配置的目标值. 该计算的输入是一组 pod 度量(每个 pod 可能有多个), 输出则是一个整数(pod 副本数量).</p> <p>当 Autoscaler 配置为只考虑单个度量时, 计算所需副本数很简单. 只要将所有 pod 的度量求和后除以 HPA 资源上配置的目标值, 再向上取整即可. 实际的计算稍微复杂一些; Autoscaler 还保证了度量数值不稳定, 迅速抖动时不会导致系统抖动(thrash).</p> <p>基于多个 pod 度量的自动伸缩(例如: CPU 使用率和每秒查询率[QPS])的计算也并不复杂. Autoscaler 单独计算每个度量的副本数, 然后取最大值(例如: 如果需要 4 个 pod 达到目标 CPU 使用率, 以及需要 3 个 pod 来达到目标 QPS, 那么 Autoscaler 将扩展到 4 个 pod). 图 15.2 展示了这个示例.</p> <p><img src="/img/image-20240225223527-5ylchx4.png" alt="image" title="图15.2 从两个度量计算副本数"></p> <blockquote><p>更新被伸缩资源的副本数</p></blockquote> <p><strong>自动伸缩操作的最后一步是更新被伸缩资源对象(比如 ReplicaSet)上的副本数字段, 然后让 ReplicaSet 控制器负责启动更多 pod 或者删除多余的 pod.</strong></p> <p>Autoscaler 控制器<strong>通过 Scale 子资源来修改被伸缩资源的 replicas 字段</strong>. 这样 Autoscaler 不必了解它所管理资源的细节, 而只需要通过 Scale 子资源暴露的界面, 就可以完成它的工作了(见图 15.3).</p> <p><img src="/img/image-20240225223614-x3hvnwc.png" alt="image" title="图15.3 HPA 只对 Scale 子资源进行更改"></p> <p>这意味着<strong>只要 API 服务器为某个可伸缩资源暴露了 Scale 子资源, Autoscaler 即可操作该资源</strong>. 目前暴露了 Scale 子资源的资源有:</p> <ul><li><strong>Deployment</strong></li> <li><strong>ReplicaSet</strong></li> <li><strong>ReplicationController</strong></li> <li><strong>StatefulSet</strong></li></ul> <p>目前也只有这些对象可以<strong>附着 Autoscaler</strong>.</p> <blockquote><p>了解整个自动伸缩过程</p></blockquote> <p>前面对自动伸缩过程的三个步骤都有所了解了, 下面用一张图表直观地感受一下<strong>自动伸缩过程中的各个组件</strong>, 如图 15.4 所示.</p> <p>从 pod 指向 cAdvisor, 再经过 Heapster, 而最终到达 HPA 的箭头代表度量数据的流向. 值得注意的是, 每个组件从其他组件拉取数据的动作是<strong>周期性</strong>的(即 cAdvisor 用一个无限循环从 pod 中采集数据; Heapster 与 HPA 控制器亦是如此). 这意味着度量数据的传播与相应动作的触发都需要相当一段时间, 不是立即发生的. 接下来实地观察 Autoscaler 行为时要注意这一点.</p> <p><img src="/img/image-20240225223742-qobb33x.png" alt="image" title="图15.4 Autoscaler 获取度量数据伸缩目标部署的方式"></p> <h6 id="_15-1-2-基于cpu使用率进行自动伸缩"><a href="#_15-1-2-基于cpu使用率进行自动伸缩" class="header-anchor">#</a> 15.1.2 基于CPU使用率进行自动伸缩</h6> <p>可能你最想用以指导自动伸缩的度量就是 pod 中<strong>进程的 CPU 使用率</strong>了. 假设你用几个 pod 来提供服务, 如果它们的 CPU 使用率达到了 100%, 显然它们已经扛不住压力了, 要么进行<strong>纵向扩容</strong>(scale up), 增加它们<strong>可用的 CPU 时间</strong>, 要么进行<strong>横向扩容</strong>(scale out), <strong>增加 pod 数量</strong>. 因为本章谈论的是 HPA, 因此仅仅关注横向扩容. 这么一来, 平均 CPU 使用率就应该下降了.</p> <p>因为 CPU 使用通常是不稳定的, 比较靠谱的做法是<strong>在 CPU 被压垮之前就横向扩容, 即</strong>可能平均负载达到或超过 80% 的时候就进行扩容. 但这里有个问题, 到底是谁的 80% 呢?</p> <p>提示: 一定把目标 CPU 使用率设置得远远低于 100%(一定不要超过 90%), 以预留充分空间给突发的流量洪峰.</p> <p>你可能还记得, 上一章中提到容器中的进程被保证能够使用该容器资源请求中所请求的 CPU 资源数量. 但在没有其他进程需要 CPU 时, 进程就能使用节点上<strong>所有</strong>可用的 CPU 资源. 如果有人说 &quot;这个 pod 用了 80% 的 CPU&quot;, 我们并不清楚对方的意思是 80% 的节点 CPU, 还是 80% 的 guaranteed CPU(资源请求量), 还是用资源限额给 pod 配置的硬上限的 80%.</p> <p>就 Autoscaler 而言, 只有 pod 的保证 CPU 用量(CPU 请求)才与确认 pod 的 CPU 使用有关. <strong>Autoscaler 对比 pod 的实际 CPU 使用与它的请求, 这意味着需要给被伸缩的 pod 设置 CPU 请求, 不管是直接设置还是通过 LimitRange 对象间接设置, 这样 Autoscaler 才能确定 CPU 使用率</strong>.</p> <blockquote><p>基于 CPU 使用率创建 HPA</p></blockquote> <p>现在来看看如何创建一个 HPA, 并让它<strong>基于 CPU 使用率来伸缩 pod</strong>. 下面将创建一个类似第 9 章中的 Deployment, 但正如讨论的, 需要确保 Deployment 所创建的<strong>所有 pod 都指定了 CPU 资源请求</strong>, 这样才有可能实现自动伸缩. 你需要给 Deployment 的 pod 模板添加一个 CPU 资源请求, 如以下代码清单所示.</p> <p><strong>代码清单-15.1 设置了 CPU 请求的 Deployment:deployment.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>         <span class="token comment"># 手动设置(初始)想要的副本数为3</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia<span class="token punctuation">:</span>v1     <span class="token comment"># 运行luksa/kubia:v1镜像</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> nodejs
        <span class="token key atrule">resources</span><span class="token punctuation">:</span>              <span class="token comment"># 每个pod请求100毫核的CPU</span>
          <span class="token key atrule">requests</span><span class="token punctuation">:</span>         
            <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m         
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>这就是一个正常的 Deployment 对象, 现在还没有启用自动伸缩. 它会运行 3 个实例的 kubia NodeJS 应用, 每个实例请求 100 毫核的 CPU.</p> <p><strong>创建了 Deployment 之后, 为了给它的 pod 启用横向自动伸缩, 需要创建一个 HorizontalpodAutoscaler(HPA)对象, 并把它指向该 Deployment</strong>. 可以给 HPA 准备 YAML manifest, 但有个办法更简单, 也就是用 kubectl autoscale 命令:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl autoscale deployment kubia <span class="token operator">--</span>cpu<span class="token operator">-</span>percent<span class="token operator">=</span><span class="token number">30</span> <span class="token operator">--</span>min<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">--</span>max<span class="token operator">=</span><span class="token number">5</span>
deployment <span class="token string">&quot;kubia&quot;</span> autoscaled
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这会帮你创建 HPA 对象, 并将<strong>叫作 kubia 的 Deployment 设置为伸缩目标</strong>. 这里还设置了 pod 的目标 CPU 使用率为 30%, 指定了副本的最小和最大数量. <strong>Autoscaler 会持续调整副本的数量以使 CPU 使用率接近 30%, 但它永远不会调整到少于 1 个或者多于 5 个</strong>.</p> <p>提示: 一定要<strong>确保自动伸缩的目标是 Deployment 而不是底层的 ReplicaSet</strong>. 这样才能确保预期的副本数量在应用更新后继续保持(记着 Deployment 会给每个应用版本创建一个新的 ReplicaSet). 手动伸缩也是同样的道理.</p> <p>下面来看看 HorizontalpodAutoscaler 资源的定义, 更深入地理解它, 如以下代码清单所示.</p> <p><strong>代码清单-15.2 一个 HorizontalpodAutoscaler 的 YAML 定义</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get hpa.v2beta1.autoscaling kubia <span class="token parameter variable">-o</span> yaml
apiVersion: autoscaling/v2beta1     <span class="token comment"># HPA资源位于autoscaling这个API组中</span>
kind: HorizontalPodAutoscaler        
metadata:
  name: kubia       <span class="token comment"># 每个HPA都有一个名称(并不一定非要像这里一样与Deployment的名称一致)</span>
  <span class="token punctuation">..</span>.
spec:
  maxReplicas: <span class="token number">5</span>    <span class="token comment"># 指定的最大副本数</span>
  metrics:          <span class="token comment"># 让Autoscaler调整pod数量以使每个pod都使用所请求CPU的30%                     </span>
  - resource:                          
      name: cpu                        
      targetAverageUtilization: <span class="token number">30</span>     
    type: Resource                     
  minReplicas: <span class="token number">1</span>     <span class="token comment"># 指定的最小副本数</span>
  scaleTargetRef:                      
    apiVersion: extensions/v1beta1     <span class="token comment"># 该Autoscaler将作用的目标资源   </span>
    kind: Deployment                   
    name: kubia                        
status:
  currentMetrics: <span class="token punctuation">[</span><span class="token punctuation">]</span>         <span class="token comment"># Autoscaler当前的状态</span>
  currentReplicas: <span class="token number">3</span>                   
  desiredReplicas: <span class="token number">0</span>                     
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>注意: HPA 资源存在多个版本: 新的 autoscaling/v2beta1 和旧的 autoscaling/v1. 此处请求的是新版资源.</p> <blockquote><p>观察第一个自动伸缩事件</p></blockquote> <p>cAdvisor 获取 CPU 度量与 Heapster 收集这些度量都需要一阵子, 之后 Autoscaler 才能采取行动. 在这段时间里, 如果用 kubectl get 显示 HPA 资源, TARGETS 列就会显示 <code>unknown</code>​:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> hpa
<span class="token constant">NAME</span>      <span class="token constant">REFERENCE</span>          <span class="token constant">TARGETS</span>           <span class="token constant">MINPODS</span>   <span class="token constant">MAXPODS</span>   <span class="token constant">REPLICAS</span>
kubia     Deployment<span class="token operator">/</span>kubia   <span class="token operator">&lt;</span>unknown<span class="token operator">&gt;</span> <span class="token operator">/</span> <span class="token number">30</span><span class="token operator">%</span>   <span class="token number">1</span>         <span class="token number">5</span>         <span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>因为在运行三个没有请求的 pod, 它们的 CPU 使用率应该接近 0, 应该预期 Autoscaler 将它们收缩到 1 个 pod, 因为即便只有一个 pod, CPU 使用率仍然会<strong>低于</strong> 30% 的目标值.</p> <p>确实, Autoscaler 就是这么做的. 它很快就把 Deployment <strong>收缩到单个副本</strong>:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> deployment
<span class="token constant">NAME</span>      <span class="token constant">DESIRED</span>   <span class="token constant">CURRENT</span>   <span class="token constant">UP</span><span class="token operator">-</span><span class="token constant">TO</span><span class="token operator">-</span><span class="token constant">DATE</span>   <span class="token constant">AVAILABLE</span>   <span class="token constant">AGE</span>
kubia     <span class="token number">1</span>         <span class="token number">1</span>         <span class="token number">1</span>            <span class="token number">1</span>           23m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>记住, <mark><strong>Autoscaler 只会在 Deployment 上调节预期的副本数量. 接下来由 Deployment 控制器负责更新 ReplicaSet 对象上的副本数量, 从而使 ReplicaSet 控制器删除多余的两个 pod 而留下一个</strong></mark>.</p> <p>可以使用 kubectl describe 来观察 HorizontalpodAutoscaler 的更多信息, 以及它底层控制器的工作, 如以下代码清单所示.</p> <p><strong>代码清单-15.3 用 kubectl describe 检查一个 HorizontalpodAutoscaler</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl describe hpa
<span class="token literal-property property">Name</span><span class="token operator">:</span>                             kubia
<span class="token literal-property property">Namespace</span><span class="token operator">:</span>                        <span class="token keyword">default</span>
<span class="token literal-property property">Labels</span><span class="token operator">:</span>                           <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token literal-property property">Annotations</span><span class="token operator">:</span>                      <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
<span class="token literal-property property">CreationTimestamp</span><span class="token operator">:</span>                Sat<span class="token punctuation">,</span> <span class="token number">03</span> Jun <span class="token number">2017</span> <span class="token number">12</span><span class="token operator">:</span><span class="token number">59</span><span class="token operator">:</span><span class="token number">57</span> <span class="token operator">+</span><span class="token number">0200</span>
<span class="token literal-property property">Reference</span><span class="token operator">:</span>                        Deployment<span class="token operator">/</span>kubia
<span class="token literal-property property">Metrics</span><span class="token operator">:</span>                          <span class="token punctuation">(</span> current <span class="token operator">/</span> target <span class="token punctuation">)</span>
  resource cpu on <span class="token function">pods</span>
  <span class="token punctuation">(</span><span class="token keyword">as</span> a percentage <span class="token keyword">of</span> request<span class="token punctuation">)</span><span class="token operator">:</span>   <span class="token number">0</span><span class="token operator">%</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">30</span><span class="token operator">%</span>
Min replicas<span class="token operator">:</span>                     <span class="token number">1</span>
Max replicas<span class="token operator">:</span>                     <span class="token number">5</span>
<span class="token literal-property property">Events</span><span class="token operator">:</span>
From                        Reason              Message
<span class="token operator">--</span><span class="token operator">--</span>                        <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>              <span class="token operator">--</span><span class="token operator">-</span>
horizontal<span class="token operator">-</span>pod<span class="token operator">-</span>autoscaler   SuccessfulRescale   New size<span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">;</span> reason<span class="token operator">:</span> All
                                                metrics below target
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>注意: 为使可读性更佳, 输出被重新排版了.</p> <p>把注意力转移到代码清单底部的事件列表. 可以看到因为<strong>所有度量都低于目标值, HPA 已经成功收缩到单个副本</strong>了.</p> <blockquote><p>触发一次自动扩容</p></blockquote> <p>前面已经目击了第一个自动伸缩事件(一个收缩事件). 现在要往 pod 发送请求, 增加它的 CPU 使用率, 随后就应该看到 Autoscaler 检测到这一切并启动更多的 pod.</p> <p>需要通过一个 Service 来暴露 pod, 以便用单一的 URL 访问到所有 pod. 你可能还记得做这件事最简单的方法就是 kubectl expose:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl expose deployment kubia <span class="token operator">--</span>port<span class="token operator">=</span><span class="token number">80</span> <span class="token operator">--</span>target<span class="token operator">-</span>port<span class="token operator">=</span><span class="token number">8080</span>
service <span class="token string">&quot;kubia&quot;</span> exposed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在向 pod 发送请求之前, 你可能希望在另一个终端里运行以下命令, 来观察 HPA 与 Deployment 上发生了什么, 如以下代码清单所示.</p> <p><strong>代码清单-15.4 并行观察多个资源</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ watch <span class="token operator">-</span>n <span class="token number">1</span> kubectl <span class="token keyword">get</span> hpa<span class="token punctuation">,</span>deployment
Every <span class="token number">1</span><span class="token punctuation">.</span>0s<span class="token operator">:</span> kubectl <span class="token keyword">get</span> hpa<span class="token punctuation">,</span>deployment

<span class="token constant">NAME</span>        <span class="token constant">REFERENCE</span>          <span class="token constant">TARGETS</span>    <span class="token constant">MINPODS</span>   <span class="token constant">MAXPODS</span>   <span class="token constant">REPLICAS</span>  <span class="token constant">AGE</span>
hpa<span class="token operator">/</span>kubia   Deployment<span class="token operator">/</span>kubia   <span class="token number">0</span><span class="token operator">%</span> <span class="token operator">/</span> <span class="token number">30</span><span class="token operator">%</span>   <span class="token number">1</span>         <span class="token number">5</span>         <span class="token number">1</span>         45m

<span class="token constant">NAME</span>           <span class="token constant">DESIRED</span>   <span class="token constant">CURRENT</span>   <span class="token constant">UP</span><span class="token operator">-</span><span class="token constant">TO</span><span class="token operator">-</span><span class="token constant">DATE</span>   <span class="token constant">AVAILABLE</span>   <span class="token constant">AGE</span>
deploy<span class="token operator">/</span>kubia   <span class="token number">1</span>         <span class="token number">1</span>         <span class="token number">1</span>            <span class="token number">1</span>           56m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>提示: 用逗号分隔资源类型可以让 kubectl get 一次列举多个资源类型.</p> <p>在运行产生负载的 pod 的同时, 注意观察这两个对象的状态. 在另一个终端里运行以下命令:</p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl run <span class="token operator">-</span>it <span class="token operator">--</span>rm <span class="token operator">--</span>restart<span class="token operator">=</span>Never loadgenerator <span class="token operator">--</span>image<span class="token operator">=</span>busybox
 <span class="token operator">--</span> sh <span class="token operator">-</span>c <span class="token string">&quot;while true; do wget -O - -q http://kubia.default; done&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这会运行一个 pod 重复请求 kubia 服务. 在运行 kubectl exec 命令时见过几次 -it 选项了. 如你所见, 它对 kubectl run 也适用. 它允许你将控制台附加到被观察的进程, 不仅允许你直接观察进程的输出, 而且在你按下 CTRL+C 组合键时还会直接终止进程. --rm 选项使得 pod 在退出之后自动被删除; -restart=Never 选项则使 kubectl run 命令直接创建一个非托管的 pod, 而不是通过一个你用不着的 Deployment 对象间接创建. 对于需要在集群中执行命令, 又不想在已有的 pod 之上运行的情形, 这组选项很实用. 它们不仅与本地运行的效果相同, 甚至在运行结束之后还会把现场清理干净!</p> <blockquote><p>观察 Autoscaler 扩容 Deployment</p></blockquote> <p>随着负载生成 pod 的运行, 可以观察到它. 一开始都在请求目前唯一的 pod. 与此前一样, 度量更新需要一些时间, 但等到它们更新的时候就可以<strong>看到 Autoscaler 增加副本数</strong>了. 在笔者的环境中, pod 的 CPU 使用率一开始升高到了 108%, 使得 Autoscaler 增加 pod 数量到 4. 于是单个 pod 的 CPU 使用率降低到了 74%, 并最终稳定在 26% 左右.</p> <p>可以再次用 kubectl describe 检查 Autoscaler 事件, 看看它都在干嘛(以下代码清单仅仅展示了最关键的部分信息).</p> <p><strong>代码清单-15.5 一个 HPA 的事件列表</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>From    Reason              Message
<span class="token operator">--</span><span class="token operator">--</span>    <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>              <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span>
h<span class="token operator">-</span>p<span class="token operator">-</span>a   SuccessfulRescale   New size<span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">;</span> reason<span class="token operator">:</span> All metrics below target
h<span class="token operator">-</span>p<span class="token operator">-</span>a   SuccessfulRescale   New size<span class="token operator">:</span> <span class="token number">4</span><span class="token punctuation">;</span> reason<span class="token operator">:</span> cpu resource <span class="token function">utilization</span>
                            <span class="token punctuation">(</span>percentage <span class="token keyword">of</span> request<span class="token punctuation">)</span> above target
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>一开始只有一个 pod 时, 平均 CPU 使用率达到了 108%, 超过了 100%, 不知你有没有感觉奇怪? 记着, <mark><strong>容器的 CPU 使用率是它实际的 CPU 使用除以它的 CPU 请求. CPU 请求定义了容器可用的最少而非最多 CPU 资源数量, 因此一个容器可能使用的 CPU 比请求的还要多, 从而使百分比超过 100%</strong></mark> .</p> <p>在进入下一个话题之前, 先做一些简单的计算, 看看 Autoscaler 是如何得出需要 4 个副本的结论的. 最开始只有 1 个副本处理请求, 它的 CPU 使用率飙升到了 108%. 用目标 CPU 使用率百分比 30 去除 108, 得到了3.6; Autoscaler 将它向上取整, 得到了 4. 如果用 4 去除 108, 会得到 27%; 如果 Autoscaler 扩容到 4 个 pod, 它们的平均 CPU 使用率预期应该在 27% 左右, 这很接近目标值 30%, 也跟实际观察到的 CPU 使用率几乎吻合.</p> <blockquote><p>了解伸缩操作的最大速率</p></blockquote> <p>在笔者的实验中, CPU 使用率飙升到了 108%, 但通常来讲, 初始的 CPU 使用率尖峰可能更高. 然而即使初始平均 CPU 使用率确实更高(比方说 150%), 需要 5 个副本才能达到 30% 的目标, Autoscaler 在第一步仍然只会扩容到 4 个 pod. 这是因为 Autoscaler 在<strong>单次扩容操作中可增加的副本数受到限制</strong>. 如果当前副本数大于2, Autoscaler 单次操作至多使副本数翻倍; 如果副本数只有 1 或 2, Autoscaler 最多扩容到 4 个副本.</p> <p>另外, <strong>Autoscaler 两次扩容操作之间的时间间隔也有限制</strong>. 目前, 只有当 3 分钟内没有任何伸缩操作时才会触发扩容, 缩容操作频率更低--5分钟. 记住这一点, 这样当看到度量数据很明显应该触发伸缩却没有触发的时候, 就不会感到奇怪了.</p> <blockquote><p>修改一个已有 HPA 对象的目标度量值</p></blockquote> <p>作为这一节的结束, 再来做最后一个练习. 可能一开始设置的目标值 30% 有点太低了, 下面把它提高到 60%. 可以使用 kubectl edit 命令来完成这项工作. 文本编辑器打开之后, 把 targetAverageUtilization 字段改为60, 如以下代码清单所示.</p> <p><strong>代码清单-15.6 通过编辑 HPA 资源来提高目标 CPU 使用率</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token operator">...</span>
<span class="token literal-property property">spec</span><span class="token operator">:</span>
  <span class="token literal-property property">maxReplicas</span><span class="token operator">:</span> <span class="token number">5</span>
  <span class="token literal-property property">metrics</span><span class="token operator">:</span>
  <span class="token operator">-</span> resource<span class="token operator">:</span>
      <span class="token literal-property property">name</span><span class="token operator">:</span> cpu
      <span class="token literal-property property">targetAverageUtilization</span><span class="token operator">:</span> <span class="token number">60</span>     # <span class="token number">30</span>改成<span class="token number">60</span>
    <span class="token literal-property property">type</span><span class="token operator">:</span> Resource
<span class="token operator">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>正如大多数其他资源一样, 在修改资源之后, <strong>Autoscaler 控制器会检测到这一变更, 并执行相应动作</strong>. 也可以先删除 HPA 资源再用新的值创建一个, 因为删除 HPA 资源只会禁用目标资源的自动伸缩(本例中为一个 Deployment), 而它的伸缩规模会保持在删除资源的时刻. 在为 Deployment 创建一个新的 HPA 资源之后, 自动伸缩过程就会继续进行.</p> <h6 id="_15-1-3-基于内存使用进行自动伸缩"><a href="#_15-1-3-基于内存使用进行自动伸缩" class="header-anchor">#</a> 15.1.3 基于内存使用进行自动伸缩</h6> <p><strong>基于内存的自动伸缩比基于 CPU 的困难很多</strong>. 主要原因在于, 扩容之后原有的 pod 需要有办法<strong>释放内存</strong>. 这只能由应用完成, 系统无法代劳. 系统所能做的只有杀死并重启应用, 希望它能比之前少占用一些内存; 但如果应用使用了跟之前一样多的内存, Autoscaler 就会扩容, 扩容, 再扩容, 直到达到 HPA 资源上配置的最大 pod 数量. 显然没有人想要这种行为. 基于内存使用的自动伸缩在 Kubernetes 1.8 中得到支持, 配置方法与基于 CPU 的自动伸缩完全相同.</p> <h6 id="_15-1-4-基于其他自定义度量进行自动伸缩"><a href="#_15-1-4-基于其他自定义度量进行自动伸缩" class="header-anchor">#</a> 15.1.4 基于其他自定义度量进行自动伸缩</h6> <p>前面看到了基于 CPU 使用率伸缩 pod 有多简单; 最早的时候只有这一种可用的自动伸缩方案. 要使 Autoscaler 使用应用自定义的度量来进行自动伸缩决策, 这一过程十分复杂. 最早的 Autoscaler 设计并不能轻易支持单纯基于 CPU 伸缩以外的场景, 这驱使 Kubernetes 自动伸缩特别小组(SIG)完全<strong>重新设计了 Autoscaler</strong>.</p> <p>如果你好奇最初的 Autoscaler 使用自定义度量究竟有多难, 邀你阅读笔者的博文 &quot;Kubernetes autoscaling based on custom metrics without using a host port&quot;. 你会了解到笔者在尝试配置基于自定义度量自动伸缩的过程中遇到的千辛万苦. 幸运的是新版 Kubernetes 没有这些问题, 笔者会在一篇新的博文中讲述该主题.</p> <p>这里在此不用完整例子展开说明, 而是快速过一下如何配置 Autoscaler 使用不同的度量源. 先观察一下在前一个例子中是如何定义要使用的度量的. 以下代码清单展示了<strong>之前的 HPA 对象是怎么被配置为使用 CPU 使用率度量的</strong>.</p> <p><strong>代码清单-15.7 配置为基于 CPU 自动伸缩的 HorizontalpodAutoscaler 定义</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">maxReplicas</span><span class="token punctuation">:</span> <span class="token number">5</span>
  <span class="token key atrule">metrics</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Resource                   <span class="token comment"># 定义metric类型</span>
    <span class="token key atrule">resource</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> cpu                      <span class="token comment"># 使用情况会被监控的资源</span>
      <span class="token key atrule">targetAverageUtilization</span><span class="token punctuation">:</span> <span class="token number">30</span>   <span class="token comment"># 资源的目标使用量</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>如你所见, metrics 字段允许定义多个度量供使用. 在代码清单中使用了单个度量. 每个条目都指定相应度量的类型--本例中为一个 Resource 度量. 可以在 HPA 对象中使用三种度量:</p> <ul><li>定义 metric 类型</li> <li>使用情况会被监控的资源</li> <li>资源的目标使用量</li></ul> <blockquote><p>了解 Resource 度量类型</p></blockquote> <p>Resource 类型使 Autoscaler 基于一个资源度量做出自动伸缩决策, 在容器的资源请求中指定的那些度量即为一例. 这一类型的使用方式已经看过了, 所以重点关注另外两种类型.</p> <blockquote><p>了解 Pods 度量类型</p></blockquote> <p><strong>Pods 类型用来引用任何其他种类的(包括自定义的)与 pod 直接相关的度量</strong>. 上文提过的每秒查询次数(QPS), 或者消息队列中的消息数量(当消息队列服务运行在 pod 之中)都属于这种度量. 要配置 Autoscaler 使用 pod 的 QPS 度量, HPA 对象的 metrics 字段中就需要包含以下代码清单所示的条目.</p> <p><strong>代码清单-15.8 在 HPA 中引用一个自定义 pod 度量</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">metrics</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Pods                    <span class="token comment"># 定义一个pod度量</span>
    <span class="token key atrule">resource</span><span class="token punctuation">:</span>
      <span class="token key atrule">metricName</span><span class="token punctuation">:</span> qps             <span class="token comment"># 度量的名称</span>
      <span class="token key atrule">targetAverageValue</span><span class="token punctuation">:</span> <span class="token number">100</span>     <span class="token comment"># 所有被涵盖pod内的目标平均值</span>
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>代码清单中的示例配置 Autoscaler, 使该 HPA 控制的 ReplicaSet(或其他)控制器下所辖 pod 的平均 QPS 维持 100 的水平.</p> <blockquote><p>了解 Object 度量类型</p></blockquote> <p><strong>Object 度量类型被用来让 Autoscaler 基于并非直接与 pod 关联的度量来进行伸缩</strong>. 比方说, 你可能希望基于另一个集群对象, 比如 Ingress 对象, 来伸缩你的 pod. 这度量可能是代码清单 15.8 中的 QPS, 可能是平均请求延迟, 或者完全是不相干的其他东西.</p> <p>与此前的例子不同, 使用 Object 度量类型时, Autoscaler 只会从这单个对象中获取单个度量数据; 在此前的例子中, Autoscaler 需要从所有下属 pod 中获取度量, 并使用它们的平均值. 你需要在 HPA 对象的定义中指定目标对象与目标值. 以下代码清单即为一例.</p> <p><strong>代码清单-15.9 在 HPA 中引用其他对象的度量</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token punctuation">...</span>
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">metrics</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Object                           <span class="token comment"># 使用某个特定对象的度量</span>
    <span class="token key atrule">resource</span><span class="token punctuation">:</span>
      <span class="token key atrule">metricName</span><span class="token punctuation">:</span> latencyMillis            <span class="token comment"># 度量的名称</span>
      <span class="token key atrule">target</span><span class="token punctuation">:</span>
        <span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1     <span class="token comment"># Autoscaler需要从中获取独立的特定对象</span>
        <span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress                  
        <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend                 
      <span class="token key atrule">targetValue</span><span class="token punctuation">:</span> <span class="token number">20</span>                      <span class="token comment"># Autoscaler应该使该度量尽量接近这个值</span>
  <span class="token key atrule">scaleTargetRef</span><span class="token punctuation">:</span>                          <span class="token comment"># Autoscaler将要管理的可伸缩资源</span>
    <span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1     
    <span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment                   
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia                          
<span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>该例中 HPA 被配置为使用 Ingress 对象 frontend 的 latencyMillis 度量, 目标值为 20. HPA 会监控该 Ingress 对象的度量, 如果该度量超过了目标值太多, autoscaler 便会对 kubia Deployment 资源进行扩容了.</p> <h6 id="_15-1-5-确定哪些度量适合用于自动伸缩"><a href="#_15-1-5-确定哪些度量适合用于自动伸缩" class="header-anchor">#</a> 15.1.5 确定哪些度量适合用于自动伸缩</h6> <p>你要明白, <strong>不是所有度量都适合作为自动伸缩的基础</strong>. 正如之前提到的, pod 中容器的内存占用并不是自动伸缩的一个好度量. <mark><strong>如果增加副本数不能导致被观测度量平均值的线性(或者至少接近线性)下降, 那么 autoscaler 就不能正常工作</strong></mark>.</p> <p>比方说, 如果只有一个 pod 实例, 度量数值为 X, 这时 autoscaler 扩容到了 2 个副本, 度量数值就需要落在接近 X/2 的位置. 每秒查询次数(QPS)就是这么一种自定义度量, 对 web 应用而言即为应用每秒接收的请求数. 增大副本数总会导致 QPS 成比例下降, 因为同样多的请求数现在被更多数量的 pod 处理了.</p> <p><strong>在决定基于应用自有的自定义度量来伸缩它之前, 一定要思考 pod 数量增加或减少时, 它的值会如何变化.</strong></p> <h6 id="_15-1-6-缩容到0个副本"><a href="#_15-1-6-缩容到0个副本" class="header-anchor">#</a> 15.1.6 缩容到0个副本</h6> <p>HPA 目前不允许设置 minReplicas 字段为 0, 所以 <strong>autoscaler 永远不会缩容到 0 个副本, 即便 pod 什么都没做也不会</strong>. 允许 pod 数量缩容到 0 可以大幅提升硬件利用率: 如果你运行的服务几个小时甚至几天才会收到一次请求, 就没有道理留着它们一直运行, 占用本来可以给其他服务利用的资源; 然而一旦客户端请求进来了, 你仍然还想让这些服务马上可用.</p> <p>这叫空载(idling)与解除空载(un-idling), 即允许提供特定服务的 pod 被缩容到 0 副本. 在新的请求到来时, 请求会先被阻塞, 直到 pod 被启动, 从而请求被转发到新的 pod 为止.</p> <p><strong>Kubernetes 目前暂时没有提供这个特性, 但在未来会实现</strong>. 可以检查 Kubernetes 文档来看看空载特性有没有被实现.</p> <h5 id="_15-2-pod的纵向自动伸缩"><a href="#_15-2-pod的纵向自动伸缩" class="header-anchor">#</a> 15.2 pod的纵向自动伸缩</h5> <p>横向伸缩很棒, 但并不是所有应用都能被横向伸缩. 对这些应用而言, 唯一的选项是<strong>纵向伸缩--给它们更多 CPU 和(或)内存</strong>. 因为一个节点所拥有的资源通常都比单个 pod 请求的要多, 所以应该几乎总能纵向扩容一个 pod, 对不对?</p> <p>因为 pod 的资源请求是通过 pod manifest 的字段配置的, 纵向伸缩 pod 将会通过改变这些字段来实现. 笔者这里说的是 &quot;将会&quot;, 因为目前还不可能改变已有 pod 的资源请求和限制. 在笔者动笔之前(已经是一年多之前了), 笔者确信等到写到这一章的时候, Kubernetes 应该已经支持靠谱的纵向 pod 自动伸缩了, 因此笔者在目录计划中包含了这部分内容.</p> <h6 id="_15-2-1-自动配置资源请求"><a href="#_15-2-1-自动配置资源请求" class="header-anchor">#</a> 15.2.1 自动配置资源请求</h6> <p>这是一个实验性的特性, 如果新创建的 pod 的容器没有明确设置 CPU 与内存请求, 该特性即会代为设置. 这一特性由一个叫作 InitialResources 的准入控制(Admission Control)插件提供. 当一个没有资源请求的 pod 被创建时, 该插件会根据 pod 容器的历史资源使用数据(随容器镜像, tag 而变)来设置资源请求.</p> <p>可以不用指定资源请求就部署 pod, 而靠 Kubernetes 来最终得出每个容器的资源需求有多少. 实质上, Kubernetes 是在纵向伸缩这些 pod. 比方说, 如果一个容器总是内存不足, 下次创建一个包含该容器镜像的 pod 的时候, 它的内存资源请求就会被自动调高了.</p> <h6 id="_15-2-2-修改运行中pod的资源请求"><a href="#_15-2-2-修改运行中pod的资源请求" class="header-anchor">#</a> 15.2.2 修改运行中pod的资源请求</h6> <p>有朝一日, 同样的机制也会用于修改已有 pod 的资源请求, 这意味着在 pod 运行的同时也可以被该机制纵向伸缩. 在笔者写作此节之时, 一份新的纵向 pod 自动伸缩提案正在定稿中. 请参考 Kubernetes 文档来检查纵向 pod 自动伸缩实现了没有.</p> <h5 id="_15-3-集群节点的横向伸缩"><a href="#_15-3-集群节点的横向伸缩" class="header-anchor">#</a> 15.3 集群节点的横向伸缩</h5> <p>HPA 在需要的时候会创建更多的 pod 实例. 但<strong>万一所有的节点都满了, 放不下更多 pod 了, 怎么办</strong>? 显然这个问题并不局限于 Autoscaler 创建新 pod 实例的场景. 即便是手动创建 pod, 也可能碰到因为资源被已有 pod 使用殆尽, 以至于没有节点能接收新 pod 的情况.</p> <p>在这种情况下, <strong>需要删除一些已有的 pod, 或者纵向缩容它们, 抑或向集群中添加更多节点</strong>. 如果你的 Kubernetes 集群运行在自建(on premise)基础架构上, 你得添加一台物理机, 并将其加入集群. 但如果你的集群运行于云端基础架构之上, 添加新的节点通常就是点击几下鼠标, 或者向云端做 API 调用. 这可以自动化的, 对吧?</p> <p>Kubernetes 支持在需要时<strong>立即自动从云服务提供者请求更多节点</strong>. 该特性由 Cluster Autoscaler 执行.</p> <h6 id="_15-3-1-cluster-autoscaler介绍"><a href="#_15-3-1-cluster-autoscaler介绍" class="header-anchor">#</a> 15.3.1 Cluster Autoscaler介绍</h6> <p>Cluster Autoscaler 负责在由于节点资源不足, 而无法调度某 pod 到已有节点时, 自动部署新节点. 它也会在节点长时间使用率低下的情况下下线节点.</p> <blockquote><p>从云端基础架构请求新节点</p></blockquote> <p>如果在一个 pod 被创建之后, Scheduler 无法将其调度到任何一个已有节点, 一个新节点就会被创建. Cluster Autoscaler 会注意此类 pod, 并请求云服务提供者启动一个新节点. 但在这么做之前, 它会检查新节点有没有可能容纳这个(些)pod, 毕竟如果新节点本来就不可能容纳它们, 就没必要启动这么一个节点了.</p> <p><strong>云服务提供者通常把相同规格(或者有相同特性)的节点聚合成组</strong>. 因此 Cluster Autoscaler 不能单纯地说 &quot;给我多一个节点&quot;, 它还需要指明节点类型.</p> <p>Cluster Autoscaler 通过检查可用的节点分组来确定是否有至少一种节点类型能容纳未被调度的 pod. 如果只存在唯一一个此种节点分组, Cluster Autoscaler 就可以增加节点分组的大小, 让云服务提供商给分组中增加一个节点. 但如果存在多个满足条件的节点分组, Cluster Autoscaler 就必须挑一个最合适的. 这里 &quot;最合适&quot; 的精确含义显然必须是可配置的. 在最坏的情况下, 它会随机挑选一个. 图 15.5 简单描述了 Cluster Autoscaler 面对一个不可调度 pod 时是如何反应的.</p> <p>新节点启动后, 其上运行的 Kubelet 会联系 API 服务器, 创建一个 Node 资源以注册该节点. 从这一刻起, 该节点即成为 Kubernetes 集群的一部分, 可以调度 pod 于其上了.</p> <p>简单吧? 那么缩容呢?</p> <p><img src="/img/image-20240228092615-gtd2ipr.png" alt="image" title="图15.5 Cluster Autoscaler 发现不可调度到已有节点的 pod 时会触发集群扩容"></p> <blockquote><p>归还节点</p></blockquote> <p><strong>当节点利用率不足时, Cluster Autoscaler 也需要能够减少节点的数目</strong>. Cluster Autoscaler 通过监控所有节点上请求的 CPU 与内存来实现这一点. 如果某个节点上所有 pod 请求的 CPU, 内存都不到 50%, 该节点即被认定为不再需要.</p> <p>这并不是决定是否要归还某一节点的唯一因素. Cluster Autoscaler 也会检查是否有系统 pod(仅仅)运行在该节点上(这并不包括每个节点上都运行的服务, 比如 DaemonSet 所部署的服务). 如果节点上有系统 pod 在运行, 该节点就不会被归还. 对非托管 pod, 以及有本地存储的 pod 也是如此, 否则就会造成这些 pod 提供的服务中断. 换句话说, 只有当 Cluster Autoscaler 知道节点上运行的 pod 能够重新调度到其他节点, 该节点才会被归还.</p> <p>当一个节点被选中下线, 它首先会被标记为不可调度, 随后运行其上的 pod 将被疏散至其他节点. 因为所有这些 pod 都属于 ReplicaSet 或者其他控制器, 它们的替代 pod 会被创建并调度到其他剩下的节点(这就是为何正被下线的节点要先标记为不可调度的原因).</p> <blockquote><p>手动标记节点为不可调度, 排空节点</p></blockquote> <p>节点也可以手动被标记为不可调度并排空. 不涉及细节, 这些工作可用以下 kubectl 命令完成:</p> <ul><li>​<code>kubectl cordon &lt;node&gt;</code>​ 标记节点为不可调度(但对其上的 pod 不做任何事).</li> <li>​<code>kubectl drain &lt;node&gt;</code>​ 标记节点为不可调度, 随后疏散其上所有 pod.</li></ul> <p>两种情形下, 在你用 <code>kubectl uncordon &lt;node&gt;</code>​ 解除节点的不可调度状态之前, 不会有新 pod 被调度到该节点.</p> <h6 id="_15-3-2-启用cluster-autoscaler"><a href="#_15-3-2-启用cluster-autoscaler" class="header-anchor">#</a> 15.3.2 启用Cluster Autoscaler</h6> <p><strong>集群自动伸缩</strong>在以下云服务提供商可用:</p> <ul><li>Google Kubernetes Engine(GKE)</li> <li>Google Compute Engine(GCE)</li> <li>Amazon Web Services(AWS)</li> <li>Microsoft Azure</li></ul> <p>启动 Cluster Autoscaler 的方式取决于你的 Kubernetes 集群运行在哪.</p> <h5 id="_15-4-本章小结"><a href="#_15-4-本章小结" class="header-anchor">#</a> 15.4 本章小结</h5> <p>本章展示了 Kubernetes 能够如何伸缩 pod 以及节点. 你学到了</p> <ul><li><strong>配置 pod 的自动横向伸缩很简单, 只要创建一个 HorizontalpodAutoscaler 对象, 将它指向一个 Deployment, ReplicaSet, ReplicationController, 并设置 pod 的目标 CPU 使用率即可.</strong></li> <li>除了让 HPA 基于 pod 的 CPU 使用率进行伸缩操作, 还可以配置它基于应用自身提供的自定义度量, 或者在集群中部署的其他对象的度量来自动伸缩.</li> <li>目前还不能进行纵向 pod 自动伸缩.</li> <li>如果你的 Kubernetes 集群运行在支持的云服务提供者之上, 甚至集群节点也可以自动伸缩.</li> <li>可以用带有 -it 和 --rm 选项的 kubectl run 命令在 pod 中运行一次性的进程, 并在按下 CTRL+C 组合键时自动停止并删除该临时 pod.</li></ul> <p>下一章中将探索一些高级调度特性. 例如, 不让某些 pod 运行在特定节点上, 将一些 pod 紧密或者分开调度.</p> <h4 id="_16-高级调度"><a href="#_16-高级调度" class="header-anchor">#</a> 16.高级调度</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>使用节点污点和 pod 容忍度组织 pod 调度到特定节点</li> <li>将节点亲缘性规则作为节点选择器的—种替代</li> <li>使用节点亲缘性进行多个 pod 的共同调度</li> <li>使用节点非亲缘性来分离多个 pod</li></ul> <p><strong>Kubernetes 允许你去影响 pod 被调度到哪个节点</strong>. 起初, 只能通过在 pod 规范里指定节点选择器来实现, 后面其他的机制逐渐加入来扩容这项功能, 本章将包括这些内容.</p> <h5 id="_16-1-使用污点和容忍度阻止节点调度到特定节点"><a href="#_16-1-使用污点和容忍度阻止节点调度到特定节点" class="header-anchor">#</a> 16.1 使用污点和容忍度阻止节点调度到特定节点</h5> <p>首先要介绍的高级调度的两个特性是<mark><strong>节点污点(taints), 以及 pod 对于污点的容忍度, 这些特性被用于限制哪些 pod 可以被调度到某一个节点. 只有当一个 pod 容忍某个节点的污点, 这个 pod 才能被调度到该节点.</strong></mark></p> <p>这与使用节点选择器和节点亲缘性有些许不同, 本章后面部分会介绍到. <strong>节点选择器和节点亲缘性规则, 是通过明确的在 pod 中添加的信息, 来决定一个 pod 可以或者不可以被调度到哪些节点上. 而污点则是在不修改已有 pod 信息的前提下, 通过在节点上添加污点信息, 来拒绝 pod 在某些节点上的部署</strong>.</p> <h6 id="_16-1-1-介绍污点和容忍度"><a href="#_16-1-1-介绍污点和容忍度" class="header-anchor">#</a> 16.1.1 介绍污点和容忍度</h6> <p>学习节点污点的最佳路径就是看一个已有的污点. 附录 B 展示了如何使用 kubeadm 工具去设置一个多节点的集群, 默认情况下, 这样一个集群中的<strong>主节点需要设置污点, 这样才能保证只有控制面板 pod 才能部署在主节点上</strong>.</p> <blockquote><p>显示节点的污点信息</p></blockquote> <p>可以通过 kubectl describe node 查看节点的污点信息, 如以下代码清单所示.</p> <p><strong>代码清单-16.1 显示通过 kubeadm 创建的集群中的主节点信息</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe <span class="token function">node</span> master.k8s
Name:         master.k8s
Role:
Labels:       beta.kubernetes.io/arch<span class="token operator">=</span>amd64
              beta.kubernetes.io/os<span class="token operator">=</span>linux
              kubernetes.io/hostname<span class="token operator">=</span>master.k8s
              node-role.kubernetes.io/master<span class="token operator">=</span>
Annotations:  node.alpha.kubernetes.io/ttl<span class="token operator">=</span><span class="token number">0</span>
              volumes.kubernetes.io/controller-managed-attach-detach<span class="token operator">=</span>true
Taints:       node-role.kubernetes.io/master:NoSchedule   <span class="token comment"># 主节点包含一个污点</span>
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>主节点包含一个污点, 污点包含了一个 key, value, 以及一个 effect, 表现为 <code>&lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</code>​. 上面显示的主节点的污点信息, 包含一个为 noderole.kubernetes.io/master 的 key, 一个空的 value, 以及值为 NoSchedule 的 effect.</p> <p><strong>这个污点将阻止 pod 调度到这个节点上面, 除非有 pod 能容忍这个污点, 而通常容忍这个污点的 pod 都是系统级别 pod</strong>(见图 16.1).</p> <p><img src="/img/image-20240228092706-mqfgtcd.png" alt="image" title="图16.1 一个 pod 只有容忍了节点的污点, 才能被调度到该节点上面"></p> <blockquote><p>显示 pod 的污点容忍度</p></blockquote> <p>在一个通过 kubeadm 初始化的集群中, kube-proxy 集群组件以 pod 的形式运行在每个节点上, 其中也包括主节点. 因为以 pod 形式运行的主节点组件同时也需要访问 Kubernetes 服务. <strong>为了确保 kube-proxy pod 也能够运行在主节点上, 该 pod 需要添加相应的污点容忍度</strong>. 该 pod 整体包含了 3 个污点容忍度, 如以下代码清单所示.</p> <p><strong>代码清单-16.2 一个 pod 的污点容忍度</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe po kube-proxy-80wqm <span class="token parameter variable">-n</span> kube-system
<span class="token punctuation">..</span>.
Tolerations:    node-role.kubernetes.io/master<span class="token operator">=</span>:NoSchedule
                node.alpha.kubernetes.io/notReady<span class="token operator">=</span>:Exists:NoExecute
                node.alpha.kubernetes.io/unreachable<span class="token operator">=</span>:Exists:NoExecute
<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>如你所见, <mark><strong>第一个污点容忍度匹配了主节点的污点, 表示允许这个 kube-proxy pod 被调度到主节点上</strong></mark>.</p> <p>注意: 尽管在 pod 的污点容忍度中显示了等号, 但是在节点的污点信息中却没有. 当污点或者污点容忍度中的 value 为 null 时, kubectl 故意将污点和污点容忍度进行不同形式的显示.</p> <blockquote><p>了解污点的效果</p></blockquote> <p>另外两个在 kube-proxy pod 上的污点定义了<strong>当节点状态是没有 ready 或者是 unreachable 时, 该 pod 允许运行在该节点多长时间</strong>(时间用秒来表示, 这里没有显示, 但是在 pod YAML 中可以看到). 这两个污点容忍度使用的效果是 <strong>NoExecute</strong> 而不是 NoSchedule.</p> <p>每一个污点都可以关联一个效果, 效果包含了以下三种:</p> <ul><li><strong>NoSchedule 表示如果 pod 没有容忍这些污点, pod 则不能被调度到包含这些污点的节点上.</strong></li> <li><strong>PreferNoSchedule 是 NoSchedule 的一个宽松的版本, 表示尽量阻止 pod 被调度到这个节点上, 但是如果没有其他节点可以调度, pod 依然会被调度到这个节点上.</strong></li> <li><strong>NoExecute 不同于 NoSchedule 以及 PreferNoSchedule, 后两者只在调度期间起作用, 而 NoExecute 也会影响正在节点上运行着的 pod. 如果在一个节点上添加了 NoExecute 污点, 那些在该节点上运行着的 pod, 如果没有容忍这个 NoExecute 污点, 将会从这个节点去除.</strong></li></ul> <h6 id="_16-1-2-在节点上添加自定义污点"><a href="#_16-1-2-在节点上添加自定义污点" class="header-anchor">#</a> 16.1.2 在节点上添加自定义污点</h6> <p>假设你有一个单独的 Kubernetes 集群, 上面同时有<strong>生产环境和非生产环境的流量</strong>. 其中最重要的一点是, <mark><strong>非生产环境的 pod 不能运行在生产环境的节点上. 可以通过在生产环境的节点上添加污点来满足这个要求</strong></mark>, 可以使用 kubectl taint 命令来添加污点:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl taint <span class="token function">node</span> node1.k8s node-type<span class="token operator">=</span>production:NoSchedule
<span class="token function">node</span> <span class="token string">&quot;node1.k8s&quot;</span> tainted
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这个命令添加了一个 taint, 其中, key 为 node-type, value 为 production, 效果为 NoSchedule. 如果现在部署一个常规 pod 的多个副本, 就会发现没有一个 pod 被部署到添加了污点信息的节点上面, 如以下代码清单所示.</p> <p><strong>代码清单-16.3 部署没有污点容忍度的 pod</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl run test <span class="token operator">--</span>image busybox <span class="token operator">--</span>replicas <span class="token number">5</span> <span class="token operator">--</span> sleep <span class="token number">99999</span>
deployment <span class="token string">&quot;test&quot;</span> created

$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>o wide
<span class="token constant">NAME</span>                <span class="token constant">READY</span>  <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>   <span class="token constant">AGE</span>   <span class="token constant">IP</span>          <span class="token constant">NODE</span>
test<span class="token operator">-</span><span class="token number">196686</span><span class="token operator">-</span>46ngl   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>          12s   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.1</span>   node2<span class="token punctuation">.</span>k8s
test<span class="token operator">-</span><span class="token number">196686</span><span class="token operator">-</span>73p89   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>          12s   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.7</span>   node2<span class="token punctuation">.</span>k8s
test<span class="token operator">-</span><span class="token number">196686</span><span class="token operator">-</span><span class="token number">77280</span>   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>          12s   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.6</span>   node2<span class="token punctuation">.</span>k8s
test<span class="token operator">-</span><span class="token number">196686</span><span class="token operator">-</span>h9m8f   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>          12s   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.5</span>   node2<span class="token punctuation">.</span>k8s
test<span class="token operator">-</span><span class="token number">196686</span><span class="token operator">-</span>p85ll   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>          12s   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.4</span>   node2<span class="token punctuation">.</span>k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>现在, <strong>没人能够随意地将 pod 部署到生产环境节点上</strong>了.</p> <h6 id="_16-1-3-在pod上添加污点容忍度"><a href="#_16-1-3-在pod上添加污点容忍度" class="header-anchor">#</a> 16.1.3 在pod上添加污点容忍度</h6> <p><strong>为了将生产环境 pod 部署到生成环境节点上, pod 需要能容忍那些你添加在节点上的污点</strong>. 生产环境 pod 的清单里面需要增加以下的 YAML 代码片段.</p> <p><strong>代码清单-16.4 标记污点容忍度的生产环境部署: production-deployment.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> prod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">5</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token punctuation">...</span>
      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>type     <span class="token comment"># 此处的污点容忍度允许pod被调度到生产环境节点上</span>
        <span class="token key atrule">Operator</span><span class="token punctuation">:</span> Equal    
        <span class="token key atrule">value</span><span class="token punctuation">:</span> production  
        <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>如果运行了这个部署, 将看到 pod 就会被调度到生产环境节点, 如以下代码清单所示.</p> <p><strong>代码清单-16.5 包含污点容忍度的 pod 被调度到生产环境节点 node1</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-o</span> wide
NAME                READY  STATUS    RESTARTS   AGE   IP          NODE
prod-350605-1ph5h   <span class="token number">0</span>/1    Running   <span class="token number">0</span>          16s   <span class="token number">10.44</span>.0.3   node1.k8s
prod-350605-ctqcr   <span class="token number">1</span>/1    Running   <span class="token number">0</span>          16s   <span class="token number">10.47</span>.0.4   node2.k8s
prod-350605-f7pcc   <span class="token number">0</span>/1    Running   <span class="token number">0</span>          17s   <span class="token number">10.44</span>.0.6   node1.k8s
prod-350605-k7c8g   <span class="token number">1</span>/1    Running   <span class="token number">0</span>          17s   <span class="token number">10.47</span>.0.9   node2.k8s
prod-350605-rp1nv   <span class="token number">0</span>/1    Running   <span class="token number">0</span>          17s   <span class="token number">10.44</span>.0.4   node1.k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>正如你所见, 生产环境 pod 也被调度到了非生产环境 node2. 为了防止这种情况发生, <strong>也需要在非生产环境的节点设置污点信息</strong>, 例如 <code>node-type=nonproduction:NoSchedule</code>​. 那么, 你也需要在非生产环境 pod 上添加了对应的污点容忍度.</p> <h6 id="_16-1-4-了解污点和污点容忍度的使用场景"><a href="#_16-1-4-了解污点和污点容忍度的使用场景" class="header-anchor">#</a> 16.1.4 了解污点和污点容忍度的使用场景</h6> <p><mark><strong>节点可以拥有多个污点信息, 而 pod 也可以有多个污点容忍度</strong></mark>. 正如你所见, 污点可以只有一个 key 和一个效果, 而不必设置 value. <strong>污点容忍度可以通过设置 Equal 操作符 Equal 操作符来指定匹配的 value(默认情况下的操作符), 或者也可以通过设置 Exists 操作符来匹配污点的 key</strong>.</p> <blockquote><p>在调度时使用污点和容忍度</p></blockquote> <p><mark><strong>污点可以用来组织新 pod 的调度(使用 NoSchedule 效果), 或者定义非优先调度的节点(使用 PreferNoSchedule 效果), 甚至是将已有的 pod 从当前节点剔除.</strong></mark></p> <p>可以用任何你觉得合适的方式去设置污点和容忍度. 例如, 可以将一个集群分成多个部分, 只允许开发团队将 pod 调度到他们特定的节点上. 当部分节点提供了某种特殊硬件, 并且只有部分 pod 需要使用到这些硬件的时候, 也可以通过设置污点和容忍度的方式来实现.</p> <blockquote><p>配置节点失效之后的 pod 重新调度最长等待时间</p></blockquote> <p>也可以配置一个容忍度, 用于当某个 pod 运行所在的节点变成 unready 或者 unreachable 状态时, Kubernetes 可以等待该 pod 被调度到其他节点的最长等待时间. 如果查看其中一个 pod 的容忍度信息, 将看到两条容忍度信息, 如以下代码清单所示.</p> <p><strong>代码清单-16.6 带有默认容忍度的 pod</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po prod-350605-1ph5h <span class="token parameter variable">-o</span> yaml
<span class="token punctuation">..</span>.
  tolerations:
  - effect: NoExecute       <span class="token comment"># 该pod允许所在节点处于notReady状态为300秒, 之后pod将被重新调度</span>
    key: node.alpha.kubernetes.io/notReady     
    operator: Exists                           
    tolerationSeconds: <span class="token number">300</span>                     
  - effect: NoExecute       <span class="token comment"># 同样的配置应用于节点处于unreachable状态</span>
    key: node.alpha.kubernetes.io/unreachable  
    operator: Exists                           
    tolerationSeconds: <span class="token number">300</span>                      
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>这两个容忍度表示, 该 pod 将容忍所在节点处于 notReady 或者 unreachable 状态维持 300 秒. 当 Kubernetes 控制器检测到有节点处于 notReady 或者 unreachable 状态时, 将会等待 300 秒, 如果状态持续的话, 之后将把该 pod 重新调度到其他节点上</strong>.</p> <p>当没有定义这两个容忍度时, 他们会<strong>自动添加</strong>到 pod 上. 如果你觉得对于你的 pod 来说, 5 分钟太长的话, 可以在 pod 描述中显式地将这两个容忍度设置得更短一些.</p> <h5 id="_16-2-使用节点亲缘性将pod调度到特定节点上"><a href="#_16-2-使用节点亲缘性将pod调度到特定节点上" class="header-anchor">#</a> 16.2 使用节点亲缘性将pod调度到特定节点上</h5> <p>正如目前所学到的, <strong>污点可以用来让 pod 远离特定的节点</strong>. 下面将学习一种更新的机制, 叫作<mark><strong>节点亲缘性(node affinity), 这种机制允许你通知 Kubernetes 将 pod 只调度到某个几点子集上面</strong></mark>.</p> <blockquote><p>对比节点亲缘性和节点选择器</p></blockquote> <p>在早期版本的 Kubernetes 中, 初始的节点亲缘性机制, 就是 pod 描述中的 nodeSelector 字段. 节点必须包含所有 pod 对应字段中的指定 label, 才能成为 pod 调度的目标节点.</p> <p>节点选择器实现简单, 但是它不能满足你的所有需求. 正因为如此, 一种更强大的机制被引入. <strong>节点选择器最终会被弃用, 所以现在了解新的节点亲缘性机制就变得重要起来</strong>.</p> <p>与节点选择器类似, <strong>每个 pod 可以定义自己的节点亲缘性规则</strong>. 这些规则可以指定硬性限制或者偏好. 如果指定一种偏好的话, 你<mark><strong>将告知 Kubernetes 对于某个特定的 pod, 它更倾向于调度到某些节点上, 之后 Kubernetes 将尽量把这个 pod 调度到这些节点上面. 如果没法实现的话, pod 将被调度到其他某个节点上</strong></mark>.</p> <blockquote><p>检查默认的节点标签</p></blockquote> <p><strong>节点亲缘性根据节点的标签来进行选择, 这点跟节点选择器是一致的</strong>. 当了解到如何使用节点亲缘性之后, 下面来检查下一个 Google Kubernetes 引擎集群(GKE)中节点的标签, 来看一下它们默认的标签是什么, 如以下代码清单所示.</p> <p><strong>代码清单-16.7 GKE 节点的默认标签</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe <span class="token function">node</span> gke-kubia-default-pool-db274c5a-mjnf
Name:     gke-kubia-default-pool-db274c5a-mjnf
Role:
Labels:   beta.kubernetes.io/arch<span class="token operator">=</span>amd64
          beta.kubernetes.io/fluentd-ds-ready<span class="token operator">=</span>true
          beta.kubernetes.io/instance-type<span class="token operator">=</span>f1-micro
          beta.kubernetes.io/os<span class="token operator">=</span>linux
          cloud.google.com/gke-nodepool<span class="token operator">=</span>default-pool
          failure-domain.beta.kubernetes.io/region<span class="token operator">=</span>europe-west1 <span class="token comment"># 这三个标签对于节点亲缘性来说最为重要    </span>
          failure-domain.beta.kubernetes.io/zone<span class="token operator">=</span>europe-west1-d     
          kubernetes.io/hostname<span class="token operator">=</span>gke-kubia-default-pool-db274c5a-mjnf   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>这个节点有很多标签, 但涉及节点亲缘性和 pod 亲缘性时, 最后三个标签是最重要的</strong>. 这三个标签的含义如下:</p> <ul><li>failure-domain.beta.kubernetes.io/region 表示该节点所在的<strong>地理地域</strong>.</li> <li>failure-domain.beta.kubernetes.io/zone 表示该节点所在的<strong>可用性区域</strong>(availability zone).</li> <li>kubernetes.io/hostname 很显然是该<strong>节点的主机名</strong>.</li></ul> <p><strong>这三个以及其他标签, 将被用于 pod 亲缘性规则</strong>. 在第三章中已经学会如何给一个节点添加自定义标签, 并且在 pod 的节点选择器中使用它. 可以通过给 pod 加上节点选择器的方式, 将节点部署到含有这个自定义标签的节点上. 下面将学习到怎么用节点亲缘性规则实现同样的功能.</p> <h6 id="_16-2-1-指定强制性节点亲缘性规则"><a href="#_16-2-1-指定强制性节点亲缘性规则" class="header-anchor">#</a> 16.2.1 指定强制性节点亲缘性规则</h6> <p>在第 3 章的例子中, 使用了节点选择器使得需要 GPU 的 pod 只被调度到有 GPU 的节点上. 包含了 nodeSelector 字段的 pod 描述如以下代码清单所示.</p> <p><strong>代码清单-16.8 使用了节点选择器的 pod:kubia-gpu-nodeselector.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>gpu
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>     <span class="token comment"># 这个pod会被调度到包含了gpu=ture标签的节点上</span>
    <span class="token key atrule">gpu</span><span class="token punctuation">:</span> <span class="token string">&quot;true&quot;</span>        
  <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>nodeSelector 字段表示, pod 只能被部署在包含了 gpu=true 标签的节点上. 如果<strong>将节点选择器替换为节点亲缘性规则</strong>, pod 定义将会如以下代码清单所示.</p> <p><strong>代码清单-16.9 使用了节点亲缘性规则的 pod:kubia-gpu-nodeaffinity.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia<span class="token punctuation">-</span>gpu
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
    <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>    <span class="token comment"># 节点亲缘性配置</span>
      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>
        <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> gpu
            <span class="token key atrule">operator</span><span class="token punctuation">:</span> In
            <span class="token key atrule">values</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token string">&quot;true&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>首先可以注意到的是, 这种写法比简单的节点选择器要复杂得多, 但这是因为它的<strong>表达能力更强</strong>, 下面详细看一下这个规则.</p> <blockquote><p>较长的节点亲缘性属性名的意义</p></blockquote> <p>正如你所看到的, 这个 pod 的描述<strong>包含了 affinity 字段, 该字段又包含了 nodeAffinity 字段</strong>, 这个字段有一个极其长的名字, 所以先重点关注这个.</p> <p>把这个名字分成两部分, 然后分别看下它们的含义:</p> <ul><li>​<code>requiredDuringScheduling...</code>​ 表明了该字段下定义的规则, 为了让 pod 能调度到该节点上, <strong>明确指出了该节点必须包含的标签</strong>.</li> <li>​<code>...IgnoredDuringExecution</code>​ 表明了该字段下定义的规则, <strong>不会影响已经在节点上运行着的 pod</strong>.</li></ul> <p>目前, 当你知道当前的<strong>亲缘性规则只会影响正在被调度的 pod, 并且不会导致已经在运行的 pod 被剔除</strong>时, 情况可能会更简单一些. 这就是为什么目前的规则都是以 IgnoredDuringExecution 结尾的. 最终, Kubernetes 也会支持 RequiredDuringExecution, 表示如果去除掉节点上的某个标签, <strong>那些需要节点包含该标签的 pod 将会被剔除</strong>. 正如笔者所说, Kubernetes 目前还不支持次特性. 所以可以暂时不去关心这个长字段的第二部分.</p> <blockquote><p>了解节点选择器条件</p></blockquote> <p>记住上一节所解释的内容, 将更容易理解 nodeSelectorTerms 和 matchExpressions 字段, 这两个字段定义了<strong>节点的标签必须满足哪一种表达式, 才能满足 pod 调度的条件</strong>. 样例中的单个表达式比较容易理解, 节点必须包含一个叫作 gpu 的标签, 并且这个标签的值必须是 true.</p> <p>因此, 这个 pod 只会被调度到包含 gpu=true 的节点上, 如图 16.2 所示.</p> <p><img src="/img/image-20240226214841-qbzfxgo.png" alt="image" title="图16.2 一个 pod 的节点亲缘性指定了节点必须包含哪些标签才能满足 pod 调度的条件"></p> <p>现在, 更有趣的部分来了, <strong>节点亲缘性也可以在调度时指定节点的优先级</strong>, 这将在接下来的部分看到.</p> <h6 id="_16-2-2-调度pod时优先考虑某些节点"><a href="#_16-2-2-调度pod时优先考虑某些节点" class="header-anchor">#</a> 16.2.2 调度pod时优先考虑某些节点</h6> <p>最近介绍的<strong>节点亲缘性的最大好处就是,</strong> <mark><strong>当调度某一个 pod 时, 指定调度器可以优先考虑哪些节点</strong></mark>​ <strong>, 这个功能是通过 preferredDuringSchedulingIgnored DuringExecution 字段来实现的</strong>.</p> <p>想象一下你拥有一个跨越多个国家的多个数据中心, 每一个数据中心代表了一个单独的可用性区域. 在每个区域中, 你有一些特定的机器, 只提供给你自己或者你的合作公司使用. 现在, 你想要部署一些 pod, <strong>希望将 pod 优先部署</strong>在区域 zone1, 并且是为你公司部署预留的机器上. 如果你的机器没有足够的空间用于这些 pod, 或者出于其他一些重要的原因不希望这些 pod 调度到上面, 那么就会调度到其他区域的其他机器上面, 这种情况你也是可以接受的. 节点亲缘性就可以实现这样的功能.</p> <blockquote><p>给节点加上标签</p></blockquote> <p><strong>首先, 节点必须加上合适的标签. 每个节点需要包含两个标签, 一个用于表示所在的这个节点所归属的可用性区域, 另一个用于表示这是一个独占的节点还是一个共享的节点.</strong></p> <p>附录 B 解释了如何在本地 VM 中设置一个三个节点的集群(一个主节点和两个工作节点). 在接下来的例子中, 笔者将使用这个集群中的两个工作节点, 当然也可以使用 GKE 或者其他多节点的集群. Minikube 并不是运行这些例子的最佳选择, 因为它只运行了一个节点.</p> <p>首先, 给这些节点加上标签, 如以下代码清单所示.</p> <p><strong>代码清单-16.10 给节点加上标签</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl label node node1<span class="token punctuation">.</span>k8s availability<span class="token operator">-</span>zone<span class="token operator">=</span>zone1
node <span class="token string">&quot;node1.k8s&quot;</span> labeled
$ kubectl label node node1<span class="token punctuation">.</span>k8s share<span class="token operator">-</span>type<span class="token operator">=</span>dedicated
node <span class="token string">&quot;node1.k8s&quot;</span> labeled
$ kubectl label node node2<span class="token punctuation">.</span>k8s availability<span class="token operator">-</span>zone<span class="token operator">=</span>zone2
node <span class="token string">&quot;node2.k8s&quot;</span> labeled
$ kubectl label node node2<span class="token punctuation">.</span>k8s share<span class="token operator">-</span>type<span class="token operator">=</span>shared
node <span class="token string">&quot;node2.k8s&quot;</span> labeled
$ kubectl <span class="token keyword">get</span> node <span class="token operator">-</span><span class="token constant">L</span> availability<span class="token operator">-</span>zone <span class="token operator">-</span><span class="token constant">L</span> share<span class="token operator">-</span>type
<span class="token constant">NAME</span>         <span class="token constant">STATUS</span>    <span class="token constant">AGE</span>       <span class="token constant">VERSION</span>   <span class="token constant">AVAILABILITY</span><span class="token operator">-</span><span class="token constant">ZONE</span>   <span class="token constant">SHARE</span><span class="token operator">-</span><span class="token constant">TYPE</span>
master<span class="token punctuation">.</span>k8s   Ready     4d        v1<span class="token punctuation">.</span><span class="token number">6.4</span>    <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>              <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
node1<span class="token punctuation">.</span>k8s    Ready     4d        v1<span class="token punctuation">.</span><span class="token number">6.4</span>    zone1               dedicated
node2<span class="token punctuation">.</span>k8s    Ready     4d        v1<span class="token punctuation">.</span><span class="token number">6.4</span>    zone2               shared
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><blockquote><p>指定优先级节点亲缘性规则</p></blockquote> <p>当这些节点的标签设置好, 现在可以<strong>创建一个 Deployment, 其中优先选择 zone1 中的 dedicated 节点</strong>. 下面的代码清单显示了这个 Deployment 的描述.</p> <p><strong>代码清单-16.11 含有优先级节点亲缘性规则的 Deployment: preferreddeployment.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pref
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
        <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>
          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>  <span class="token comment"># 指定优先级, 这不是必须的</span>
          <span class="token punctuation">-</span> <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">80</span>    <span class="token comment"># 节点优先调度到zone1, 这是权重最高的偏好                   </span>
            <span class="token key atrule">preference</span><span class="token punctuation">:</span>                                   
              <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                           
              <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> availability<span class="token punctuation">-</span>zone                    
                <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                              
                <span class="token key atrule">values</span><span class="token punctuation">:</span>                                   
                <span class="token punctuation">-</span> zone1                                   
          <span class="token punctuation">-</span> <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">20</span>     <span class="token comment"># 同时优先调度pod到独占节点, 但是该优先级的权重进位zone优先级的四分之一</span>
            <span class="token key atrule">preference</span><span class="token punctuation">:</span>                                   
              <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                           
              <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>type                           
                <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                              
                <span class="token key atrule">values</span><span class="token punctuation">:</span>                                   
                <span class="token punctuation">-</span> dedicated                               
      <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>仔细查看上面的代码清单, 这里定义了一个<strong>节点亲缘性优先级</strong>, 而不是强制要求. 这里想要 pod 被调度到包含标签 availability-zone=zone1 以及 share-type=dedicated 的节点上. 第一个优先级规则是相对重要的, 因此将其 weight 设置为 80, 而第二个优先级规则就不那么重要(weight 设置为 20).</p> <blockquote><p>了解节点优先级是如何工作的</p></blockquote> <p>如果集群包含多个节点, 当调度上面的代码清单中的 Deployment pod 时, 节点将会分成 4 个组, 如图 16.3 所示. <strong>那些包含 availability-zone 以及 share-type 标签, 并且匹配 pod 亲缘性的节点, 将排在最前面</strong>. 然后, 由于 pod 的节点亲缘性规则配置的权重, 接下来是 zone1 的 shared 节点, 然后是其他区域的 dedicated 节点, 优先级最低的是剩下的其他节点.</p> <p><img src="/img/image-20240226221056-zjupst4.png" alt="image" title="图16.3 基于 pod 节点亲缘性优先级对节点进行排序"></p> <blockquote><p>在一个包含两个节点的集群中部署节点</p></blockquote> <p>如果在一个包含<strong>两个节点</strong>的集群中创建该部署, 那么看到的最多的应该是 pod 被部署在了 node1 上面. 检查下面的代码清单看情况是否属实.</p> <p><strong>代码清单-16.12 查看 pod 调度情况</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>o wide
<span class="token constant">NAME</span>                <span class="token constant">READY</span>   <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>  <span class="token constant">AGE</span>   <span class="token constant">IP</span>          <span class="token constant">NODE</span>
pref<span class="token operator">-</span><span class="token number">607515</span><span class="token operator">-</span>1rnwv   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>     Running   <span class="token number">0</span>         4m    <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.1</span>   node2<span class="token punctuation">.</span>k8s
pref<span class="token operator">-</span><span class="token number">607515</span><span class="token operator">-</span>27wp0   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>     Running   <span class="token number">0</span>         4m    <span class="token number">10.44</span><span class="token number">.0</span><span class="token number">.8</span>   node1<span class="token punctuation">.</span>k8s  # 四个node1
pref<span class="token operator">-</span><span class="token number">607515</span><span class="token operator">-</span>5xd0z   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>     Running   <span class="token number">0</span>         4m    <span class="token number">10.44</span><span class="token number">.0</span><span class="token number">.5</span>   node1<span class="token punctuation">.</span>k8s
pref<span class="token operator">-</span><span class="token number">607515</span><span class="token operator">-</span>jx9wt   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>     Running   <span class="token number">0</span>         4m    <span class="token number">10.44</span><span class="token number">.0</span><span class="token number">.4</span>   node1<span class="token punctuation">.</span>k8s
pref<span class="token operator">-</span><span class="token number">607515</span><span class="token operator">-</span>mlgqm   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>     Running   <span class="token number">0</span>         4m    <span class="token number">10.44</span><span class="token number">.0</span><span class="token number">.6</span>   node1<span class="token punctuation">.</span>k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>5个 pod 被创建, 其中 4 个部署在了 node1, 1 个部署在了 node2. 为什么会有 1 个 pod 会被调度到 node2而不是 node1? 原因是除了节点亲缘性的优先级函数, 调度器还会使用<strong>其他的优先级函数</strong>来决定节点被调度到哪. 其中之一就是 Selector SpreadPriority 函数, 这个函数确保了属于同一个 ReplicaSet 或者 Service 的 pod, 将<strong>分散部署在不同节点上, 以避免单个节点失效导致这个服务也宕机</strong>. 这就是有 1个 pod 被调度到 node2 的最大可能.</p> <p>可以去试着扩容部署至 20 个实例或更多, 将看到大多数的 pod 被调度到 node1. 在笔者的测试中, 20 个实例只有 2 个被调度到了 node2. 如果没有设置任何节点亲缘性优先级, pod 将会被均匀地分配在两个节点上面.</p> <h5 id="_16-3-使用pod亲缘性与非亲缘性对pod进行协同部署"><a href="#_16-3-使用pod亲缘性与非亲缘性对pod进行协同部署" class="header-anchor">#</a> 16.3 使用pod亲缘性与非亲缘性对pod进行协同部署</h5> <p>前面已经了解了节点亲缘性规则是如何影响 pod 被调度到哪个节点. 但这些规则<strong>只影响了 pod 和节点之间的亲缘性</strong>. 然而, 有些时候也希望能有能力<mark><strong>指定 pod 自身之间的亲缘性</strong></mark>.</p> <p>举例来说, <strong>想象一下你有一个前端 pod 和一个后端 pod, 将这些节点部署得比较靠近, 可以降低延时, 提高应用的性能</strong>. 可以使用节点亲缘性规则来确保这两个 pod 被调度到同一个节点, 同一个机架, 同一个数据中心. 但是, 之后还需要指定调度到具体哪个节点, 哪个机架或者哪个数据中心. 因此, 这不是一个最佳的解决方案. 更好的做法应该是, <mark><strong>让 Kubernetes 将你的 pod 部署在任何它觉得合适的地方, 同时确保 2 个 pod 是靠近的. 这种功能可以通过 pod 亲缘性来实现</strong></mark>, 下面从一个例子来了解更多吧.</p> <h6 id="_16-3-1-使用pod间亲缘性将多个pod部署在同一个节点上"><a href="#_16-3-1-使用pod间亲缘性将多个pod部署在同一个节点上" class="header-anchor">#</a> 16.3.1 使用pod间亲缘性将多个pod部署在同一个节点上</h6> <p>下面将部署 1 个后端 pod 和 5 个包含 pod 亲缘性配置的前端 pod 实例, 使得这些前端实例将被部署在后端 pod 所在的同一个节点上.</p> <p>首先来部署后端 pod:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl run backend <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>backend <span class="token parameter variable">--image</span> busybox -- <span class="token function">sleep</span> <span class="token number">999999</span>
deployment <span class="token string">&quot;backend&quot;</span> created
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>该部署并没有什么特别的, 唯一需要注意的是通过 -l 选项添加的 <strong>app=backend 标签</strong>, 这个标签将在前端 pod 的 <strong>podAffinity 配置</strong>中使用到.</p> <blockquote><p>在 pod 定义中指定 pod 亲缘性</p></blockquote> <p>前端 pod 的描述如以下代码清单所示.</p> <p><strong>代码清单-16.13 使用 podAffinity 的 pod:frontend-podaffinity-host.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">5</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
        <span class="token key atrule">podAffinity</span><span class="token punctuation">:</span>            <span class="token comment"># 定义节点亲缘性规则</span>
          <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>  <span class="token comment"># 定义一个强制性要求, 而不是偏好</span>
          <span class="token punctuation">-</span> <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname  <span class="token comment"># 本次部署的pod, 必须被调度到匹配pod选择器的节点上</span>
            <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                              
              <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>                              
                <span class="token key atrule">app</span><span class="token punctuation">:</span> backend                            
      <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>代码清单显示了, 该部署将创建<strong>包含强制性</strong>要求的 pod, 其中<strong>要求 pod 将被调度到和其他包含 app=backend 标签的 pod 所在的相同节点上(通过 topologyKey 字段指定)</strong> , 如图 16.4 所示.</p> <p><img src="/img/image-20240226221826-oo88kdn.png" alt="image" title="图16.4 pod 亲缘性允许 pod 被调度到那些包含指定标签的 pod 所在的节点上"></p> <p>注意: 除了使用简单的 matchLabels 字段, 也可以使用表达能力更强的 <strong>matchExpressions</strong> 字段.</p> <blockquote><p>部署包含 pod 亲缘性的 pod</p></blockquote> <p>在创建此次部署之前, 先来看一下之前的后端 pod 被调度到了哪个节点上:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po <span class="token parameter variable">-o</span> wide
NAME                   READY  STATUS   RESTARTS  AGE  IP         NODE
backend-257820-qhqj6   <span class="token number">1</span>/1    Running  <span class="token number">0</span>         8m   <span class="token number">10.47</span>.0.1  node2.k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>当创建前端 pod 时, 它们应该也会被调度到 node2 上. 接着开始创建 Deployment, 然后看 pod 被调度到了哪里. 结果显示在下面的代码清单中.</p> <p><strong>代码清单-16.14 部署前端 pod, 观察 pod 被调度到哪些节点</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl create <span class="token operator">-</span>f frontend<span class="token operator">-</span>podaffinity<span class="token operator">-</span>host<span class="token punctuation">.</span>yaml
deployment <span class="token string">&quot;frontend&quot;</span> created

$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>o wide
<span class="token constant">NAME</span>                   <span class="token constant">READY</span>  <span class="token constant">STATUS</span>    <span class="token constant">RESTARTS</span>  <span class="token constant">AGE</span>  <span class="token constant">IP</span>         <span class="token constant">NODE</span>
backend<span class="token operator">-</span><span class="token number">257820</span><span class="token operator">-</span>qhqj6   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>         8m   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.1</span>  node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">121895</span><span class="token operator">-</span>2c1ts  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>         13s  <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.6</span>  node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">121895</span><span class="token operator">-</span>776m7  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>         13s  <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.4</span>  node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">121895</span><span class="token operator">-</span>7ffsm  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>         13s  <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.8</span>  node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">121895</span><span class="token operator">-</span>fpgm6  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>         13s  <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.7</span>  node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">121895</span><span class="token operator">-</span>vb9ll  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running   <span class="token number">0</span>         13s  <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.5</span>  node2<span class="token punctuation">.</span>k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>所有的前端 pod 确实都被调度到了和后端 pod 相同的节点上. <strong>当调度前端 pod 时, 调度器首先找出所有匹配前端 pod 的 podAffinity 配置中 labelSelector 的 pod, 之后将前端 pod 调度到相同的节点上</strong>.</p> <blockquote><p>了解调度器如何使用 pod 亲缘性规则</p></blockquote> <p>有趣的是, <strong>如果现在删除了后端 pod, 调度器会将该 pod 调度到 node2, 即便后端 pod 本身没有定义任何 pod 亲缘性规则(只有前端 pod 设置了规则)</strong> . 这种情况很合理, 因为假设后端 pod 被误删除而被调度到其他节点上, 前端 pod 的亲缘性规则就被<strong>打破</strong>了.</p> <p>如果增加调度器的日志级别检查它的日志的话, 可以确定调度器是会考虑其他 pod 的亲缘性规则的. 下面的代码清单显示了相关的日志.</p> <p><strong>代码清单-16.15 调度器日志显示了后端 pod 被调度到 node2的原因</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code><span class="token operator">...</span> Attempting to schedule pod<span class="token operator">:</span> <span class="token keyword">default</span><span class="token operator">/</span>backend<span class="token operator">-</span><span class="token number">257820</span><span class="token operator">-</span>qhqj6
<span class="token operator">...</span> <span class="token operator">...</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node2<span class="token punctuation">.</span>k8s<span class="token operator">:</span> Taint Toleration Priority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node1<span class="token punctuation">.</span>k8s<span class="token operator">:</span> Taint Toleration Priority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node2<span class="token punctuation">.</span>k8s<span class="token operator">:</span> InterPodAffinityPriority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>  # 分数<span class="token number">10</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node1<span class="token punctuation">.</span>k8s<span class="token operator">:</span> InterPodAffinityPriority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>   # 分数<span class="token number">0</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node2<span class="token punctuation">.</span>k8s<span class="token operator">:</span> SelectorSpreadPriority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node1<span class="token punctuation">.</span>k8s<span class="token operator">:</span> SelectorSpreadPriority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node2<span class="token punctuation">.</span>k8s<span class="token operator">:</span> NodeAffinityPriority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token operator">...</span> backend<span class="token operator">-</span>qhqj6 <span class="token operator">-</span><span class="token operator">&gt;</span> node1<span class="token punctuation">.</span>k8s<span class="token operator">:</span> NodeAffinityPriority<span class="token punctuation">,</span> <span class="token literal-property property">Score</span><span class="token operator">:</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token operator">...</span> Host node2<span class="token punctuation">.</span><span class="token parameter">k8s</span> <span class="token operator">=&gt;</span> Score <span class="token number">100030</span>
<span class="token operator">...</span> Host node1<span class="token punctuation">.</span><span class="token parameter">k8s</span> <span class="token operator">=&gt;</span> Score <span class="token number">100022</span>
<span class="token operator">...</span> Attempting to bind backend<span class="token operator">-</span><span class="token number">257820</span><span class="token operator">-</span>qhqj6 to node2<span class="token punctuation">.</span>k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>如果关注有注释的两行日志, 会发现当调度后端 pod 时, 由于 pod 间亲缘性, <strong>node2 获得了比 node1 更高的分数</strong>.</p> <h6 id="_16-3-2-将pod部署在同一机柜-可用性区域或者地理地域"><a href="#_16-3-2-将pod部署在同一机柜-可用性区域或者地理地域" class="header-anchor">#</a> 16.3.2 将pod部署在同一机柜,可用性区域或者地理地域</h6> <p>在前面的例子中, 使用了 podAffinity 将前端 pod 和后端 pod 部署在了同一个节点上. 你可能不希望所有的前端 pod 都部署在同一个节点上, 但仍希望和后端 pod 保持足够近, 比如在<strong>同一个可用性区域</strong>中.</p> <blockquote><p>在同一个可用性区域中协同部署 pod</p></blockquote> <p>笔者正在使用的集群运行在本机的三个虚拟机中, 因此可以说所有节点都运行在同一个可用性区域中. 但是, 如果这些节点运行在不同的可用性区域中, 那么需要将 topologyKey 属性设置为 <strong>failure-domain.beta.kubernetes.io/zone</strong>, 以确保前端 pod 和后端 pod 运行在同一个可用性区域中.</p> <blockquote><p>在同一个地域中协同部署 pod</p></blockquote> <p>为了允许将 pod 部署在同一个地域而不是区域内(云服务提供商通常拥有多个地理地域的数据中心, 每个地理地域会被划分成多个可用性区域), 那么需要将 topologyKey 属性设置为 <strong>failure-domain.beta.kubernetes.io/region</strong>.</p> <blockquote><p>了解 topologyKey 是如何工作的</p></blockquote> <p>topologyKey 的工作方式很简单, 目前提到的 3 个键并没有什么特别的. 如果你愿意, <strong>可以任意设置自定义的键</strong>, 例如 rack, 为了让 pod 能部署到同一个机柜. <strong>唯一的前置条件就是, 在你的节点上加上 rack 标签</strong>. 这种场景将在图 16.5 中进行展示.</p> <p><img src="/img/image-20240226222207-lzxs1dm.png" alt="image" title="图16.5 podAffinity 中的 topologyKey 决定了 pod 被调度的范围"></p> <p>举例来说, 你有 20 个节点, 每 10 个节点在同一个机柜中, 你将前 10 个节点加上标签 rack=rack1, 另外 10 个加上标签 rack=rack2. 接着, 当定义 pod 的 podAffinity 时, <strong>将 toplogyKey 设置为 rack</strong>.</p> <p>当调度器决定 pod 调度到哪里时, 它首先检查 pod 的 podAffinity 配置, 找出那些<strong>符合标签选择器的 pod</strong>, 接着查询这些 pod 运行在哪些节点上. 特别的是, 它会寻找标签能匹配 podAffinity 配置中 topologyKey 的节点. 接着, 它会<strong>优先选择所有的标签匹配 pod 的值的节点</strong>. 在图 16.5 中, 标签选择器匹配了运行在 Node 12 的后端 pod, 那个节点 rack 标签的值等于 rack2. 所以, 当调度 1 个前端 pod 时, 调度器只会在包含标签 rack=rack2 的节点中进行选择.</p> <p>注意: <strong>在调度时, 默认情况下, 标签选择器只有匹配同一命名空间中的 pod. 但是, 可以通过在 labelSelector 同一级添加 namespaces 字段, 实现从其他的命名空间选择 pod 的功能</strong>.</p> <h6 id="_16-3-3-表达pod亲缘性优先级取代强制性要求"><a href="#_16-3-3-表达pod亲缘性优先级取代强制性要求" class="header-anchor">#</a> 16.3.3 表达pod亲缘性优先级取代强制性要求</h6> <p>前面谈论了节点亲缘性, <strong>了解了 nodeAffinity 可以表示一种强制性要求, 表示 pod 只能被调度到符合节点亲缘性规则的节点上. 它也可以表示一种节点优先级, 用于告知调度器将 pod 调度到某些节点上, 同时也满足当这些节点出于各种原因无法满足 pod 要求时, 将 pod 调度到其他节点上</strong>.</p> <p>这种特性同样适用于 podAffinity, 你可以告诉调度器, 优先将前端 pod 调度到和后端 pod 相同的节点上, 但是如果不满足需求, 调度到其他节点上也是可以的. 一个使用了 <strong>preferredDuringSchedulingIgnoredDuringExecutionpod</strong> 亲缘性规则的 Deployment 的样例如以下代码清单所示.</p> <p><strong>代码清单-16.16 pod 亲缘性优先级</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">5</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token punctuation">...</span>
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
        <span class="token key atrule">podAffinity</span><span class="token punctuation">:</span>
          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>    <span class="token comment"># 使用了preferred而不是required</span>
          <span class="token punctuation">-</span> <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">80</span>           <span class="token comment"># weight和podAffinityTerm设置为和之前例子中一样的值</span>
            <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>                              
              <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname         
              <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                              
                <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>                              
                  <span class="token key atrule">app</span><span class="token punctuation">:</span> backend                              
      <span class="token key atrule">containers</span><span class="token punctuation">:</span> <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>跟 nodeAffinity 优先级规则一样, 需要为一个规则设置一个<strong>权重</strong>. 同时也需要设置 topologyKey 和 labelSelector, 正如 podAffinity 规则中的强制性要求一样. 图 16.6 展示了这种场景.</p> <p><img src="/img/image-20240226222510-3wergdq.png" alt="image" title="图16.6 pod 亲缘性可以用来告知调度器优先考虑那些有包含某些标签的 pod 正在运行着的节点"></p> <p>正如 nodeAffinity 样例, 部署将 4 个 pod 调度到和后端 pod 一样的节点, 另外一个调度到了其他节点(如下面的代码清单所示).</p> <p><strong>代码清单-16.17 使用 podAffinity 优先级的 pod 部署</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>o wide
<span class="token constant">NAME</span>                   <span class="token constant">READY</span>  <span class="token constant">STATUS</span>   <span class="token constant">RESTARTS</span>  <span class="token constant">AGE</span>  <span class="token constant">IP</span>          <span class="token constant">NODE</span>
backend<span class="token operator">-</span><span class="token number">257820</span><span class="token operator">-</span>ssrgj   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         1h   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.9</span>   node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">941083</span><span class="token operator">-</span>3mff9  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         8m   <span class="token number">10.44</span><span class="token number">.0</span><span class="token number">.4</span>   node1<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">941083</span><span class="token operator">-</span>7fp7d  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         8m   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.6</span>   node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">941083</span><span class="token operator">-</span>cq23b  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         8m   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.1</span>   node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">941083</span><span class="token operator">-</span>m70sw  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         8m   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.5</span>   node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">941083</span><span class="token operator">-</span>wsjv8  <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         8m   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.4</span>   node2<span class="token punctuation">.</span>k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><h6 id="_16-3-4-利用pod的非亲缘性分开调度pod"><a href="#_16-3-4-利用pod的非亲缘性分开调度pod" class="header-anchor">#</a> 16.3.4 利用pod的非亲缘性分开调度pod</h6> <p>现在已经知道了如何告诉调度器对 pod 进行协同部署, 但有时候你的需求却恰恰相反, 你<mark><strong>可能希望 pod 远离彼此</strong></mark>. 这种特性叫作 <mark><strong>pod 非亲缘性</strong></mark>. 它和 pod 亲缘性的表示方式一样, 只不过是<strong>将 podAffinity 字段换成 podAntiAffinity</strong>, 这将导致调度器永远不会选择那些有包含 podAntiAffinity 匹配标签的 pod 所在的节点, 如图 16.7 所示.</p> <p><img src="/img/image-20240226222611-02niunt.png" alt="image" title="图16.7 使用 pod 非亲缘性使得 pod 原理包含某些标签 pod 所在的节点"></p> <p>一个为什么需要使用 pod 非亲缘性的例子, 就是<strong>当两个集合的 pod, 如果运行在同一个节点上会影响彼此的性能</strong>. 在这种情况下, 需要告知调度器永远不要将这些 pod 部署在同一个节点上. 另一个例子是强制让调度器将同一组的 pod 分在在不同的可用性区域或者地域, 这样让整个区域或地域失效之后, 不会使得整个服务完全不可用.</p> <blockquote><p>使用非亲缘性分散一个部署中的 pod</p></blockquote> <p>下面来看一下如何强制前端 pod 被调度到不同节点上. 下面的代码清单展示了 pod 的非亲缘性是如何配置的.</p> <p><strong>代码清单-16.18 包含非亲缘性的 pod:frontend-podantiaffinity-host.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> extensions/v1beta1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">5</span>
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>           <span class="token comment"># 前端pod有app=frontend标签                                       </span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend                                       
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>
        <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>      <span class="token comment"># 定义pod非亲缘性的强制性要求</span>
          <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>   
          <span class="token punctuation">-</span> <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname  <span class="token comment"># 一个前端pod必须不能调度到有app=frontend标签的pod运行的节点上</span>
            <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                                  
              <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>                                  
                <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend                               
      <span class="token key atrule">containers</span><span class="token punctuation">:</span> <span class="token punctuation">...</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>这次, 需要定义 <strong>podAntiAffinity</strong> 而不是 podAffinity, 并且将 labelSelector 和 Deployment 创建的 pod 匹配. 下面看一下当创建了该 Deployment 之后会发生什么, 创建的 pod 如下面的代码清单所示.</p> <p><strong>代码清单-16.19 Deployment 创建的 pod</strong></p> <div class="language-js line-numbers-mode"><pre class="language-js"><code>$ kubectl <span class="token keyword">get</span> po <span class="token operator">-</span>l app<span class="token operator">=</span>frontend <span class="token operator">-</span>o wide
<span class="token constant">NAME</span>                    <span class="token constant">READY</span>  <span class="token constant">STATUS</span>   <span class="token constant">RESTARTS</span>  <span class="token constant">AGE</span>  <span class="token constant">IP</span>         <span class="token constant">NODE</span>
frontend<span class="token operator">-</span><span class="token number">286632</span><span class="token operator">-</span>0lffz   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>    Pending  <span class="token number">0</span>         1m   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
frontend<span class="token operator">-</span><span class="token number">286632</span><span class="token operator">-</span>2rkcz   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         1m   <span class="token number">10.47</span><span class="token number">.0</span><span class="token number">.1</span>  node2<span class="token punctuation">.</span>k8s
frontend<span class="token operator">-</span><span class="token number">286632</span><span class="token operator">-</span>4nwhp   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>    Pending  <span class="token number">0</span>         1m   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
frontend<span class="token operator">-</span><span class="token number">286632</span><span class="token operator">-</span>h4686   <span class="token number">0</span><span class="token operator">/</span><span class="token number">1</span>    Pending  <span class="token number">0</span>         1m   <span class="token operator">&lt;</span>none<span class="token operator">&gt;</span>
frontend<span class="token operator">-</span><span class="token number">286632</span><span class="token operator">-</span>st222   <span class="token number">1</span><span class="token operator">/</span><span class="token number">1</span>    Running  <span class="token number">0</span>         1m   <span class="token number">10.44</span><span class="token number">.0</span><span class="token number">.4</span>  node1<span class="token punctuation">.</span>k8s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>正如你所见, 只有 2 个 pod 被调度, 一个在 node1 上, 另一个在 node2 上. 剩下的 3 个 pod 均处于 Pending 状态, 因为<strong>调度器不允许这些 pod 调度到同一个节点上</strong>.</p> <blockquote><p>理解 pod 非亲缘性优先级</p></blockquote> <p>在这种情况下, 你可能应该制定<strong>软性要求</strong>(使用 preferredDuringSchedulingIgnoredDuringExecution 字段). 毕竟, 如果有 2 个前端 pod 运行在同一个节点上也不是什么大问题. 但是如果运行在同一个节点上会造成问题的场景下, 使用 requiredDuringScheduling 就比较合适了.</p> <p>与使用 pod 亲缘性一样, <strong>topologyKey 字段决定了 pod 不能被调度的范围</strong>. 可以使用这个字段决定 pod 不能被调度到同一个机柜, 可用性区域, 地域, 或者任何你创建的自定义节点标签标示的范围.</p> <h5 id="_16-4-本章小结"><a href="#_16-4-本章小结" class="header-anchor">#</a> 16.4 本章小结</h5> <p>本章了解了</p> <ul><li>如何通过节点的标签或是上面运行的 pod, 保证不把 pod 调度到某个节点上, 或者只把 pod 调度到特定节点上.</li> <li><strong>如果在节点上添加了 1 个污点信息, 除非 pod 容忍这些污点, 否则 pod 不会被调度到该节点上.</strong></li> <li>有 3 种污点类型: <strong>NoSchedule 完全阻止调度, PreferNoSchedule 不强制阻止调度, NoExecute 会将已经在运行的 pod 从节点上剔除</strong>. NoExecute 污点同时还可以设置, 当 pod 运行的节点变成 unreachable 或者 unready 状态, pod 需要重新调度时, 控制面板的最长等待时间.</li> <li><strong>节点亲缘性允许你去指定 pod 应该被调度到哪些节点上. 它可以被用作一种强制性要求, 也可以作为一种节点的优先级.</strong></li> <li><strong>pod 亲缘性被用于告知调度器将 pod 调度到和另一个 pod 相同的一节点(基于 pod 的标签)上.</strong></li> <li><strong>pod 亲缘性的 topologyKey 表示了被调度的 pod 和另一个 pod 的距离(在同一个节点, 同一个机柜, 同一个可用性局域或者可用性地域).</strong></li> <li><strong>pod 非亲缘性可以被用于将 pod 调度到远离某些 pod 的节点.</strong></li> <li>和节点亲缘性一样, pod 亲缘性和非亲缘性可以设置是强制性要求还是优先选择.</li></ul> <p>下一章将学习在 Kubernetes 环境中开发应用, 以及将应用平滑运行的最佳实践.</p> <h4 id="_17-开发应用的最佳实践"><a href="#_17-开发应用的最佳实践" class="header-anchor">#</a> 17.开发应用的最佳实践</h4> <blockquote><p>本章内容涵盖</p></blockquote> <ul><li>了解在一个典型应用中会出现哪些 Kubernetes 的资源:</li> <li>添加 pod 启动后和停止前的生命周期钩子</li> <li>在不断开客户端连接的情况下妥善地停止应用</li> <li>在 Kubernetes 中如何方便地管理应用</li> <li>在 pod 中使用 init 容器</li> <li>使用 Minikube 在本地进行应用开发</li></ul> <p>到目前为止, 已经探索了每个单独的资源的功能以及其使用方法. 现在来看看如何将它们和运行在 Kubernetes 上面的一个典型的应用结合在一起. 下面也将看看如何让一个应用可以顺利运行. 毕竟, 这是使用 Kubernetes 的重点, 不是吗?</p> <h5 id="_17-1-集中一切资源"><a href="#_17-1-集中一切资源" class="header-anchor">#</a> 17.1 集中一切资源</h5> <p>首先看看一个实际应用程序的各个组成部分. 这也会让你有机会看看你是否记得迄今为止所学到的一切, 并且能够从全局来审视它们. 图 17.1 显示了<strong>一个典型应用中所使用的各个 Kubernetes 组件</strong>.</p> <p><img src="/img/image-20240226223346-efzinyq.png" alt="image" title="图17.1 一个典型应用中的资源"></p> <p><mark><strong>一个典型的应用 manifest 包含了一个或者多个 Deployment 和 StatefulSet 对象. 这些对象中包含了一个或者多个容器的 pod 模板, 每个容器都有一个存活探针, 并且为容器提供的服务(如果有的话)提供就绪探针. 提供服务的 pod 是通过一个或者多个服务来暴露自己的. 当需要从集群外访问这些服务的时候, 要么将这些服务配置为 LoadBalancer 或者 NodePort 类型的服务, 要么通过 Ingress 资源来开放服务.</strong></mark></p> <p><mark><strong>pod 模板(从中创建 pod 的配置文件)通常会引用两种类型的私密凭据(Secret). 一种是从私有镜像仓库拉取镜像时使用的; 另一种是 pod 中运行的进程直接使用的. 私密凭据本身通常不是应用 manifest 的一部分, 因为它们不是由应用开发者来配置, 而是由运维团队来配置的. 私密凭据通常会被分配给 ServiceAccount, 然后 ServiceAccount 会被分配给每个单独的 pod.</strong></mark></p> <p><mark><strong>一个应用还包含一个或者多个 ConfigMap 对象, 可以用它们来初始化环境变量, 或者在 pod 中以 configMap 卷来挂载. 有一些 pod 会使用额外的卷, 例如 emptyDir 或 gitRepo 卷, 而需要持久化存储的 pod 则需要 persistentVolumeClaim 卷. PersistentVolumeClaim 也是一个应用 manifest 的一部分, 而被 PersistentVolumeClaim 所引用的 StorageClass 则是由系统管理员事先创建的.</strong></mark></p> <p><mark><strong>在某些情况下, 一个应用还需要使用任务(Jobs)和定时任务(CronJobs). 守护进程集(DaemonSet)通常不是应用部署的一部分, 但是通常由系统管理员创建, 以在全部或者部分节点上运行系统服务. 水平 pod 扩容器(HorizontalpodAutoscaler)可以由开发者包含在应用 manifest 中或者后续由运维团队添加到系统中. 集群管理员还会创建 LimitRange 和 ResourceQuota 对象, 以控制每个 pod 和所有的 pod(作为一个整体)的计算资源使用情况.</strong></mark></p> <p><mark><strong>在应用部署后, 各种 Kubernetes 控制器会自动创建其他的对象. 其中包括端点控制器(Endpoint controller)创建的服务端点(Endpoint)对象, 部署控制器(Deployment controller)创建的 ReplicaSet 对象, 以及由 ReplicaSet(或者 Job, CronJob, StatefulSet, DaemonSet)创建的实际的 pod 对象.</strong></mark></p> <p><mark><strong>资源通常通过一个或者多个标签来组织. 这不仅仅适用于 pod, 同时也适用于其他的资源. 除了标签, 大多数的资源还包含一个描述资源的注解, 列出负责该资源的人员或者团队的联系信息, 或者为管理者和其他的工具提供额外的元数据.</strong></mark></p> <p><strong>pod 是所有一切资源的中心</strong>, 毫无疑问是 Kubernetes 中最重要的资源. 毕竟每个应用都运行在 pod 中. 为了确保你知道如何开发能充分利用应用所在环境资源的应用, 最后再<strong>从应用的角度来仔细看一下 pod</strong>.</p> <h5 id="_17-2-了解pod的生命周期"><a href="#_17-2-了解pod的生命周期" class="header-anchor">#</a> 17.2 了解pod的生命周期</h5> <p>之前说过, 可以将 pod 比作只运行单个应用的虚拟机. 尽管在 pod 中运行的应用和虚拟机中运行的应用没什么不同, 但是还是存在显著的差异. 其中一个例子就是 <strong>pod 中运行的应用随时可能会被杀死, 因为 Kubernetes 需要将这个 pod 调度到另外一个节点, 或者是请求缩容</strong>. 接下来将探讨这方面的内容.</p> <h6 id="_17-2-1-应用必须预料到会被杀死或者重新调度"><a href="#_17-2-1-应用必须预料到会被杀死或者重新调度" class="header-anchor">#</a> 17.2.1 应用必须预料到会被杀死或者重新调度</h6> <p>在 Kubernetes 之外, 运行在虚拟机中的应用很少会被从一台机器迁移到另外一台. 当一个操作者迁移应用的时候, 他们可以重新配置应用并且手动检查应用是否在新的位置正常运行. 借助于 Kubernetes, <strong>应用可以更加频繁地进行自动迁移而无须人工介入, 也就是说没有人会再对应用进行配置并且确保它们在迁移之后能够正常运行. 这就意味着应用开发者必须允许他们的应用可以被相对频繁地迁移</strong>.</p> <blockquote><p>预料到本地 IP 和主机名会发生变化</p></blockquote> <p>当一个 pod 被杀死并且在其他地方运行之后(技术上来讲是一个新的 pod 替换了旧的 pod, 旧 pod 没有被迁移), 它不仅拥有了一个新的 IP 地址还有了一个新的名称和主机名. 大部分无状态的应用都可以处理这种场景同时不会有不利的影响, 但是有状态服务通常不能. 前面已经了解到有状态应用可以通过一个 StatefulSet 来运行, StatefulSet 会保证在将应用调度到新的节点并启动之后, 它可以看到和之前一样的主机名和持久化状态. 当然 pod 的 IP 还是会发生变化, 应用必须能够应对这种变化. <strong>因此应用开发者在一个集群应用中不应该依赖成员的 IP 地址来构建彼此的关系, 另外如果使用主机名来构建关系, 必须使用 StatefulSet</strong>.</p> <blockquote><p>预料到写入磁盘的数据会消失</p></blockquote> <p>还有一件事情需要记住的是, 在应用往磁盘写入数据的情况下, <strong>当应用在新的 pod 中启动后这些数据可能会丢失, 除非你将持久化的存储挂载到应用的数据写入路径</strong>. 在 pod 被重新调度的时候, 数据丢失是一定的, 但是即使在没有调度的情况下, 写入磁盘的文件仍然会丢失. 甚至是在单个 pod 的生命周期过程中, pod 中的应用写入磁盘的文件也会丢失. 下面通过一个例子来解释一下这个问题.</p> <p>假设有个应用, 它的启动过程是比较耗时的而且需要很多的计算操作. 为了能够让这个应用在后续的启动中更快, 开发者一般会<strong>把启动过程中的一些计算结果缓存到磁盘上</strong>(例如启动时扫描所有的用作注解的 Java 类然后把结果写入到索引文件). 由于在 Kubernetes 中应用默认运行在容器中, 这些文件会被写入到容器的文件系统中. 如果这个时候容器重启了, 这些文件都会丢失, 因为新的容器启动的时候会使用一个全新的可写入层(参考图 17.2).</p> <p>不要忘了, 单个容器可能因为各种原因被重启, 例如进程崩溃了, 例如存活探针返回失败了, 或者是因为节点内存逐步耗尽, 进程被 OOMKiller 杀死了. 当上述情况发生的时候, pod 还是一样, 但是容器却是全新的了. <strong>Kubelet 不会一个容器运行多次, 而是会重新创建一个容器</strong>.</p> <blockquote><p>使用存储卷来跨容器持久化数据</p></blockquote> <p>当 pod 的容器重启后, 本例中的应用<strong>仍然需要</strong>执行有大量计算过程的启动程序. 这个或许不是你所期望的. 为了保证这种情况下数据不丢失, 就需要至少使用一个 pod 级别的卷. <strong>因为卷的存在和销毁与 pod 生命周期是一致的, 所以新的容器将可以重用之前容器写到卷上的数据</strong>(见图 17.3).</p> <p><img src="/img/image-20240226232329-cns9vqd.png" alt="image" title="图17.2 写入到容器文件系统的文件在容器重启之后都丢失了"></p> <p><img src="/img/image-20240226232348-0nl5m9h.png" alt="image" title="图 17.3 使用存储卷来跨容器持久化数据"></p> <p>有时候使用存储卷来跨容器存储数据是个好办法, 但是也不总是如此. 万一由于数据损坏而导致新创建的进程再次崩溃呢? 这会导致一个持续性的循环崩溃(pod 会提示 CrashLoopBackOff 状态). 如果不使用存储卷的话, 新的容器会从零开始启动, 并且很可能不会崩溃. 使用存储卷来跨容器存储数据是把双刃剑. 因此需要仔细思考是否使用它们.</p> <h6 id="_17-2-2-重新调度死亡的或者部分死亡的pod"><a href="#_17-2-2-重新调度死亡的或者部分死亡的pod" class="header-anchor">#</a> 17.2.2 重新调度死亡的或者部分死亡的pod</h6> <p><strong>如果一个 pod 的容器一直处于崩溃状态, Kubelet 将会一直不停地重启它们</strong>. 每次重启的时间间隔将会以指数级增加, 直到达到 5 分钟. 在这个 5 分钟的时间间隔中, pod 基本上是死亡了, 因为它们的容器进程没有运行. 公平来讲, 如果是个多容器的 pod, 其中的一些容器可能是正常运行的, 所以这个 pod 只是<strong>部分死亡</strong>了. 但是如果 pod 中仅包含一个容器, 那么这个 pod 是完全死亡的而且已经毫无用处了, 因为里面已经没有进程在运行了.</p> <p>你或许会奇怪, 为什么这些 pod 不会被自动移除或者重新调度, 尽管它们是 ReplicaSet 或者相似控制器的一部分. <strong>如果你创建了一个期望副本数是 3 的 ReplicaSet, 当那些 pod 中的一个容器开始崩溃, Kubernetes 将不会删除或者替换这个 pod. 结果就是这个 ReplicaSet 只剩下了两个正确运行的副本, 而不是期望的三个</strong>(见图 17.4).</p> <p><img src="/img/image-20240226232418-cjv4x2k.png" alt="image" title="图17.4 ReplicaSet 控制器没有重新调度死亡的 pod"></p> <p>你或许期望能够删除这个 pod 然后重新启动一个可以在其他节点上成功运行的 pod. 毕竟这个容器可能是因为一个节点相关的问题而导致的崩溃, 这个问题在其他的节点上不会出现. 很遗憾, 并不是这样的. <mark><strong>ReplicaSet 本身并不关心 pod 是否处于死亡状态, 它只关心 pod 的数量是否匹配期望的副本数量, 在这种情况下, 副本数量确实是匹配的</strong></mark>.</p> <p>如果你想自己研究一下, 这里有一个 ReplicaSet 的 YAML manifest 文件, 它里面定义的 pod 会不停地崩溃(这个文件是代码归档中的 replicaset-crashingpods.yaml). 如果创建了这个 ReplicaSet 然后检查一下创建的 pod, 你会看到如下的代码清单.</p> <p><strong>代码清单-17.1 ReplicaSet 和 持续崩溃的 pod</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME                  READY     STATUS             RESTARTS   AGE 
crashing-pods-f1tcd   <span class="token number">0</span>/1       CrashLoopBackOff   <span class="token number">5</span>          6m  <span class="token comment"># pod的状态表示Kubelet在延迟重启, 因为容器一直处于崩溃状态 </span>
crashing-pods-k7l6k   <span class="token number">0</span>/1       CrashLoopBackOff   <span class="token number">5</span>          6m
crashing-pods-z7l3v   <span class="token number">0</span>/1       CrashLoopBackOff   <span class="token number">5</span>          6m

$ kubectl describe rs crashing-pods
Name:           crashing-pods
Replicas:       <span class="token number">3</span> current / <span class="token number">3</span> desired     <span class="token comment"># 控制器没有才去任何动作, 因为目前的副本数量和期望的相符</span>
Pods Status:    <span class="token number">3</span> Running / <span class="token number">0</span> Waiting / <span class="token number">0</span> Succeeded / <span class="token number">0</span> Failed  <span class="token comment"># 显示有三个副本正在运行中</span>

$ kubectl describe po crashing-pods-f1tcd
Name:           crashing-pods-f1tcd
Namespace:      default
Node:           minikube/192.168.99.102
Start Time:     Thu, 02 Mar <span class="token number">2017</span> <span class="token number">14</span>:02:23 +0100
Labels:         <span class="token assign-left variable">app</span><span class="token operator">=</span>crashing-pods
Status:         Running         <span class="token comment"># Kubelet describe也显示pod的状态是运行中</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>在某种程度上, 可以理解为什么 Kubernetes 会这样做. 容器将会每 5 分钟重启一次, 在这个过程中 Kubernetes 期望崩溃的底层原因会被解决. 这个机制依据的基本原理就是将 pod 重新调度到其他节点通常并不会解决崩溃的问题, 因为应用运行在容器的内部, 所有的节点理论上应该都是相同的. 虽然上面的情况并不总是如此, 但是大多数情况下都是这样.</p> <h6 id="_17-2-3-以固定顺序启动pod"><a href="#_17-2-3-以固定顺序启动pod" class="header-anchor">#</a> 17.2.3 以固定顺序启动pod</h6> <p>pod 中运行的应用和手动运行的应用之间的另外一个不同就是运维人员在手动部署应用的时候知道应用之间的依赖关系, 这样他们就可以按照顺序来启动应用.</p> <blockquote><p>了解 pod 是如何启动的</p></blockquote> <p>当使用 Kubernetes 来运行多个 pod 的应用的时候, <strong>Kubernetes 没有内置的方法来先运行某些 pod 然后等这些 pod 运行成功后再运行其他 pod</strong>. 当然也可以先发布第一个应用的配置, 然后等待 pod 启动完毕再发布第二个应用的配置. 但是你的整个系统通常都是定义在一个单独的 YAML 或者 JSON 文件中, 这些文件包含了多个 pod, 服务或者其他对象的定义.</p> <p>Kubernetes API 服务器确实是按照 YAML/JSON 文件中定义的对象的顺序来进行处理的, 但是仅仅意味着它们在被写入到 etcd 的时候是有顺序的. <strong>无法确保 pod 会按照那个顺序启动</strong>.</p> <p>但是你可以<strong>阻止一个主容器的启动, 直到它的预置条件被满足. 这个是通过</strong>​<mark><strong>在 pod 中包含一个叫作 init 的容器</strong></mark>​<strong>来实现的</strong>.</p> <blockquote><p>init 容器介绍</p></blockquote> <p>除了常规的容器, pod 还可以包括 init 容器. 如容器名所示, 它们<strong>可以用来初始化 pod, 这通常意味着向容器的存储卷中写入数据, 然后将这个存储卷挂载到主容器中</strong>.</p> <p><mark><strong>一个 pod 可以拥有任意数量的 init 容器. init 容器是顺序执行的, 并且仅当最后一个 init 容器执行完毕才会去启动主容器. 换句话说, init 容器也可以用来延迟 pod 的主容器的启动. 例如, 直到满足某一个条件的时候. init 容器可以一直等待直到主容器所依赖的服务启动完成并可以提供服务. 当这个服务启动并且可以提供服务之后, init 容器就执行结束了, 然后主容器就可以启动了. 这样主容器就不会发生在所依赖服务准备好之前使用它的情况了.</strong></mark></p> <p>下面来看一个 pod 使用 init 容器来延迟主容器启动的例子. 还记得第 7 章中创建的名叫 fortune 的 pod 吗? 它是一个能够返回给客户端请求一个人生格言作为响应的 web 服务. 现在假设有一个叫作 fortune-client 的 pod, 它的<strong>主容器需要依赖 fortune 服务先启动并且运行之后才能启动</strong>. 可以给 fortune-client 的 pod 添加一个 init 容器, 这个容器主要<strong>检查发送给 fortune 服务的请求是否被响应</strong>. 如果没有响应, 那么这个 init 容器将一直重试. 当这个 init 容器获得响应之后, 它的执行就结束了然后让主容器启动.</p> <blockquote><p>将 init 容器加入 pod</p></blockquote> <p>init 容器可以在 pod spec 文件中像主容器那样定义, 不过是通过字段 spec.initContainers 来定义的. 可以在本书的代码归档中找到 fortune-client pod 完整的 YAML 定义文件. 下面的代码清单展示了 init 容器定义的部分.</p> <p><strong>代码清单-17.2 pod 中定义的 init 容器: fortune-client.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">initContainers</span><span class="token punctuation">:</span>   <span class="token comment"># 定义一个init容器, 而不是常规的容器</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> init
    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c  
    <span class="token punctuation">-</span> 'while true; do echo &quot;Waiting for fortune service to come up<span class="token punctuation">...</span>&quot;;  
           wget http<span class="token punctuation">:</span>//fortune <span class="token punctuation">-</span>q <span class="token punctuation">-</span>T 1 <span class="token punctuation">-</span>O /dev/null <span class="token punctuation">&gt;</span>/dev/null 2<span class="token punctuation">&gt;</span>/dev/null   
           <span class="token important">&amp;&amp;</span> break; sleep 1; done; echo &quot;Service is up<span class="token tag">!</span> Starting main   
           container.&quot;'   <span class="token comment"># init容器运行一个循环, 并且在fortune服务启动之后循环才退出</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>当部署这个 pod 的时候, <strong>只有 pod 的 init 容器会启动起来</strong>. 这个可以通过命令 kubectl get 查看 pod 的状态来展示:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl get po
NAME             READY     STATUS     RESTARTS   AGE
fortune-client   <span class="token number">0</span>/1       Init:0/1   <span class="token number">0</span>          1m   <span class="token comment"># 只有init容器启动了</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>STATUS 列展示了目前没有 init 容器执行完毕. 可以通过 kubectl logs 命令来查看 init 容器的日志:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl logs fortune-client <span class="token parameter variable">-c</span> init
Waiting <span class="token keyword">for</span> fortune <span class="token function">service</span> to come up<span class="token punctuation">..</span>.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>当运行 kubectl logs 命令的时候, 需要通过选项 -c 来指定 init 容器的名称(在这个例子中, pod 的 init 容器的名称就叫作 init, 如代码清单 17.2 所示).</p> <p>主容器直到部署的 fortune 服务和 fortune-server pod 启动之后才会运行. 这些配置内容都在文件 fortune-server.yaml 中.</p> <blockquote><p>处理 pod 内部依赖的最佳实践</p></blockquote> <p>前面已经了解如何通过 init 容器来延迟 pod 主容器的启动, 直到预置的条件被满足(例如, 为了确保 pod 所依赖的服务已经准备好), 但是更佳的情况是<strong>构建一个不需要它所依赖的服务都准备好后才能启动的应用</strong>. 毕竟, 这些服务在后面也有可能下线, 但是这个时候应用已经在运行中了.</p> <p><strong>应用需要自身能够应对它所依赖的服务没有准备好的情况</strong>. 另外不要忘了 Readiness 探针. 如果一个应用在其中一个依赖缺失的情况下无法工作, 那么它需要通过它的 Readiness 探针来通知这个情况, 这样 Kubernetes 也会知道这个应用没有准备好. 需要这样做的原因不仅仅是因为这个就绪探针收到的信号会阻止应用成为一个服务端点, 另外还因为 Deployment 控制器在滚动升级的时候会使用应用的就绪探针, 因此可以避免错误版本的出现.</p> <h6 id="_17-2-4-增加生命周期钩子"><a href="#_17-2-4-增加生命周期钩子" class="header-anchor">#</a> 17.2.4 增加生命周期钩子</h6> <p>前面已经讨论了如果使用 init 容器来介入 pod 的启动过程, 另外 pod 还允许你<strong>定义两种类型的生命周期钩子</strong>:</p> <ul><li><mark><strong>启动后(Post-start)钩子</strong></mark></li> <li><mark><strong>停止前(Pre-stop)钩子</strong></mark></li></ul> <p>这些生命周期的钩子是<strong>基于每个容器</strong>来指定的, 和 init 容器不同的是, <strong>init 容器是应用到整个 pod</strong>. 这些钩子, 如它们的名字所示, 是<strong>在容器启动后和停止前执行</strong>的.</p> <p>生命周期钩子与存活探针和就绪探针相似的是它们都可以:</p> <ul><li><strong>在容器内部执行一个命令</strong></li> <li><strong>向一个 URL 发送 HTTP GET 请求</strong></li></ul> <p>下面分别来看一下这两个钩子, 看看它们是如何在容器的生命周期中起作用的.</p> <blockquote><p>使用启动后容器生命周期钩子</p></blockquote> <p><strong>启动后钩子是在容器的主进程启动之后立即执行的. 可以用它在应用启动时做一些额外的工作</strong>. 当然, 如果你是容器中运行的应用的开发者, 可以在应用的代码中加入这些操作. 但如果你在<mark><strong>运行一个其他人开发的应用, 大部分情况下并不想(或者无法)修改它的源代码. 启动后钩子可以让你在不改动应用的情况下, 运行一些额外的命令. 这些命令可能包括向外部监听器发送应用已启动的信号, 或者是初始化应用以使得应用能够顺利运行</strong></mark>.</p> <p>这个<strong>钩子和主进程是并行执行</strong>的. 钩子的名称或许有误导性, 因为它并不是等到主进程完全启动后(如果这个进程有一个初始化的过程, Kubelet 显然不会等待这个过程完成, 因为它并不知道什么时候会完成)才执行的.</p> <p>即使钩子是以<strong>异步方式</strong>运行的, 它确实通过两种方式来影响容器. <strong>在钩子执行完毕之前, 容器会一直停留在 Waiting 状态, 其原因是 ContainerCreating. 因此, pod 的状态会是 Pending 而不是 Running. 如果钩子运行失败或者返回了非零的状态码, 主容器会被杀死</strong>.</p> <p>一个包含启动后钩子的 pod manifest 内容如下面的代码清单所示.</p> <p><strong>代码清单-17.3 一个包含启动后生命周期钩子的 pod:poststart-hook.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>with<span class="token punctuation">-</span>poststart<span class="token punctuation">-</span>hook
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> luksa/kubia
    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubia
    <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>       <span class="token comment"># 钩子是在容器启动时执行的</span>
      <span class="token key atrule">postStart</span><span class="token punctuation">:</span>                                                     
        <span class="token key atrule">exec</span><span class="token punctuation">:</span>        <span class="token comment"># 它在容器内部执行 /bin 目录下的postStart.sh脚本</span>
          <span class="token key atrule">command</span><span class="token punctuation">:</span>                                                   
          <span class="token punctuation">-</span> sh                                                       
          <span class="token punctuation">-</span> <span class="token punctuation">-</span>c                                                       
          <span class="token punctuation">-</span> <span class="token string">&quot;echo 'hook will fail with exit code 15'; sleep 5; exit 15&quot;</span>  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>在这个例子中, <strong>命令 echo, sleep 和 exit 是在容器创建时和容器的主进程一起执行的</strong>. 典型情况下, 一般并不会像这样来执行命令, 而是通过存储在容器镜像中的 shell 脚本或者二进制可执行文件来运行.</p> <p>遗憾的是, 如果钩子程序启动的进程将日志输出到标准输出终端, 你将无法在任何地方看到它们. 这样就会导致调试生命周期钩子程序非常痛苦. 如果钩子程序失败了, 就仅仅会在 pod 的事件中看到一个 FailedPostStartHook 的告警信息(可以通过命令 kubectl describe pod 来查看). 稍等一会儿, 就可以看到更多关于钩子为什么失败的信息, 如下面的代码清单所示.</p> <p><strong>代码清单-17.4 pod 的事件显示了基于命令的钩子程序的退出码</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FailedSync   Error syncing pod, skipping: failed to <span class="token string">&quot;StartContainer&quot;</span> <span class="token keyword">for</span>
             <span class="token string">&quot;kubia&quot;</span> with PostStart handler: <span class="token builtin class-name">command</span> <span class="token string">'sh -c echo '</span>hook
             will fail with <span class="token builtin class-name">exit</span> code <span class="token number">15</span><span class="token string">'; sleep 5 ; exit 15'</span> exited
             with <span class="token number">15</span>: <span class="token builtin class-name">:</span> <span class="token string">&quot;PostStart Hook Failed&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>最后一行的数字 15 就是命令的退出码. 当使用 HTTP GET 请求作为钩子的时候, 失败原因可能类似于如下代码清单(可以从本书的代码归档中找到文件 poststart-hook-httpget.yaml 并部署一下).</p> <p><strong>代码清单-17.5 pod 的事件显示了基于 HTTP GET 的钩子程序的失败原因</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>FailedSync   Error syncing pod, skipping: failed to <span class="token string">&quot;StartContainer&quot;</span> <span class="token keyword">for</span>
             <span class="token string">&quot;kubia&quot;</span> with PostStart handler: Get
             http://10.32.0.2:9090/postStart: dial tcp <span class="token number">10.32</span>.0.2:9090:
             getsockopt: connection refused: <span class="token string">&quot;PostStart Hook Failed&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>注意: 这个启动后钩子是故意地使用错误的端口 9090 而不是正确的端口 8080 来演示钩子失败时会发生什么情况的.</p> <p>基于命令的启动后钩子输出到标准输出终端和错误输出终端的内容在任何地方都不会记录, 因此你或许想把钩子程序的进程输出记录到容器的文件系统文件中, 这样可以通过如下的命令来查看文件的内容:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> my-pod <span class="token function">cat</span> logfile.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>如果容器因为各种原因重启了(包括由于钩子执行失败导致的), 这个文件在你能够查看之前就消失了. 这种情况下, 可以通过<strong>给容器挂载一个 emptyDir 卷, 并且让钩子程序向这个存储卷写入内容</strong>来解决.</p> <blockquote><p>使用停止前容器生命周期钩子</p></blockquote> <p><strong>停止前钩子是在容器被终止之前立即执行的. 当一个容器需要终止运行的时候, Kubelet 在配置了停止前钩子的时候就会执行这个停止前钩子, 并且仅在执行完钩子程序后才会向容器进程发送 SIGTERM 信号(如果这个进程没有优雅地终止运行, 则会被杀死).</strong></p> <p><mark><strong>停止前钩子在容器收到 SIGTERM 信号后没有优雅地关闭的时候, 可以利用它来触发容器以优雅的方式关闭. 这些钩子也可以在容器终止之前执行任意的操作, 并且并不需要在应用内部实现这些操作(当你在运行一个第三方应用, 并且在无法访问应用或者修改应用源码的情况下很有用).</strong></mark></p> <p>在 pod 的 manifest 中配置停止前钩子和增加一个启动后钩子方法差不多. 上面的例子演示了执行命令的启动后钩子, 这里我们来看看执行一个 HTTP GET 请求的停止前钩子. 下面的代码清单演示了如何在 pod 中定义一个停止前 HTTP GET 的钩子.</p> <p><strong>代码清单-17.6 停止前钩子的 YAML 配置片段: pre-stop-hookhttpget.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>
  <span class="token key atrule">preStop</span><span class="token punctuation">:</span>         <span class="token comment"># 这是一个执行HTTP GET请求的停止前钩子</span>
    <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>          
      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>   <span class="token comment"># 这个请求发送到http://pod_IP:8080/shutdown</span>
      <span class="token key atrule">path</span><span class="token punctuation">:</span> shutdown    
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这个代码清单中定义的停止前钩子在 Kubelet 开始终止容器的时候就立即执行到 <code>http://pod_IP:8080/shutdown</code>​ 的 HTTP GET 请求. 除了代码清单中所示的 port 和 path, 还可以设置 scheme(HTTP 或 HTTPS)和 host, 当然也可以设置发送出去的请求的 httpHeaders. 默认情况下, host 的值是 pod 的 IP 地址. 确保请求不会发送到 localhost, 因为 localhost 表示节点, 而不是 pod.</p> <p>和启动后钩子不同的是, <mark><strong>无论钩子执行是否成功容器都会被终止</strong></mark>. 无论是 HTTP 返回的错误状态码或者基于命令的钩子返回的非零退出码都不会阻止容器的终止. 如果停止前钩子执行失败了, 就会在 pod 的事件中看到一个 FailedPreStopHook 的告警, 但是因为 pod 不久就会被删除了(毕竟是 pod 的删除动作触发的停止前钩子的执行), 你或许都看不到停止前钩子执行失败了.</p> <p>提示: 如果停止前钩子的成功执行对系统的行为很重要, 请确认这个钩子是否成功执行了. 笔者遇到过停止前钩子根本没有执行而开发者都没有注意到的情况.</p> <blockquote><p>在应用没有收到 SIGTERM 信号时使用停止前钩子</p></blockquote> <p>很多开发者在定义停止前钩子的时候会犯错误, 他们在钩子中只向应用发送了 SIGTERM 信号. 他们这样做是因为他们没有看到他们的应用接收到 Kubelet 发送的 SIGTERM 信号. 应用没有接收到信号的原因并不是 Kubernetes 没有发送信号, 而是因为<strong>在容器内部信号没有被传递给应用的进程</strong>. 如果容器镜像配置是通过执行一个 shell 进程, 然后在 shell 进程内部执行应用进程, 那么这个信号就被这个 shell 进程吞没了, 这样就不会传递给子进程.</p> <p>在这种情况下, 合理的做法是<strong>让 shell 进程传递这个信号给应用进程, 而不是添加一个停止前钩子来发送信号给应用进程</strong>. 可以通过在作为主进程执行的 shell 进程内处理信号并把它传递给应用进程的方式来实现. 或者如果无法配置容器镜像执行 shell 进程, 而是通过直接运行应用的二进制文件, 可以通过在 Dockerfile 中使用 ENTRYPOINT 或者 CMD 的 exec 方式来实现, 即 <code>ENTRYPOINT[&quot;/mybinary&quot;]</code>​ 而不是 <code>ENTRYPOINT /mybinary</code>​.</p> <p>在通过第一种方式运行二进制文件 mybinary 的容器中, 这个进程就是容器的主进程, 而在第二种方式中, 是先运行一个 shell 作为主进程, 然后 mybinary 进程作为 shell 进程的子进程运行.</p> <blockquote><p>了解生命周期钩子是针对容器而不是 pod</p></blockquote> <p>作为对启动后和停止前钩子最后的思考, 笔者会强调的是这些<mark><strong>生命周期的钩子是针对容器而不是 pod 的</strong></mark>. 不应该使用停止前钩子来运行那些需要在 pod 终止的时候执行的操作. 原因是停止前钩子只会在容器被终止前调用(大部分可能是因为存活探针失败导致的终止). 这个过程会在 pod 的生命周期中发生多次, 而不仅仅是在 pod 被关闭的时候.</p> <h6 id="_17-2-5-了解pod的关闭"><a href="#_17-2-5-了解pod的关闭" class="header-anchor">#</a> 17.2.5 了解pod的关闭</h6> <p>前面已经接触过关于 pod 终止的话题, 所以这里会进一步探讨相关细节来<mark><strong>看看 pod 关闭的时候具体发生了什么</strong></mark>. 这个对理解如何干净地关闭 pod 中运行的应用很重要.</p> <p>下面从头开始, <strong>pod 的关闭是通过 API 服务器删除 pod 的对象来触发的. 当接收到 HTTP DELETE 请求后, API 服务器还没有删除 pod 对象, 而是给 pod 设置一个 deletionTimestamp 值. 拥有 deletionTimestamp 的 pod 就开始停止了</strong>.</p> <p>当 Kubelet 意识到需要终止 pod 的时候, 它开始<strong>终止 pod 中的每个容器</strong>. Kubelet 会<strong>给每个容器一定的时间来优雅地停止</strong>. 这个时间叫作<strong>终止宽限期(Termination Grace Period)</strong> , 每个 pod 可以单独配置. 在终止进程开始之后, 计时器就开始计时, 接着按照顺序执行以下事件:</p> <ol><li>执行停止前钩子(如果配置了的话), 然后等待它执行完毕</li> <li>向容器的主进程发送 SIGTERM 信号</li> <li>等待容器优雅地关闭或者等待终止宽限期超时</li> <li>如果容器主进程没有优雅地关闭, 使用 SIGKILL 信号强制终止进程</li></ol> <p>事件的顺序如图 17.5 所示.</p> <p><img src="/img/image-20240227204353-8fmx8nf.png" alt="image" title="图17.5 容器停止顺序"></p> <blockquote><p>指定终止宽限期</p></blockquote> <p>终止宽限期可以通过 pod spec 中的 <strong>spec.terminationGracePeriodSeconds</strong> <strong>字段</strong>来设置. 默认情况下, 值为 30, 表示容器在被强制终止之前会有 <strong>30 秒</strong>的时间来自行优雅地终止.</p> <p>提示: 应该将终止宽限时间设置得足够长, 这样容器进程才可以在这个时间段内完成清理工作.</p> <p>在删除 pod 的时候, pod spec 中指定的终止宽限时间也可以通过如下方式来覆盖:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po mypod --grace-period<span class="token operator">=</span><span class="token number">5</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个命令将会让 Kubectl <strong>等待 5 秒钟, 让 pod 自行关闭</strong>. 当 pod 所有的容器都停止后, Kubelet 会通知 API 服务器, 然后 pod 资源最终都会被删除. 可以强制 API 服务器立即删除 pod 资源, 而不用等待确认. 可以通过设置宽限时间为 0, 然后增加一个 --force 选项来实现:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl delete po mypod --grace-period<span class="token operator">=</span><span class="token number">0</span> <span class="token parameter variable">--force</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在使用这个选项的时候需要注意, 尤其是 StatefulSet 的 pod. StatefulSet 控制器会非常小心地避免在同一时间运行相同 pod 的两个实例(两个 pod 拥有相同的序号, 名称, 并且挂载到相同的 PersistentVolume). 强制删除一个 pod 会导致控制器不会等待被删的 pod 里面的容器完成关闭就创建一个替代的 pod. 换句话说, <strong>相同 pod 的两个实例可能在同一时间运行, 这样会导致有状态的集群服务工作异常</strong>. 只有在确认 pod 不会再运行, 或者无法和集群中的其他成员通信(可以通过托管 pod 的节点网络连接失败并且无法重连来确认)的情况下再强制删除有状态的 pod.</p> <p>现在你已经了解了容器关闭的方式, 接下来<strong>从应用的角度来看一下应用应该如何处理容器的关闭流程</strong>.</p> <blockquote><p>在应用中合理地处理容器关闭操作</p></blockquote> <p><mark><strong>应用应该通过启动关闭流程来响应 SIGTERM 信号, 并且在流程结束后终止运行. 除了处理 SIGTERM 信号, 应用还可以通过停止前钩子来收到关闭通知. 在这两种情况下, 应用只有固定的时间来干净地终止运行.</strong></mark></p> <p>但是如果无法预测应用需要多长时间来干净地终止运行怎么办呢? 例如, 假设应用是一个分布式数据存储. 在缩容的时候, 其中一个 pod 的实例会被删除然后关闭. 在这个关闭的过程中, 这个 pod 需要将它的数据迁移到其他存活的 pod 上面以确保数据不会丢失. 那么这个 pod 是否应该在接收到终止信号的时候就开始迁移数据(无论是通过 SIGTERM 信号还是停止前钩子)?</p> <p>完全不是! 这种做法是不推荐的, 理由至少有两点:</p> <ul><li>一个容器终止运行并不一定代表整个 pod 被终止了.</li> <li>无法保证这个关闭流程能够在进程被杀死之前执行完毕.</li></ul> <p>第二种场景不仅会在应用在超过终止宽限期还没有优雅地关闭时发生, 还会在容器关闭过程中运行 pod 的节点出现故障时发生. 即使这个时候节点又重启了, Kubelet 不会重启容器的关闭流程(甚至都不会再启动这个容器了). 这样就无法保证 pod 可以完成它整个关闭的流程.</p> <blockquote><p>将重要的关闭流程替换为专注于关闭流程的 pod</p></blockquote> <p><strong>如何确认一个必须运行完毕的重要的关闭流程真的运行完毕了呢</strong>(例如, 确认一个 pod 的数据成功迁移到了另外一个 pod)?</p> <p>这个问题的合理的解决方案是<strong>用一个专门的持续运行中的 pod 来持续检查是否存在孤立的数据</strong>. 当这个 pod 发现孤立的数据的时候, 它就可以把它们迁移到仍存活的 pod. 当然不一定是一个持续运行的 pod, 也可以使用 CronJob 资源来周期性地运行这个 pod.</p> <p>你或许以为 StatefulSet 在这里会有用处, 但实际上并不是这样. 如你所记起的那样, 给 StatefulSet 缩容会导致 PersistentVolumeClaim 处于孤立状态, 这会导致存储在 PersistentVolumeClaim 中的数据搁浅. 当然, 在后续的扩容过程中, PersistentVolume 会被附加到新的 pod 实例中, 但是万一这个扩容操作永远不会发生(或者很久之后才会发生)呢? 因此, 当在使用 StatefulSet 的时候或许想运行一个数据迁移的 pod(这种场景如图 17.6 所示). 为了避免应用在升级过程中出现数据迁移, 专门用于数据迁移的 pod 可以在数据迁移之前配置一个等待时间, 让有状态的 pod 有时间启动起来.</p> <p><img src="/img/image-20240227204825-svcxnzp.png" alt="image" title="图17.6 使用专门的 pod 来迁移数据"></p> <h5 id="_17-3-确保所有的客户端请求都得到了妥善处理"><a href="#_17-3-确保所有的客户端请求都得到了妥善处理" class="header-anchor">#</a> 17.3 确保所有的客户端请求都得到了妥善处理</h5> <p>现在已经了解清楚如何干净地关闭 pod 了. 下面<strong>从 pod 的客户端角度来看看 pod 的生命周期</strong>(使用 pod 提供的服务的客户端). 了解这一点很重要, 如果你希望 pod 扩容或者缩容的时候客户端不会遇到问题的话.</p> <p>毋庸赘言, 大家都希望<strong>所有的客户端请求都能够得到妥善的处理</strong>. 你显然<strong>不希望 pod 在启动或者关闭过程中出现断开连接的情况</strong>. Kubernetes 本身并没有避免这种事情的发生. 你的应用需要遵循一些规则来避免遇到连接断开的情况. 首先重点看一下如何在 pod 启动的时候, 确保所有的连接都被妥善处理了.</p> <h6 id="_17-3-1-在pod启动时避免客户端连接断开"><a href="#_17-3-1-在pod启动时避免客户端连接断开" class="header-anchor">#</a> 17.3.1 在pod启动时避免客户端连接断开</h6> <p>确保 pod 启动的时候每个连接都被妥善处理很容易, 只要理解了服务和服务端点是如何工作的. <strong>当一个 pod 启动的时候, 它以服务端点的方式提供给所有的服务, 这些服务的标签选择器和 pod 的标签匹配</strong>. 在第 5 章说过, pod 需要发送信号给 Kubernetes 通知它自己已经准备好了. <strong>pod 在准备好之后, 它才能变成一个服务端点, 否则无法接收任何客户端的连接请求</strong>.</p> <p>如果在 pod spec 中没有指定就绪探针, 那么 pod 总是被认为是准备好了的. 当第一个 kube-proxy 在它的节点上面更新了 iptables 规则之后, 并且第一个客户端 pod 开始连接服务的时候, 这个默认被认为是准备好了的 pod 几乎会立即开始接收请求. 如果应用这个时候还没有准备好接收连接, 那么所有的客户端都会看到 &quot;连接被拒绝&quot; 一类的错误信息.</p> <p><mark><strong>需要做的是当且仅当应用准备好处理进来的请求的时候, 才去让就绪探针返回成功. 好的实践第一步是添加一个指向应用根 URL 的 HTTP GET 请求的就绪探针</strong></mark>. 在很多情况下, 这样做就足够了, 免得你还需要在应用中实现一个特殊的 readiness endpoint.</p> <h6 id="_17-3-2-在pod关闭时避免客户端连接断开"><a href="#_17-3-2-在pod关闭时避免客户端连接断开" class="header-anchor">#</a> 17.3.2 在pod关闭时避免客户端连接断开</h6> <p>现在来看一下<strong>当 pod 被删除, pod 的容器被终止的时候会发生什么</strong>. 前面已经讨论过 pod 的容器应该如何在它们收到 SIGTERM 信号的时候干净地关闭(或者容器的停止前钩子被执行的时候). 但是这就能确保所有的客户端请求都被妥善处理了吗?</p> <p><mark><strong>当应用接收到终止信号的时候应该如何做呢? 它应该继续接收请求么? 那些已经被接收但是还没有处理完毕的请求该怎么办呢? 那些打开的 HTTP 长连接(连接上已经没有活跃的请求了)该怎么办呢</strong></mark>? 在回答这些问题之前, 需要详细地看一下当 pod 删除的时候, 集群中的一连串事件是如何发生的.</p> <blockquote><p>了解 pod 删除时发生的一连串事件</p></blockquote> <p>在第 11 章中, 已经深入地研究了一下 Kubernetes 集群的组成部分. 需要一直记住的是这些<strong>组件都是运行在不同机器上面的不同的进程</strong>. 它们并不是在一个庞大的单一进程中. 让集群中的所有组件同步到一致的集群状态需要时间. 下面通过 pod 删除时集群中发生的<strong>一连串事件</strong>来探究一下真相.</p> <p><strong>当 API 服务器接收到删除 pod 的请求之后, 它首先修改了 etcd 中的状态并且把删除事件通知给观察者. 其中的两个观察者就是 Kubelet 和端点控制器(Endpoint Controller)</strong> . 图 17.7 展示了并行发生的两串事件(用 A 或 B 标识).</p> <p><img src="/img/image-20240227210219-f63tj3u.png" alt="image" title="图17.7 pod 删除时发生的一连串事件"></p> <p>在标识为 A 的一串事件中, 当 Kubelet 接收到 pod 应该被终止的通知的时候, 它初始化了 17.2.5 中讲解过的关闭动作序列(<strong>执行停止前钩子, 发送 SIGTERM 信号, 等待一段时间, 然后在容器没有自我终止时强制杀死容器</strong>). 如果应用立即停止接收客户端的请求以作为对 SIGTERM 信号的响应, 那么任何尝试连接到应用的请求都会收到 Connection Refused 的错误. 从 pod 被删除到发生这个情况的时间相对来说特别短, 因为这是 API 服务器和 Kubelet 之间的直接通信.</p> <p>那么, 再看看另外一串事件中发生了什么, 就是<strong>在 pod 被从 iptables 规则中移除之前的那些事件</strong>(图中标识为 B 的序列). 当端点控制器(在 Kubernetes 的控制面板的 Controller Manager 中运行)接收到 pod 要被删除的通知时, 它<strong>从所有 pod 所在的服务中移除了这个 pod 的服务端点. 它通过向 API 服务器发送 REST 请求来修改 Endpoint API 对象. 然后 API 服务器会通知所有的客户端关注这个 Endpoint 对象. 其中的一些观察者都是运行在工作节点上面的 kube-proxy 服务. 每个 kube-proxy 服务都会在自己的节点上更新 iptables 规则, 以阻止新的连接被转发到这些处于停止状态的 pod 上</strong>. 这里一个重要的细节是, <strong>移除 iptables 规则对已存在的连接没有影响, 已经连接到 pod 的客户端仍然可以通过这些连接向 pod 发送额外的请求</strong>.</p> <p>上面的两串事件是<strong>并行发生</strong>的. 最有可能的是, 关闭 pod 中应用进程所消耗的时间比完成 iptables 规则更新所需要的时间稍微短一点. 导致 iptables 规则更新的那一串事件相对比较长(见图 17.8), 因为这些事件必须先到达 Endpoint 控制器, 然后 Endpoint 控制器向 API 服务器发送新的请求, 然后 API 服务器必须修改 kube-proxy, 最后 kube-proxy 再修改 iptables 规则. 存在一个很大的可能性是 SIGTERM 信号会在 iptables 规则更新到所有的节点之前发送出去.</p> <p>最终的结果是, 在<strong>发送终止信号给 pod 之后, pod 仍然可以接收客户端请求</strong>. 如果应用立即关闭服务端套接字, 停止接收请求的话, 这会导致客户端收到 &quot;连接被拒绝&quot; 一类的错误(这个情形和 pod 启动时应用还无法立即接收请求, 并且还没有给 pod 定义一个就绪探针时发生的一样).</p> <p><img src="/img/image-20240227210248-kivxdw3.png" alt="image" title="图17.8 pod 删除时事件发生的时间线"></p> <blockquote><p>解决问题</p></blockquote> <p>用 Google 搜索这个问题的解决方案看上去就是<strong>给 pod 添加一个就绪探针</strong>来解决问题. 假设所需要做的事情就是在 pod 接收到 SIGTERM 信号的时候就绪探针开始失败. 这会导致 pod 从服务的端点中被移除. 但是<strong>这个移除动作只会在就绪探针持续性失败一段时间后才会发生(可以在就绪探针的 spec 中配置), 并且这个移除动作还是需要先到达 kube-proxy 然后 iptables 规则才会移除这个 pod</strong>.</p> <p>实际上, 就绪探针完全不影响这个过程. <strong>端点控制器在接收到 pod 要被删除(当 pod spec 中的 deletionTimestamp 字段不再是 null)的通知的时候就会从 Endpoint 中移除 pod</strong>. 从那个时候开始, 就绪探针的结果已经无关紧要了.</p> <p><mark><strong>那么这个问题的合适的解决方案是什么呢? 如何保证所有的请求都被处理了呢?</strong></mark></p> <p>很明显, <strong>pod 必须在接收到终止信号之后仍然保持接收连接直到所有的 kubeproxy 完成了 iptables 规则的更新. 当然, 不仅仅是 kube-proxy, 这里还会有 Ingress 控制器或者负载均衡器直接把请求转发给 pod 而不经过 Service(iptables). 这也包括使用客户端负载均衡的客户端. 为了确保不会有客户端遇到连接断开的情况, 需要等到它们通知你它们不会再转发请求给 pod 的时候</strong>.</p> <p>这是不可能的, 因为这些组件分布在不同的机器上面. 即使知道每一个组件的位置并且可以等到它们都来通知你可以关闭 pod 了, 万一其中有一个组件未响应呢? 这个时候, 需要等待这个回复多长时间? 记住, 在这个时间段内, 你延阻了关闭的过程.</p> <p>可以做的唯一的合理的事情就是<mark><strong>等待足够长的时间让所有的 kube-proxy 可以完成它们的工作</strong></mark>. 那么多长时间才是足够的呢? 在大部分场景下, 几秒钟应该就足够了, 但是无法保证每次都是足够的. 当 API 服务器或者端点控制器过载的时候, 通知到达 kube-proxy 的时间会更长. 你<strong>无法完美地解决这个问题, 理解这一点很重要</strong>, 但是即使增加 5 秒或者 10 秒延迟也会极大提升用户体验. 可以用长一点的延迟时间, 但是别太长, 因为这会导致容器无法正常关闭, 而且会导致 pod 被删除很长一段时间后还显示在列表里面, 这个会给删除 pod 的用户带来困扰.</p> <blockquote><p>小结</p></blockquote> <p>简要概括一下, <strong>妥善关闭一个应用包括如下步骤</strong>:</p> <ul><li><p><strong>等待几秒钟, 然后停止接收新的连接.</strong></p></li> <li><p><strong>关闭所有没有请求过来的长连接.</strong></p></li> <li><p><strong>等待所有的请求都完成.</strong></p> <p>‍</p></li> <li><p><strong>然后完全关闭应用.</strong></p> <p>‍</p></li></ul> <p>为了理解这个过程中连接和请求都发生了什么, 请仔细看一下图 17.9.</p> <p><img src="/img/image-20240227210502-spcbmc5.png" alt="image" title="图17.9 在接收到终止信号后妥善地处理已存在和建立的连接"></p> <p>这个过程不像进程接收到终止信号立即退出那么简单, 不是吗? 真的值得这么做吗? 这个取决于你. 但是至少可以添加一个停止前钩子来等待几秒钟再退出, 或许就像下面代码清单中所示的一样.</p> <p><strong>代码清单-17.7 用于避免连接断开的停止前钩子</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>
  <span class="token key atrule">preStop</span><span class="token punctuation">:</span>
    <span class="token key atrule">exec</span><span class="token punctuation">:</span>
      <span class="token key atrule">command</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> sh
      <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
      <span class="token punctuation">-</span> <span class="token string">&quot;sleep 5&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这样就不需要修改代码了. 如果应用已经能够确保所有的进来的请求都得到了处理, 那么这个停止前钩子带来的等待已经足够了.</p> <h5 id="_17-4-让应用在kubernetes中方便运行和管理"><a href="#_17-4-让应用在kubernetes中方便运行和管理" class="header-anchor">#</a> 17.4 让应用在Kubernetes中方便运行和管理</h5> <p>希望你目前已经对如何妥善处理客户端请求有了一个更加清晰的了解. 下面看看<strong>如何构建方便在 Kubernetes 中管理的应用</strong>.</p> <h6 id="_17-4-1-构建可管理的容器镜像"><a href="#_17-4-1-构建可管理的容器镜像" class="header-anchor">#</a> 17.4.1 构建可管理的容器镜像</h6> <p>当把应用打包进镜像的时候, 可以包括应用的二进制文件和它的依赖库, 或者可以将一个完整的操作系统和应用打包在一起. 很多人都会这样做, 尽管很多时候并不需要这样.</p> <p>镜像里面的操作系统中的每个文件你都需要吗? 或许并不是这样. 大多数文件都不会用到而且仅仅会让你的镜像变得比需要的大. 当然, 镜像的分层会让每个独立的层只会被下载一次, 但是当 pod 第一次被调度到节点的时候, 你也不希望等待过长的时间.</p> <p>部署新的 pod 或者扩展它们会很快. 这个要求镜像足够小而且不包容任何无用的东西. 如果使用 Go 语言来构建应用, 那么镜像除了应用的可执行二进制文件外不需要任何东西. <strong>这样基于 Go 语言的容器镜像就会非常小, 很适合 Kubernetes</strong>.</p> <p>提示: 在这些镜像的 Dockerfile 中使用 FROM scratch 指令.</p> <p>但是在实践中, 就会发现这些最小化构建的镜像非常难以调试. 当需要运行一些工具, 例如 ping, dig, curl 或者容器中其他类似的命令的时候, 就会意识到让容器再至少包含这些工具的最小集合有多重要. 笔者无法告诉你应该在你的镜像中包含哪些工具, 不包含哪些工具, 因为一切取决于你的需求, 你需要自己发现最适合自己的方式.</p> <h6 id="_17-4-2-合理地给镜像打标签-正确地使用imagepullpolicy"><a href="#_17-4-2-合理地给镜像打标签-正确地使用imagepullpolicy" class="header-anchor">#</a> 17.4.2 合理地给镜像打标签,正确地使用ImagePullPolicy</h6> <p>你很快就会发现在 pod manifest 中使用 latest 来引用镜像会出问题, 因为你<strong>无法知道每个 pod 副本中运行的镜像版本</strong>. 即使开始的时候所有的 pod 副本都运行相同的镜像版本, 当再以标签 latest 来推送一个新的镜像版本的时候, 如果 pod 被重新调度了(或者你扩容了 Deployment), 新的 pod 就会运行新的镜像版本, 而旧的 pod 还是运行旧的镜像版本. 另外, <strong>使用 latest 标签会导致无法回退到之前的版本(除非你重新推送了旧的镜像)</strong> .</p> <p><mark><strong>必须使用能够指明具体版本的标签而不是 latest</strong></mark>. 记住如果使用的是可更改的标签(总是向相同的标签推送更改), 那么需要在 pod spec 中将 imagePullPolicy 设置为 Always. 但是如果在生产环境中使用这种方式, 需要注意它的附加说明. 如果镜像的拉取策略设置为 Always 的话, 容器的运行时在遇到新的 pod 需要部署的时候都会去联系镜像注册中心. 这会拖慢 pod 的启动速度, 因为节点需要去检查镜像是否已经被修改了. 更糟糕的是, 当镜像注册中心无法连接到的时候, 这个策略会导致新的 pod 无法启动.</p> <h6 id="_17-4-3-使用多维度而不是单维度的标签"><a href="#_17-4-3-使用多维度而不是单维度的标签" class="header-anchor">#</a> 17.4.3 使用多维度而不是单维度的标签</h6> <p><strong>别忘了给所有的资源都打上标签, 而不仅仅是 pod</strong>. 确保你给每个资源添加了多个标签, 这样就可以通过不同的维度来选择它们了. 在资源数量飞速增长的时候, 你(或者运维团队)会感激你自己的.</p> <p>标签可以包含如下的内容:</p> <ul><li><strong>资源所属的应用(或者微服务)的名称</strong></li> <li><strong>应用层级(前端, 后端, 等等)</strong></li> <li><strong>运行环境(开发, 测试, 预发布, 生产, 等等)</strong></li> <li><strong>版本号</strong></li> <li><strong>发布类型(稳定版, 金丝雀, 蓝绿开发中的绿色或者蓝色, 等等)</strong></li> <li><strong>租户(如果你在每个租户中运行不同的 pod 而不是使用命名空间)</strong></li> <li><strong>分片(带分片的系统)</strong></li></ul> <p>标签管理可以让你以组而不是隔离的方式来管理资源, 从而很容易了解资源的归属.</p> <h6 id="_17-4-4-通过注解描述每个资源"><a href="#_17-4-4-通过注解描述每个资源" class="header-anchor">#</a> 17.4.4 通过注解描述每个资源</h6> <p>可以<strong>使用注解来给你的资源添加额外的信息. 资源至少应该包括一个描述资源的注解和一个描述资源负责人的注解</strong>.</p> <p>在微服务框架中, pod 应该包含一个注解来描述该 pod 依赖的其他服务的名称. 这样就很容易展现 pod 之间的依赖关系了. 其他的注解可以包括构建和版本信息, 以及其他工具或者图形界面会使用到的元信息(图标名称等).</p> <p>标签和注解都可以让你更加容易地管理运行中的应用, 但是没有什么比应用开始崩溃而你对原因一无所知更糟糕的了.</p> <h6 id="_17-4-5-给进程终止提供更多的信息"><a href="#_17-4-5-给进程终止提供更多的信息" class="header-anchor">#</a> 17.4.5 给进程终止提供更多的信息</h6> <p>没有什么比调查容器为什么终止运行(或者持续终止运行)更加令人沮丧的了, 尤其是在最糟糕的时候发生这个情况. 对运维人员好一点吧, 把<strong>所有必需的调试信息都写到日志文件中</strong>.</p> <p>为了让诊断过程更容易, 可以使用 Kubernetes 的另一个特性, 这个特性可以<strong>在 pod 状态中很容易地显示出容器终止的原因. 可以让容器中的进程向容器的文件系统中指定文件写入一个终止消息. 这个文件的内容会在容器终止后被 Kubelet 读取, 然后显示在 kubectl describe pod 中. 如果一个应用使用这种机制的话, 操作人员无须去查看容器的日志就可以很快地看到应用为什么终止了</strong>.</p> <p>这个进程需要写入终止消息的文件默认路径是 <code>/dev/termination-log</code>​, 当然这个路径也可以在 pod spec 中容器定义的部分设置 terminationMessagePath 字段来自定义.</p> <p>可以通过运行一个容器会立即死亡的 pod 来实际看一下这个过程, 如下面的代码清单所示.</p> <p><strong>代码清单-17.8 写终止消息的 pod:termination-message.yaml</strong></p> <div class="language-yml line-numbers-mode"><pre class="language-yml"><code><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span>with<span class="token punctuation">-</span>termination<span class="token punctuation">-</span>message
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox
    <span class="token key atrule">name</span><span class="token punctuation">:</span> main
    <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /var/termination<span class="token punctuation">-</span>reason  <span class="token comment"># 在覆盖默认的终止消息写入文件路径</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span>
    <span class="token punctuation">-</span> sh
    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c
    <span class="token punctuation">-</span> 'echo &quot;I''ve had enough&quot; <span class="token punctuation">&gt;</span> /var/termination<span class="token punctuation">-</span>reason ; exit 1'  <span class="token comment"># 容器会在退出之前将这个消息写入文件</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>当运行这个 pod 的时候, 会很快看到 pod 的状态变成 CrashLoopBackOff. 这个时候如果使用 kubectl describe, 就会看到容器为什么死亡了, 而不需要去深入到它的日志中, 如下面的代码清单所示.</p> <p><strong>代码清单-17.9 使用 kubectl describe 来查看容器的终止消息</strong></p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl describe po
Name:           pod-with-termination-message
<span class="token punctuation">..</span>.
Containers:
<span class="token punctuation">..</span>.
    State:      Waiting
      Reason:   CrashLoopBackOff
    Last State: Terminated
      Reason:   Error
      Message:  I've had enough     <span class="token comment"># 可以不用查看日志就知道容器死亡的原因</span>
      Exit Code:        <span class="token number">1</span>
      Started:          Tue, <span class="token number">21</span> Feb <span class="token number">2017</span> <span class="token number">21</span>:38:31 +0100
      Finished:         Tue, <span class="token number">21</span> Feb <span class="token number">2017</span> <span class="token number">21</span>:38:31 +0100
    Ready:              False
    Restart Count:      <span class="token number">6</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>如你所见, 容器进程写入文件 <code>/var/termination-reason</code>​ 中的终止消息 &quot;I′ve had enough&quot; 显示在了容器的 <strong>Last State</strong> 中. 注意, 这个机制并不仅仅适用于崩溃的容器. 它<strong>也可以用在那些运行一个可完成的任务并且成功终止的容器中</strong>(可以在文件 termination-message-success.yaml 中找到示例).</p> <p>这种机制对已终止运行的容器非常有用, 然而你或许会同意类似的机制对于显示运行中的应用的特定状态信息(不仅仅是已终止的容器)也很有用. Kubernetes 暂时不提供类似功能, 而且笔者也不清楚是否有计划引入.</p> <p>注意: 如果容器没有向任何文件写入消息, 可以将 terminationMessage Policy 字段的值设置为 FallbackToLogsOnError. 在这种情况下, <strong>容器的最后几行日志会被当作终止消息</strong>(当然仅当容器没有成功终止的情况下).</p> <h6 id="_17-4-6-处理应用日志"><a href="#_17-4-6-处理应用日志" class="header-anchor">#</a> 17.4.6 处理应用日志</h6> <p>当讨论应用的日志记录时, 需要再次强调应用应该将日志写到标准输出终端而不是文件中. 这样可以很容易地通过 kubectl logs 命令来查看应用日志.</p> <p>提示: 如果一个容器崩溃了, 然后使用一个新的容器替代它, 就会看到新的容器的日志. 如果希望看到之前容器的日志, 那么在使用 kubectl logs 命令的时候, 加上选项 --previous.</p> <p>如果应用把日志写到了文件而不是标准输出终端, 那么可以使用另外一种方法来查看日志:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token builtin class-name">exec</span> <span class="token operator">&lt;</span>pod<span class="token operator">&gt;</span> <span class="token function">cat</span> <span class="token operator">&lt;</span>logfile<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个命令会在容器内部执行 cat 命令, 把日志流返回给 kubectl, 然后 kubectl 将它们显示在你的终端.</p> <blockquote><p>将日志或者其他文件复制到容器或者从容器中复制出来</p></blockquote> <p>也可以使用 kubectl cp 命令将日志文件复制到本地机器, 这个目前还没有介绍过. 这个命令允许从容器中复制文件或者将文件复制到容器中. 例如, 如果一个 pod 名叫 foo-pod, 它只有一个容器, 并且这个容器有个文件叫作 <code>/var/log/foo.log</code>​, 那么可以使用下面的命令将这个文件传送到本地机器:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token function">cp</span> foo-pod:/var/log/foo.log foo.log
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>将文件从你的本地机器复制到 pod 中, 可以指定 pod 的名字作为第二个参数:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ kubectl <span class="token function">cp</span> localfile foo-pod:/etc/remotefile
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个命令把本地文件 localfile 复制到了 pod 的容器里面, 路径是 <code>/etc/remotefile</code>​. 如果 pod 中有多个容器, 可以使用 -c containerName 选项来指定具体的容器.</p> <blockquote><p>使用集中式日志记录</p></blockquote> <p>在一个生产环境系统中, 你希望使用一个<strong>集中式的面向集群的日志解决方案</strong>, 所有的日志都会被收集并且(永久地)存储在一个中心化的位置. 这样可以查看历史日志, 分析趋势. 如果没有这个系统, pod 的日志只有在 pod 存在的时候才存在. 当 pod 被删除之后, 它的日志也会被删除.</p> <p>Kubernetes 本身并不提供任何集中式的日志记录, <strong>必须通过其他的组件来支持所有容器日志的集中式的存储和分析, 这些组件通常在集群中以普通的 pod 方式运行</strong>.</p> <p>你或许已经听说过由 ElasticSearch, Logstash 和 Kibana 组成的 ELK 栈. 一个稍微更改的变种是 <strong>EFK</strong> 栈, 其中 Logstash 被 FluentD 替换了.</p> <p>当使用 EFK 作为集中式日志记录的时候, <strong>每个 Kubernetes 集群节点都会运行一个 FluentD 的代理(通过使用 DaemonSet 作为 pod 来部署), 这个代理负责从容器搜集日志, 给日志打上和 pod 相关的信息, 然后把它们发送给 ElasticSearch, 然后由 ElasticSearch 来永久地存储它们. ElasticSearch 在集群中也是作为 pod 部署的. 这些日志可以通过 Kibana 在 Web 浏览器中查看和分析, Kibana 是一个可视化 ElasticSearch 数据的工具. 它经常也是作为 pod 来运行的, 并且通过一个服务暴露出来</strong>. EFK 的三个组件如下图所示.</p> <p><img src="/img/image-20240227211340-p6wfe3c.png" alt="image" title="图17.10 使用 FluentD, ElasticSearch 和 Kibana 的集中式日志记录"></p> <h5 id="_17-5-开发和测试的最佳实践"><a href="#_17-5-开发和测试的最佳实践" class="header-anchor">#</a> 17.5 开发和测试的最佳实践</h5> <p>前面已经讲解了开发应用时需要注意的事项, 但还没有谈到帮助你简化这些过程的开发和测试流程. 这里笔者不打算讲得太详细, 因为每个人都需要找到适合他们自己的最佳方式, 但是这里有几个方案的基本出发点供参考.</p> <h6 id="_17-5-1-开发过程中在kubernetes之外运行应用"><a href="#_17-5-1-开发过程中在kubernetes之外运行应用" class="header-anchor">#</a> 17.5.1 开发过程中在Kubernetes之外运行应用</h6> <p>当在开发一个即将在生产环境的 Kubernetes 中运行的应用时, 是否意味着需要在开发的时候就在 Kubernetes 中运行它呢? 并不一定. 如果一定需要在每次小的修改后构建应用, 构建容器镜像, 然后推送到镜像中心, 再重新部署 pod 服务, 那么整个开发过程会非常缓慢而痛苦. 幸运的是, 你并不需要经历这些麻烦.</p> <p>可以一直在自己的本地机器上面开发和运行应用, 和过去的方式一样. 毕竟, Kubernetes 上面运行的应用也只是集群中某个节点上面运行的一个普通的(隔离的)进程. 如果应用依赖了 Kubernetes 环境提供的一些功能, 可以很容易地在本地开发的机器上面复制这个功能.</p> <p>这里甚至没有讨论到在容器中运行应用的部分. 大多数时间你不需要这样做, 通常可以直接在你的 IDE 里面运行你的应用.</p> <h6 id="_17-5-2-在开发过程中使用minikube"><a href="#_17-5-2-在开发过程中使用minikube" class="header-anchor">#</a> 17.5.2 在开发过程中使用Minikube</h6> <p>在开发过程中并不会强迫你在 Kubernetes 中运行应用. 但是你仍然可以这样做, 来看看在 Kubernetes 中应用是如何运行的.</p> <p>你或许已经使用过 Minikube 来运行本书中的例子. 尽管 Minikube 集群只运行一个工作节点, 它也是在 Kubernetes 中尝试运行你的应用(以及开发组成完整应用的资源 manifest)的一个有价值的方法. Minikube 没有提供一个完整的多节点 Kubernetes 集群的功能, 但是在大多数的场景下, 这并不影响什么.</p> <blockquote><p>将本地文件挂载到 Minikube VM 然后再挂载到容器中</p></blockquote> <p>如果你正在使用 Minikube 进行开发, 并且希望在 Kubernetes 集群中试验应用的每个更改, 可以使用 minikube mount 命令将本地的文件系统挂载到 Minikube VM 中, 然后通过一个 hostPath 卷挂载到容器中. 可以在 Minikube 文档中找到更多的使用说明, 文档链接在 https://github.com/kubernetes/minikube/tree/master/docs 上.</p> <blockquote><p>在 Minikube VM 中使用 Docker Daemon 来构建镜像</p></blockquote> <p>如果你正在使用 Minikube 开发应用, 并且计划在每个更改之后都构建一个镜像, 可以在 Minikube VM 中使用 Docker Daemon 来进行镜像构建, 而不是通过本地的 Docker Daemon 构建然后再推送到镜像中心, 最后拉取到 Minikube VM 中. 为了使用 Minikube 的 Docker Daemon, 只需要将你的 DOCKER_HOST 环境变量指向它. 幸运的是, 这个做起来实际上比听上去容易多了, 只需要在本地机器上运行下面的命令:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token builtin class-name">eval</span> <span class="token variable"><span class="token variable">$(</span>minikube docker-env<span class="token variable">)</span></span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个命令会帮你设置所有需要的环境变量, 然后你就可以像 Docker Daemon 运行在你本地的时候那样构建镜像了. 构建完镜像之后, 不需要再去推送镜像, 因为它已经存储在 Minikube VM 中了, 这样新的 pod 就可以立即使用这个镜像了. 如果你的 pod 已经在运行了, 那么可以删除它们或者杀死容器让它们重启.</p> <blockquote><p>在本地构建镜像然后直接复制到 Minikube VM 中</p></blockquote> <p>如果你无法使用 Minikube VM 内部的 Docker Daemon 来构建镜像, 这里仍然有方法来避免将镜像推送到镜像中心, 然后使用运行在 Minikube VM 内部的 Kubelet 拉取镜像这样的流程. 如果你在本地机器构建好了镜像, 可以使用下面的命令将镜像直接复制到 Minikube VM 中:</p> <div class="language-sh line-numbers-mode"><pre class="language-sh"><code>$ <span class="token function">docker</span> save <span class="token operator">&lt;</span>image<span class="token operator">&gt;</span> <span class="token operator">|</span> <span class="token punctuation">(</span>eval <span class="token variable"><span class="token variable">$(</span>minikube docker-env<span class="token variable">)</span></span> <span class="token operator">&amp;&amp;</span> <span class="token function">docker</span> load<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>和之前一样, 这个镜像也可以在 pod 中立即使用了. 这里注意确保 pod spec 中的 imagePullPolicy 不要设置为 Always, 因为这会导致从外部镜像中心拉取镜像, 从而导致你复制过去的镜像的更改丢失.</p> <blockquote><p>将 Minikube 和 Kubernetes 集群结合起来</p></blockquote> <p>你在使用 Minikube 开发应用的时候几乎没有任何限制, 甚至可以将 Minikube 集群和一个 Kubernetes 集群结合起来. 笔者有的时候在本地的 Minikube 集群运行开发中的服务, 然后让它们和部署在千里之外的远程多节点 Kubernetes 集群中的其他服务进行通信.</p> <p>当笔者完成开发工作之后, 可以几乎不用修改什么就将本地的服务迁移到远程集群中, 并且由于 Kubernetes 将底层基础框架的复杂性进行抽象和应用独立开来, 所以运行过程也没有什么问题.</p> <h6 id="_17-5-3-发布版本和自动部署资源清单"><a href="#_17-5-3-发布版本和自动部署资源清单" class="header-anchor">#</a> 17.5.3 发布版本和自动部署资源清单</h6> <p>因为 Kubernetes 采用的是指令式模型, 你不必判断出部署的资源的当前状态, 然后向它们发送命令来将资源状态切换到你期望的那样. 需要做的就是告诉 Kubernetes 你希望的状态, 然后 Kubernetes 会采取相关的必要措施来将集群的状态切换到你期望的样子.</p> <p><strong>可以将资源的 manifest 存放到一个版本控制系统中, 这样可以方便做代码审查, 审计跟踪, 或者任何需要的时候回退更改. 在每次提交更改之后, 可以使用 kubectl apply 命令将更改反映到部署的资源中.</strong></p> <p>如果你运行了一个定时代理(或者是代理检测到新的提交的时候)来从版本控制系统(VCS)中检出资源 manifest, 然后调用 apply 命令应用更改, 那么可以简单通过向 VCS 提交更改来管理你运行中的应用, 这样就不需要手动和 Kubernetes API 服务器进行通信了.</p> <p>可以使用不同的代码分支来部署开发, 测试, 预发布和生产集群等环境(或者是同一个集群中不同的命名空间)下的资源 manifest.</p> <h6 id="_17-5-4-利用持续集成和持续交付"><a href="#_17-5-4-利用持续集成和持续交付" class="header-anchor">#</a> 17.5.4 利用持续集成和持续交付</h6> <p>之前讨论过 Kubernetes 资源的自动部署, 但是或许你希望能够建立一个<strong>完整的 CI/CD 工作流来构建应用的二进制文件, 容器镜像以及资源配置, 然后部署到一个或者多个 Kubernetes 集群中</strong>.</p> <p>可以在网上找到很多讨论这方面的资源. 这里特别介绍一个叫作 <strong>Fabric8</strong> 的项目(http://fabric8.io), 这是一个 Kubernetes 的集成开发平台. 这个平台包括著名的自动化集成系统 Jenkins, 以及其他很多提供完整的 CI/CD 工作流的工具, 这些工具面向 DevOps 风格的开发, 部署, 以及 Kubernetes 上微服务的管理.</p> <p>如果你希望构建自己的解决方案, 建议看看一个讨论这个话题的 Google Cloud 平台的在线实验室. 它的地址在 https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes 上.</p> <h5 id="_17-6-本章小结"><a href="#_17-6-本章小结" class="header-anchor">#</a> 17.6 本章小结</h5> <p>希望本章的内容已经帮助你更加深入地了解了 Kubernetes 是如何工作的, 并且能够帮助你自如地构建应用并部署到 Kubernetes 集群. 本章的目标是:</p> <ul><li>向你展示本书中覆盖的<strong>所有的资源是如何组织在一起, 构成一个运行在 Kubernetes 上的典型的应用</strong>的.</li> <li>促使你思考很少在机器间进行迁移的应用和运行在 pod 中经常会被调度的应用之间的区别.</li> <li>帮助理解为什么拥有多组件的应用(或者微服务)不应该依赖具体的服务启动顺序.</li> <li><strong>介绍 init 容器</strong>, 它可以帮助你初始化一个 pod 或者延迟 pod 主容器的启动直到预置的条件被满足.</li> <li>讲解<strong>容器生命周期钩子</strong>并且应该在何时使用它们.</li> <li>对 Kubernetes 组件的分布式特性所引发的结果, 以及 Kubernetes 的最终一致性模型进行深入的了解.</li> <li>学习如何在不断开客户端连接的情况下, <strong>妥善地关闭你的应用</strong>.</li> <li>给你几个让应用方便管理的小提示, 包括控制镜像大小, 给资源添加注解以及多维度的标签, 以及方便查看应用终止的原因.</li> <li>讲解如何开发 Kubernetes 应用, 以及如何在本地或者 Minikube 中运行, 然后再部署到多节点的集群中.</li></ul> <p>‍</p> <p>‍</p> <p>‍</p> <p>‍</p></div></div>  <div class="page-edit"><div class="edit-link"><a href="https://github.com/xugaoyi/vuepress-theme-vdoing/edit/main/docs/30.系统/3100.容器/10.容器/200.Kubernetes实战🌸.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <!----></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/98e5e4/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">容器实战高手课(极客时间)🌸</div></a> <a href="/pages/f35c72/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">深入剖析Kubernetes(极客时间)🌸</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/98e5e4/" class="prev">容器实战高手课(极客时间)🌸</a></span> <span class="next"><a href="/pages/f35c72/">深入剖析Kubernetes(极客时间)🌸</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="mailto:1174520425@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/nanodaemony" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2019-2025
    <span>达尔文的猹 | MIT License</span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.3f3e0e10.js" defer></script><script src="/assets/js/2.e9fcb30c.js" defer></script><script src="/assets/js/3.1998f389.js" defer></script><script src="/assets/js/193.4a06bc12.js" defer></script>
  </body>
</html>
